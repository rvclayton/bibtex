.so bibtex.header
		  
@Article{pvsame,
  author       = "Boehm, Barry~W. and Gray, Terence~E. and Seewaldt, Thomas",
  title        = "Prototyping Versus Specifying:  {A} Multiproject Experiment",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 3,
  pages        = "290-302",
  month        = may,
  keywords     = "prototyping, specifying, requirements analysis, software
    engineering, software engineering education, software management, software
    metrics, cocomo, user interfaces",
  abstract     = "In this experiment, seven software teams developed versions
    of the same small-size (2000-4000 source instruction) application software
    product.  Four teams used the Specifying approach.  Three teams used the
    Prototyping approach.  The main results of the experiment were the
    following.  1) Prototyping yielded products with roughly equivalent
    performance, but with about 40 percent less code and 45 percent less
    effort.  2) The prototyped products rated somewhat lower on functionality
    and robustness, but higher on ease of use and ease of learning.  3)
    Specifying produced more coherent designs and software that was easier to
    integrate.  The paper presents the experimental data supporting these and a
    number of additional conclusions.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010238"
}
@Article{acsosapbhfi,
  author       = "Arnold, Matthew and Fink, Stephen and Sarkar, Vivek and Sweeney, Peter~F.",
  title        = "{A} Comparative Study of Static and Profile-Based Heuristics for Inlining",
  journal      = sigplan # " (" # pot # "Workshop on Dynamic and Adaptive
		  Compilation and Optimization - Dynamo '00)",
  year         = 2000,
  volume       = 35,
  number       = 7,
  pages        = "52--64",
  month        = jul,
  keywords     = "call graphs, inlining, program optimization, static analysis,
    dynamic analysis",
  abstract     = "In this paper, we present a comparative study of static and 
    profile-based heuristics for inlining.  Our motivation for this study is to
    use the results to design the best inlining algorithm that we can for the
    Jalapeño dynamic optimizing compiler for Java [6].  We use a well-known
    approximation algorithm for the KNAPSACK problem as a common
    “meta-algorithm” for the inlining heuristics studied in this paper.  We
    present performance results for an implementation of these inlining
    heuristics in the Jalapeño dynamic optimizing compiler.  Our performance
    results show that the inlining heuristics studied in this paper can lead to
    significant speedups in execution time (up to 1.68x) even with modest
    limits on code size expansion (at most 10%).",  
  location     = "http://dx.doi.org/10.1145/351403.351416"
}

@Article{cfsrmc,
  author       = "Iannino, Anthony and Musa, John~D. and Okumoto, Kazuhira and Littlewood, Bev",
  title        = "Criteria for Software Reliability Model Comparisons",
  journal      = tse,
  year         = 1983,
  volume       = 8,
  number       = 3,
  pages        = "12--16",
  month        = jul,
  keywords     = "model comparisons, predictive validity, software failures,
    software reliability, capability, assumption quality, applicability,
    simplicity",
  abstract     = "A set of criteria is proposed for the comparison of software
    reliability models.  The intention is to provide a logically organized
    basis for determining the superior models and for the presentation of model
    characteristics.  It is hoped that in the future, a software manager will
    be able to more easily select the model most suitable for his/her
    requirements from among the preferred ones.", 
  location     = "http://dx.doi.org/10.1145/1010891.1010893"
}

@Article{dotpedm,
  author       = "Barach, David~R. and Taenzer, David~H. and Wells, Robert~E.",
  title        = "Design of the {PEN} Editor Display Module",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN SIGOA Symposium on Text Manipulation)",
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "130--136",
  month        = jun,
  keywords     = "software design, terminal handling, optimization",
  abstract     = "PEN, a new portable video editor, uses a number of simple but
    effective techniques.  Most are not new, but are unavailable in the
    literature.  We will describe our goals for PEN's display module, discuss
    implementation alternatives and describe in detail the techniques used in
    the editor.", 
  location     = "http://dx.doi.org/10.1145/800209.806464"
}
@Book{dsdi,
  author       = "Daniel~C. Dennett",
  title        = "Darwin's Dangerous Idea",
  publisher    = "Simon \& Schuster",
  year         = 1995,
  address      = nyny,
  keywords     = "evolution, darwin, scientific debate",
  location     = "QH 375 D45"
}

@Article{rtpt,
  author       = "Thomas~H. {Cheatham, Jr.}",
  title        = "Reusability Through Program Transformation",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "574--588",
  month        = sep,
  keywords     = "programming environments, porgram transformations, rapid
    prototyping, reusability, specification languages",
  abstract     = "We describe a methodology and supporting programming
    environment that provide for reuse of abstract programs.  Abstract programs
    are written using notations and constructs natural to the problem domain in
    a language realized by syntactic extension of a base language.  Program
    transformations are employed to refine an abstract program into its
    concrete counterpart.  We discuss the use of the methodology in the setting
    of rapid prototyping and custom tailoring.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010282"
}

@Article{arajg,
  author       = "James Gosling",
  title        = "{A} Redisplay Algorithm",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN SIGOA Symposium on Text Manipulation)",
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "123--129",
  month        = jun,
  keywords     = "dynamic programming",
  abstract     = "This paper presents an algorithm for updating the image
    displayed on a conventional video terminal.  It assumes that the terminal
    is capable of doing the usual insert/delete line and insert/delete
    character operations.  It takes as input a description of the image
    currently on the screen and a description of the new image desired and
    produces a series of operations to do the desired transformation in a
    near-optimal manner.  The algorithm is interesting because it applies
    results from the theoretical string-to-string correction problem (a
    generalization of the problem of finding a longest common subsequence), to
    a problem that is usually approached with crude ad-hoc techniques.", 
  location     = "http://dx.doi.org/10.1145/872730.806463"
}

@Article{otlbpitf,
  author       = "Achugbue, James~O.",
  title        = "On the Line Breaking Problem in Text Formatting",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN SIGOA Symposium on Text Manipulation)",
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "117--122",
  month        = jun,
  keywords     = "line breaking, text formatting, dynamic programming",
  abstract     = "A basic problem in text formatting is that of determining the
    break points for separating a string of words into lines to obtain a
    formatted paragraph.  When formatted text is required to be aligned with
    both the left and right margins, the choice of break points greatly affects
    the quality of the formatted document.  This paper presents and discusses
    solutions to the line breaking problem.  These include the usual
    line-by-line method, a dynamic programming approach, and a new algorithm
    which is optimal and runs almost as fast as the line-by-line method.", 
  location     = "http://dx.doi.org/10.1145/800209.806462"
}

@Article{esopk,
  author       = "Soloway, Elliot and Ehrlich, Kate",
  title        = "Emperical Studies of Programming Knowledge",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "595--609",
  month        = sep,
  keywords     = "cognitive models of programming, novice/expert differences,
    program comprehension, software psychology",
  abstract     = "We suggest that expert programmers have and use two types of 
    programming knowledge: 1) programming plans, which are generic program
    fragments that represent stereotypic action sequences in programming, and
    2) rules of programming discourse, which capture the conventions in
    programming and govern the composition of the plans into programs.  We
    report here on two empirical studies that attempt to evaluate the above
    hypothesis.  Results from these studies do in fact support our claim.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010283"
}

@Article{prtpt,
  author       = "Boyle, James~M. and Muralidharan, Monagur~N.",
  title        = "Program Reusability through Program Transformation",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "574--588",
  month        = sep,
  keywords     = "abstract programming, canonical forms, optimization, program
    transformation, pure applicative lisp, rewrite rules, stepwise refinement,
    tampr",
  abstract     = "How can a program written in pure applicative LISP be reused
    in a Fortran environment? One answer is by automatically transforming it
    from LISP into Fortran.  In this paper we discuss a practical application
    of this technique-one that yields an efficient Fortran program.  We view
    this process as an example of abstract programming, in which the LISP
    program constitutes an abstract specification for the Fortran version.  The
    idea of strategy-a strategy for getting from LISP to Fortran-is basic to
    designing and applying the transformations.  One strategic insight is that
    the task is easier if the LISP program is converted to ``recursive''
    Fortran, and then the recursive Fortran program is converted to
    nonrecursive standard Fortran.  Another strategic insight is that much of
    the task can be accomplished by converting the program from one canonical
    form to another.  Developing a strategy also involves making various
    implementation decisions.  One advantage of program transformation
    methodology is that it exposes such decisions for examination and review.
    Another is that it enables optimizations to be detected and implemented
    easily.  Once a strategy has been discovered, it can be implemented by
    means of rewrite-rule transformations using the TAMPR program
    transformation system.  The transformational approach to program reuse
    based on this strategy has a measure of elegance.  It is also practical-the
    resulting Fortran program is 25 percent faster than its compiled LISP
    counterpart, even without extensive optimization.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010281"
}

@Article{tioeaiaidps,
  author       = "Hammer, Michael and Ilson, Richard and Anderson, Tim and Gilbert, Edward and Good, Michael and Niamir, Bahram and Rosentein, Larry and Schoichet, Sandor",
  title        = "The Implementation of {E}tude, An integrated and Interactive Document Production System",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN SIGOA Symposium on Text Manipulation)",
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "117--122",
  month        = jun,
  keywords     = "software design, user-interface design, document
    representation",
  abstract     = "Etude is an experimental text processing system that is being
    developed in order to formulate and evaluate new approaches to the design
    of user interfaces for office automation tools.  The primary design goal
    for Etude is to provide the user with substantial functionality in the
    editing and formatting of documents in the context of a system that is easy
    to learn and use.", 
  location     = "http://dx.doi.org/10.1145/800209.806465"
}

@Article{aisercdcall,
  author       = "Ben-David, Amram and Ben-Porath, Moshe~I. and Loeb, Jonah~Z. and Rich, Michael",
  title        = "An Industrial Software Engineering Retraining Course:  Development Considerations and Lessons Learned",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "748--755",
  month        = nov,
  keywords     = "",
  abstract     = "Israel Aircraft Industries has recently been conducting a novel
    six-month intensive course to retrain practicing engineers to become
    software engineers working on embedded computer systems.  The first course
    was concluded in January 1982 and the second course began in November 1982.
    This paper describes the objectives, educational philosophy, course
    content, and practical experience of the first course.  It also describes
    how the second course was modified as a result of the lessons learned from
    the successes and failures of the first course.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010303"
}

@Article{acgpoi,
  author       = "Corrigan, Neil~B. and Starkey, J.~Denbigh",
  title        = "{A} Concurrent General Purpose Operator Interface",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "738--748",
  month        = nov,
  keywords     = "computer graphics, concurrent pascal, concurrent programming,
    interactive system, man-machine interface, operator interface, process
    control",
  abstract     = "Compact interactive control consoles are rephcing traditional
    control rooms as operator interfaces for physical processes.  In the irust
    major application of concurrent programming outside the area of operating
    systems, this paper presents a design for a general purpose operator
    interface which uses a color graphics terminal with a touch-sensitive
    screen as the control console.  Operators interact with a process through a
    collection of application-dependent displays generated interactively by
    users familiar with the physical process.  The use of concurrent
    programming results in a straightforward and reliable design which may
    easily be extended to support multiple devices of varying types in the
    control console.  An implementation of the Operator Interface in Concurrent
    Pascal currently in progress is also discussed.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010302"
}

@Article{eoerbufcp,
  author       = "Kang~G. Shin and Yann-Hang Lee",
  title        = "Evaluation of Error Recovery Blocks Used for Cooperating Processes",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "692--700",
  month        = nov,
  keywords     = "backward error recovery, conversation scheme, domino effect,
    pseudorecovery points and line(s), recovery block(s), recovery line(s),
    rollback propagations",
  abstract     = "Three alternatives for implementing recovery blocks (RB's) 
    are conceivable for backward error recovery in concurrent processing.
    These are the asynchronous, synchronous, and the pseudorecovery point
    implementations.  Asynchronous RB's are based on the concept of maximum
    autonomy in each of concurrent processes.  Consequently, establishment of
    RB's in a process is made independently of others and unbounded rollback
    propagations become a serious problem.  In order to completely avoid
    unbounded rollback propagations, it is necessary to synchronize the
    establishment of recovery blocks in all cooperating processes.  Process
    autonomy is sacrificed and processes are forced to wait for commitments
    from others to establish a recovery line, leading to inefficiency in time
    utilization.  As a compromise between asynchronous and synchronous RB's we
    propose to insert pseudorecovery points (PRP's) so that unbounded rollback
    propagations may be avoided while maintaining process autonomy.  We
    developed probabilistic models for analyzing these three methods under
    standard assumptions in computer performance analysis, i.e., exponential
    distributions for related random variables.  With these models we have
    estimated 1) the interval between two successive recovery lines for
    asynchronous RB's, 2) mean loss in computation power for the synchronized
    method, and 3) additional overhead and rollback distance in case PRP's are
    used.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010298"
}

@Article{rtem,
  author       = "Bernhard Plattner",
  title        = "Real-Time Execution Monitoring",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "756--764",
  month        = nov,
  keywords     = "debugging, monitor, performance evaluation, process
    interaction, process monitor, real-time monitoring, timing",
  abstract     = "Today's programming methodology emphasizes the study of static
    aspects of programs.  In practice, however, monitoring a program in
    execution, i.e., monitoring a process, is routinely done by any programmer
    whose task it is to produce a reliable piece of software.  There are two
    reasons why one might want to examine the dynamic aspects of a program:
    first, to evaluate the performance of a program, and hence to assess its
    overall behavior; and second, to demonstrate the presence of programming
    errors, isolate erroneous program code, and correct it.  This latter task
    is commonly called ``debugging a program'' and requires a detailed insight
    into the innards of a program being executed.  Today, many computer systems
    are being used to measure and control real-world processes.  The pace of
    execution of these systems and their control programs is therefore bound to
    timing constraints imposed by the real-world process.  As a step towards
    solving the problems associated with execution monitoring of real-time
    programs, we develop a set of appropriate concepts and define the basic
    requirements for a real-time monitoring facility.  As a test case for the
    theoretical treatment of the topic, we design hardware and software for an
    experimental real-time monitoring system and describe its implementation.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010304"
}

@Article{dcobnfgpc,
  author       = "Alan Mainwaring and David~E. Culler",
  title        = "Design Challenges of Virtual Networks:  Fast, General-Purpose Communication",
  journal      = sigplan # " (" # pot # "Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming)",
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "119--130",
  month        = aug,
  keywords     = "virtual networks, high-performance clusters, direct network
    access, application programming interfaces, system resource management,
    protocol architecture and implementation",
  abstract     = "Virtual networks provide applications with the illusion of 
    having their own dedicated, high-performance networks, although network
    interfaces posses limited, shared resources.  We present the design of a
    large-scale virtual network system and examine the integration of
    communication programming interface, system resource management, and
    network interface operation.  Our implementation on a cluster of 100
    workstations quantifies the impact of virtualization on small message
    latencies and throughputs, shows full hardware performance is delivered to
    dedicated applications and time-shared workloads, and shows robust
    performance under demanding workloads that overcommit interface
    resources.", 
  location     = "http://dx.doi.org/10.1145/329366.301115"
}

@Article{aeiojsrmi,
  author       = "Maassen, Jason and {van Nieuwpoort}, Rob and Veldema, Ronald and Bal, Henri~E. and Plaat, Aske",
  title        = "An Efficient Implementation of {J}ava's {R}emote {M}ethod {I}nvocation",
  journal      = sigplan # " (" # pot # "Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming)",
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "173--182",
  month        = aug,
  keywords     = "java, rmi, performance, native-code compilation",
  abstract     = "Java offers interesting opportunities for parallel computing.
    In particular, Java Remote Method Invocation provides an unusually flexible
    kind of Remote Procedure Call.  Unlike RPC, RMI supports polymorphism,
    which requires the system to be able to download remote classes into a
    running application.  Sun's RMI implementation achieves this kind of
    flexibility by passing around object type information and processing it at
    run time, which causes a major run time overhead.  Using Sun's JDK 1.1.4 on
    a Pentium Pro/Myri.net cluster, for example, the latency for a null RMI
    (without parameters or a return value) is 1228 &mu;sec, which is about a
    factor of 40 higher than that of a user-level RPC.  In this paper, we study
    an alternative approach for implementing RMI, based on native compilation.
    This approach allows for better optimization, eliminates the need for
    processing of type information at run time, and makes a light weight
    communication protocol possible.  We have built a Java system based on a
    native compiler, which supports both compile time and run time generation
    of marshallers.  We find that almost all of the run time overhead of RMI
    can be pushed to compile time.  With this approach, the latency of a null
    RMI is reduced to 34 &mu;sec, while still supporting polymorphic RMIs (and
    allowing interoperability with other JVMs).", 
  location     = "http://dx.doi.org/10.1145/301104.301120"
}

@Article{cmobst,
  author       = "Udi Manber",
  title        = "Concurrent Maintenance of Binary Search Trees",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "777--784",
  month        = nov,
  keywords     = "concurrent algorithms, data structures, distributed
    algorithms, locking, transactions, trees",
  abstract     = "The problem of providing efficient concurrent access for 
    independent processes to a dynamic search structure is the topic of this
    paper.  We develop concurrent algorithms for search, update, insert, and
    delete in a simple variation of binary search trees, called external trees.
    The algorithm for deletion, which is usually the most difficult operation,
    is relatively easy in this data structure.  The advantages of the data
    structure and the algorithms are that they are simple, flexible, and
    efficient, so that they can be used as a part in the design of more
    complicated concurrent algorithms where maintaining a dynamic search
    structure is necessary.  In order to increase the efficiency of the
    algorithms we introduce maintenance processes that independently reorganize
    the data structure and relieve the user processes of nonurgent operations.
    We also discuss questions of transactions in a dynamic environment and
    replicated copies of the data structure.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010306"
}

@Article{mfdabiat,
  author       = "Steven~M. German",
  title        = "Monitoring for Deadlock and Blocking in {A}da Tasking",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "764--777",
  month        = nov,
  keywords     = "concurrent algorithms, concurrent programming languages,
    correctness proofs of concurrent programs, deadlock detection, exceptions,
    program transformations, semantics of ada tasking, state graph models, task
    identifiers", 
  abstract     = "We present a deadlock monitoring algorithm for Ada tasking
    programs which is based on transforming the source program.  The
    transformations introduce a new task called the monitor, which receives
    information from all other tasks about their tasking activities.  The
    monitor detects deadlocks consisting of circular entry calls as well as
    some noncircular blocking situations.  The correctness of the program
    transformations is formulated and proved using an operational state graph
    model of tasking.  The main issue in the correctness proof is to show that
    the deadlock monitor algorithm works correctly without having simultaneous
    information about the state of the program.  In the course of this work, we
    have developed some useful techniques for programming tasking applications,
    such as a method for uniformly introducing task identifiers.  We argue that
    the ease of finding and justifying program transformations is a good test
    of the generality and uniformity of a programming language.  The complexity
    of the full Ada language makes it difficult to safely apply
    transformational methods to arbitrary programs.  We discuss several
    problems with the current semantics of Ada's tasks.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010305"
}

@Article{apodaca,
  author       = "Rugina, Radu and Rinard, Martin",
  title        = "Automatic Parallelization of Divide and Conquer Algorithms",
  journal      = sigplan # " (" # pot # "Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP '99)",
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "72--83",
  month        = aug,
  keywords     = "parallelization, region analysis, pointer analysis, bounds
    analysis, initial value analysis, correlation analysis",
  abstract     = "Divide and conquer algorithms are a good match for modern
    parallel machines: they tend to have large amounts of inherent parallelism
    and they work well with caches and deep memory hierarchies.  But these
    algorithms pose challenging problems for parallelizing compilers.  They are
    usually coded as recursive procedures and often use pointers into
    dynamically allocated memory blocks and pointer arithmetic.  All of these
    features are incompatible with the analysis algorithms in traditional
    parallelizing compilers.This paper presents the design and implementation
    of a compiler that is designed to parallelize divide and conquer algorithms
    whose subproblems access disjoint regions of dynamically allocated arrays.
    The foundation of the compiler is a flow-sensitive, context-sensitive, and
    interprocedural pointer analysis algorithm.  A range of symbolic analysis
    algorithms build on the pointer analysis information to extract symbolic
    bounds for the memory regions accessed by (potentially recursive)
    procedures that use pointers and pointer arithmetic.  The symbolic bounds
    information allows the compiler to find procedure calls that can execute in
    parallel without violating the data dependences.  The compiler generates
    code that executes these calls in parallel.  We have used the compiler to
    parallelize several programs that use divide and conquer algorithms.  Our
    results show that the programs perform well and exhibit good speedup.", 
  location     = "http://dx.doi.org/10.1145/329366.301111"
}

@Article{amfcvsed,
  author       = "Victor~R. Basili and David~M. Weiss",
  title        = "A Methodology for Collecting Valid Software Engineering Data",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "728--738",
  month        = nov,
  keywords     = "data collection, data collection methodology, error analysis,
    error classification, software engineering experiments",
  abstract     = "An effective data collection method for evaluating software
    development methodologies and for studying the software development process
    is described.  The method uses goal-directed data collection to evaluate
    methodologies with respect to the claims made for them.  Such claims are
    used as a basis for defining the goals of the data collection, establishing
    a list of questions of interest to be answered by data analysis, defining a
    set of data categorization schemes, and designing a data collection form.
    The data to be collected are based on the changes made to the software
    during development, and are obtained when the changes are made.  To ensure
    accuracy of the data, validation is performed concurrently with software
    development and data collection.  Validation is based on interviews with
    those people supplying the data.  Results from using the methodology show
    that data validation is a necessary part of change data collection.
    Without it, as much as 50 percent of the data may be erroneous.
    Feasibility of the data collection methodology was demonstrated by applying
    it to five different projects in two different environments.  The
    application showed that the methodology was both feasible and useful.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010301",
  location     = "http://www.cs.umd.edu/projects/SoftEng/ESEG/papers/82.21.pdf"
}

@Article{bcafpp,
  author       = "Lee, Jaejin and Padua, David~A. and Midkiff, Samuel~P.",
  title        = "Basic Compiler Algorithms for Parallel Programs",
  journal      = sigplan # " (" # pot # "Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP '99)",
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "1--12",
  month        = aug,
  keywords     = "delay set analysis, concurrent control flow graphs,
    concurrent static single-assignment form, copy propagation, dead-code
    elimination, global value numbering, cse, ",
  abstract     = "Traditional compiler techniques developed for sequential
    programs do not guarantee the correctness (sequential consistency) of
    compiler transformations when applied to parallel programs.  This is
    because traditional compilers for sequential programs do not account for
    the updates to a shared variable by different threads.  We present a
    concurrent static single assignment (CSSA) form for parallel programs
    containing cobegin/coend and parallel do constructs and post/wait
    synchronization primitives.  Based on the CSSA form, we present copy
    propagation and dead code elimination techniques.  Also, a global value
    numbering technique that detects equivalent variables in parallel programs
    is presented.  By using global value numbering and the CSSA form, we extend
    classical common subexpression elimination, redundant load/store
    elimination, and loop invariant detection to parallel programs without
    violating sequential consistency.  These optimization techniques are the
    most commonly used techniques for sequential programs.  By extending these
    techniques to parallel programs, we can guarantee the correctness of the
    optimized program and maintain single processor performance in a
    multiprocessor environment.", 
  location     = "http://dx.doi.org/10.1145/301104.301105"
}

@Article{omfold,
  author       = "Stefanovi{\' c}, Darko and McKinley, Kathryn~S. and Moss, J.~Eliot~B.",
  title        = "On Models for Object Lifetime Distributions",
  journal      = sigplan # " (" # pot # " Second International Symposium on Memory Management, ISMM '00)",
  year         = 2001,
  volume       = 36,
  number       = 1,
  pages        = "137--142",
  month        = jan,
  keywords     = "object lifetimes, lifetime distributions, garbage collection
    modeling, storage management", 
  abstract     = "Analytical models of memory object lifetimes are appealing
    because having them would enable mathematical analysis or fast simulation
    of the memory management behavior of programs.  In this paper, we
    investigate models for object lifetimes drawn from programs in
    object-oriented languages such as Java and Smalltalk.  We present certain
    postulated analytical models and compare them with observed lifetimes for
    58 programs.  We find that observed lifetime distributions do not match
    previously proposed object lifetime models, but do agree in salient shape
    characteristics with the gamma distribution family used in statistical
    survival analysis for general populations.", 
  location     = "http://dx.doi.org/10.1145/362426.362477"
}

@Article{cdpmfs,
  author       = "Haj-Yihia, Jawad and Asher, Yosi Ben and Rotem, Efraim and Yasin, Ahmad and Ginosar, Ran",
  title        = "Compiler-Directed Power Management for Superscalars",
  journal      = "ACM Transactions on Architecture and Code Optimization",
  year         = 2015,
  volume       = 11,
  number       = 4,
  pages        = "48:1--48:21",
  month        = jan,
  keywords     = "static analysis, power management, power demand modeling, cpu
    architectures, power distribution networks",
  abstract     = "Modern superscalar CPUs contain large complex structures and
    diverse execution units, consuming wide dynamic power range.  Building a
    power delivery network for the worst-case power consumption is not energy
    efficient and often is impossible to fit in small systems.  Instantaneous
    power excursions can cause voltage droops.  Power management algorithms are
    too slow to respond to instantaneous events.  In this article, we propose a
    novel compiler-directed framework to address this problem.  The framework
    is validated on a 4th Generation Intel® Core™ processor and with simulator
    on output trace.  Up to 16% performance speedup is measured over baseline
    for the SPEC CPU2006 benchmarks.",
  location     = "http://dx.doi.org/10.1145/2685393"
}

@Article{oret,
  author       = "Ntafos, Simon~C.",
  title        = "On Required Element Testing",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "795--803",
  month        = nov,
  keywords     = "data-flow analysis, required element testing, testing
    strategies", 
  abstract     = "In this paper we introduce two classes of program testing
    strategies that consist of specifying a set of required elements for the
    program and then covering those elements with appropriate test inputs.  In
    general, a required element has a structural and a functional component and
    is covered by a test case if the test case causes the features specified in
    the structural component to be executed under the conditions specified in
    the functional component.  Data flow analysis is used to specify the
    structural component and data flow interactions are used as a basis for
    developing the functional component.  The strategies are illustrated with
    examples and some experimental evaluations of their effectiveness are
    presented.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010308"
}

@Article{iaotfgcfj,
  author       = "Domani, Tamar and Kolodner, Elliot~K. and Lewis, Ethan and
    Salant, Eliot~E. and Barabash, Katherine and Lahan, Itai and Levanoni,
    Yossi and Petrank, Erez and Yanorer, Igor", 
  title        = "Implementing an On-the-fly Garbage Collector for {J}ava",
  journal      = sigplan # " (" # pot # " Second International Symposium on Memory Management, ISMM '00)",
  year         = 2001,
  volume       = 36,
  number       = 1,
  pages        = "155--166",
  month        = jan,
  keywords     = "java, programming languages, memory management, garbage
    collection, concurrent garbage collection, on-the-fly garbage collection",
  abstract     = "Java uses garbage collection (GC) for the automatic
    reclamation of computer memory no longer required by a running application.
    GC implementations for Java Virtual Machines (JVM) are typically designed
    for single processor machines, and do not necessarily perform well for a
    server program with many threads running on a multiprocessor.  We designed
    and implemented an on-the-fly GC, based on the algorithm of Doligez, Leroy
    and Gonthier [13, 12] (DLG), for Java in this environment.  An on-the-fly
    collector, a collector that does not stop the program threads, allows all
    processors to be utilized during collection and provides uniform response
    times.  We extended and adapted DLG for Java (e.g., adding support for weak
    references) and for modern multiprocessors without sequential consistency,
    and added performance improvements (e.g., to keep track of the objects
    remaining to be traced).  We compared the performance of our implementation
    with stop-the-world mark-sweep GC.  Our measurements show that the
    performance advantage for our collector increases as the number of threads
    increase and that it provides uniformly low response times.", 
  location     = "http://dx.doi.org/10.1145/362426.362484"
}

@Article{upgist,
  author       = "Sirer, Emin G{\" u}n and Bershad, Brian~N.",
  title        = "Using Production Grammars in Software Testing",
  journal      = sigplan # " (" # pot # " Second Conference on Domain-Specific Languages)" ,
  year         = 1999,
  volume       = 35,
  number       = 1,
  pages        = "1--13",
  month        = jan,
  keywords     = "testing, virtual machines, production grammars, test
    generation, comparative testing, self-describing test cases",
  abstract     = "Extensible typesafe systems, such as Java, rely critically on
    a large and complex software base for their overall protection and
    integrity, and are therefore difficult to test and verify.  Traditional
    testing techniques, such as manual test generation and formal verification,
    are too time consuming, expensive, and imprecise, or work only on abstract
    models of the implementation and are too simplistic.  Consequently,
    commercial virtual machines deployed so far have exhibited numerous bugs
    and security holes.In this paper, we discuss our experience with using
    production grammars in testing large, complex and safety-critical software
    systems.  Specifically, we describe lava, a domain specific language we
    have developed for specifying production grammars, and relate our
    experience with using lava to generate effective test suites for the Java
    virtual machine.  We demonstrate the effectiveness of production grammars
    in generating complex test cases that can, when combined with comparative
    and variant testing techniques, achieve high code and value coverage.  We
    also describe an extension to production grammars that enables concurrent
    generation of certificates for test cases.  A certificate is a behavioral
    description that specifies the intended outcome of the generated test case,
    and therefore acts as an oracle by which the correctness of the tested
    system can be evaluated in isolation.  We report the results of applying
    these testing techniques to commercial Java implementations.  We conclude
    that the use of production grammars in combination with other automated
    testing techniques is a powerful and effective method for testing software
    systems, and is enabled by a special purpose language for specifying
    extended production grammars.", 
  location     = "http://dx.doi.org/10.1145/331963.331965"
}

@Article{ujrtaelp,
  author       = "Dale Parson",
  title        = "Using Java Reflection to Automate Extension Langugae Parsing",
  journal      = sigplan # " (" # pot # " Second Conference on Domain-Specific Languages)" ,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "67--80",
  month        = jan,
  keywords     = "reflection, embedded languages, scripting languages, tcl,
    java, c++",
  abstract     = "An extension language is an interpreted programming language
    designed to be embedded in a domain-specific framework.  The addition of
    domain-specific primitive operations to an embedded extension language
    transforms that vanilla extension language into a domain-specific language.
    The LUxWORKS processor simulator and debugger from Lucent uses Tcl as its
    extension language.  After an overview of extension language embedding and
    LUxWORKS experience, this paper looks at using Java reflection and related
    mechanisms to solve three limitations in extension language - domain
    framework interaction.  The three limitations are gradual accumulation of
    ad hoc interface code connecting an extension language to a domain
    framework, over-coupling of a domain framework to a specific extension
    language, and inefficient command interpretation.  Java reflection consists
    of a set of programming interfaces through which a software module in a
    Java system can discover the structure of classes, methods and their
    associations in the system.  Java reflection and a naming convention for
    primitive domain operations eliminate ad hoc interface code by supporting
    recursive inspection of a domain command interface and translation of
    extension language objects into domain objects.  Java reflection,
    name-based dynamic class loading, and a language-neutral extension language
    abstraction eliminate language over-coupling by transforming the specific
    extension language into a run-time parameter.  Java reflection and command
    objects eliminate inefficiency by bypassing the extension language
    interpreter for stereotyped commands.  Overall, Java reflection helps to
    eliminate these limitations by supporting reorganization and elimination of
    handwritten code, and by streamlining interpretation.", 
  location     = "http://www.cs.tufts.edu/~nr/cs257/archive/dale-parson/dsl99lux.ps.gz"
}

@Article{aasohpaiv,
  author       = "Higashino, Teruo and Mori, Masaaki and Sugiyama, Yuji and Taniguchi, Kenichi and Kasami, Tadao",
  title        = "An Algebraic Specification of {HDLC} Procedures and Its Verification",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "825--836",
  month        = nov,
  keywords     = "algebraic specifications, church-rosser property, hdlc
    procedures, term rewriting system, verification",
  abstract     = "It is well known that algebraic specification methods are 
    promising for specifying programs and for verifying their various
    properties formally.  In this paper, an algebraic specification of
    information transfer procedures of high-level data link control (HDLC)
    procedures is presented and some of the main properties of the
    specification are shown.  First, we introduce abstract states, state
    transition functions, and output functions corresponding to elementary
    notions extracted from the description of HDLC procedures in ISO 3309-1979
    (E) and ISO 4335-1979 (E).  Second, we show axioms which represent the
    relations between the values of functions before and after the state
    transitions.  Then, it is proved that the specification is ``consistent,''
    ``sufficiently complete,'' and ``nonredundant.'' Also it is shown that an
    implementation which realizes the specification is naturally derived.  In
    the last section, verification of various properties of HDLC procedures is
    formulated in the same framework as the algebraic specification, and some
    verification examples are presented.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010311"
}

@Article{fra,
  author       = "Elliott, Conal and Hudak, Paul",
  title        = "Functional Reactive Animation",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN International Conference on Functional Programming, ICFP '97)",
  year         = 1997,
  volume       = 32,
  number       = 8,
  pages        = "263--273",
  month        = aug,
  keywords     = "functional programming, reactive programming, animation",
  abstract     = "Fran (Functional Reactive Animation) is a collection of data
    types and functions for composing richly interactive, multimedia
    animations.  The key ideas in Fran are its notions of behaviors and events.
    Behaviors are time-varying, reactive values, while events are sets of
    arbitrarily complex conditions, carrying possibly rich information.  Most
    traditional values can be treated as behaviors, and when images are thus
    treated, they become animations.  Although these notions are captured as
    data types rather than a programming language, we provide them with a
    denotational semantics, including a proper treatment of real time, to guide
    reasoning and implementation.  A method to effectively and efficiently
    perform event detection using interval analysis is also described, which
    relies on the partial information structure on the domain of event times.
    Fran has been implemented in Hugs, yielding surprisingly good performance
    for an interpreter-based system.  Several examples are given, including the
    ability to describe physical phenomena involving gravity, springs,
    velocity, acceleration, etc.  using ordinary differential equations.", 
  location     = "http://dx.doi.org/10.1145/258949.258973"
}

@Article{dsec,
  author       = "Leijen, Daan and Meijer, Erik",
  title        = "Domain Specific Embedded Compilers",
  journal      = sigplan # " (" # pot # "Second Conference on Domain-Specific Languages)" ,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "109--122",
  month        = jan,
  keywords     = "haskell, embedded languages, little languages, phantom
    types",
  abstract     = "Domain-specific embedded languages (DSELs) expressed in 
    higher-order, typed (HOT) languages provide a composable framework for
    domain-specific abstractions.  Such a framework is of greater utility than
    a collection of stand-alone domain-specific languages.  Usually, embedded
    domain specific languages are build on top of a set of domain specific
    primitive functions that are ultimately implemented using some form of
    foreign function call.  We sketch a general design pattern/or embedding
    client-server style services into Haskell using a domain specific embedded
    compiler for the server's source language.  In particular we apply this
    idea to implement Haskell/DB, a domain specific embdedded compiler that
    dynamically generates of SQL queries from monad comprehensions, which are
    then executed on an arbitrary ODBC database server.", 
  location     = "http://dx.doi.org/10.1145/331963.331977", 
  location     = "https://www.usenix.org/events/dsl99/full_papers/leijen/leijen.pdf"
}

@Article{shlrs,
  author       = "Dennis~W. Leinbaugh",
  title        = "Selectors: High-Level Resource Schedulers",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "810--825",
  month        = nov,
  keywords     = "nonprocedural langauge, process synchronization, protected
  resource, resource scheduling, resource sharing, starvation",
  abstract     = "Resource sharing problems can be described in three basically
    independent modular components.  The constraints the resource places upon
    sharing because of physcal limitations and consistency requirements.  The
    desired ordering of resource requests to achieve efficiency-either
    efficiency of resource utilization or efficiency of processes making the
    requests.  Modifications to the ordering to prevent starvation of processes
    waiting for requests which might otherwise never receive service.  A
    high-level nonprocedural language to specify these components of resource
    sharing problems is described.  General deadlock and starvation properties
    of selectors are proven.  Solutions to several classic resource sharing
    problems are shown to illustrate the expressiveness of this language.
    Proof techniques for this high-level language are introduced to show how to
    prove particular selectors are or are not deadlock and starvation free.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010310"
}

@Article{vveis,
  author       = "James Jennings and Eric Beuscher",
  title        = "Verischemelog: {V}erilog Embedded in {S}cheme",
  journal      = sigplan # " (" # pot # "Second Conference on Domain-Specific Languages)" ,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "109--122",
  month        = jan,
  keywords     = "scheme, verilog, macros, extension languages, embedded
    languages, hardware description languages",
  abstract     = "Verischemelog (pronounced with 5 syllables, 
    ver-uh-scheme-uh-log) is a language and programming environment embedded in
    Scheme for designing digital electronic hardware systems and for
    controlling the simulatin of these circuits.  Simulation is performed by a
    separate program, often a commercial product.  Verischemelog compiles to
    Verilog, an industry standard language accepted by several commercial and
    public domain simulators.  Because many design elements are easily
    parameterized, design engineers currently write scripts which generate
    hardware description code in Verilog.  These scripts work by textual
    substitution, and are typically ad-hoc and quite limited.  Preprocessors
    for Verilog, on the other hand, are hampered by their macro-expansion
    languages, which support few data types and lack procedures.  Verischemelog
    obviates the need for scripts and prepocessors by providing a hardware
    description language with list-based syntax, and Scheme to manipulate it.
    An interactive development environment gives early and specific feedback
    about errors, and structured access to the compiler and run-time
    environment provide a high degree of reconfigurability and extensibility of
    Verischemelog.", 
  location     = "http://usenix.org/publications/library/proceedings/dsl99/full_papers/jennings/jennings.pdf"
}

@Article{cctvpfcfsm,
  author       = "Mohammed~C. Gouda",
  title        = "Closed Covers:  To Verify Progress for Communicating Finite State Machines",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "846--855",
  month        = nov,
  keywords     = "communicating finite state machines, communication progress,
    communication protocols, verification techniques, automata, data
    structures, protocols, resource management, system recovery",
  abstract     = {Consider communicating finite state machines which exchange
    messages over unbounded FIFO channels.  We discuss a technique to verify
    that the communication between a given pair of such machines will progress
    indefinitely; this implies that the communication is free from deadlocks
    and unspecified receptions.  The technique is based on finding a set of
    global states for the communicating pair such that the following two
    conditions (along with other conditions) are satisfied: 1) the initial
    global state is in that set; and 2) starting from any global state in that
    set, an "acyclic version" of the communicating pair must reach a global
    state in that set.  We call such a set a closed cover, and show that the
    existence of a closed cover for a communicating pair is sufficient to
    guarantee indefinite communication progress.  We also show that in many
    practical instances, if the communication is guaranteed to progress
    indefinitely, then the existence of a closed cover is necessary.},
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010313",
  location     = "http://www.cs.utexas.edu/~gouda/papers/journal/10-whole.pdf"
}

@Article{aooassl,
  author       = "Giuseppe Serazzi and Maria Caizarossa",
  title        = "Adaptive Optimization of a System's Load",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  month        = nov,
  pages        = "837--845",
  keywords     = "adaptive control, adaptive scheduling algorithm, asymptotic
    bound analyasis, load balancing, real-time performance optimization",
  abstract     = "Applications of modeling techniques based on queueing theory
    to computer system performance analysis normally assume the existence of
    steady-state conditions.  However, these conditions are often violated
    since the unpredictable composition of workload causes peaks having highly
    variable intensities and durations.  Furthermore, computer system
    performance is highly dependent on how the system reacts to workload
    fluctuations.  Automatic control mechanisms are required to take care of
    the high variance of resource demands.  Real-time optimization of the
    overall performance of a computer system requires the introduction of
    adaptive control on the controlled functions, An adaptive scheduling
    algorithm which controls the input of the system in order to maximize a
    given performance criterion, such as the system throughput, is presented.
    The system load is adjusted depending on the characteristics of both the
    mix of jobs in execution and the mix of jobs submitted to the system and
    waiting in the input queue.  The asymptotic analysis of the performance
    bounds provides useful information about the limits on the performance
    indexes that can be achieved with a multiclass workload.  The evaluation of
    the adaptive control system is performed through simulation experiments
    using data collected from two real workloads.  This technique could be used
    to optimize the throughput of a centralized system as well as for the
    automatic load balancing in a distributed environment.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010312"
}

@Article{acm,
  author       = "Khayat, Mohammad G.",
  title        = "{A} Concurrency Measure",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "804--810",
  month        = nov,
  keywords     = "compatibility, concurrency control, degree of concurrency,
    dining philosophers problem, interprocess communication, maximal
    compatibility, parallel processing, readers/writers problem,
    synchronization policies, update synchronization",
  abstract     = "With the new advents of technology and the availability of 
    microprocessors and minicomputers, parallel and distributed processing is
    gaining widespread acceptability.  In such systems resources are shared
    among a number of processes.  Accesses to the resources must be
    synchronized in order to guarantee proper operation of a system.  In this
    research work, a measure, called maximal compatibility, is developed to
    measure the degree of concurrency (parallelism) a synchronization policy
    achieves.  A set of accesses is considered compatible if it only contains
    accesses that are permitted to occur simultaneously.  A policy is maximally
    compatible if it allows every compatible set of accesses to occur
    simultaneously and if the maximum number of requests is always satisfied
    without allowing incompatible accesses to occur simultaneously.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010309"
}

@Article{dore,
  author       = "Janusz~A. Brzozowski",
  title        = "Derivatives of Regular Expressions",
  journal      = jacm,
  year         = 1964,
  volume       = 11,
  number       = 4,
  pages        = "481--494",
  month        = oct,
  keywords     = "symbolic manipulation, algebraic manipulation",
  abstract     = "",
  location     = "http://dx.doi.org/10.1145/321239.321249"
}

@Article{cpnala,
  author       = "David~B. Benson",
  title        = "Counting Paths:  Nondeterminism as Linear Algebra",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "785--794",
  month        = nov,
  keywords     = "convergent iterative programs, deterministic divergence,
    nondeterministic divergence, nondeterministic programs, nonnegative
    matrices, semirings, computer errors, concurrent computing, control
    systems, convergence, design engineering, distributed computing, linear
    algebra, operating systems, programming profession, redundancy",
  abstract     = "Nondeterminism is considered to be ignorance about the actual
    state transition sequence performed during a computation.  The number of
    distinct potential paths from state i to j forms a matrix [nij].  The
    behavior of a nondeterministic program is defined to be this multiplicity
    matrix of the state transitions.  The standard programming constructs have
    behaviors defined in terms of the behaviors of their constituents using
    matrix addition and multiplication only.  The spectral radius of the matrix
    assigned to an iterating component characterizes its convergence.  The
    spectral radius is shown to be either 0 or else >= 1.  The program
    converges iff the spectral radius is zero, diverges deterministically iff
    the spectral radius is one, and has a proper nondeterministic divergence
    iff the spectral radius exceeds one.  If the machine has an infinite number
    of states the characterization of convergence is given graph theoretically.
    The spectral radii of synchronous and interleaved parallel noncommunicating
    systems are easily computed in terms of the spectral radii of the
    components.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010307"
}

@Article{dsodiws,
  author       = "Fern{\' a}ndez, Mary and Suciu, Dan and Tatarinov, Igor",
  title        = "Declarative Specification of Data-Intensive Web Sites",
  journal      = sigplan # " (" # pot # "Second Conference on Domain-Specific Languages, DSL '99)",
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "135--148",
  month        = jan,
  keywords     = "domain-specific languages, functional languages,
    semistructured data, query languages, template languages",
  abstract     = "Integrated information systems are often realized as 
    data-intensive Web sites, which integrate data from multiple data sources.
    We present a system, called STRUDEL, for specifying and generating
    data-intensive Web sites.  STRUDEL separates the tasks of accessing and
    integrating a site's data sources, building its structure, and generating
    its HTML representation.  STRUDEL's declarative query language, called
    StruQL, supports the first two tasks.  Unlike ad-hoc database queries, a
    StruQL query is a software artifact that must be extensible and reusable To
    support more modular and reusable site definition queries, we extend StruQL
    with functions and describe how the new language, FunStruQL, better
    supports common site-engineering tasks, such as choosing a strategy for
    generating the site's pages dynamically and/or statically To substantiate
    STRUDEL's benefits, we describe the re-engineering of a production Web site
    using FunStruQL and show that the new site is smaller, more reusable, and
    unlike the original site, can be analyzed and optimized.", 
  location     = "http://dx.doi.org/10.1145/331960.331979"
}

@Article{narwf,
  author       = "Robert~W. Floyd",
  title        = "Nondeterministic Algorithms",
  journal      = jacm,
  year         = 1967,
  volume       = 14,
  number       = 4,
  pages        = "636--644",
  month        = oct,
  keywords     = "nondeterminism, algorithms, program transforms",
  abstract     = "Programs to solve combinatorial search problems may often be 
    simply written by using multiple-valued functions.  Such programs, although
    impossible to execute directly on conventional computers, may be converted
    in a mechanical way into conventional backtracking programs.  The process
    is illustrated with algorithms to find all solutions to the eight queens
    problem on the chessboard, and to find all simple cycles in a network.", 
  location     = "http://dx.doi.org/10.1145/321420.321422"
}

@Book{osid,
  author       = "Thomas~W. Doeppner",
  title        = "Operating Systems in Depth",
  publisher    = "Wiley",
  year         = 2011,
  keywords     = "operating systems, multithreaded programming, processor
    management, file systems, memory management, security, networking,
    distributed file systems",
  location     = "QA 76.76.O63 D64"
}

@Book{aiama,
  author       = "Stuart Russell and Peter Norvig",
  title        = "Artificial Intelligence: {A} Modern Approach",
  publisher    = "Prentice Hall",
  year         = 2010,
  address      = srnj,
  edition      = "third",
  keywords     = "artificial intelligence, agents, search, constraint
    satisfaction, first-order logic, inference, planning, knowledge
    representations, uncertainty, probabilistic reasoning, decision making,
    leaning, natural language processing, perception, robotics",
  location     = "Q 335 R86"
}

@Book{isppap,
  author       = "Robert~J. Schalkoff",
  title        = "Intelligent Systems:  Principles, Pradigms, and Pragmatics",
  publisher    = "Jones and Bartlett",
  year         = 2011,
  address      = "Sudbury, " # MA,
  keywords     = "search, constraint satisfaction, natural language
    understanding, production systems, soar, uncertainty, fuzzy systems,
    planning, neural networks, learning, evolutionary computing",
  location     = "QA 76.76 I58 S323"
}

@Book{tiloeb,
  author       = "David Bromwich",
  title        = "The Intellectual Life of Edmund Burke",
  publisher    = "Harvard University Press",
  year         = 2014,
  address      = cma,
  keywords     = "edmund burke, aesthetics, the revolutionary war, democracy,
    political thought, politics",
  location     = "DA 506 B9 B69"
}

@Book{tips,
  author       = "Paul Vaderlind and Richard~K. Guy and Loren~C. Larsen",
  title        = "The Inquisitive Problem Solver",
  publisher    = "The Mathematical Association of America",
  year         = 2002,
  address      = "Washington, D.C.",
  keywords     = "mathematics, problems, problem solving",
  location     = "QA 43.V28 2002"
}

@Book{bbrbp,
  author       = "Robert~B. Parker",
  title        = "Bad Business",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2004,
  address      = nyny,
  keywords     = "financial chicanery, murrdaar",
  location     = "PS 3566.A686 B34"
}

@Book{hifh,
  author       = "Helen Macdonald",
  title        = "{H} Is for Hawk",
  publisher    = "Grove Press",
  year         = 2014,
  address      = nyny,
  keywords     = "hawks, death, history",
  location     = "QL 696.F3 M324"
}

@InBook{lphoor,
  author       = "Hamdy~A. Taha",
  title        = "Handbook of Operations Research",
  chapter      = "II-1: Linear Programming",
  publisher    = "Van Nostrand Reinhold",
  year         = 1978,
  editor       = "Joseph~J. Moder and Salah~E. Elmaghraby",
  pages        = "85--119",
  address      = nyny,
  keywords     = "linear programming, the simplex method, duality theory,
    sensitivity analyses",
  location     = "T 57.6.H35"
}

@TechReport{rsurr,
  author       = "Fr{\' e}d{\' e}ric Boussinot",
  title        = "{RC} Semantics using Rewriting Rules",
  institution  = "Centre de Math{\' e}matiques Appliqu{\' e}es, Ecole des Mines
  de Paris",
  year         = 1992,
  number       = "18--92",
  address      = "Sophia-Antipolis, France",
  month        = "23 " # sep,
  keywords     = "rewriting rules, semantics, rc, reactive semantics",
  abstract     = "This paper describes a formal semantics for a new
     programming language called reactive C.  This language is an extension of
     C to program reactive systems i.e.  systems that react to sequences of
     activations from the external world.  Reactive statements are introduced
     to code these systems.  The semantics of reactive statements is described
     in an operational framework using conditional rewriting rules.", 
  location     = "http://www-sop.inria.fr/meije/rp/RapportsRecherche/rapport18-92.pdf"
}

@TechReport{pbacus4,
  author       = "Mario Wolczko and Randall~B. Smith",
  title        = "Prototype-Based Application Construction Using {S}elf 4.0",
  institution  = "Sun Microsystems Laboratories",
  year         = 1995,
  number       = "SMLI 95-0257",
  address      = mvca,
  keywords     = "self, prototypical programming",
  location     = "https://www.cs.ucsb.edu/~urs/oocsb/self/release/Self-4.0/manuals/tutorial.ps.gz"
}

@InProceedings{eyawtkasbwata,
  author       = "David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios",
  title        = "Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask",
  booktitle    = port # "Twenty-Fourth ACM Symposium on Operating System Principles",
  year         = 2013,
  pages        = "33--48",
  address      = "Framington, Pennsylvania",
  month        = "3--6 " # nov,
  keywords     = "synchronization, machine architecture, operon, xeon",
  abstract     = "This paper presents the most exhaustive study of 
    synchronization to date.  We span multiple layers, from hardware
    cache-coherence protocols up to high-level concurrent software.  We do so
    on different types of architectures, from single-socket -- uniform and
    non-uniform -- to multi-socket -- directory and broadcast-based --
    many-cores.  We draw a set of observations that, roughly speaking, imply
    that scalability of synchronization is mainly a property of the hardware.",
  location     = "http://dx.doi.org/10.1145/2517349.2522714"
}

@InProceedings{oteogij,
  author       = "Ran Shaham, Elliot~K. Kolodner and Mooly Sagiv",
  title        = "On the Effectiveness of {GC} in {J}ava",
  booktitle    = pot # "International Symposium on Memory Management",
  year         = 2000,
  pages        = "12--17",
  address      = "Minneapolis, MN",
  month        = "15--16 " # oct,
  keywords     = "java, garbage collection, dynamic analysis",
  abstract     = "We study the effectiveness of garbage collection (GC)
    algorithms by measuring the time difference between the actual collection
    time of an object and the potential earliest collection time for that
    object.  Our ultimate goal is to use this study in order to develop static
    analysis techniques that can be used together with GC to allow earlier
    reclamation of objects.  The results may also be used to pinpoint
    application source code that could be rewritten in a way that would allow
    more timely GC.  Specifically, we compare the objects reachable from the
    root set to the ones that are actually used again.  The idea is that GC
    could reclaim unused objects even if they are reachable from the root set.
    Thus, our experiments indicate a kind of upper bound on storage savings
    that could be achieved.  We also try to characterize these objects in order
    to understand the potential benefits of various static analysis algorithms.
    The Java Virtual Machine (JVM) was instrumented to measure objects that are
    reachable, but not used again, and to characterize these objects.
    Experimental results are shown for the SPECjvm98 benchmark suite.  The
    potential memory savings for these benchmarks range from 23% to 74%.", 
  location     = "http://dx.doi.org/10.1145/362422.362430"
}

@InProceedings{dmdcsm,
  author       = "Watabe, Kazuo and Sakata, Shiro and Maeno, Kazutoshi and Fukuoka, Hideyuki and Ohmori, Toyoko",
  title        = "Distributed Multiparty Desktop Conferencing System: {MERMAID}",
  booktitle    = pot # "1990 ACM Conference on Computer-Supported Cooperative Work (CSCW '90)",
  year         = 1990,
  pages        = "27--38",
  address      = laca,
  month        = "7--10 " # oct,
  keywords     = "client-server, shared workspaces, video conferencing",
  abstract     = "This describes a distributed multiparty desktop conferencing
    system (MERMAID) and presents its preliminary brief evaluation, obtained as
    a result of daily use.  MERMAID, which is designed based on group
    collaboration system architecture, provides an environment for widely
    distributed participants, seated at their desks, to hold real-time
    conferences by interchanging information through video, voice, and
    multimedia documents.  This system is implemented by using narrow-band
    ISDN, high-speed data network, and UNIX-based EWSs with electronic writing
    pads, image scanners, video cameras, microphone-installed loudspeakers,
    etc.  The system provides participants with the means for sharing
    information in such multimedia forms as video images, voice, text,
    graphics, still images, and hand drawn figures.", 
  location     = "http://dx.doi.org/10.1145/99332.99338"
}

