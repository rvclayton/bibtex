.so bibtex.header

@string{asplos87 = sigplan # " (" # pot # "Second International Conference on " # asplos # ", ASPLOS II)"}
@string{icfp02 = sigplan # " (" # pot # "Seventh ACM SIGPLAN International Conference on Functional Programming, ICFP '02)" }
@string{osdi96 = osr # " (" # pot # "Second USENIX Symposium on Operating Systems Design and Implementation, OSDI '96)"}
@string{ppeals88 = sigplan # " (" # pot # "ACM\slash SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems, PPEALS '88)"}
@string{sosp89    = osr # " (" # pot # "Twelfth" # sosp # ", SOSP '89)"}
@string{sosp91    = osr # " (" # pot # "Thirteenth" # sosp # ", SOSP '91)"}
@string{sosp93    = osr # " (" # pot # "Fourteenth" # sosp # ", SOSP '932)"}
@string{usenixw92 = pot # "Winter 1992 USENIX Conference"}
		  
		  
@Book{tcwekm,
  author       = "Ella~K. Maillart",
  title        = "The Cruel Way",
  subtitle     = "Switzerland to Afghanistan in a Ford, 1939",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "travel, middle east",
  location     = "DS 352.M18"
}

@Book{taeojl,
  author       = "Yves Beauchemin",
  title        = "The Accidental Education of Jerome Lupien",
  publisher    = "House of Anansi Press",
  year         = 2018,
  address      = "Canada",
  keywords     = "political machinations, lobbyists, con games",
  location     = "PS 8553.E172 E4813"
}

@Book{himt,
  author       = "Matt Taibbi",
  title        = "Hate Inc.",
  publisher    = "OR Books",
  year         = 2019,
  address      = nyny,
  keywords     = "media, propaganda, chomsky, maddow, journalism, the profit motive",
  location     = "9781949017250"
}

@Book{eolp,
  author       = "Christopher John Hogger",
  title        = "Essentials of Logic Programming",
  publisher    = oup,
  year         = 1990,
  series       = "Graduate Texts in Computer Science",
  address      = nyny,
  keywords     = "logic programming, first-order logic, causal-form logic,
    herbrand domain, sld resolution, definite program semantics, finite
    failure, program verification",
  location     = "QA 76.63 H64"
}

@Book{tmjc,
  author       = "Jorge Comensal",
  title        = "The Mutations",
  publisher    = fsg,
  year         = 2019,
  address      = nyny,
  keywords     = "cancer, survivorship, psychotherapy",
  location     = "PQ 7298.413.O438 M8813 "
}

@Book{ojhhdb,
  author       = "John~H. Halpern and David Blistein",
  title        = "Opium",
  subtitle     = "How an Ancient Flower Shaped and Poisoned Our World",
  publisher    = "Hachette",
  year         = 2019,
  address      = nyny,
  keywords     = "opium, addiction, poppies, drugs, medicine",
  location     = "HV 5816.H35"
}

@Book{tlpmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Laughing Policeman",
  publisher    = "Vintage",
  year         = 1977,
  price        = "$1.65",
  address      = nyny,
  keywords     = "murrdaar, doggedness, lone wolves",
  location     = "PT 9876.29.J63"
}

@Book{topsad,
  author       = "Edward Yourdon",
  title        = "Techniques of Program Structure and Design",
  publisher    = ph,
  year         = 1975,
  address      = ecnj,
  keywords     = "computer programs, top-down design, modular programming,
    structured programming, antidebugging, program testing, debugging,
    programming style",
  location     = "QA 76.6.6.Y68"
}

@Book{natb,
  author       = "Lars Iyer",
  title        = "Nietzsche and the Burbs",
  publisher    = "Melville House",
  year         = 2019,
  address      = "Brooklyn, N.Y.",
  keywords     = "suburban life, wasted youth",
  location     = ""
}

@Book{hwsts,
  author       = "Thomas Hockey",
  title        = "How We See the Sky",
  subtitle     = "A Naked-Eye Tour of Day and Night",
  publisher    = ucp,
  year         = 2011,
  address      = chil,
  keywords     = "the night sky, stars, planets, the moon",
  location     = "QB 44.3 H635"
}

@Book{htrm,
  author       = "Randall Munroe",
  title        = "How To",
  subtitle     = "Absurd Scientific Advice for Common Real-World Problems",
  publisher    = "Riverhead Books",
  year         = 2019,
  address      = nyny,
  keywords     = "jumping, pool parties, digging holes, piano playing,
    emergency landings, crossing rivers, moving, stability, lava moats,
    throwing, football, weather prediction, physics, algebra, modeling.",
  location     = ""
}

@Book{tnbcw,
  author       = "Colson Whitehead",
  title        = "The Nickel Boys",
  publisher    = "Doubleday",
  year         = 2019,
  address      = nyny,
  keywords     = "racism, florida, juvenile detention, the past",
  location     = "PS 3573.H4768 N53"
}

@Book{ystb,
  author       = "Wendy Lesser",
  title        = "You Say to Brick",
  subtitle     = "The Life of Louis Kahn",
  publisher    = fsg,
  year         = 2017,
  address      = nyny,
  keywords     = "louis kahn, american architecture",
  location     = "NA 737.K32 L48"
}

@Book{cnast,
  author       = "Andrew~S. Tanenbaum and David~J. Wetherall",
  title        = "Computer Networks",
  publisher    = ph,
  year         = 2011,
  address      = boma,
  keywords     = "iso osi stack, network security",
  location     = "TK 5105.5.T36"
}

@Article{famw,
  author       = "Charles~P. Thacker and Lawrence~C. Stewart",
  title        = "Firefly:  {A} Multiprocessor Workstation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "164--172",
  month        = oct,
  keywords     = "multiprocessor architecture, vax, caching, trade-offs,
    simulation",
  abstract     = "Firefly is a shared-memory multiprocessor workstation that
    contains from one to seven MicroVAX 78032 processors, each with a floating
    point unit and a sixteen kilobyte cache.  The caches are coherent, so that
    all processors see a consistent view of main memory.  A system may contain
    from four to sixteen megabytes of storage.  Input-output is done via a
    standard DEC QBus.  Input-output devices are an Ethernet controller, fixed
    disks, and a monochrome 1024 x 768 display with keyboard and mouse.
    Optional hardware includes a high resolution color display and a controller
    for high capacity disks.  Figure 1 is a system block diagram.The Firefly
    runs a software system that emulates the Ultrix system call interface.  It
    also supports medium- and coarse-grained multiprocessing through multiple
    threads of control in a single address space.  Communications are
    implemented uniformly through the use of remote procedure calls.This paper
    describes the goals, architecture, implementation and performance analysis
    of the Firefly.  It then presents some measurements of hardware
    performance, and discusses the degree to which SRC has been successful in
    producing software to take advantage of multiprocessing.", 
  location     = "https://doi.org/10.1145/36177.36199", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-23.html"
}

@Article{paritv8p,
  author       = "Douglas~W. Clark",
  title        = "Pipelining and Performance in the {VAX} 8800 Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "173--177",
  month        = oct,
  keywords     = "pipelining, microcode, micropipelining, instruction set
    architecture, performance", 
  abstract     = "The VAX 8800 family (models 8800, 8700, 8550), currently the
    fastest computers in the VAX product line, achieve their speed through a
    combination of fast cycle time and deep pipelining.  Rather than pipeline
    highly variable VAX instructions as such, the 8800 design pipelines uniform
    microinstructions whose addresses are generated by instruction unit
    hardware.  This design approach helps achieve a fast cycle time, which is
    the prime determinan of performance.  Some preliminary measurements of
    cycles per average instruction are reported.", 
  location     = "https://doi.org/10.1145/36177.36200"
}

@Article{avafatsc,
  author       = "Robert~P. Colwell and Robert~P. Nix and John~J. O'Donnell and David~B. Papworth and Paul~K. Rodman",
  title        = "{A} {VLIW} Architecture for a Trace Scheduling Compiler",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "180--192",
  month        = oct,
  keywords     = "vliw architecture, pipelining, compilation, trace scheduling,
    scientific computation, instruction set architecture, branching",
  abstract     = "Very Long Instruction Word (VLIW) architectures were promised
    to deliver far more than the factor of two or three that current
    architectures achieve from overlapped execution.  Using a new type of
    compiler which compacts ordinary sequential code into long instruction
    words, a VLIW machine was expected to provide from ten to thirty times the
    performance of a more conventional machine built of the same implementation
    technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE
    along with its companion Trace Scheduling compacting compiler.  This new
    machine has fulfilled the performance promises that were made. Using many
    fast functional units in parallel, this machine extends some of the basic
    Reduced-Instruction-Set precepts: the architecture is load/store, the
    microarchitecture is exposed to the compiler, there is no microcode, and
    there is almost no hardware devoted to synchronization, arbitration, or
    interlocking of any kind (the compiler has sole responsibility for runtime 
    resource usage).  This paper discusses the design of this machine and
    presents some initial performance results.", 
  location     = "https://doi.org/10.1145/36206.36201"
}

@Article{dttstcplitcm,
  author       = "David~R. Ditzel and Hubert~R. McLellan and Alan~D. Berenbaum",
  title        = "Design Tradeoffs to Support the {C} Programming Language in the {CRISP} Microprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "158--162",
  month        = oct,
  keywords     = "system architecture, language support, context switching,
    code density, stack cache, compiler support, code analysis, path length",
  abstract     = "The CRISP Microprocessor contains a number of new
    architectural features to achieve high performance and support the C
    programming language.  The instruction set was designed to be independent
    of architectural tradeoffs used in any single implementation.  This paper
    describes the particular tradeoffs used in the implementation of a 172,163
    transistor 32-bit single chip microprocessor.  Many tradeoffs were used in
    the design of CRISP, this paper tries to focus on those particular to C. ", 
  location     = "https://doi.org/10.1145/36177.36198"
}

@Article{arafsc,
  author       = "Richard~B. Kieburtz",
  title        = "{A} {RISC} Architecture for Symbolic Computation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "146--155",
  month        = oct,
  keywords     = "graph reduction, combinators, g-machines, tagged data types,
    pipelining",
  abstract     = "The G-machine is a language-directed processor architecture
    designed to support graph reduction as a model of computation.  It can
    carry out lazy evaluation of functional language programs and can evaluate
    programs in which logical variables are used.  To support these language
    features, the abstract machine requires tagged memory and executes some
    rather complex instructions, such as to evaluate a function
    application.This paper explores an implementation of the G-machine as a
    high performance RISC architecture.  Complex instructions can be
    represented by RISC code without experiencing a large expansion of code
    volume.  The instruction pipeline is discussed in some detail.  The
    processor is intended to be integrated into a standard, 32-bit memory
    architecture.  Tagged memory is supported by aggregating data with tags in
    a cache.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{rvcfpacs,
  author       = "Gaetano Borriello and Andrew~R. Cherenson and Peter Bernard Danzig and Michael Newell Nelson",
  title        = "{RISCs} vs. {CISCs} for {Prolog}: {A} Case Study",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "136--145",
  month        = oct,
  keywords     = "prolog, logic programming, abstract machines, compilation,
    spur, tagged data types, unification, backtracking, coprocessors",
  abstract     = "This paper compares the performance of executing compiled
    Prolog code on two different architectures under development at
    U. C. Berkeley.  The first is the PLM, a special-purpose CISC architecture
    intended as a coprocessor for a host machine.  The second is SPUR, a
    general-purpose RISC architecture that supports tagged data.  Fourteen
    standard benchmark programs were run on both the PLM and SPUR simulators.
    The compiled code for SPUR was obtained by simple macro-expansion of PLM
    code generated by the PLM Prolog compiler.  The two implementations are
    compared with regard to static and dynamic program size, execution speed,
    and memory system performance.  On average, the macrocoded SPUR
    implementation has a static code size 14 times larger than the PLM,
    executes 16 times more instructions, yet requires only 2.3 times the number
    of machine cycles (or has the performance of 0.43 PLMs).  When memory
    system performance is taken into account, SPUR is equivalent to 0.29 PLMs.
    Optimizations of the macro-expanded code and minor architectural changes to
    SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks.
    Thus a tagged RISC architecture can execute Prolog at least half as fast as
    a special-purpose CISC architecture for Prolog.", 
  location     = "https://doi.org/10.1145/36177.36196"
}

@Article{tmeuailatmd,
  author       = "David~W. Wall and Michael~L. Powell",
  title        = "The {Mahler} Experience:  Using an Intermediate Language as the Machine Description",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "100--104",
  month        = oct,
  keywords     = "intermediate language, machine independence, compilers,
    global optimizations, link-time optimizations, instruction scheduling,
    pipelining, abstract machines",
  abstract     = "Division of a compiler into a front end and a back end that 
    communicate via an intermediate language is a well-known technique.  We go
    farther and use the intermediate language as the official description of a
    family of machines with simple instruction sets and addressing
    capabilities, hiding some of the inconvenient details of the real machine
    from the users and the front end compilers.To do this credibly, we have had
    to hide not only the existence of the details but also the performance
    consequences of hiding them.  The back end that compiles and links the
    intermediate language tries to produce code that does not suffer a
    performance penalty because of the details that were hidden from the front
    end compiler.  To accomplish this, we have used a number of link-time
    optimizations, including instruction scheduling and interprocedural
    register allocation, to hide the existence of such idiosyncracies as
    delayed branches and non-infinite register sets.  For the most part we have
    been successful.", 
  location     = "https://doi.org/10.1145/36177.36190",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-1.pdf"
}

@Article{asosctfps,
  author       = "Shlomo Weiss and James~E. Smith",
  title        = "{A} Study of Scalar Compilation Techniques for Pipelined Supercomputers",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "cray computers, loop unrolling, software pipelining,
    performance, scientific computing, optimization, machine architectures,
    register files, vectorization",
  abstract     = "This paper studies two compilation techniques for enhancing
    scalar performance in high-speed scientific processors: software pipelining
    and loop unrolling.  We study the impact of the architecture (size of the
    register file) and of the hardware (size of instruction buffer) on the
    efficiency of loop unrolling.  We also develop a methodology for
    classifying software pipelining techniques.  For loop unrolling, a
    straightforward scheduling algorithm is shown to produce near-optimal
    results when not inhibited by recurrences or memory hazards.  Our study
    indicates that the performance produced with a modified CRAY-1S scalar
    architecture and a code scheduler utilizing loop unrolling is comparable to
    the performance achieved by the CRAY-1S with a vector unit and the CFT
    vectorizing compiler.", 
  location     = "https://doi.org/10.1145/79505.79508"
}

@Article{cs8tar,
  author       = "William~R. Bush and A.~Dain Samples and David Ungar and Paul~N. Hilfinger",
  title        = "Compiling {Smalltalk-80} to a {RISC}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "risc architecture, bytecode, compilation, register windows,
    soar, dynamic languages, caching, performance",
  abstract     = "The Smalltalk On A RISC project at U.  C.  Berkeley proves
    that a high-level object-oriented language can attain high performance on a
    modified reduced instruction set architecture.  The single most important
    optimization is the removal of a layer of interpretation, compiling the
    bytecoded virtual machine instructions into low-level, register-based,
    hardware instructions.  This paper describes the compiler and how it was
    affected by SOAR architectural features.  The compiler generates code of
    reasonable density and speed.  Because of Smalltalk-80's semantics,
    relatively few optimizations are possible, but hardware and software
    mechanisms at runtime offset these limitations.  Register allocation for an
    architecture with register windows comprises the major task of the
    compiler.  Performance analysis suggests that SOAR is not simple enough;
    several hardware features could be efficiently replaced by instruction
    sequences constructed by the compiler.", 
  location     = "https://doi.org/10.1145/36177.36192"
}

@Article{hmamae,
  author       = "F.~Chow and S.~Correll and M.~Himelstein and E.~Killian and L.~Weber",
  title        = "How Many Addressing Modes are Enough?",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "117--121",
  month        = oct,
  keywords     = "risc, addressing modes, addressing architecture, offset
    indexing, optimizations, performance, simplicity",
  abstract     = "Programs naturally require a variety of memory-addressing
    modes.  It isn't necessary to provide them in hardware, however, if a
    compiler can synthesize them from a few primitive modes.  This not only
    simplifies the hardware, but also permits the compiler to use its
    understanding of the program to economize on the modes which it uses.  We
    present some compilation techniques that allow the compiler to deal
    effectively with a single addressing mode in a target RISC processor.  We
    also give measurements to show the benefits of such techniques, and to
    support our assertion that a single addressing mode is adequate for a
    general purpose processor, provided that mode incorporates both a pointer
    and an offset.", 
  location     = "https://doi.org/10.1145/36177.36193"
}

@Article{salatsp,
  author       = "Henry Massalin",
  title        = "Superoptimizer --- {A} Look at the Smallest Program",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "122--126",
  month        = oct,
  keywords     = "optimization, exhaustive search, probabilistic testing,
    assembly code",
  abstract     = "Given an instruction set, the superoptimizer finds the
    shortest program to compute a function.  Startling programs have been
    generated, many of them engaging in convoluted bit-fiddling bearing little
    resemblance to the source programs which defined the functions.  The key
    idea in the superoptimizer is a probabilistic test that makes exhaustive
    searches practical for programs of useful size.  The search space is
    defined by the processor's instruction set, which may include the whole
    set, but it is typically restricted to a subset.  By constraining the
    instructions and observing the effect on the output program, one can gain
    insight into the design of instruction sets.  In addition, superoptimized
    programs may be used by peephole optimizers to improve the quality of
    generated code, or by assembly language programmers to improve manually
    written code.", 
  location     = "https://doi.org/10.1145/36177.36194"
}

@Article{paaeotpm,
  author       = "Kazuo Taki and Katsuto Nakajima and Hiroshi Nakashima and Morihiro Ikeda",
  title        = "Performance and Architectural Evaluation of the {PSI Machine}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "128--135",
  month        = oct,
  keywords     = "kl0, prolog, logic programming, caching, branching, psi machine",
  abstract     = "We evaluated a Prolog machine PSI (Personal Sequential
    Inference machine) for the purpose of improving and redesigning it.  In
    this evaluation, we measured the execution speed and the dynamic
    characteristics of cache memory, register file, and branching hardware
    introduced for high-speed execution of Prolog programs.Execution speed of
    the PSI firmware interpreter was found to be comparable to that of the
    DEC-10 Prolog compiled code on the DEC-2060.  It was also found that PSI
    was faster than DEC for executing programs containing much unification and
    backtracking that require runtime processing.With the cache memory, the hit
    ratio for application programs was found higher than 96%; this demonstrates
    that the Prolog execution has much memory access locality.  The memory
    access frequency and the appearance ratio between Read and Write command
    were also investigated.Concerning the register file, use rate of each
    dedicated access mode was measured and effect of each mode was discussed.
    In the branching function we confirmed a high appearance rate of
    conditional branches and multi-way branches based on tag values.", 
  location     = "https://doi.org/10.1145/36177.36195"
}

@Article{cbatccaisohci,
  author       = "Sam Wineburg and Susan Mosborg and Dan Porat and Ariel Duncan",
  title        = "Common Belief and the Cultural Curriculum:  An Intergenerational Study of Historical Consciousness",
  journal      = "American Education Research Journal",
  year         = 2007,
  volume       = 44,
  number       = 1,
  pages        = "40--76",
  month        = mar,
  keywords     = "history instruction, collective memory, vietnam war, united
    states history, curricula, veterans, soldiers, student protests, political
    protests, textbooks, forest gump",
  abstract     = "How is historical knowledge transmitted across generations?
    What is the role of schooling in that transmission? The authors address
    these questions by reporting on a thirty-month longitudinal study into how
    home, school, and larger society served as contexts for the development of
    historical consciousness among adolescents.  Fifteen families drawn from
    three different school communities participated.  By adopting an
    intergenerational approach, the authors sought to understand how the
    defining moments of one generation-its lived history'-becomes the available
    history to the next.  In this article, the authors focus on what parents
    and children shared about one of the most formative historical events in
    parents' lives: the Vietnam War.  Drawing on notions of collective memory,
    as articulated by the French sociologist Maurice Halbwachs, the authors
    sought to understand which stories, archived in historical memory and
    available to the disciplinary community, are remembered and used by those
    beyond its borders.  In contrast, which stories are no longer widely
    shared, eclipsed by time's passage and unable to cross the bridge
    separating generation from generation? The authors conclude by discussing
    the forces that act to historicize today's youth and suggest how educators
    might marshal these forces-rather than spurning or simply ignoring them-to
    advance young people's historical understanding.",
  location     = "https://www.jstor.org/stable/30069471?sa=X&ved=2ahUKEwiOxfHPx__mAhVph-AKHfibCVcQFjAAegQICBAB"
}

@Article{imadothpa,
  author       = "Daniel~J. Magenheimer and Liz Peters and Karl Pettis and Dan Zuras",
  title        = "Integer Multiplication and Division on the {HP Precision Architecture}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "90--99",
  month        = oct,
  keywords     = "multiplication, division, approximations",
  abstract     = "In recent years, many architectural design efforts have
    focused on maximizing performance for frequently executed, simple
    instructions.  Although these efforts have resulted in machines with better
    average price/performance ratios, certain complex instructions and, thus,
    certain classes of programs which heavily depend on these instructions may
    suffer by comparison.  Integer multiplication and division are one such set
    of complex instructions.  This paper describes how a small set of primitive
    instructions combined with careful frequency analysis and clever
    programming allows the Hewlett-Packard Precision Architecture integer
    multiplication and division implementation to provide adequate performance
    at little or no hardware cost.", 
  location     = "https://doi.org/10.1145/36206.36189"
}

@Article{aecfipooai4,
  author       = "Christos John Georgiou and Stewart~L. Palmer and P.~L. Rosenfeld",
  title        = "An Experimental Coprocessor for Implementing Persistent Objects on an {IBM} 4381",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "84--87",
  month        = oct,
  keywords     = "coprocessors, persistent objects, system architecture",
  abstract     = "In this paper we describe an experimental coprocessor for an
    IBM 4381 that is designed to facilitate the exploration of persistent objects.",
  location     = "https://doi.org/10.1145/36177.36188"
}

@Article{chsfsdap,
  author       = "Thomas~A. Cargill and Burt~N. Locanthi",
  title        = "Cheap Hardware Support for Software Debugging and Profiling",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "82--83",
  month        = oct,
  keywords     = "debugging, watch points, reverse execution",
  abstract     = "We wish to determine the effectiveness of some simple
    hardware for debugging and profiling compiled programs on a conventional
    processor. The hardware cost is small -- a counter decremented on each
    instruction that raises an exception when its value becomes zero. With the
    counter a debugger can provide data watchpoints and reverse execution: a
    profiler can measure the total instruction cost of a code segment and
    sample the program counter accurately. Such a counter has been included on
    a single-board MC68020 workstation, for which system software is currently
    being written. We will report our progress at the symposium.", 
  location     = "https://doi.org/10.1145/36177.36187"
}

@Article{cotplictp,
  author       = "Pei~Jyun Leu and Bharat Bhargava",
  title        = "Clarification of Two-Phase Locking in Concurrent Transaction Processing",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "122--125",
  month        = jan,
  keywords     = "two-phase locking, atomic operations, transactions, read
    locks, relaxation, serializability, write locks, logs",
  abstract     = "The authors propose a formal definition of the two-phase
    locking class derived from the semantic description of the two-phase
    locking protocol, and prove that this definition is equivalent to that
    given by C.H.  Papadimitriou (1979).  They present: (1) a precise
    definition of the two phase locking; (2) a clarification of the occurrence
    and the order ofall events such as lock points, unlock points, read
    operations, and write operations of conflicting transactions; and (3) by
    relaxing some conditions in the given definition, the derivation of a new
    class called restricted-non-two-phase locking (RN2PL), which is a superset
    of the class two-phase locking (2PL) but a subset of the class
    D-serializable (DSR) given by Papadimitriou.",
  location     = "https://doi.org/10.1109/32.4629"
}

@Article{cfmvac,
  author       = "James~R. Goodman",
  title        = "Coherency for Multiprocessor Virtual Address Caches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "72--80",
  month        = oct,
  keywords     = "caches, coherency, virtual storage, virtual addressing, tlbs,
    snooping",
  abstract     = "A multiprocessor cache memory system is described that
    supplies data to the processor based on virtual addresses, but maintains
    consistency in the main memory, both across caches and across virtual
    address spaces.  Pages in the same or different address spaces may be
    mapped to share a single physical page.  The same hardware is used for
    maintaining consistency both among caches and among virtual addresses.
    Three different notions of a cache block are defined: (1) the unit for
    transferring data to/from main storage, (2) the unit over which tag
    information is maintained, and (3) the unit over which consistency is
    maintained.  The relation among these block sizes is explored, and it is
    shown that they can be optimized independently.  It is shown that the use
    of large address blocks results in low overhead for the virtual address
    cache.", 
  location     = "https://doi.org/10.1145/36177.36186"
}

@Article{tdprra,
  author       = "Russell~R. Atkinson and Edward~M. McCreight",
  title        = "The {Dragon} Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "65--69",
  month        = oct,
  keywords     = "processor design, instruction sets, procedure calls, dorado,
    instruction density, chip packaging",
  abstract     = "The Xerox PARC Dragon is a VLSI research computer that uses
    several techniques to achieve dense code and fast procedure calls in a
    system that can support multiple processors on a central high bandwidth
    memory bus.",
  location     = "https://doi.org/10.1145/36205.36185"
}

@Article{teoiscopsamp,
  author       = "Jack~W. Davidson and Richard~A. Vaughan",
  title        = "The Effect of Instruction Set Complexity on Program Size and Memory Performance",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "60--64",
  month        = oct,
  keywords     = "instruction set design, risc, cisc, memory pressure, code
    size, portable compilation",
  abstract     = "One potential disadvantage a machines with a simple
    instruction set is object-program size may be substantially larger than
    those for a machine with a complex instruction set.  Groups of simple
    instructions are required to implement the same functions performed by a
    single instruction from a complex instruction set.  In addition, the
    tendency of simple instructions to be fixed length with a few instruction
    formats also increases object-program size.  Larger object-program size
    could adversely affect memory performance and bus traffic.  This paper
    reports the results of experiments to isolate and determine the effect of
    instruction set complexity on cache memory performance and bus traffic.
    Three high-level language compilers were constructed for machines with
    instruction sets of varying complexity.  Using a set of benchmark programs,
    we evaluated the effect of instruction set complexity had on program size.
    Five of the programs were used to perform a set of trace-driven simulations
    to study each machine's cache and bus performance.  While we found that the
    miss ratio is affected by object program size, it appears that this can be
    corrected by increasing the cache size.  Our measurements of bus traffic,
    however, show that even with large caches, machines with simple instruction
    sets can expect substantially more main memory reads than machines with
    complex instruction sets.",  
  location     = "https://doi.org/10.1145/36205.36184"
}

@Article{aafdfdpd,
  author       = "Mike Adler",
  title        = "An Algebra for Data Flow Diagram Process Decomposition",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "169--183",
  month        = feb,
  keywords     = "algebra, automatic process decomposition, data flow diagram,
    dfd, directed acyclic graph, graph-based grammar, software engineering,
    structured analysis",
  abstract     = "Data flow diagram process decomposition, as applied in the
    analysis phase of software engineering, is a top-down method that takes a
    process, and its input and output data flows, and logically implements the
    process as a network of smaller processes.  The decomposition is generally
    performed in an ad hoc manner by an analyst applying heuristics, expertise,
    and knowledge to the problem.  An algebra that formalizes process
    decomposition is presented using the De Marco representation scheme.  In
    this algebra, the analyst relates the disjoint input and output sets of a
    single process by specifying the elements of an input/output connectivity
    matrix.  A directed acyclic graph is constructed from the matrix and is the
    decomposition of the process.  The graph basis, grammar matrix, and graph
    interpretations, and the operators of the algebra are discussed.  A
    decomposition procedure for applying the algebra, prototype, and production
    tools and outlook are also discussed.", 
  location     = "https://doi.org/10.1109/32.4636"
}

@Article{tatcilhasa,
  author       = "Peter Steenkiste and John Hennessy",
  title        = "Tags and Type Checking in {LISP}:  Hardware and Software Approaches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "50--59",
  month        = oct,
  keywords     = "lisp, type checking, generic operations, tagged
    architectures, performance",
  abstract     = "One  major factor distinguishing LISP from other languages
    (e.g., Pascal, C, Fortran) is the need for run-time type checking.  Run-time
    type checking is implemented by adding to each data     object a tag that
    encodes type information.  Tags must be compared for type compatibility,
    removed when using the data, and inserted when new data items are created.
    This tag manipulation, together with other work related to dynamic type
    checking and generic operations, constitutes a significant component of the
    execution time of LISP programs.  This has led both to the development of
    LISP machines that support tag checking in hardware and to the avoidance of
    type checking by users running on stock hardware.  To understand the role
    and necessity of special-purpose hardware for tag handling, we first
    measure the cost of type checking operations for a group of LISP programs.
    We then examine hardware and software implementations of tag operations and
    estimate the cost of tag handling with the different tag implementation
    schemes.  The data shows that minimal levels of support provide most of the
    benefits, and that tag operations can be relatively inexpensive, even when
    no special hardware support is present.",  
  location     = "https://doi.org/10.1145/36206.36183"
}

@Article{aaftdeotfpl,
  author       = "John~R. Hayes and Martin~E. Fraeman and Robert~L. Williams and Thomas Zaremba",
  title        = "An Architecture for the Direct Execution of the {Forth} Programming Language",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "42--49",
  month        = oct,
  keywords     = "zero-address languages, stack management, embedded systems,
    instruction set architecture, data paths",
  abstract     = "We have developed a simple direct execution architecture for
    a 32-bit Forth microprocessor.  The processor can directly access a linear
    address space of over 4 gigawords.  Two instruction types are defined; a
    subroutine call, and a user defined microcode instruction.  On-chip stack
    caches allow most Forth primitives to execute in a single cycle.", 
  location     = "https://doi.org/10.1145/36206.36182"
}

@Article{asfmppohs,
  author       = "Roberto Bisiani and Alessandro Forin",
  title        = "Architectural Support for Multilanguage Parallel Programming on Heterogeneous Systems",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "21--30",
  month        = oct,
  keywords     = "common runtimes, shared storage, remote operations, mach",
  abstract     = "We have designed and implemented a software facility, called
    Agora, that supports the development of parallel applications written in
    multiple languages.  At the core of Agora there is a mechanism that allows
    concurrent computations to share data structures independently of the
    computer architecture they are executed on.  Concurrent computations
    exchange control information by using a pattern-directed technique.  This
    paper describes the Agora shared memory and its software implementation on
    both tightly and loosely-coupled architectures.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{mivmmfpuama,
  author       = "Richard Rashid and Avadis Tevanian and Michael Young and David Golub and Robert Baron and David Black and William Bolosky and Jonathan Chew",
  title        = "Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "31--39",
  month        = oct,
  keywords     = "mach, virtual storage, machine independence, portability,
    shared storage, address mapping, paging",
  abstract     = "This paper describes the design and implementation of virtual
    memory management within the CMU Mach Operating System and the experiences
    gained by the Mach kernel group in porting that system to a variety of
    architectures.  As of this writing, Mach runs on more than half a dozen
    uniprocessors and multiprocessors including the VAX family of uniprocessors
    and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the
    Sequent Balance 21000 and several experimental computers.  Although these
    systems vary considerably in the kind of hardware support for memory
    management they provide, the machine-dependent portion of Mach virtual
    memory consists of a single code module and its related header file.  This
    separation of software memory management from hardware support has been
    accomplished without sacrificing system performance.  In addition to
    improving portability, it makes possible a relatively unbiased examination
    of the pros and cons of various hardware memory management schemes,
    especially as they apply to the support of multiprocessors.", 
  location     = "https://doi.org/10.1145/36177.36181"
}

@Article{vafam,
  author       = "Bob Beck and Bob Kasten and Shreekant Thakkar",
  title        = "{VLSI} Assist for a Multiprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "10--20",
  month        = oct,
  keywords     = "caching, co-processors, ipc, synchronization, buses,
    interrupt handling, system configuration",
  abstract     = "Multiprocessors have long been of interest to computer
    community.  They provide the potential for accelerating applications through
    parallelism and increased throughput for large multi-user system.  Three
    factors have limited the commercial success of multiprocessor systems;
    entry cost, range of performance, and ease of application.  Advances in
    very large scale integration (VLSI) and in computer aided design (CAD) have
    removed these limitations, making possible a new class of multiprocessor
    systems based on VLSI components.  A set of requirements for building an
    efficient shared multiprocessor system are discussed, including: low-level
    mutual exclusion, interrupt distribution, inter-processor signaling,
    process dispatching, caching, and system configuration.  A system that
    meets these requirements is described and evaluated.", 
  location     = "https://doi.org/10.1145/36177.36179"
}

@Article{clir,
  author       = "Matthew Flatt",
  title        = "Creating Languages in {Racket}",
  journal      = "ACM Queue",
  year         = 2011,
  volume       = 9,
  number       = 11,
  month        = nov,
  keywords     = "racket, language design, macros, dsl, modules",
  abstract     = "Choosing the right tool for a simple job is easy: a
    screwdriver is usually the best option when you need to change the battery
    in a toy, and grep is the obvious choice to check for a word in a text
    document.  For more complex tasks, the choice of tool is rarely so
    straightforward—all the more so for a programming task, where programmers
    have an unparalleled ability to construct their own tools.  Programmers
    frequently solve programming problems by creating new tool programs, such
    as scripts that generate source code from tables of data.", 
  location     = "https://doi.org/10.1145/2063166.2068896", 
  location     = "https://queue.acm.org/detail.cfm?id=2068896"
}

@Article{hafplaplfha,
  author       = "Niklaus Wirth",
  title        = "Hardware Architecture for Programming Languages and Programming Languages for Hardware Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "2--8",
  month        = oct,
  keywords     = "abstractions, hardware design, complexity, mathematical
    formalisms, system state, correctness reasoning, programming languages",
  abstract     = "Programming Languages and Operating Systems introduce
    abstractions which allow the programmer to ignore details of an
    implementation.  Support of an abstraction must not only concentrate on
    promoting the efficiency of an implementation, but also on providing the
    necessary guards against violations of the abstractions.  In the frantic
    drive for efficiency the second goal has been neglected.  There are
    indications that recent designs which are claimed to be both simple and
    powerful, achieve efficiency by shifting the complex issues of code
    generation and of appropriate guards onto compilers.Complexity has become
    the common hallmark of software as well as hardware designs.  It cannot be
    mastered by the common practices of testing and simulation.  Hardware
    design may profit from developments in programming methodology by adopting
    proof techniques similar to those used in programming.", 
  location     = "https://doi.org/10.1145/36177.36178"
}

@Article{atosigpdcs,
  author       = "Thomas~L. Casavant and Jon~G. Kuhl",
  title        = "{A} Taxonomy of Scheduling in General-PUrpose Distributed Computing Systems",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "141--154",
  month        = feb,
  keywords     = "distributed operating systems, distributed resource
    management, general-purpose distributed computing systems, scheduling, task
    allocation, taxonomy",
  abstract     = "One measure of the usefulness of a general-purpose
    distributed computing system is the system's ability to provide a level of
    performance commensurate to the degree of multiplicity of resources present
    in the system.  A taxonomy of approaches to the resource management problem
    is presented in an attempt to provide a common terminology and
    classification mechanism necessary in addressing this problem.  The
    taxonomy, while presented and discussed in terms of distributed scheduling,
    is also applicable to most types of resource management.", 
  location     = "https://doi.org/10.1109/32.4634"
}

@Article{idfaa,
  author       = "Barbara~G. Ryder and Marvin~C. Paull",
  title        = "Incremental Data-Flow Algorithm Algorithms",
  journal      = toplas,
  year         = 1988,
  volume       = 10,
  number       = 1,
  pages        = "1--50",
  month        = jan,
  keywords     = "incremental algorithms, data-flow equations, interval
    analysis",
  abstract     = "An incremental update algorithm modifies the solution of a
    problem that has been changed, rather than re-solving the entire problem.
    ACINCF and ACINCB are incremental update algorithms for forward and
    backward data-flow analysis, respectively, based on our equations model of
    Allen-Cocke interval analysis.  In addition, we have studied their
    performance on a “nontoy” structured programming language L.  Given a set
    of localized program changes in a program written in L, we identify a
    priori the nodes in its flow graph whose corresponding data-flow equations
    may be affected by the changes.  We characterize these possibly affected
    nodes by their corresponding program structures and their relation to the
    original change sites, and do so without actually performing the
    incremental updates.  Our results can be refined to characterize the
    reduced equations possibly affected if structured loop exit mechanisms are
    used, either singly or together, thereby relating richness of programming
    language usage to the ease of incremental updating.", 
  location     = "https://doi.org/10.1145/42192.42193"
}

@Article{mmiaes,
  author       = "Christopher Rosebrugh and Eng-Kee Kwang",
  title        = "Multiple Microcontrollers in an Embedded System",
  journal      = ddj,
  year         = 1992,
  volume       = 17,
  number       = 1,
  pages        = "48--57",
  month        = jan,
  keywords     = "hardware design, ",
  abstract     = "A case study in system architecture and embedded hardware design"
}

@Article{j14pjsacp1,
  author       = "Wm. Paul Rogers",
  title        = "{J2SE} 1.4 Premieres {Java}'s Assertion Capabilities, Part 1",
  journal      = "Java World",
  year         = 2001,
  month        = November,
  keywords     = "java, assertions",
  location     = "https://www.javaworld.com/article/2075803/j2se-1-4-premieres-java-s-assertion-capabilities--part-1.html"
}

@TechReport{aqmafd30,
  author       = "Greg White",
  title        = "Active Queue Management Algorithms for {DOCSIS} 3.0",
  subtitle     = "A Simulation Study of CoDel, SfQ-CoDel and PIE in DOCSIS 3.0 Networks",
  institution  = "Access Network Technologies, CableLabs",
  year         = 2013,
  month        = apr,
  keywords     = "latency, buffering, buffer bloat, queue drop algorithms, pie,
    codel, sfq-codel, packet loss, tcp, performance",
  abstract     = "This paper describes the results of a simulation study of
    three active queue management algorithms applied to the upstream
    transmission buffer in a DOCSIS 3.  cable modem.  This paper is a follow-on
    to an earlier study which examined the Controlled Delay (CoDel) active
    queue management algorithm in a simulated DOCSIS 3.  cable modem.  This
    expanded study looks at CoDel in more depth, and compares it to two other
    promising active queue management algorithms, Stochastic Flow Queue - CoDel
    (SFQ- CoDel) and Proportional Integral Enhanced (PIE).  These three queue
    management algorithms are compared to existing (tail drop) buffering
    implementations that exist in current cable modems across a range of
    latency-sensitive applications.  It is demonstrated that current cable
    modem implementations result in severe degradation of user experience for
    latency-sensitive applications in situations where the user is
    simultaneously uploading a file via TCP.  The goal of the active queue
    managers in this study is to prevent the degradation of latency sensitive
    applications, while not impacting the TCP upload performance.  The
    Stochastic Flow Queue - Controlled Delay active queue manager displays
    extremely good performance in most traffic scenarios, enabling up to 2x
    reduction in latency for gaming traffic, x reduction in web page load time,
    and pristine VoIP quality, all while minimally impacting TCP upload
    performance.  The Proportional Integral Enhanced active queue manager
    similarly provided good performance, and is optimized for efficient
    implementation in existing cable modems.", 
  location     = "https://www.cablelabs.com/wp-content/uploads/2014/05/Active_Queue_Management_Algorithms_DOCSIS_3_0.pdf"
}

@TechReport{apfdsifcip,
  author       = "Joel Moses",
  title        = "{A} Program for Drilling Students in Freshman Calculus Integration Problems",
  institution  = "Artificial Intelligence Project, Project MAC, " # mit,
  year         = 1968,
  type         = "Memo",
  number       = 158,
  address      = cma,
  month        = mar,
  keywords     = "integration, problem solving, testing",
  abstract     = "The SARGE program is a prototype of a program which is
    intended to be used as an adjacent to regular classroom work in freshman
    calculus.  Using SARGE, students can type their step-by-step solution to an
    indefinite integration problem, and can have the correctness of their
    solution determined by the system.  The syntax for these steps comes quite
    close to normal mathematical notation, given the limitations of typewriter
    input.  The methods of solution is pretty much unrestricted as long as no
    mistakes are made along the way.  If a mistake is made, SARGE will catch it
    and yield an error message.  The student may modify the incorrect step, or
    he may ask the program for advice on how the mistake arose by typing
    'help'.  At present the program is weak in generating explanations for
    mistakes.  Sometimes the 'help' mechanisms will just yield a response which
    will indicate the way in which the erroneous step can be corrected.  In
    order to improve the explanation mechanism one would need a sophisticated
    analysis of students solutions to homework or quiz problems.  Experience
    with the behavior of students with SARGE, which is nil at present, should
    also help in accomplishing this goal.  SARGE is available as SARGE SAVED in
    T302 2517.", 
  location     = "https://dspace.mit.edu/bitstream/handle/1721.1/6163/AIM-158.pdf"
}

@InProceedings{cmultsygbtr,
  author       = "Stanley~P. Hanks",
  title        = "Creating {MAN}s using {LAN} Technology:  Sometimes You Gotta Break the Rules",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "439--451",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "man, lans, bridging, security, encapsulation, fddi, ethernet,
    vpn, standards",
  abstract     = "Commercially available, off-the-shelf internetworking
    products provide good mechanisms for the creation of limited-throughput
    metropolitan and wide area networks.  However, for many applications either
    the throughput obtained from T1 or slower communication circuits is
    inadequate, or the expense of obtaining high throughput using multiple T1
    or whole DS-3 circuits is prohibitive.  Proposed service offerings such as
    802.6, SMDS, frame relay, or SONET promise adequate speed metropolitan area
    network connectivity at a reasonable price.  Today these services arc not
    available, or where they are available are limited to T1 speeds.
    Metropolitan Fiber Systems owns over 17,000 miles of fiber optic cable in
    12 cities.  Most of this capacity is currently devoted to providing leased
    T1 and DS-3 circuits, and a significant portion of those circuits are for
    data transmission.  This provided a unique opportunity to address the
    question of how to provide LAN speed connectivity in and between
    metropolitan areas using commercially available products.  This paper
    discusses the development of the very high speed MAN connectivity service
    offerings based on FDDI provided by MFS.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ahotcosalu1tj1,
  author       = "Alan~E. Kaplan",
  title        = "{A} History of the {COSNIX} Operating System: Assembly Language " # unix # " 1971 to {July}, 1991",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "429--437",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "history, fortran, disk performance, database systems",
  abstract     = "From 1971 until July, 1991 a variant of the assembly language
    version of the Unix operating system (Unix-A) was used to run a transaction
    processing system called COSMOS (Computer System for Mainframe Operations)
    in the Regional Bell Operating Companies.  At one time about seven hundred
    such systems were running on PDP 11//45 and PDP 11/70 computers.  This talk
    describes the history and development of that Unix operating system
    variant, called COSNIX, and explains some of the reasons for its success.
    I hope, also, that it gives a feeling of the challenges involved in
    producing a viable system during the days when computing resources were
    much more severely limited than they are today.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{fsfhs,
  author       = "Henry Spencer",
  title        = "Faster String Function",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "419--428",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "string functions, optimization, performance, parallel execution",
  abstract     = "The string functions provided by ANSI C and by traditional
    Unix C libraries are usually not as well-optimized as they could be.
    Careful tuning of inner loops is common, and on some processors it is
    profitable to rewrite them in assembler to exploit special instructions,
    but on most systems operations are still done a character at a time.  Given
    fairly lenient assumptions about the architecture, versions that operate a
    word at a time are possible.  Word-at-a-time processing is superficially
    difficult for C strings, since they arc terminated by a single null that is
    awkward to detect within a word.  However, carefully chosen combinations of
    logical and arithmetic operations can do such detection at a cost of 3-6
    operations per word, depending on data constraints and architecture,
    without relying on any architecture-specific specialized instructions or
    data paths.  This technique has been around as occasionally-heard folklore
    for some time, but does not appear to have been investigated in detail.
    The resulting word-at-a-time string functions are conspicuously faster than
    the usual ones for long strings.  The crossover point is typically 20-30
    characters, and the asymptotic speed advantage can be as much as a factor
    of 5, although a factor of 2-3 is more typical on 100-character operands.
    For specialized requirements where customized interfaces and customized
    code are permissible, rather higher factors are possible.  Certain problems
    occur, notably higher startup overhead, difficulties with unaligned
    strings, and the prevalence of relatively short strings as operands to some
    string functions.  The case for the fast functions is mixed, and an
    adaptive algorithm is needed to maximize overall performance.  It would
    also be useful to package the algorithms for use in custom string code,
    although this is somewhat challenging.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{rsis50,
  author       = "Sandeep Khanna and Michael Sebree and John Zolnowsky",
  title        = "Realtime Scheduling in {SunOS} 5.0",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "375--390",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time, scheduling, kernel threads, interrupts",
  abstract     = "We describe the fundamental mechanisms in SunOS 5.0 to
    provide realtime scheduling functionality.  Our primary goal was to provide
    bounded behavior for dispatching or blocking threads.  To achieve this goal
    we have modified the kernel to be fully preemptive, guaranteeing dispatch
    after both synchronous and asynchronous wakeups.  We have also worked
    toward controlling priority inversion in the kernel.  The result is a
    kernel capable of delivering realtime scheduling and bounded response to a
    large class of user level applications.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cancpmtppl,
  author       = "Sharon Hopkins",
  title        = "Camels and Needles: Computer Poetry Meets the {Perl} Programming Language",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "391--404",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "perl, poetry, free verse",
  abstract     = "Although various forms of literature have been created with
    the assistance of a computer, and even been generated by computer programs,
    it is only recently that literary works have actually been written in a
    computer language.  A computer-language poem need not necessarily produce
    any output, it may succeed merely by fooling the parser into thinking it is
    an ordinary program.  'Ihe Perl programming language has proved well-suited
    to the creation of computer-language poetry.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{3atofs,
  author       = "W.~D. Roome",
  title        = "{3DFS}: {A} Time-Oriented File Server",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "405--418",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, archiving, nfs, backups, optical storage, caching",
  abstract     = "3DFS is a network file server that provides time-oriented
    access to files and directories.  3DFS allows a user to read the version of
    a file as it existed on a particular day in the past, or to list the files
    in a directory on some prior date.  3DFS saves the daily incremental
    backups from other file systems (Sun file servers, Vaxen,...), and creates
    an on-line file system from these dumps.  3DFS uses optical disks in an
    automated jukebox, so no operator intervention is required.  3DFS uses the
    Sun NFS™ protocol, and looks like any other NFS server.  Any UNIX® command
    can read files in 3DFS, and users mount 3DFS just like any file server.
    Because 3DFS provides on-line access to old versions, users can access
    those versions in-place, without copying them to magnetic disk.  This paper
    describes 3DFS, its implementation, and our experience with it.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ntbpnm,
  author       = "Matt Blaze",
  title        = "{NFS} Tracing by Passive Network Monitoring",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "333--343",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "nfs, filesystem performance, network monitoring",
  abstract     = "Traces of filesystem activity have proven to be useful for a
    wide variety of purposes, ranging from quantitative analysis of system
    behavior to trace-driven simulation of filesystem algorithms.  Such traces
    can be difficult to obtain, however, usually entailing modification of the
    filesystems to be monitored and runtime overhead for the period of the
    trace.  Largely because of these difficulties, a surprisingly small number
    of filesystem traces have been conducted, and few sample workloads are
    available to filesystem researchers.  This paper describes a portable
    toolkit for deriving approximate traces of NFS activity by non-intrusively
    monitoring the Ethernet traffic to and from the file server.  The toolkit
    uses a promiscuous Ethernet listener interface (such as the Packetfilter)
    to read and reconstruct NFS-related RPC packets intended for the server.
    It produces traces of the NFS activity as well as a plausible set of
    corresponding client system calls.  The tool is currently in use at
    Princeton and other sites, and is available via anonymous ftp.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ccfcsius,
  author       = "Joseph~L. Hellerstein",
  title        = "Control Considerations for {CPU} Scheduling in " # unix # " Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "359--374",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "scheduling, priority scheduling, compute-bound jobs",
  abstract     = "Managing UNIX systems often involves setting service rate
    objectives, such as specifying that application A should receive 50% of the
    central processing unit (CPU).  In most UNIX systems, the only way to
    control CPU usage is by adjusting nice values; unfortunately, the
    relationship between nice values and process service rates has been poorly
    understood.  This paper develops an analytic model that relates service
    rate objectives for compute-bound processes to nice values and three
    scheduler parameters: R (the rate at which priority increases for each
    quantum of CPU consumed), D (the decay factor), and T (the number of quanta
    that expire before CPU usages are decayed); the model is evaluated using
    measurements of a workstation running IBM's Advanced Interactive Executive
    (AIX) 3.1 Operating System.  Based on the model, we develop an algorithm
    that calculates nice values that achieve service rate objectives for
    compute-bound processes.  Experiments conducted on a production AIX 3.1
    system suggest that our algorithm works well in practice.  In addition, we
    use the model to obtain insights into the control implications of parameter
    settings.  For example, we show that the nice mechanism is often less
    effective on faster processors since T tends to increase with processor
    speed; this increases the fraction of time during which processes with
    larger nice values execute, and hence limits the extent to which their
    service rates can be controlled.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{atdaonaaciads,
  author       = "Ken~W. Shirriff and John~K. Ousterhout 
",
  title        = "{A} Trace-Driven Analysis of Name and Attribute Caching in a Distributed System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "315--330",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "caching, network file system, name look-up, performance,
    server load, cache coherence, process migration, trace-driven simulation,
    sprite",
  abstract     = "This paper presents the results of simulating file name and
    attribute caching on client machines in a distributed file system.  The
    simulation used trace data gathered on a network of about 40 workstations.
    Caching was found to be advantageous: a cache on each client containing
    just 10 directories had a 91% hit rate on name lookups.  Entry-based name
    caches (holding individual directory entries) had poorer performance for
    several reasons, resulting in a maximum hit rate of about 83%.  File
    attribute caching obtained a 90% hit rate with a cache on each machine of
    the attributes for 30 files.  The simulations show that maintaining cache
    consistency between machines is not a significant problem; only 1 in 400
    name component lookups required invalidation of a remotely cached entry.
    Process migration to remote machines had little effect on caching.  Caching
    was less successful in heavily shared and modified directories such as
    /tmp, but there weren’t enough references to /tmp overall to affect the
    results significantly.  We estimate that adding name and attribute caching
    to the Sprite operating system could reduce server load by 36% and the
    number of network packets by 30%.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/nameUsenix92.ps.Z"
}

@InProceedings{toatfmaaosj,
  author       = "Matt~W. Mutka and Philip~K. McKinley ",
  title        = "The {OPENSIM} Approach:  Tools for Management and Analysis of Simulation Jobs",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "291--304",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, workstation clusters, scheduling, work sharing, guis,
    simulations",
  abstract     = "This paper presents the design, implementation, and usage of
    OpenSim.  OpenSim provides new tools and integrates existing tools into an
    environment in order to establish a comprehensive facility for performing
    simulation work.  First, OpenSim provides a graphical user interface to
    users for creating input files for simulations and managing output files
    produced from simulations.  Second, tools are provided to help a user
    easily generate plots from sets of output files assocated with a simulation
    project.  Third, OpenSim addresses a common problem for many simulation
    users, namely, lack of computing capacity to serve the jobs.  In order to
    solve this problem, OpenSim integrates Condor, an existing system that
    clusters idle workstations into a processor bank, into its environment so
    that users have access to a large amount of computing capacity without
    interfering with the local usage of workstations by their owners.  Finally,
    since users often plan their schedules according to the deadlines required
    for their jobs, OpenSim enhances Condor so that users can request jobs to
    be scheduled within a deadline.  Therefore, a user can expect that the
    amount of computing capacity required for a simulation project will be
    available before a specified deadline.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mlcidfs,
  author       = "D.~Muntz and Peter Honeyman",
  title        = "Multi-level Caching in Distributed File Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "305--313",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "intermediate caches, distributed file systems, performance,
    host caches, hit rates, trace-driven simulations",
  abstract     = "We are investigating the potential for a hierarchy of
    intermediate file servers to address scaling problems in increasingly large
    distributed file systems.  To this end, we have run trace-driven
    simulations based on data from DEC-SRC and our own data collection to
    determine the potential of caching-only intermediate servers.  The degree
    of sharing among clients is central to the effectiveness of an intermediate
    server.  This turns out to be quite low in the traces available to us.  All
    told, fewer than 10% of block accesses are to files shared by more than one
    file system client.  Our simulations show that even with an infinite cache
    at an intermediate server, cache hit rates are disappointingly low.  For
    client caches as small as 20M, we observe hit rates under 19%.  As client
    cache sizes increase, the hit rate at an intermediate server approaches the
    degree of sharing among all clients.  On the other hand, the intermediate
    server does appear to be effective in boosting the performance and
    scalability of upstream file servers by substantially reducing the request
    rate presented to them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "http://www.citi.umich.edu/techreports/reports/citi-tr-91-3.pdf"
}

@InProceedings{obf,
  author       = "Mitch Bradley",
  title        = "{Open Boot} Firmwear",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "223--235",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "forth, firmwear, boot-time configuration",
  abstract     = "Open Boot is a software architecture for the firmware that
    controls a computer before the operating system has begun execution.  The
    Open Boot firmware design is based on a machine-independent interactive
    programming language (Forth).  Open Boot includes features for
    self-identifying plug-in devices with device-resident boot drivers, support
    for disk, tape, and network booting, hardware configuration reporting, and
    debugging tools for hardware, software, and firmware.  Open Boot is the
    basis for the device identification and booting capabilities of SBus.  An
    IEEE standards effort for boot firmware based on Open Boot is underway.
    The Futurebus+ and VME-D bus standards include support for Open Boot.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{scapmotuk,
  author       = "Michael Litzkow and Marvin Solomon ",
  title        = "Supporting Checkpointing and Process Migration Outside the " # Unix # " Kernel",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "281--290",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, checkpointing, process migration, kernel
    interception, state transfer, coredumps",
  abstract     = "We have implemented both checkpointing and migration of
    processes under UNIX as a part of the Condor package.  Checkpointing,
    remote execution, and process migration are different, but closely related
    ideas; the relationship between these ideas is explored.  A unique feature
    of the Condor implementation of these items is that they are accomplished
    entirely at user level.  Costs and benefits of implementing these features
    without kernel support are presented.  Portability issues, and the
    mechanisms we have devised to deal with these issues, are discussed in
    concrete terms.  The limitations of our implementation, and possible
    avenues to relieve some of these limitations, are presented.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcacidce,
  author       = "Douglas Rosenthal and Wayne Allen and Kenneth Fiduk",
  title        = "Process Control and Communication in Distributed {CAD} Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "271--281",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "ipc, load balancing, process control, message passing,
    distributed computing",
  abstract     = "The MCC Computer-Aided Design (CAD) Framework Process Control
    System (PCS) provides distributed process control and communication
    services in heterogeneous network environments.  The PCS also provides
    network-wide load balancing via an efficient and flexible process placement
    mechanism.  The PCS services enable design tools and CAD framework
    components to leverage the resources of distributed computing networks,
    while supporting various degrees of interaction through distributed,
    real-time communication.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lasodc,
  author       = "Robert~M. English and Alexander~A. Stepanov",
  title        = "Loge:  {A} Self-Organizing Disk Controller",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "237--251",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "While the task of organizing data on the disk has
    traditionally been performed by the file system, the disk controller is in
    many respects better suited to the task.  In this paper, we describe Loge,
    a disk controller that uses internal indirection, accurate physical
    information, and reliable metadata storage to improve I/O performance.  Our
    simulations show that Loge improves overall disk performance, doubles write
    performance, and can, in some cases, improve read performance.  The Loge
    disk controller operates through standard device interfaces, enabling it to
    be used on standard systems without software modification.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{hawsibtifn,
  author       = "Bruce Nelson and Yu-Ping Cheng",
  title        = "How and Why {SCSI} is Better than {IPI} for {NFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "253--270",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "disk controllers, traffic patterns",
  abstract     = "Disk drives are often dismissed as mundane devices, but they
    are actually interesting, complicated, and misunderstood.  In traditional
    Unix servers, disk storage subsystems are usually optimized for
    sequential-transfer performance.  Perhaps counter-intuitively, however, NFS
    file servers exhibit marked random-access disk traffic.  This report
    investigates this apparent contradiction and shows that disk-drive
    concurrency not disk transfer rate—is the important factor in disk storage
    performance for most NFS network servers.  The investigation begins with a
    concrete and detailed comparison of both performance-oriented and
    nonperformance-oriented technical specifications of both SCSI and IPI drive
    and interface types.  It offers a thorough empirical evaluation of SCSI
    disk drive performance, varying parameters such as synchronous or
    asynchronous bus transfers, random and sequential access patterns, and
    multiplicity of drives per SCSI channel.  It discusses (nonempirically)
    similar characteristics for IPI-2 drives.  The report concludes with
    benchmarked comparisons of NFS file servers using SCSI-based disk arrays
    and IPI-2 subsystems.  The results show that NFS heavy-load throughput
    using SCSI disk arrays scales linearly with extra drives, whereas IPI-2
    throughput scales less than proportionally with extra drives.  This SCSI
    scalability advantage, combined with SCSI’s appealing price-performance and
    price-capacity, make SCSI disks a superior choice for NFS servers.  IPI-2
    drives, with their optional high transfer rates, remain an excellent choice
    for compute-oriented servers executing large-file applications where high
    sequential throughput is essential.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{iiiocas,
  author       = "Murthy Devarakonda and Arup Mukherjee",
  title        = "Issues in Implementation of Cache-Affinity Scheduling",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "345--357",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "affinity scheduling, cache management, thread scheduling,
    performance", 
  abstract     = "In a shared memory multiprocessor, a thread may have an
    affinity to a processor because of the data remaining in the processor’s
    cache from a previous dispatch.  We show that two basic problems should be
    addressed in a Unix-like system to exploit cache affinity for improved
    performance: First, the limitation of the Unix dispatcher model (“processor
    seeking a thread”); Second, pseudo-affinity caused by low-cost waiting
    techniques used in a threads package such as C Threads.  We demonstrate
    that the affinity scheduling is most effective when used in a threads
    package that supports multiplexing of user threads on kernel threads.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{at,
  author       = "Jay Littman",
  title        = "Applying Threads",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "209--221",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multithreading, synchronization, deadlock, performance",
  abstract     = "Multithreading components of a software system can increase
    performance, but it can also increase complexity.  At Hewlett-Packard, we
    have developed a workstation based medical product, called the Monitoring
    Full Disclosure Review Station, or M1251A, that uses multithreading to
    achieve performance requirements.  The M1251A continuously acquires
    physiological waveforms and arrhythmia information for presentation to a
    clinician in an intensive care unit.  This paper describes the benefits the
    M1251A gains from multithreading, identifies the problems the development
    team had with multithreading, and explains how those problems were
    resolved.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aspmap,
  author       = "Bernhard Wagner and Bruce~K. Haddon",
  title        = "Application Software:  Product Management and Privileges",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "system administration, portability, standards",
  abstract     = "Application programs for UNIX are increasingly making greater
    demands upon the system structure, are adhering less well to admittedly
    implicit guidelines, or, are being inexactly transliterated from the
    paradigms of other systems.  These influences add to the administrative
    problems and load, and, in some cases, are exacerbating security risks.
    The administrative problems and corresponding solutions are presented here
    in a twofold manner: Firstly, by the description of our use of methods that
    separate administration of the programs and files that make up an
    application suite both from system administration and from eacli other.  We
    argue that thus a sort ol modularity takes place in the software
    administration.  The goal is the lack of need for super user privilege, so
    that this separation improves the overall security of the system.
    Secondly, we list a number of features that we support as being essential,
    a list of requirements to be fulfilled by all application programs written
    for UNIX systems.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcsr,
  author       = "Spencer Rugaber",
  title        = "Program Comprehension",
  booktitle    = "Encyclopedia of Computer Science and Technology",
  year         = 1995,
  editor       = "Allen Kent and James~G. Williams",
  pages        = "341--368",
  publisher    = "Marcel Dekker",
  address      = nyny,
  keywords     = "program comprehension, cognitive models"
}

@InProceedings{rwpfm,
  author       = "Robin Schaufler",
  title        = "Realtime Workstation Performance for {MIDI}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "139--",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time systems, midi, system performance, benchmarking",
  abstract     = "MIDI studio applications require 1 millisecond accuracy in
    timing transmission and receipt of MIDI messages.  Past MIDI
    implementations on UNIX™ have either used the Roland MPU-401 coprocessor to
    do accurate timing, or have not had timing tests published for them.
    Timing MIDI I/O on the host processor allows for more flexible scheduling
    policies than the MPU-401, but many people expressed skepticism that it
    could be done with sufficient accuracy and efficiency because of UNIX™
    virtual memory and pre-emptive scheduling.  This paper describes studies we
    did on providing millisecond accuracy on the host processor of a Silicon
    Graphics Iris Indigo running IRIX, the Silicon Graphics version of UNIX.
    Our measurements show that millisecond accuracy is feasible on IRIX™
    without modifying the kernel.  The paper goes on to describe how the
    studies relate to other time based media.  With a small set of real time
    features, UNIX can really sing.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aafapmt,
  author       = "Sun Wu and Udi Manber",
  title        = "{\tt agrep} --- Fast Approximate Pattern-Matching Tool",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "153--162",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "approximate string matching, boyer-more, knuth-morris-pratt",
  abstract     = "Searching for a pattern in a text file is a very common
    operation in many applications ranging from text editors and databases to
    applications in molecular biology.  In many instances the pattern does not
    appear in the text exaedy.  Errors in the text or in the query can result
    from misspelling or from experimental errors (e.g., when the text is a DNA
    sequence).  The use of such approximate pattern matching has been limited
    until now to specific applications.  Most text editors and searching
    programs do not support searching with errors because of the complexity
    involved in implementing it.  In this paper we describe a new tool, called
    agrep, for approximate pattern matching.  Agrep is based on a new efficient
    and flexible algorithm for approximate string matching.  Agrep is also
    competitive with other tools for exact string matching; it include many
    options that make searching more powerful and convenient.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aewbiwacileas,
  author       = "Bill Cheswick",
  title        = "An Evening with {Berferd} In Which a Cracker is Lured, Endured, and Studied",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "163--173",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "cracking, jails, honeypots, law",
  abstract     = "On 7 January 1991 a cracker, believing he had discovered the
    famous sendmail DEBUG hole in our Internet gateway machine, attempted to
    obtain a copy of our password file.  I sent him one.  For several months we
    led this cracker on a merry chase in order to trace his location and learn
    his techniques.  This paper is a chronicle of the cracker’s 'successes' and
    disappointments, the bait and traps used to lure and detect him, and the
    chroot 'Jail' we built to watch his activities.  We concluded that our
    cracker had a lot of time and persistence, and a good list of security
    holes to use once he obtained a login on a machine.  With these holes he
    could often subvert the uucp and bin accounts in short order, and then
    root.  Our cracker was interested in military targets and new machines to
    help launder his connections.  This is a draft of a paper accepted for the
    January 1992 San Francisco Usenix.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{haph,
  author       = "Peter Honeyman and L.~B. Huston and M.~T. Stolarchuk",
  title        = "Hijacking {AFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "175--181",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "network security, mitm attack, afs, network protocols,
    challenge/response oracle",
  abstract     = "We have identified several techniques that allow uncontrolled
    access to files managed by AFS 3.0.  One method relies on administrative
    (or root) access to a user’s workstation.  Defending against this sort of
    attack is difficult.  Another class of attacks comes from promiscuous
    access to the physical network.  Stronger cryptographic protocols, such as
    those employed by AFS 3.1, obviate this problem.  These exercises help us
    understand vulnerabilities in the distributed systems that we employ (and
    deploy), and offer guidelines for securing them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aibaflsdse,
  author       = "Dale Skeen",
  title        = "An Information Bus Architecture for Large-Scale, Decision-Support Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information bus, publish-subscribe",
  abstract     = "Some of the promising industries for commercializing UNIX are
    those requiring real-time decision support, such as trading rooms, factory
    automation, process control, and network management.  These large-scale,
    real-time environments present challenging technical problems of high data
    volumes, split-second response times, and high availability.  Moreover,
    these environments demand flexible architectures that can support a rapidly
    changing set of application requirements.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

