.so bibtex.header

@string{asplos91 = sigplan # " (" # pot # "Fourth International Conference on " # asplos # ", ASPLOS IV)"}
@string{asplos00 = sigplan # " (" # pot # "Ninth International Conference on " # asplos # ", ASPLOS IX)"}
@string{icfp02 = sigplan # " (" # pot # "Seventh ACM SIGPLAN International Conference on Functional Programming, ICFP '02)" }
@string{osdi96 = osr # " (" # pot # "Second USENIX Symposium on Operating Systems Design and Implementation, OSDI '96)"}
@string{ppeals88 = sigplan # " (" # pot # "ACM\slash SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems, PPEALS '88)"}
@string{sosp81    = osr # " (" # pot # "Eighth" # sosp # ", SOSP '81)"}
@string{usenixw92 = pot # "Winter 1992 USENIX Conference"}
@string{usenixs94 = pot # " USENIX Summer 1994 Technical Conference"}
@string{usenix95  = pot # "1995 USENIX Technical Conference"}
		  
		  
@Book{tcwekm,
  author       = "Ella~K. Maillart",
  title        = "The Cruel Way",
  subtitle     = "Switzerland to Afghanistan in a Ford, 1939",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "travel, middle east",
  location     = "DS 352.M18"
}

@Book{taeojl,
  author       = "Yves Beauchemin",
  title        = "The Accidental Education of Jerome Lupien",
  publisher    = "House of Anansi Press",
  year         = 2018,
  address      = "Canada",
  keywords     = "political machinations, lobbyists, con games",
  location     = "PS 8553.E172 E4813"
}

@Book{himt,
  author       = "Matt Taibbi",
  title        = "Hate Inc.",
  publisher    = "OR Books",
  year         = 2019,
  address      = nyny,
  keywords     = "media, propaganda, chomsky, maddow, journalism, the profit motive",
  location     = "9781949017250"
}

@Book{eolp,
  author       = "Christopher John Hogger",
  title        = "Essentials of Logic Programming",
  publisher    = oup,
  year         = 1990,
  series       = "Graduate Texts in Computer Science",
  address      = nyny,
  keywords     = "logic programming, first-order logic, causal-form logic,
    herbrand domain, sld resolution, definite program semantics, finite
    failure, program verification",
  location     = "QA 76.63 H64"
}

@Book{tmjc,
  author       = "Jorge Comensal",
  title        = "The Mutations",
  publisher    = fsg,
  year         = 2019,
  address      = nyny,
  keywords     = "cancer, survivorship, psychotherapy",
  location     = "PQ 7298.413.O438 M8813 "
}

@Book{ojhhdb,
  author       = "John~H. Halpern and David Blistein",
  title        = "Opium",
  subtitle     = "How an Ancient Flower Shaped and Poisoned Our World",
  publisher    = "Hachette",
  year         = 2019,
  address      = nyny,
  keywords     = "opium, addiction, poppies, drugs, medicine",
  location     = "HV 5816.H35"
}

@Book{tlpmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Laughing Policeman",
  publisher    = "Vintage",
  year         = 1977,
  price        = "$1.65",
  address      = nyny,
  keywords     = "murrdaar, doggedness, lone wolves",
  location     = "PT 9876.29.J63"
}

@Book{topsad,
  author       = "Edward Yourdon",
  title        = "Techniques of Program Structure and Design",
  publisher    = ph,
  year         = 1975,
  address      = ecnj,
  keywords     = "computer programs, top-down design, modular programming,
    structured programming, antidebugging, program testing, debugging,
    programming style",
  location     = "QA 76.6.6.Y68"
}

@Book{natb,
  author       = "Lars Iyer",
  title        = "Nietzsche and the Burbs",
  publisher    = "Melville House",
  year         = 2019,
  address      = "Brooklyn, N.Y.",
  keywords     = "suburban life, wasted youth",
  location     = ""
}

@Book{hwsts,
  author       = "Thomas Hockey",
  title        = "How We See the Sky",
  subtitle     = "A Naked-Eye Tour of Day and Night",
  publisher    = ucp,
  year         = 2011,
  address      = chil,
  keywords     = "the night sky, stars, planets, the moon",
  location     = "QB 44.3 H635"
}

@Book{htrm,
  author       = "Randall Munroe",
  title        = "How To",
  subtitle     = "Absurd Scientific Advice for Common Real-World Problems",
  publisher    = "Riverhead Books",
  year         = 2019,
  address      = nyny,
  keywords     = "jumping, pool parties, digging holes, piano playing,
    emergency landings, crossing rivers, moving, stability, lava moats,
    throwing, football, weather prediction, physics, algebra, modeling.",
  location     = ""
}

@Book{tnbcw,
  author       = "Colson Whitehead",
  title        = "The Nickel Boys",
  publisher    = "Doubleday",
  year         = 2019,
  address      = nyny,
  keywords     = "racism, florida, juvenile detention, the past",
  location     = "PS 3573.H4768 N53"
}

@Book{ystb,
  author       = "Wendy Lesser",
  title        = "You Say to Brick",
  subtitle     = "The Life of Louis Kahn",
  publisher    = fsg,
  year         = 2017,
  address      = nyny,
  keywords     = "louis kahn, american architecture",
  location     = "NA 737.K32 L48"
}

@Book{cnast,
  author       = "Andrew~S. Tanenbaum and David~J. Wetherall",
  title        = "Computer Networks",
  publisher    = ph,
  year         = 2011,
  address      = boma,
  keywords     = "iso osi stack, network security",
  location     = "TK 5105.5.T36"
}

@Book{tjs,
  author       = "John Suchet",
  title        = "Tchaikovsky",
  title        = "The Man Revealed",
  publisher    = "Pegasus Books",
  year         = 2018,
  address      = nyny,
  keywords     = "p.i. tchaikovsky, russian romanticism, music composition",
  location     = "ML 410.C4 S8"
}

@Book{bleh,
  author       = "Elizabeth Hand",
  title        = "Blacklight",
  publisher    = "HarperColins",
  year         = 1999,
  address      = nyny,
  keywords     = "occult, small-town life, into the weird",
  location     = "PS 3558.A4619 B53"
}

@Book{qsr,
  author       = "Salman Rushdie",
  title        = "Quichotte",
  publisher    = "Random House",
  year         = 2019,
  address      = nyny,
  keywords     = "quests, the near future, metafiction",
  location     = "PR 6068,U757 Q53"
}

@Book{gase,
  author       = "Shimon Even",
  title        = "Graph Algorithms",
  publisher    = "Computer Science Press",
  year         = 1979,
  address      = "Potomac, Maryland",
  keywords     = "graph theory, paths, trees, depth-first search, ordered
    trees, maximum flow, planar graphs, graph planarity tests, np completeness",
  location     = "QA 166.E93"
}

@Book{ckmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "Cop Killer",
  publisher    = "Vintage",
  year         = 1975,
  address      = nyny,
  price        = "$1.75",
  keywords     = "murrdaar, coincidences",
  location     = "PT 9876.29.J63"
}

@Book{teor,
  author       = "Stephen~P. Kershaw",
  title        = "The Enemies of Rome",
  subtitle     = "The Barbarian Rebellion Against the Roman Empire",
  publisher    = "Pegasus Books",
  year         = 2019,
  address      = nyny,
  keywords     = "ancient rome, warfare, barbarians",
  location     = ""
}

@Book{ldods,
  author       = "Arthur~D. Friedman",
  title        = "Logical Design of Digital Systems",
  publisher    = "Computer Science Press",
  year         = 1975,
  address      = "Woodland Hills, California",
  keywords     = "number systems, nondecimal arithmetic, codes, combinational
    circuits, sequential circuits, circuit design",
  location     = "TK 7868.S9 F74"
}

@Book{tcdtrhpcp,
  author       = "Peter Buse",
  title        = "The Camera Does the Rest",
  subtitle     = "How Polaroid Changed Photography",
  publisher    = ucp,
  year         = 2016,
  address      = chil,
  keywords     = "photography, polaroid, technology, history, marketing",
  location     = "TR 269.B87"
}

@Book{tlabt,
  author       = "Barbara Taylor",
  title        = "The Last Asylum",
  subtitle     = "A Memoir of Madness in Our Times",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "mental health, psychotherapy",
  location     = "RC 451.4.W6 T38"
}

@Book{awg,
  author       = "William Gibson",
  title        = "Agency",
  publisher    = "Berkley",
  year         = 2020,
  address      = nyny,
  keywords     = "travel broadens the mind, temporal meddling",
  location     = "PS 3557.I2264 A34"
}

@Book{dswj,
  author       = "John~R. Hubbard",
  title        = "Data Structures with {Java}",
  publisher    = "Schaum's Outline Series",
  year         = 2007,
  address      = nyny,
  keywords     = "java, data structures, oo programming, arrays, linked data
    structures, java collection framework, stacks, queues, lists, hash tables,
    recursion, trees, binary trees, search trees, heaps and priority queues,
    sorting, graphs, mathematics",
  location     = "QA 76.73.J38 H82"
}

@Book{gjg,
  author       = "James Gleick",
  title        = "Genius",
  subtitle     = "The Life and Science of Richard Feynman",
  publisher    = "Vintage Books",
  year         = 1993,
  address      = nyny,
  keywords     = "richard feynman, 2nd-half 20th century atomic physics, biography",
  location     = "QC 16.F49 G55"
}

@Book{hotd,
  author       = "Joe Meno",
  title        = "Hairstyles of the Damned",
  publisher    = "Punk Planet Books",
  year         = 2004,
  address      = chil,
  keywords     = "teenage wasteland, lurv",
  location     = "PS 3563.E53 H35"
}

@Book{afane,
  author       = "David~C. Korten",
  title        = "Agenda for a New Economy",
  subtitle     = "From Phantom Wealth to Real Wealth",
  publisher    = "Berrett-Koehler Publishers",
  year         = 2010,
  address      = sfca,
  keywords     = "economics, development, society, finance, ",
  location     = "HC 106.83.K67"
}

@Book{esaaf,
  author       = "\AE leen Frisch",
  title        = "Essential System Administration",
  publisher    = "O'Reilly \& Associates, Inc.",
  year         = 1991,
  address      = seca,
  keywords     = "unix, system administration, startup, shutdown, user
    accounts, security, automation, managing system resources, filesystems,
    disks, backup, restore, terminals, modems, printers, spooling, tcp/ip
    network management, accounting, bourne shell programming",
  location     = "QA 76.76.O63 F782"
}

@Book{ttcjd,
  author       = "Jared Diamond",
  title        = "The Third Chimpanzee",
  subtitle     = "The Evolution and Future of the Human Animal",
  publisher    = "Harper Perennial",
  year         = 1992,
  address      = nyny,
  keywords     = "human development, language, warfare, sex, evolution",
  location     = "GN 281.D53"
}

@Book{aese,
  author       = "William~C. Deitz",
  title        = "At Empire's Edge",
  publisher    = "Ace Books",
  year         = 2009,
  address      = nyny,
  keywords     = "empath cops, empath killer shape shifters, intergalactic
    corruption", 
  location     = "PS 3554.I388 A84"
}

@Book{ltzbee,
  author       = "Bret Easton Ellis",
  title        = "Less Than Zero",
  publisher    = "Vintage",
  year         = 1985,
  address      = nyny,
  keywords     = "sex, drugs, rock and roll, ennui",
  location     = "PS 3555.L5937 L4"
}

@Book{epekb,
  author       = "Kent Beck",
  title        = "Extreme Programming Explained",
  publisher    = aw,
  year         = 2000,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, economics, design",
  location     = "QA 76.76.D47 B434"
}

@Book{ttmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Terrorists",
  publisher    = "Vintage",
  year         = 1976,
  address      = nyny,
  keywords     = "terrorism",
  location     = "PT 9876.29.J63"
}

@Book{ttatb,
  author       = "David~P. Billington",
  title        = "The Tower and the Bridge",
  subtitle     = "The New Art of Structural Engineering",
  publisher    = pup,
  year         = 1983,
  address      = prnj,
  keywords     = "structural engineering, structural art",
  location     = "TA 636.B54"
}

@Book{lgew,
  author       = "Elizabeth Wilson",
  title        = "Love Game",
  subtitle     = "A History of Tennis from Victorian Pastime to Global Phenomenon",
  publisher    = ucp,
  year         = 2014,
  address      = chil,
  keywords     = "tennis, commerce, amateurism, professionalism",
  location     = "GV 992.W558"
}

@Book{waacf,
  author       = "Daryl Gregory",
  title        = "We are All Completely Fine",
  publisher    = "Tachyon",
  year         = 2014,
  address      = sfca,
  keywords     = "monsters, the talking cure",
  location     = "978-1-61696-173-5"
}

@Book{epirj,
  author       = "Ron Jeffries and Ann Anderson and Chet Hendrickson",
  title        = "Extreme Programming Installed",
  publisher    = aw,
  year         = 2001,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, extreme programming, experience, practice",
  location     = "QA 76.76 D47 J44"
}

@Book{epewcw,
  author       = "William~C. Wake",
  title        = "Extreme Programming Explored",
  publisher    = aw,
  year         = 2002,
  series       = "The XP Series",
  address      = boma,
  keywords     = "extreme programming, software development, programming,
    teamwork, process",
  location     = "QA 76.76 D47 W34"
}

@Book{lmbfwy,
  author       = "Richard Ford",
  title        = "Let Me Be Frank With You",
  publisher    = "Ecco",
  year         = 2014,
  address      = nyny,
  keywords     = "new jersey, super storm sandy, real estate, disaster,
    divorce, murderous passions, death-bed confessions",
  location     = "PS 3556.O713 L48"
}

@Book{butl,
  author       = "Tanith Lee",
  title        = "Black Unicorn",
  publisher    = "Atheneum",
  year         = 1991,
  address      = nyny,
  keywords     = "quests, unicorns",
  location     = "PZ 7.L5149 Bl"
}

@Book{okotew,
  author       = "Bertrand Russell",
  title        = "Our Knowledge of the External World",
  publisher    = "The New American Library",
  year         = 1960,
  address      = nyny,
  month        = sep,
  price        = "$0.60",
  keywords     = "philosophy, science, logic, infinity, free will,
    epistemology, continuity, causality",
  location     = "B 1649.R93 O8"
}

@Book{cgfjp,
  author       = "Leen Ammeraal and Kang Zhang",
  title        = "Computer Graphics for Java Programmers",
  publisher    = "Springer Science+Business Media",
  year         = 2017,
  address      = nyny,
  edition      = "third",
  keywords     = "geometry, transformations, 2d algorithms, perspective, 3d
    algorithms, hidden line removal, hidden face removal, color, texture,
    shading, fractals",
  location     = "T 385.A488"
}

@Book{tpwmdg,
  author       = "Michael~D. Gordin",
  title        = "The Pseudoscience Wars",
  subtitle     = "Immanuel Velikovsky and the Birth of the Modern Fringe",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "psuedoscience, astronomy, history, catastrophism, bible
    interpretation",
  location     = "Q 172.5.P77 G674"
}

@Book{mdsacg,
  author       = "Kurt Mehlhorn",
  title        = "Multi-Dimensional Searching and Computational Geometry",
  publisher    = sv,
  year         = 1984,
  volume       = 3,
  series       = "EACTS Monographs on Theoretical Computer Science",
  address      = bege,
  keywords     = "multidimensional data structures, computational geometry,
    algorithms",
  location     = "0-387-13642-8"
}

@Book{msbtm,
  author       = "Thomas McMahon",
  title        = "McKay's Bees",
  publisher    = ucp,
  year         = 1979,
  address      = chil,
  keywords     = "frontier and pioneer life, bee culture, beekeepers, kansas",
  location     = "PS 3563.A3188 M39"
}

@Book{pldab,
  author       = "Arnold Businger",
  title        = "{PORTAL} Language Description",
  publisher    = sv,
  year         = 1988,
  volume       = "198",
  series       = lncs,
  address      = bege,
  edition      = "second",
  keywords     = "real-time systems, language definition",
  location     = "QA 76.73.P66 B87 "
}

@Book{ltdpl,
  author       = "William~W. Wadage and Edward~A. Ashcroft",
  title        = "Lucid, the Dataflow Programming Language",
  publisher    = "Academic Press",
  year         = 1985,
  volume       = "22",
  series       = "APIC Studies in Data Processing",
  address      = loen,
  keywords     = "iswim, luswim, iterations, program transformations",
  location     = "QA 76.7"
}

@Book{laofcm,
  author       = "Francisis~G. McCabe",
  title        = "Logic and Objects",
  publisher    = phi,
  year         = 1992,
  address      = "Hertfordshire, U.K.",
  keywords     = "logic programming, scheduler, semantics, implementation",
  location     = "QA 76.63.M42"
}

@Book{ncehm,
  author       = "Edward~H. Miller",
  title        = "Nut Country",
  subtitle     = "Right-Wing Dallas and the Birth of the Southern Strategy",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "american political history, dallas texas, republicans,
    ultraconservatives, the southern strategy, racism",
  location     = "JK 2359.D35 M55"
}

@Book{gmmem,
  author       = "Michael~E. Martenson",
  title        = "Geometric Modeling",
  publisher    = "John Wiley \& Sons",
  year         = 1985,
  address      = nyny,
  keywords     = "curves, surfaces, solids, analytic properties, relational
    properties, intersections, transformations, solid modeling fundamentals,
    solid model construction, global properties, computer graphics, cad/cam",
  location     = "QA 447.M62"
}

@Book{itor,
  author       = "Frederick~S. Hillier and Gerald~J. Lieberman",
  title        = "Introduction to Operations Research",
  publisher    = "McGraw-Hill",
  year         = 2001,
  address      = boma,
  keywords     = "modeling, linear programming, simplex method, duality theory,
    sensitivity analysis, transportation problems, assignment problems, network
    optimization, project management, pert/cpm, dynamic programming, integer
    programming, nonlinear programming, game theory, decision analysis, markov
    chains, queueing theory, inventory theory, forecasting, markov decision
    processes, simulation",
  location     = "T 57.6 M53"
}

@Book{skr,
  author       = "Karen Russell",
  title        = "Swamplandia!",
  publisher    = "Vintage",
  year         = 2011,
  address      = nyny,
  keywords     = "florida, gator rasslin', ",
  location     = "PS 3618.U755 S39"
}

@Book{canpjdc,
  author       = "Jeffery~D. Clements",
  title        = "Corporations are not People",
  publisher    = "Berrett-Koehler",
  year         = 2014,
  address      = sfca,
  edition      = "second",
  keywords     = "corporate governance, constitutional law, the judicial
    system, the powell doctrine",
  location     = "JK 467.C55"
}

@Article{famw,
  author       = "Charles~P. Thacker and Lawrence~C. Stewart",
  title        = "Firefly:  {A} Multiprocessor Workstation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "164--172",
  month        = oct,
  keywords     = "multiprocessor architecture, vax, caching, trade-offs,
    simulation",
  abstract     = "Firefly is a shared-memory multiprocessor workstation that
    contains from one to seven MicroVAX 78032 processors, each with a floating
    point unit and a sixteen kilobyte cache.  The caches are coherent, so that
    all processors see a consistent view of main memory.  A system may contain
    from four to sixteen megabytes of storage.  Input-output is done via a
    standard DEC QBus.  Input-output devices are an Ethernet controller, fixed
    disks, and a monochrome 1024 x 768 display with keyboard and mouse.
    Optional hardware includes a high resolution color display and a controller
    for high capacity disks.  Figure 1 is a system block diagram.The Firefly
    runs a software system that emulates the Ultrix system call interface.  It
    also supports medium- and coarse-grained multiprocessing through multiple
    threads of control in a single address space.  Communications are
    implemented uniformly through the use of remote procedure calls.This paper
    describes the goals, architecture, implementation and performance analysis
    of the Firefly.  It then presents some measurements of hardware
    performance, and discusses the degree to which SRC has been successful in
    producing software to take advantage of multiprocessing.", 
  location     = "https://doi.org/10.1145/36177.36199", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-23.html"
}

@Article{paritv8p,
  author       = "Douglas~W. Clark",
  title        = "Pipelining and Performance in the {VAX} 8800 Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "173--177",
  month        = oct,
  keywords     = "pipelining, microcode, micropipelining, instruction set
    architecture, performance", 
  abstract     = "The VAX 8800 family (models 8800, 8700, 8550), currently the
    fastest computers in the VAX product line, achieve their speed through a
    combination of fast cycle time and deep pipelining.  Rather than pipeline
    highly variable VAX instructions as such, the 8800 design pipelines uniform
    microinstructions whose addresses are generated by instruction unit
    hardware.  This design approach helps achieve a fast cycle time, which is
    the prime determinan of performance.  Some preliminary measurements of
    cycles per average instruction are reported.", 
  location     = "https://doi.org/10.1145/36177.36200"
}

@Article{avafatsc,
  author       = "Robert~P. Colwell and Robert~P. Nix and John~J. O'Donnell and David~B. Papworth and Paul~K. Rodman",
  title        = "{A} {VLIW} Architecture for a Trace Scheduling Compiler",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "180--192",
  month        = oct,
  keywords     = "vliw architecture, pipelining, compilation, trace scheduling,
    scientific computation, instruction set architecture, branching",
  abstract     = "Very Long Instruction Word (VLIW) architectures were promised
    to deliver far more than the factor of two or three that current
    architectures achieve from overlapped execution.  Using a new type of
    compiler which compacts ordinary sequential code into long instruction
    words, a VLIW machine was expected to provide from ten to thirty times the
    performance of a more conventional machine built of the same implementation
    technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE
    along with its companion Trace Scheduling compacting compiler.  This new
    machine has fulfilled the performance promises that were made. Using many
    fast functional units in parallel, this machine extends some of the basic
    Reduced-Instruction-Set precepts: the architecture is load/store, the
    microarchitecture is exposed to the compiler, there is no microcode, and
    there is almost no hardware devoted to synchronization, arbitration, or
    interlocking of any kind (the compiler has sole responsibility for runtime 
    resource usage).  This paper discusses the design of this machine and
    presents some initial performance results.", 
  location     = "https://doi.org/10.1145/36206.36201"
}

@Article{dttstcplitcm,
  author       = "David~R. Ditzel and Hubert~R. McLellan and Alan~D. Berenbaum",
  title        = "Design Tradeoffs to Support the {C} Programming Language in the {CRISP} Microprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "158--162",
  month        = oct,
  keywords     = "system architecture, language support, context switching,
    code density, stack cache, compiler support, code analysis, path length",
  abstract     = "The CRISP Microprocessor contains a number of new
    architectural features to achieve high performance and support the C
    programming language.  The instruction set was designed to be independent
    of architectural tradeoffs used in any single implementation.  This paper
    describes the particular tradeoffs used in the implementation of a 172,163
    transistor 32-bit single chip microprocessor.  Many tradeoffs were used in
    the design of CRISP, this paper tries to focus on those particular to C.Â ", 
  location     = "https://doi.org/10.1145/36177.36198"
}

@Article{arafsc,
  author       = "Richard~B. Kieburtz",
  title        = "{A} {RISC} Architecture for Symbolic Computation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "146--155",
  month        = oct,
  keywords     = "graph reduction, combinators, g-machines, tagged data types,
    pipelining",
  abstract     = "The G-machine is a language-directed processor architecture
    designed to support graph reduction as a model of computation.  It can
    carry out lazy evaluation of functional language programs and can evaluate
    programs in which logical variables are used.  To support these language
    features, the abstract machine requires tagged memory and executes some
    rather complex instructions, such as to evaluate a function
    application.This paper explores an implementation of the G-machine as a
    high performance RISC architecture.  Complex instructions can be
    represented by RISC code without experiencing a large expansion of code
    volume.  The instruction pipeline is discussed in some detail.  The
    processor is intended to be integrated into a standard, 32-bit memory
    architecture.  Tagged memory is supported by aggregating data with tags in
    a cache.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{rvcfpacs,
  author       = "Gaetano Borriello and Andrew~R. Cherenson and Peter Bernard Danzig and Michael Newell Nelson",
  title        = "{RISCs} vs. {CISCs} for {Prolog}: {A} Case Study",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "136--145",
  month        = oct,
  keywords     = "prolog, logic programming, abstract machines, compilation,
    spur, tagged data types, unification, backtracking, coprocessors",
  abstract     = "This paper compares the performance of executing compiled
    Prolog code on two different architectures under development at
    U. C. Berkeley.  The first is the PLM, a special-purpose CISC architecture
    intended as a coprocessor for a host machine.  The second is SPUR, a
    general-purpose RISC architecture that supports tagged data.  Fourteen
    standard benchmark programs were run on both the PLM and SPUR simulators.
    The compiled code for SPUR was obtained by simple macro-expansion of PLM
    code generated by the PLM Prolog compiler.  The two implementations are
    compared with regard to static and dynamic program size, execution speed,
    and memory system performance.  On average, the macrocoded SPUR
    implementation has a static code size 14 times larger than the PLM,
    executes 16 times more instructions, yet requires only 2.3 times the number
    of machine cycles (or has the performance of 0.43 PLMs).  When memory
    system performance is taken into account, SPUR is equivalent to 0.29 PLMs.
    Optimizations of the macro-expanded code and minor architectural changes to
    SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks.
    Thus a tagged RISC architecture can execute Prolog at least half as fast as
    a special-purpose CISC architecture for Prolog.", 
  location     = "https://doi.org/10.1145/36177.36196"
}

@Article{tmeuailatmd,
  author       = "David~W. Wall and Michael~L. Powell",
  title        = "The {Mahler} Experience:  Using an Intermediate Language as the Machine Description",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "100--104",
  month        = oct,
  keywords     = "intermediate language, machine independence, compilers,
    global optimizations, link-time optimizations, instruction scheduling,
    pipelining, abstract machines",
  abstract     = "Division of a compiler into a front end and a back end that 
    communicate via an intermediate language is a well-known technique.  We go
    farther and use the intermediate language as the official description of a
    family of machines with simple instruction sets and addressing
    capabilities, hiding some of the inconvenient details of the real machine
    from the users and the front end compilers.To do this credibly, we have had
    to hide not only the existence of the details but also the performance
    consequences of hiding them.  The back end that compiles and links the
    intermediate language tries to produce code that does not suffer a
    performance penalty because of the details that were hidden from the front
    end compiler.  To accomplish this, we have used a number of link-time
    optimizations, including instruction scheduling and interprocedural
    register allocation, to hide the existence of such idiosyncracies as
    delayed branches and non-infinite register sets.  For the most part we have
    been successful.", 
  location     = "https://doi.org/10.1145/36177.36190",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-1.pdf"
}

@Article{asosctfps,
  author       = "Shlomo Weiss and James~E. Smith",
  title        = "{A} Study of Scalar Compilation Techniques for Pipelined Supercomputers",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "cray computers, loop unrolling, software pipelining,
    performance, scientific computing, optimization, machine architectures,
    register files, vectorization",
  abstract     = "This paper studies two compilation techniques for enhancing
    scalar performance in high-speed scientific processors: software pipelining
    and loop unrolling.  We study the impact of the architecture (size of the
    register file) and of the hardware (size of instruction buffer) on the
    efficiency of loop unrolling.  We also develop a methodology for
    classifying software pipelining techniques.  For loop unrolling, a
    straightforward scheduling algorithm is shown to produce near-optimal
    results when not inhibited by recurrences or memory hazards.  Our study
    indicates that the performance produced with a modified CRAY-1S scalar
    architecture and a code scheduler utilizing loop unrolling is comparable to
    the performance achieved by the CRAY-1S with a vector unit and the CFT
    vectorizing compiler.", 
  location     = "https://doi.org/10.1145/79505.79508"
}

@Article{cs8tar,
  author       = "William~R. Bush and A.~Dain Samples and David Ungar and Paul~N. Hilfinger",
  title        = "Compiling {Smalltalk-80} to a {RISC}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "risc architecture, bytecode, compilation, register windows,
    soar, dynamic languages, caching, performance",
  abstract     = "The Smalltalk On A RISC project at U.  C.  Berkeley proves
    that a high-level object-oriented language can attain high performance on a
    modified reduced instruction set architecture.  The single most important
    optimization is the removal of a layer of interpretation, compiling the
    bytecoded virtual machine instructions into low-level, register-based,
    hardware instructions.  This paper describes the compiler and how it was
    affected by SOAR architectural features.  The compiler generates code of
    reasonable density and speed.  Because of Smalltalk-80's semantics,
    relatively few optimizations are possible, but hardware and software
    mechanisms at runtime offset these limitations.  Register allocation for an
    architecture with register windows comprises the major task of the
    compiler.  Performance analysis suggests that SOAR is not simple enough;
    several hardware features could be efficiently replaced by instruction
    sequences constructed by the compiler.", 
  location     = "https://doi.org/10.1145/36177.36192"
}

@Article{hmamae,
  author       = "F.~Chow and S.~Correll and M.~Himelstein and E.~Killian and L.~Weber",
  title        = "How Many Addressing Modes are Enough?",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "117--121",
  month        = oct,
  keywords     = "risc, addressing modes, addressing architecture, offset
    indexing, optimizations, performance, simplicity",
  abstract     = "Programs naturally require a variety of memory-addressing
    modes.  It isn't necessary to provide them in hardware, however, if a
    compiler can synthesize them from a few primitive modes.  This not only
    simplifies the hardware, but also permits the compiler to use its
    understanding of the program to economize on the modes which it uses.  We
    present some compilation techniques that allow the compiler to deal
    effectively with a single addressing mode in a target RISC processor.  We
    also give measurements to show the benefits of such techniques, and to
    support our assertion that a single addressing mode is adequate for a
    general purpose processor, provided that mode incorporates both a pointer
    and an offset.", 
  location     = "https://doi.org/10.1145/36177.36193"
}

@Article{salatsp,
  author       = "Henry Massalin",
  title        = "Superoptimizer --- {A} Look at the Smallest Program",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "122--126",
  month        = oct,
  keywords     = "optimization, exhaustive search, probabilistic testing,
    assembly code",
  abstract     = "Given an instruction set, the superoptimizer finds the
    shortest program to compute a function.  Startling programs have been
    generated, many of them engaging in convoluted bit-fiddling bearing little
    resemblance to the source programs which defined the functions.  The key
    idea in the superoptimizer is a probabilistic test that makes exhaustive
    searches practical for programs of useful size.  The search space is
    defined by the processor's instruction set, which may include the whole
    set, but it is typically restricted to a subset.  By constraining the
    instructions and observing the effect on the output program, one can gain
    insight into the design of instruction sets.  In addition, superoptimized
    programs may be used by peephole optimizers to improve the quality of
    generated code, or by assembly language programmers to improve manually
    written code.", 
  location     = "https://doi.org/10.1145/36177.36194"
}

@Article{paaeotpm,
  author       = "Kazuo Taki and Katsuto Nakajima and Hiroshi Nakashima and Morihiro Ikeda",
  title        = "Performance and Architectural Evaluation of the {PSI Machine}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "128--135",
  month        = oct,
  keywords     = "kl0, prolog, logic programming, caching, branching, psi machine",
  abstract     = "We evaluated a Prolog machine PSI (Personal Sequential
    Inference machine) for the purpose of improving and redesigning it.  In
    this evaluation, we measured the execution speed and the dynamic
    characteristics of cache memory, register file, and branching hardware
    introduced for high-speed execution of Prolog programs.Execution speed of
    the PSI firmware interpreter was found to be comparable to that of the
    DEC-10 Prolog compiled code on the DEC-2060.  It was also found that PSI
    was faster than DEC for executing programs containing much unification and
    backtracking that require runtime processing.With the cache memory, the hit
    ratio for application programs was found higher than 96%; this demonstrates
    that the Prolog execution has much memory access locality.  The memory
    access frequency and the appearance ratio between Read and Write command
    were also investigated.Concerning the register file, use rate of each
    dedicated access mode was measured and effect of each mode was discussed.
    In the branching function we confirmed a high appearance rate of
    conditional branches and multi-way branches based on tag values.", 
  location     = "https://doi.org/10.1145/36177.36195"
}

@Article{cbatccaisohci,
  author       = "Sam Wineburg and Susan Mosborg and Dan Porat and Ariel Duncan",
  title        = "Common Belief and the Cultural Curriculum:  An Intergenerational Study of Historical Consciousness",
  journal      = "American Education Research Journal",
  year         = 2007,
  volume       = 44,
  number       = 1,
  pages        = "40--76",
  month        = mar,
  keywords     = "history instruction, collective memory, vietnam war, united
    states history, curricula, veterans, soldiers, student protests, political
    protests, textbooks, forest gump",
  abstract     = "How is historical knowledge transmitted across generations?
    What is the role of schooling in that transmission? The authors address
    these questions by reporting on a thirty-month longitudinal study into how
    home, school, and larger society served as contexts for the development of
    historical consciousness among adolescents.  Fifteen families drawn from
    three different school communities participated.  By adopting an
    intergenerational approach, the authors sought to understand how the
    defining moments of one generation-its lived history'-becomes the available
    history to the next.  In this article, the authors focus on what parents
    and children shared about one of the most formative historical events in
    parents' lives: the Vietnam War.  Drawing on notions of collective memory,
    as articulated by the French sociologist Maurice Halbwachs, the authors
    sought to understand which stories, archived in historical memory and
    available to the disciplinary community, are remembered and used by those
    beyond its borders.  In contrast, which stories are no longer widely
    shared, eclipsed by time's passage and unable to cross the bridge
    separating generation from generation? The authors conclude by discussing
    the forces that act to historicize today's youth and suggest how educators
    might marshal these forces-rather than spurning or simply ignoring them-to
    advance young people's historical understanding.",
  location     = "https://www.jstor.org/stable/30069471?sa=X&ved=2ahUKEwiOxfHPx__mAhVph-AKHfibCVcQFjAAegQICBAB"
}

@Article{imadothpa,
  author       = "Daniel~J. Magenheimer and Liz Peters and Karl Pettis and Dan Zuras",
  title        = "Integer Multiplication and Division on the {HP Precision Architecture}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "90--99",
  month        = oct,
  keywords     = "multiplication, division, approximations",
  abstract     = "In recent years, many architectural design efforts have
    focused on maximizing performance for frequently executed, simple
    instructions.  Although these efforts have resulted in machines with better
    average price/performance ratios, certain complex instructions and, thus,
    certain classes of programs which heavily depend on these instructions may
    suffer by comparison.  Integer multiplication and division are one such set
    of complex instructions.  This paper describes how a small set of primitive
    instructions combined with careful frequency analysis and clever
    programming allows the Hewlett-Packard Precision Architecture integer
    multiplication and division implementation to provide adequate performance
    at little or no hardware cost.", 
  location     = "https://doi.org/10.1145/36206.36189"
}

@Article{aecfipooai4,
  author       = "Christos John Georgiou and Stewart~L. Palmer and P.~L. Rosenfeld",
  title        = "An Experimental Coprocessor for Implementing Persistent Objects on an {IBM} 4381",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "84--87",
  month        = oct,
  keywords     = "coprocessors, persistent objects, system architecture",
  abstract     = "In this paper we describe an experimental coprocessor for an
    IBM 4381 that is designed to facilitate the exploration of persistent objects.",
  location     = "https://doi.org/10.1145/36177.36188"
}

@Article{chsfsdap,
  author       = "Thomas~A. Cargill and Burt~N. Locanthi",
  title        = "Cheap Hardware Support for Software Debugging and Profiling",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "82--83",
  month        = oct,
  keywords     = "debugging, watch points, reverse execution",
  abstract     = "We wish to determine the effectiveness of some simple
    hardware for debugging and profiling compiled programs on a conventional
    processor. The hardware cost is small -- a counter decremented on each
    instruction that raises an exception when its value becomes zero. With the
    counter a debugger can provide data watchpoints and reverse execution: a
    profiler can measure the total instruction cost of a code segment and
    sample the program counter accurately. Such a counter has been included on
    a single-board MC68020 workstation, for which system software is currently
    being written. We will report our progress at the symposium.", 
  location     = "https://doi.org/10.1145/36177.36187"
}

@Article{cotplictp,
  author       = "Pei~Jyun Leu and Bharat Bhargava",
  title        = "Clarification of Two-Phase Locking in Concurrent Transaction Processing",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "122--125",
  month        = jan,
  keywords     = "two-phase locking, atomic operations, transactions, read
    locks, relaxation, serializability, write locks, logs",
  abstract     = "The authors propose a formal definition of the two-phase
    locking class derived from the semantic description of the two-phase
    locking protocol, and prove that this definition is equivalent to that
    given by C.H.  Papadimitriou (1979).  They present: (1) a precise
    definition of the two phase locking; (2) a clarification of the occurrence
    and the order ofall events such as lock points, unlock points, read
    operations, and write operations of conflicting transactions; and (3) by
    relaxing some conditions in the given definition, the derivation of a new
    class called restricted-non-two-phase locking (RN2PL), which is a superset
    of the class two-phase locking (2PL) but a subset of the class
    D-serializable (DSR) given by Papadimitriou.",
  location     = "https://doi.org/10.1109/32.4629"
}

@Article{cfmvac,
  author       = "James~R. Goodman",
  title        = "Coherency for Multiprocessor Virtual Address Caches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "72--80",
  month        = oct,
  keywords     = "caches, coherency, virtual storage, virtual addressing, tlbs,
    snooping",
  abstract     = "A multiprocessor cache memory system is described that
    supplies data to the processor based on virtual addresses, but maintains
    consistency in the main memory, both across caches and across virtual
    address spaces.  Pages in the same or different address spaces may be
    mapped to share a single physical page.  The same hardware is used for
    maintaining consistency both among caches and among virtual addresses.
    Three different notions of a cache block are defined: (1) the unit for
    transferring data to/from main storage, (2) the unit over which tag
    information is maintained, and (3) the unit over which consistency is
    maintained.  The relation among these block sizes is explored, and it is
    shown that they can be optimized independently.  It is shown that the use
    of large address blocks results in low overhead for the virtual address
    cache.", 
  location     = "https://doi.org/10.1145/36177.36186"
}

@Article{tdprra,
  author       = "Russell~R. Atkinson and Edward~M. McCreight",
  title        = "The {Dragon} Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "65--69",
  month        = oct,
  keywords     = "processor design, instruction sets, procedure calls, dorado,
    instruction density, chip packaging",
  abstract     = "The Xerox PARC Dragon is a VLSI research computer that uses
    several techniques to achieve dense code and fast procedure calls in a
    system that can support multiple processors on a central high bandwidth
    memory bus.",
  location     = "https://doi.org/10.1145/36205.36185"
}

@Article{teoiscopsamp,
  author       = "Jack~W. Davidson and Richard~A. Vaughan",
  title        = "The Effect of Instruction Set Complexity on Program Size and Memory Performance",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "60--64",
  month        = oct,
  keywords     = "instruction set design, risc, cisc, memory pressure, code
    size, portable compilation",
  abstract     = "One potential disadvantage a machines with a simple
    instruction set is object-program size may be substantially larger than
    those for a machine with a complex instruction set.  Groups of simple
    instructions are required to implement the same functions performed by a
    single instruction from a complex instruction set.  In addition, the
    tendency of simple instructions to be fixed length with a few instruction
    formats also increases object-program size.  Larger object-program size
    could adversely affect memory performance and bus traffic.  This paper
    reports the results of experiments to isolate and determine the effect of
    instruction set complexity on cache memory performance and bus traffic.
    Three high-level language compilers were constructed for machines with
    instruction sets of varying complexity.  Using a set of benchmark programs,
    we evaluated the effect of instruction set complexity had on program size.
    Five of the programs were used to perform a set of trace-driven simulations
    to study each machine's cache and bus performance.  While we found that the
    miss ratio is affected by object program size, it appears that this can be
    corrected by increasing the cache size.  Our measurements of bus traffic,
    however, show that even with large caches, machines with simple instruction
    sets can expect substantially more main memory reads than machines with
    complex instruction sets.",  
  location     = "https://doi.org/10.1145/36205.36184"
}

@Article{aafdfdpd,
  author       = "Mike Adler",
  title        = "An Algebra for Data Flow Diagram Process Decomposition",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "169--183",
  month        = feb,
  keywords     = "algebra, automatic process decomposition, data flow diagram,
    dfd, directed acyclic graph, graph-based grammar, software engineering,
    structured analysis",
  abstract     = "Data flow diagram process decomposition, as applied in the
    analysis phase of software engineering, is a top-down method that takes a
    process, and its input and output data flows, and logically implements the
    process as a network of smaller processes.  The decomposition is generally
    performed in an ad hoc manner by an analyst applying heuristics, expertise,
    and knowledge to the problem.  An algebra that formalizes process
    decomposition is presented using the De Marco representation scheme.  In
    this algebra, the analyst relates the disjoint input and output sets of a
    single process by specifying the elements of an input/output connectivity
    matrix.  A directed acyclic graph is constructed from the matrix and is the
    decomposition of the process.  The graph basis, grammar matrix, and graph
    interpretations, and the operators of the algebra are discussed.  A
    decomposition procedure for applying the algebra, prototype, and production
    tools and outlook are also discussed.", 
  location     = "https://doi.org/10.1109/32.4636"
}

@Article{tatcilhasa,
  author       = "Peter Steenkiste and John Hennessy",
  title        = "Tags and Type Checking in {LISP}:  Hardware and Software Approaches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "50--59",
  month        = oct,
  keywords     = "lisp, type checking, generic operations, tagged
    architectures, performance",
  abstract     = "One  major factor distinguishing LISP from other languages
    (e.g., Pascal, C, Fortran) is the need for run-time type checking.  Run-time
    type checking is implemented by adding to each data     object a tag that
    encodes type information.  Tags must be compared for type compatibility,
    removed when using the data, and inserted when new data items are created.
    This tag manipulation, together with other work related to dynamic type
    checking and generic operations, constitutes a significant component of the
    execution time of LISP programs.  This has led both to the development of
    LISP machines that support tag checking in hardware and to the avoidance of
    type checking by users running on stock hardware.  To understand the role
    and necessity of special-purpose hardware for tag handling, we first
    measure the cost of type checking operations for a group of LISP programs.
    We then examine hardware and software implementations of tag operations and
    estimate the cost of tag handling with the different tag implementation
    schemes.  The data shows that minimal levels of support provide most of the
    benefits, and that tag operations can be relatively inexpensive, even when
    no special hardware support is present.",  
  location     = "https://doi.org/10.1145/36206.36183"
}

@Article{aaftdeotfpl,
  author       = "John~R. Hayes and Martin~E. Fraeman and Robert~L. Williams and Thomas Zaremba",
  title        = "An Architecture for the Direct Execution of the {Forth} Programming Language",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "42--49",
  month        = oct,
  keywords     = "zero-address languages, stack management, embedded systems,
    instruction set architecture, data paths",
  abstract     = "We have developed a simple direct execution architecture for
    a 32-bit Forth microprocessor.  The processor can directly access a linear
    address space of over 4 gigawords.  Two instruction types are defined; a
    subroutine call, and a user defined microcode instruction.  On-chip stack
    caches allow most Forth primitives to execute in a single cycle.", 
  location     = "https://doi.org/10.1145/36206.36182"
}

@Article{asfmppohs,
  author       = "Roberto Bisiani and Alessandro Forin",
  title        = "Architectural Support for Multilanguage Parallel Programming on Heterogeneous Systems",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "21--30",
  month        = oct,
  keywords     = "common runtimes, shared storage, remote operations, mach",
  abstract     = "We have designed and implemented a software facility, called
    Agora, that supports the development of parallel applications written in
    multiple languages.  At the core of Agora there is a mechanism that allows
    concurrent computations to share data structures independently of the
    computer architecture they are executed on.  Concurrent computations
    exchange control information by using a pattern-directed technique.  This
    paper describes the Agora shared memory and its software implementation on
    both tightly and loosely-coupled architectures.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{mivmmfpuama,
  author       = "Richard Rashid and Avadis Tevanian and Michael Young and David Golub and Robert Baron and David Black and William Bolosky and Jonathan Chew",
  title        = "Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "31--39",
  month        = oct,
  keywords     = "mach, virtual storage, machine independence, portability,
    shared storage, address mapping, paging",
  abstract     = "This paper describes the design and implementation of virtual
    memory management within the CMU Mach Operating System and the experiences
    gained by the Mach kernel group in porting that system to a variety of
    architectures.  As of this writing, Mach runs on more than half a dozen
    uniprocessors and multiprocessors including the VAX family of uniprocessors
    and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the
    Sequent Balance 21000 and several experimental computers.  Although these
    systems vary considerably in the kind of hardware support for memory
    management they provide, the machine-dependent portion of Mach virtual
    memory consists of a single code module and its related header file.  This
    separation of software memory management from hardware support has been
    accomplished without sacrificing system performance.  In addition to
    improving portability, it makes possible a relatively unbiased examination
    of the pros and cons of various hardware memory management schemes,
    especially as they apply to the support of multiprocessors.", 
  location     = "https://doi.org/10.1145/36177.36181"
}

@Article{vafam,
  author       = "Bob Beck and Bob Kasten and Shreekant Thakkar",
  title        = "{VLSI} Assist for a Multiprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "10--20",
  month        = oct,
  keywords     = "caching, co-processors, ipc, synchronization, buses,
    interrupt handling, system configuration",
  abstract     = "Multiprocessors have long been of interest to computer
    community.  They provide the potential for accelerating applications through
    parallelism and increased throughput for large multi-user system.  Three
    factors have limited the commercial success of multiprocessor systems;
    entry cost, range of performance, and ease of application.  Advances in
    large scale integration (VLSI) and in computer aided design (CAD) have
    removed these limitations, making possible a new class of multiprocessor
    systems based on VLSI components.  A set of requirements for building an
    efficient shared multiprocessor system are discussed, including: low-level
    mutual exclusion, interrupt distribution, inter-processor signaling,
    process dispatching, caching, and system configuration.  A system that
    meets these requirements is described and evaluated.", 
  location     = "https://doi.org/10.1145/36177.36179"
}

@Article{clir,
  author       = "Matthew Flatt",
  title        = "Creating Languages in {Racket}",
  journal      = "ACM Queue",
  year         = 2011,
  volume       = 9,
  number       = 11,
  month        = nov,
  keywords     = "racket, language design, macros, dsl, modules",
  abstract     = "Choosing the right tool for a simple job is easy: a
    screwdriver is usually the best option when you need to change the battery
    in a toy, and grep is the obvious choice to check for a word in a text
    document.  For more complex tasks, the choice of tool is rarely so
    straightforwardâall the more so for a programming task, where programmers
    have an unparalleled ability to construct their own tools.  Programmers
    frequently solve programming problems by creating new tool programs, such
    as scripts that generate source code from tables of data.", 
  location     = "https://doi.org/10.1145/2063166.2068896", 
  location     = "https://queue.acm.org/detail.cfm?id=2068896"
}

@Article{hafplaplfha,
  author       = "Niklaus Wirth",
  title        = "Hardware Architecture for Programming Languages and Programming Languages for Hardware Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "2--8",
  month        = oct,
  keywords     = "abstractions, hardware design, complexity, mathematical
    formalisms, system state, correctness reasoning, programming languages",
  abstract     = "Programming Languages and Operating Systems introduce
    abstractions which allow the programmer to ignore details of an
    implementation.  Support of an abstraction must not only concentrate on
    promoting the efficiency of an implementation, but also on providing the
    necessary guards against violations of the abstractions.  In the frantic
    drive for efficiency the second goal has been neglected.  There are
    indications that recent designs which are claimed to be both simple and
    powerful, achieve efficiency by shifting the complex issues of code
    generation and of appropriate guards onto compilers.Complexity has become
    the common hallmark of software as well as hardware designs.  It cannot be
    mastered by the common practices of testing and simulation.  Hardware
    design may profit from developments in programming methodology by adopting
    proof techniques similar to those used in programming.", 
  location     = "https://doi.org/10.1145/36177.36178"
}

@Article{atosigpdcs,
  author       = "Thomas~L. Casavant and Jon~G. Kuhl",
  title        = "{A} Taxonomy of Scheduling in General-Purpose Distributed Computing Systems",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "141--154",
  month        = feb,
  keywords     = "distributed operating systems, distributed resource
    management, general-purpose distributed computing systems, scheduling, task
    allocation, taxonomy",
  abstract     = "One measure of the usefulness of a general-purpose
    distributed computing system is the system's ability to provide a level of
    performance commensurate to the degree of multiplicity of resources present
    in the system.  A taxonomy of approaches to the resource management problem
    is presented in an attempt to provide a common terminology and
    classification mechanism necessary in addressing this problem.  The
    taxonomy, while presented and discussed in terms of distributed scheduling,
    is also applicable to most types of resource management.", 
  location     = "https://doi.org/10.1109/32.4634"
}

@Article{idfaa,
  author       = "Barbara~G. Ryder and Marvin~C. Paull",
  title        = "Incremental Data-Flow Algorithm Algorithms",
  journal      = toplas,
  year         = 1988,
  volume       = 10,
  number       = 1,
  pages        = "1--50",
  month        = jan,
  keywords     = "incremental algorithms, data-flow equations, interval
    analysis",
  abstract     = "An incremental update algorithm modifies the solution of a
    problem that has been changed, rather than re-solving the entire problem.
    ACINCF and ACINCB are incremental update algorithms for forward and
    backward data-flow analysis, respectively, based on our equations model of
    Allen-Cocke interval analysis.  In addition, we have studied their
    performance on a ânontoyâ structured programming language L.  Given a set
    of localized program changes in a program written in L, we identify a
    priori the nodes in its flow graph whose corresponding data-flow equations
    may be affected by the changes.  We characterize these possibly affected
    nodes by their corresponding program structures and their relation to the
    original change sites, and do so without actually performing the
    incremental updates.  Our results can be refined to characterize the
    reduced equations possibly affected if structured loop exit mechanisms are
    used, either singly or together, thereby relating richness of programming
    language usage to the ease of incremental updating.", 
  location     = "https://doi.org/10.1145/42192.42193"
}

@Article{mmiaes,
  author       = "Christopher Rosebrugh and Eng-Kee Kwang",
  title        = "Multiple Microcontrollers in an Embedded System",
  journal      = ddj,
  year         = 1992,
  volume       = 17,
  number       = 1,
  pages        = "48--57",
  month        = jan,
  keywords     = "hardware design, ",
  abstract     = "A case study in system architecture and embedded hardware design"
}

@Article{j14pjsacp1,
  author       = "Wm. Paul Rogers",
  title        = "{J2SE} 1.4 Premieres {Java}'s Assertion Capabilities, Part 1",
  journal      = "Java World",
  year         = 2001,
  month        = November,
  keywords     = "java, assertions",
  location     = "https://www.javaworld.com/article/2075803/j2se-1-4-premieres-java-s-assertion-capabilities--part-1.html"
}

@Article{mmocoaiafoed,
  author       = "Ketabchi, Mohammad~A. and Berzins, Valdis",
  title        = "Mathematical Model of Composite Objects and Its Application for Organizing Engineering Databases",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "71--84",
  month        = jan,
  keywords     = "database partitioning, composite objects, engineering
    databases, clustering concept, component aggregation, assemblies,
    equivalent objects, equivalence classes, Boolean algebra, minterms, stored
    views, relational database, design data, frequent access patterns", 
  abstract     = "The authors introduce a clustering concept called component
    aggregation which considers assemblies having the same types of parts as
    equivalent objects.  The notion of equivalent objects is used to develop a
    mathematical model of composite objects.  It is shown that the set of
    equivalence classes of objects form a Boolean algebra whose minterms
    represent the objects that are not considered composite at the current
    viewing level.  The algebraic structure of composite objects serves as a
    basis for developing a technique for organizing composite objects and
    supporting materialization of explosion views.  The technique provides a
    clustering mechanism which partitions the database into meaningful and
    application-oriented clusters, and allows any desired explosion view to be
    materialized using a minimal set of stored views.  A simplified relational
    database for design data and a set of frequent access patterns in design
    applications are outlined and used to demonstrate the benefits of database
    organizations based on the mathematical model of composite objects.", 
  location     = "https://doi.org/10.1109/32.4624"
}

@Article{elsfpsi,
  author       = "Deepinder~P. Sidhu and Carole~S. Crall",
  title        = "Executable Logic Specifications for Protocol Service Interfaces",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "98--121",
  month        = jan,
  keywords     = "automated development tools, formal description technique,
    formal modeling, protocol specification, protocol verification, state
    transitions, prolog, service specification, iso osi model, tcp, tp2, tp4",
  abstract     = "A general, formal modeling technique for protocol service
    interfaces is discussed.  An executable description of the model using a
    logic-programming-based language, Prolog, is presented.  The specification
    of protocol layers consists of two parts, the specification of the protocol
    interfaces and the specification of entities within the protocol layer.
    The specification of protocol interfaces forms the standard against which
    protocols are verified.  When a protocol has been implemented, the
    correctness of its implementation can be tested using the sequences of
    events generated at the service interface.  If the behavior of the protocol
    implementation is consistent with the behavior at the service interface,
    the implementation conforms to its standard.  To illustrate how it works,
    the model is applied to the service interfaces of protocol standards
    developed for the transport layer of the ISO/OSI architecture.  The results
    indicate that Prolog is a useful formal language for specifying
    protocol interfaces.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4626"
}

@Article{fabfpe,
  author       = "Nazim~H. Madhavji",
  title        = "Fragtypes:  {A} Basis for Programming Environments",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "85--97",
  month        = jan,
  keywords     = "modula-2, mupe-2, program fragments, programming
    environments, programming in the all, program composition, structured manipulation",
  abstract     = "The author introduces a novel basis for programming
    environments that encourages development of software in fragments of
    various types, called fragtypes.  Fragtypes range from a simple expression
    type to a complete subsystem type.  As a result, they are suited to the
    development of software in an enlarged scope that includes both programming
    in the small and programming in the large.  The author shows how proposed
    operations on fragtypes can achieve unusual effects on the software
    development process.  Fragtypes and their associated construction rules
    form the basis of the programming environment MUPE-2, which is currently
    under development at McGill University.  The target and the implementation
    language of this environment is the programming language Modula-2.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4625"
}

@Article{aaomasisuotsb,
  author       = "Robert~F. Cmelik and Shing~I. Kong and David~R. Ditzel and Edmund~J. Kelly",
  title        = "An Analysis of {MIPS} and {SPARC} Instruction Set Utiliation on the {SPEC} Benchmarks",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "290--302",
  month        = apr,
  keywords     = "instruction sets, sparc, mips, risc, autoincrement,
    compilers, delay slots",
  abstract     = "The dynamic instruction counts of MIPS and SPARC are compared
    using the SPEC benchmarks.  MIPS typically executes more user-level
    instructions than SPARC.  This difference can be accounted for by
    architectural differences, compiler differences, and library differences.
    The most significant differences are that SPARCï¿½S double-precision floating
    point load/store is an architectural advantage in the SPEC floating point
    benchmarks while MIPSï¿½s compare-and-branch instruction is an architectural
    advantage in the SPEC integer benchmarks.  After the differences in the two
    architectures are isolated, it appears that although MIPS and SPARC each
    have strengths and weaknesses in their compilers and library routines, the
    combined effect of compilers and library routines does not give either MIPS
    or SPARC a clear advantage in these areas.", 
  location     = "https://doi.org/10.1145/106974.107001"
}

@Article{pcoafotirs6,
  author       = "C.~Brian Hall and Kevin O'Brien",
  title        = "Performance Characteristics of Architectural Features of the {IBM RISC System/6000}",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "303--308",
  month        = apr,
  keywords     = "instruction sets, count registers, branching, performance,
    condition codes",
  abstract     = "The IBM RISC System/6000 has a number of architectural
    features that, are not usually found on RISC machines.  Among these are
    pre-increment and decrement forms of memory referencing instructions, a
    special purpose count register that can be used as a loop counter, and
    eight independent sets of condition code bits.  This paper examines the
    performance gained on a number of industry sta.udard benchmarks through the
    use of each of these features.", 
  location     = "https://doi.org/10.1145/106974.107002"
}

@Article{pfacaraacws,
  author       = "Dileep Bhandarkar and Douglas~W. Clark",
  title        = "Performance from Architecture:  Comparing a {RISC} and a {CISC} with Similar Hardware Organization",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "310--319",
  month        = apr,
  keywords     = "performance, mips, vax, pipelining, benchmarking, cache
    behavior, system architecture",
  abstract     = "Performance comparisons across different computer
    architectures cannot usually separate the architectural contribution from
    various implementation and technology contributions to performance.  This
    paper compares an example implementation from the RISC and CISC
    architectural schools (a MIPS M/2000 and a Digital VAX 8700) on nine of the
    ten SPEC benchmarks.  The organizational similarity of these machines
    provides an opportunity to examine the purely architectural advantages of
    RISC.  The RISC approach offers, compared with VAX, many fewer cycles per
    instruction but somewhat more instructions per program.  Using results from
    a software monitor on the MIPS machine and a hardware monitor on the VAX,
    this paper shows that the resulting advantage in cycles per program ranges
    from slightly under a factor of 2 to almost a factor of 4, with a geometric
    mean of 2.7.  It also demonstrates the correlation between cycles per
    instruction and relative instruction count.  Various reasons for this
    correlation, and for the consistent net advantage of RISC, are discussed.", 
  location     = "https://doi.org/10.1145/106972.107003"
}

@Article{pcwfai,
  author       = "Eric Freudenthal and Allan Gottlieb",
  title        = "Process Coordination with Fetch-and-Increment",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "260--268",
  month        = apr,
  keywords     = "barrier synchronization, bottleneck-free algorithms,
    fetch-and-add, fetch-and-increment, parallel access queues, process
    coordination, readers-writers problem",
  abstract     = "The fetch-and-add (F&A) operation has been used effectively
    in a number of process coordination algorithms.  In this paper we assess
    the power of fetch-and-increment (F&I) and fetch-and-decrement (F&D), which
    we view as restricted forms of F&A in which the only addends permitted are
    Â±1.  F&A-based algorithms that use only unit addends are thus trivially
    expressed with just F&I and F&D.  Our primary contributions are new F&I/F&D
    algorithms for readers/writers coordination and barrier synchronization for
    dynamically-sized groups.  We also restructure an existing F&A-based
    algorithm for queues-with-multiplicity to obtain an algorithm using just
    F&I and F&D.  When executed on certain hardware architectures, most of
    these algorithms are free of serial bottlenecks.  We also discuss a general
    technique for implementing F&A using F&I/F&D at a cost logarithmic in the
    number of processors.", 
  location     = "https://doi.org/10.1145/106974.106998", 
  location     = "https://nyuscholars.nyu.edu/en/publications/process-coordination-with-fetch-and-increment"
}

@Article{tcfarb,
  author       = "Douglas Johnson",
  title        = "The Case for a Read Barrier",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "279--287",
  month        = apr,
  keywords     = "lisp, garbage collection, generational gc, temporal gc,
    virtual memory, paging",
  abstract     = "This paper looks at the performance of two different garbage
    collection algorithms on a large and long running Lips application.
    Both algorithms use write barriers for generational collection.  Only one
    algorithm uses a rad barrier for incremental collection.  The results show
    little difference in the two algorithm's ability to collect garbage and
    some difference in memory size.  Any differences in CPU usage were too
    small to be visible with the measuring techniques used.  However, there
    were major differences in paging behavior with a read barrier permitted the
    garbage collector to work with the virtual memory manager instead of
    independently.", 
  location     = "https://doi.org/10.1145/106974.107000"
}

@Article{swcjmmc,
  author       = "John~M. Mellor-Crummey and Michael~L. Scott",
  title        = "Synchroniation Without Contention",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "269--278",
  month        = apr,
  keywords     = "bbn butterfly multiprocessor and busy-wait synchronization
    and contention-free mutual exclusion and dance hall machines and exploit
    local access and fetch_and_X instructions and large shared-memory
    multiprocessor and local access and memory consistency  and performance and
    reader-writer control and sequent symmetry and special-purpose hardware
    support", 
  abstract     = "Conventional wisdom holds that contention due to busy-wait
    synchronization is a major obstacle to scalability and acceptable
    performance in large shared-memory multiprocessors.  We argue the contrary,
    and present fast, simple algorithms for contention-free mutual exclusion,
    reader-writer control, and barrier synchronization.  These algorithms,
    based on widely available fetch_and_phi instructions, exploit local access
    to shared memory to avoid contention.  We compare our algorithms to
    previous approaches in both qualitative and quantitative terms, presenting
    their performance on the Sequent Symmetry and BBN Butterfly
    multiprocessors.  Our results highlight the importance of local access to
    shared memory, provide a case against the construction of so-called 'dance
    hall' machines, and suggest that special-purpose hardware support for
    synchronization is unlikely to be cost effective on machines with
    sequentially consistent memory.", 
  location     = "https://doi.org/10.1145/106975.106999"
}

@Article{aecbaads,
  author       = "Sang~L. Min and Jong-Deok Choi",
  title        = "An Efficient Cache-based Access Anomaly Detection Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "235--244",
  month        = apr,
  keywords     = "cache coherence, access anomalies, processor scheduling,
    cache-coherence protocols",
  abstract     = "One of the important issues in parallel program debugging is
    an efficient detection of access anomalies caused by uncoordinated access
    to shared variables.  On-the-fly detection of access anomalies has the
    major advantage that it reports only actual anomalies during execution
    while static analysis methods report all the potential anomalies, many of
    which cannot actually materialize during execution.  It also has the
    advantage that shorter traces are produced for post-mortem analysis
    purposes if an anomaly is detected.  The reason for this is that after an
    anomaly occurs, further trace information is of dubious value because the
    first anomaly may have affected subsequent program behavior.  So, once the
    first anomaly occurs, no further trace information need be generated.
    Existing methods for on-the-fly access anomaly detection suffer from
    performance penalties since the execution of the program being debugged has
    to be interrupted on every access to shared variables.  In this paper, we
    propose an efficient cache-based access anomaly detection scheme that
    piggybacks on the overhead already paid by the underlying cache coherence
    protocol.", 
  location     = "https://doi.org/10.1145/106973.106996"
}

@Article{peomcmfsmm,
  author       = "Kourosh Gharachorloo and Anoop Gupta and John Hennessy",
  title        = "Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "245--257",
  month        = apr,
  keywords     = "The memory consistency model supported by a multiprocessor
    architecture determines the amount of buffering and pipelining that may be
    used to hide or reduce the latency of memory accesses.  Several different
    consistency models have been proposed.  These range from sequential
    consistency on one end, allowing limited buffering, to release
    consistency on the other end, allowing extensive buffering and pipelining.
    The processor consistency and weak consistency models fall in between.  The
    advantage of the less strict models is increased performance potential.
    The disadvantage is increased hardware complexity and a more complex
    programming model.  To make an informed decision on the above trade-off
    requires performance data for the various models.  This paper addresses the
    issue of performance benefits from the above four consistency models.  Our
    results are based on simulation studies done for three applications.  The
    results show that in an environment where processor reads are blocking and
    writes are buffered, a significant performance increase is achieved from
    allowing reads to bypass previous writes.  Pipelining of writes, which
    determines the rate at which writes are retired from the write buffer, is
    of secondary importance.  As a result, we show that the sequential
    consistency model performs poorly relative to all other models, while the
    processor consistency model provides most of the benefits of the weak and
    release consistency models.", 
  location     = "https://doi.org/10.1145/106973.106997"
}

@Article{pacup,
  author       = "Jacques Cohen and Timothy~J. Hickey",
  title        = "Parsing and Compiling Using Prolog",
  journal      = toplas,
  year         = 1987,
  volume       = 9,
  number       = 2,
  pages        = "125--163",
  month        = apr,
  keywords     = "code generation, grammar properties, optimization, parsing,
    prolog, backtracking, definite-clause grammars, difference lists",
  abstract     = "This paper presents the material needed for exposing the
    reader to the advantages of using Prolog as a language for describing
    succinctly most of the algorithms needed in prototyping and implementing
    compilers or producing tools that facilitate this task.  The available
    published material on the subject describes one particular approach in
    implementing compilers using Prolog.  It consists of coupling actions to
    recursive descent parsers to produce syntax-trees which are subsequently
    utilized in guiding the generation of assembly language code.  Although
    this remains a worthwhile approach, there is a host of possibilities for
    Prolog usage in compiler construction.  The primary aim of this paper is to
    demonstrate the use of Prolog in parsing and compiling.  A second, but
    equally important, goal of this paper is to show that Prolog is a
    labor-saving tool in prototyping and implementing many non-numerical
    algorithms which arise in compiling, and whose description using Prolog is
    not available in the literature.  The paper discusses the use of
    unification and nondeterminism in compiler writing as well as means to
    bypass these (costly) features when they are deemed unnecessary.  Topics
    covered include bottom-up and top-down parsers, syntax-directed
    translation, grammar properties, parser generation, code generation, and
    optimizations.  Newly proposed features that are useful in compiler
    construction are also discussed.  A knowledge of Prolog is assumed.", 
  location     = "https://doi.org/10.1145/22719.22946"
}

@Article{pcoppida,
  author       = "Edward~K. Lee and Randy~H. Katz",
  title        = "Performance Consequences of Parity Placement in Disk Arrays",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "190--199",
  month        = apr,
  keywords     = "raid, error detection, parity, data mapping, performance",
  abstract     = "Due to recent advances in CPU and memory system performance,
    I/O systems are increasingly limiting the performance of modern computer
    systems.  Redundant Arrays of Inexpensive Disks (RAID) have been proposed
    by Patterson et. al. to meet the impending I/O crisis.  RAIDs substitute
    many small inexpensive disks for a few large expensive disks to provide
    higher performance (both transfer rate and I/O rate), smaller footprints
    and lower power consumption at a lower cost than the large expensive disks
    they replace, Unfortunately, with so many small disks, media availability
    becomes a serious problem.  RAIDs provide high availability by using parity
    encoding of data to survive disk failures.  As will be shown by this paper,
    the way parity is distributed in a RAID has significant consequences for
    performance.  In particular, we show that for relatively large request
    sizes of hundreds of kilobytes, the choice of parity placement
    significantly affects performance (up to 20-30 percent for the typical disk
    array configurations that are common today) and propose properties that are
    generally desirable of parity placements.",  
  location     = "https://doi.org/10.1145/106974.106992"
}

@Article{ctcocacfatlf,
  author       = "Vincent Cate and Thomas Gross",
  title        = "Combining the Concepts of Compression and Caching for a Two-Level Filesystem",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "200--211",
  month        = apr,
  keywords     = "compression algorithms, file migration, caching, temporal
    locality, lru, file migration, storage hierarchies",
  abstract     = "Caching Storage systems have always attempted to use
    properties of the files that are stored in the system to optimize access
    time, capacity, and/or cost.  Compression exploits patterns within files,
    and file migration and file caching exploit file access patterns, but the
    combination of these concepts has not been reported on before.  We discuss
    here the effectiveness of a filesystem that integrates caching and
    compression to provide two levels of file storage on disks.  This
    investigation is based on measurements that were collected on nine
    computers at three different sites.  The data indicate that automatic
    compression of least recently used files doubles the amount of data that
    can be stored on a given disk system, while incurring only a slight
    performance cost.", 
  location     = "https://doi.org/10.1145/106973.106993"
}

@Article{npatrtma,
  author       = "William~J. Bolosky and Michael~L. Scott and Robert~P. Fitzgerald and Robert~J. Fowler and Alan~L. Cox",
  title        = "{NUMA} Policies and Their Relation to Memory Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "212--221",
  month        = apr,
  keywords     = "trace-based analysis, global storage, programming style, ",
  abstract     = "Multiprocessor memory reference traces provide a wealth of
    information on the behavior of parallel programs.  We have used this
    information to explore the relationship between kernel-based NUMA
    management policies and multiprocessor memory architecture.  Our trace
    analysis techniques employ an off-line, optimal cost policy as a baseline
    against which to compare on-line policies, and as a policy-insensitive tool
    for evaluating architectural design alternatives.  We compare the
    performance of our optimal policy with that of three implementable policies
    (two of which appear in a previous work), on a variety of applications,
    with varying relative speeds for page moves and local, global, and remote
    memory references.  Our results indicate that a good NUMA policy must be
    chosen to match its machine, and confirm that such policies can be both
    simple and effective.  They also indicate that programs for NUMA machine
    must be written with care to obtain the best performance.", 
  location     = "https://doi.org/10.1145/106973.106994"
}

@Article{ldasccs,
  author       = "David Chaiken and John Kubiatowicz and Anant Agarwal",
  title        = "{LimitLESS} Directories:  A Scalable Cache Coherence Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "224--234",
  month        = apr,
  keywords     = "alewife machine, cache coherence, cache coherence protocol,
    interprocessor interrupt, performance",
  abstract     = "Caches enhance the performance of multiprocessors by reducing
    network traffic and average memory access latency.  However, cache-based
    systems must address the problem of cache coherence.  We propose the
    LimitLESS directory protocol to solve this problem.  The LimitLESS scheme
    uses a combination of hardware and software techniques to realize the
    performance of a full-map directory with the memory overhead of a limited
    directory.  This protocol is supported by Alewife, a large-scale
    multiprocessor.  We describe the architectural interfaces needed to
    implement the LimitLESS directory, and evaluate its performance through
    simulations of the Alewife machine.", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a237629.pdf",
  location     = "https://doi.org/10.1145/106973.106995"
}

@Article{iraaisfr,
  author       = "David~G. Bradlee and Susan~J. Eggers and Robert~R. Henry",
  title        = "Integrating Register Allocation and Instruction Scheduling for {RISC}s",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "122--131",
  month        = apr,
  keywords     = "instruction scheduling, register allocation, risc, pass
    phasing, code generation",
  abstract     = "To achieve high performance in uniprocessor RISC systems,
    compilers must perform both register allocation to reduce memory references
    and instruction scheduling to avoid pipeline hazards.  Compilers that
    separate the two functions should perform poorly on uniprocessor RISCS that
    support multi-cycle operations, particularly on computation-intensive
    workloads.  This is because the lack of coordination between register
    allocation and instruction scheduling results in poor use of the register
    set.  In this paper we compare three code generation strategies on three
    RISC processors that support multi-cycle operations.  The first strategy
    completely separates register allocation and instruction scheduling; the
    second logically separates the two phases, but uses a heuristic to force
    the instruction scheduler, which runs first, to adhere to the same
    restrictions as the register allocator; the third performs pre-scheduling
    to calculate schedule cost estimates that enable the register allocator to
    find a balance between using registers to avoid pipeline delays and using
    them to reduce memory references.  Our results show that separating
    register allocation and code scheduling produces inefficient code.
    However, a technique as complex as the third alternative brings little
    added benefit over the second.", 
  location     = "https://doi.org/10.1145/106972.106986"
}

@Article{cgfsaaem,
  author       = "Manuel~E. Benitez and Jack~W. Davidson",
  title        = "Code Generation for Streaming:  an Access\slash Execute Mechanism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "132--141",
  month        = apr,
  keywords     = "wm architecture, code generation, recurrence detection, ",
  abstract     = "Access/execute architectures have several advantages over
    more traditional architectures.  Because address generation and memory
    access are decoupled from operand use, memory latencies are tolerated
    better, there is more potential for concurrent operation, and it permits
    the use of specialized hardware to facilitate fast address generation.
    This paper describes the code generation and optimization algorithms that
    are used in an optimizing compiler for an architecture that contains
    explicit hardware support for the access/execute model of computation.  Of
    particular interest is the novel approach that the compiler uses to detect
    recurrence relations in programs and to generate code for them.  Because
    these relations are often used in problem domains that require significant
    computational resources, detecting and handling them can result in
    significant reductions in execution time.  While the tectilques discussed
    were originally targeted for one specific architecture, many of the
    techniques are applicable to commonly available microprocessors.  The paper
    describes the algorithms as well as our experience with using them on a
    number of machines.", 
  location     = "https://doi.org/10.1145/106975.106987"
}

@Article{eiohlpp,
  author       = "Rajive Bagrodia and Sharad Mathur",
  title        = "Efficient Implementation of High-Level Parallel Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "142--151",
  month        = apr,
  keywords     = "uc, data mapping, connection machine, copying, reducing,
    permuting, folding, matrix computations",
  abstract     = "The efficiency of a parallel program is related to the
    implementation of its data structures on the distributed (or shared) memory
    of a specific architecture.  This paper describes a declarative approach
    that may be used to modify the mapping of the program data on a specific
    architecture.  The ideas are developed in the context of a new language
    called UC and its implementation on the Connection Machine.  The paper also
    contains measurements on sample programs to illustrate the effectiveness of
    data mappings in improving the execution efficiency of example programs.", 
  location     = "https://doi.org/10.1145/106972.376053"
}

@Article{vrdfpvs,
  author       = "William Mangione-Smith and Santosh~G. Abraham and Edward~S. Davidson",
  title        = "Vector Register Design for Polycyclic Vector Scheduling",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "154--163",
  month        = apr,
  keywords     = "instruction scheduling, register naming, memory latency",
  abstract     = "Most vector compilers use a scheduling technique known as
    simple vector scheduling (SVS).  With SVS, all instructions of one loop
    iteration are issued before any succeeding iteration begins.  Long vector
    registers is the primary mechanism for increasing pipeline utilization.
    Some vector compilers use a newer technique, polycyclic vector scheduling
    (PVS), that executes multiple loop iterations concurrently.  Furthermore,
    chaining is not required for optimal performance using PVS.  While PVS code
    schedules typically perform as well as or better than SVS schedules, they
    also tend to require more vector registers.  This limits the applicability
    of PVS for current vector machines.  This paper studies how the register
    requirements of PVS code are related to machine architecture.  An
    architecture similar to the Cray-2 has been used for a series of scheduling
    experiments that cover a wide range of vector register lengths and memory
    latencies.  The results of these experiments indicate that the critical
    machine parameter for a PVS compiler is the number of available vector
    registers.  Little advantage has been found for using a vector register
    length of more than sixteen elements.", 
  location     = "https://doi.org/10.1145/106972.328664"
}

@Article{fgpwmhsacctam,
  author       = "David~E. Culler and Anurag Sah and Klaus~E. Schauser and Thorsten von Eicken and John Wawrzynek",
  title        = "Fine-grain Parallelism with Minimal Hardware Support:  {A} Compiler-Controlled Threaded Abstract Machine",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "164--175",
  month        = apr,
  keywords     = "activations, threads, quanta, id",
  abstract     = "In this paper, we present a relatively primitive execution
    model for fine-grain parallelism, in which all synchronization, scheduling,
    and storage management is explicit and under compiler control.  This is
    defined by a threaded abstract machine (TAM) with a multilevel scheduling
    hierarchy.  Considerable temporal locality of logically related threads is
    demonstrated, providing an avenue for effective register use under
    quasidynamic scheduling.  A prototype TAM instruction set, TLO, has been
    developed, along with a translator to a variety of existing sequential and
    parallel machines.  Compilation of Id, an extended functional language
    requiring fine-grain synchronization, under this model yields performance
    approaching that of conventional languages on current uniprocessors.
    Measurements suggest that the net cost of synchronization on conventional
    multiprocessors can be reduced to within a small factor of that on machines
    with elaborate hardware support, such aa proposed dataflow architectures.
    This brings into question whether tolerance to latency and inexpensive
    synchronization require specific hardware support or merely an appropriate
    compilation strategy and program representation.", 
  location     = "https://doi.org/10.1145/106972.106990"
}

@Article{loilp,
  author       = "David~W. Wall",
  title        = "Limits of Instruction-Level Parallelism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "176--188",
  month        = apr,
  keywords     = "basic blocks, branch and jump prediction, loop unrolling,
    alias analysis, register renaming, ",
  abstract     = "Growing interest in ambitious multiple-issue machines and
    heavily pipelined machines requires a careful examination of how much
    instruction level parallelism exists in typical programs.  Such an
    examination is complicated by the wide variety of hardware and software
    techniques for increasing the parallelism that can be exploited, including
    branch prediction, register renaming, and alias analysis.  By performing
    simulations based on instruction traces, we can model techniques at the
    limits of feasibility and even beyond.  Our study shows a striking
    difference between assuming that the techniques we use are perfect and
    merely assuming that they are impossibly good.  Even with impossibly good
    techniques, average parallelism rarely exceeds 7, with 5 more common.", 
  location     = "https://doi.org/10.1145/106974.106991",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-15.pdf"
}

@Article{tcpaooba,
  author       = "Monica~D. Lam and Edward~E. Rothberg and Michael~E. Wolf",
  title        = "The Cache Performance and Optimizations of Blocked Algorithms",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "63â-74",
  month        = apr,
  keywords     = "blocking, cache performance, matrix computations, data
    locality, analytic models, data access patterns",
  abstract     = "Blocking is a well-known optimization technique for improving
    the effectiveness of memory hierarchies.  Instead of operating on entire
    rows or columns of an array, blocked algorithms operate on submatrices or
    blocks, so that data loaded into the faster levels of the memory hierarchy
    are reused.  This paper presents cache performance data for blocked
    programs and evaluates several optimization to improve this performance.
    The data is obtained by a theoretical model of data conflicts in the cache,
    which has been validated by large amounts of simulation.  We show that the
    degree of cache interference is highly sensitive to the stride of data
    accesses and the size of the blocks, and can cause wide variations in
    machine performance for different matrix sizes.  The conventional wisdom of
    frying to use the entire cache, or even a fixed fraction of the cache, is
    incorrect.  If a fixed block size is used for a given cache size, the block
    size that minimizes the expected number of cache misses is very small.
    Tailoring the block size according to the matrix size and cache parameters
    can improve the average performance and reduce the variance in performance
    for different matrix sizes.  Finally, whenever possible, it is beneficial
    to copy non-contiguous reused data into consecutive locations.", 
  location     = "https://doi.org/10.1145/106972.106981"
}

@Article{teocsocp,
  author       = "Jeffrey~C. Mogul and Anita Borg",
  title        = "The Effect of Context Switches on Cache Performance",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "75--84",
  month        = apr,
  keywords     = "scheduling, context switching, trace analysis, cache
    simulations, performance",
  abstract     = "The sustained performance of fast processors is critically
    dependent on cache performance.  Cache performance in turn depends on
    locality of reference.  When an operating system switches contexts, the
    assumption of locality may be violated because the instructions and data of
    the newly-scheduled process may no longer be in the cache(s).
    Context-switching thus has a cost above that associated with that of the
    operations performed by the kernel.  We fed address traces of the processes
    running on a multi-tasking operating system through a cache simulator, to
    compute accurate cache-hit rates over short intervals.  By marking the
    output of such a simulation whenever a context switch occurs, and then
    aggregating the post-context-switch results of a large number of context
    switches, it is possible to estimate the cache performance reduction caused
    by a switch.  Depending on cache parameters the net cost of a context
    switch appears to be in the thousands of cycles, or tens to hundreds of
    microseconds.", 
  location     = "https://dl.acm.org/doi/10.1145/106974.106982"
}

@Article{apifotfism,
  author       = "David Keppel",
  title        = "{A} Portable Interface for On-The-Fly Instruction Space Modification",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "86--95",
  month        = apr,
  keywords     = "multiprocessors, protection, coherence, virtual machines,
    portability, address-space management, ",
  abstract     = "Applications such as incremental linking must modify
    instruction space during program execution.  Whenever instruction space is
    modified, machine-dependent systems issues such as instruction caching must
    be dealt with properly.  However, there are no standard idioms for
    signaling a change to the instruction space.  This paper discusses issues
    for instruction space allocation and coherence, describes a portable
    interface for modifying instruction space, and examines the details of
    several implementations of the interface.", 
  location     = "https://doi.org/10.1145/106973.106983"
}

@Article{vmpfup,
  author       = "Andrew~W. Appel and Kai Li",
  title        = "Virtual Memory Primatives for User Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "96--107",
  month        = apr,
  keywords     = "virtual memory, concurrent garbage collection, shared virtual
    storage, concurrent checkpointing, general garbage collection, persistent
    stores, extending addressability, data-compression paging, heap overflow
    detection, performance, tlb consistency, page size, ",
  abstract     = "Memory Management Units (MMUs) are traditionally used by
    operating systems to implement disk-paged virtual memory.  Some operating
    systems allow user programs to specify the protection level (inaccessible,
    read-only, read-write) of pages, and allow user programs to handle
    protection violations, but these mechanisms are not always robust,
    efficient, or well-matched to the needs of applications.  We survey several
    user-level algorithms that make use of page-protection techniques, and
    analyze their common characteristics, in an attempt to answer the question,
    'What virtual-memory primitives should the operating system provide to user
    processes, and how do today's operating systems provide them?'.", 
  location     = "https://doi.org/10.1145/106973.106984", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-276-90"
}

@Article{tioaaosd,
  author       = "Thomas~E. Anderson and Henry~M. Levy and Brian~N. Bershad and Edward~D. Lazowska",
  title        = "The Interaction of Architecture and Operating System Design",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "108--120",
  month        = apr,
  keywords     = "ipc, cross-machine communication, local communication, system
    calls, interrupt handling, virtual memory, translation buffers",
  abstract     = "Today's high-performance RISC microprocessors have been
    highly tuned for integer and floating point application performance.  These
    architectures have paid less attention to operating system requirements.
    At the same time, new operating system designs often have overlooked modern
    architectural trends which may unavoidably change the relative cost of
    certain primitive operations.  The result is that operating system
    performance is well below application code performance on contemporary
    RISCs.  This paper examines recent directions in computer architecture and
    operating systems, and the implications of changes in each domain for the
    other.  The requirements of three components of operating system design are
    discussed in detail: interprocess communication, virtual memory, and thread
    management.  For each component, we relate operating system functional and
    performance needs to the mechanisms available on commercial RISC
    architectures such as the MIPS R2000 and R3000, SUN SPARC, IBM RS6000,
    Motorola 88000, and Intel i860.  Our analysis reveals a number of specific
    reasons why the performance of oeprating system primitives on RISCs has not
    scaled with integer performance.  In addition, we identify areas in which
    architectures could better (and cost-effectively) accommodate operating
    system needs, and areas in which operating system design could accommodate
    certain necessary characteristics of cost-effective high-performance
    microprocessors.", 
  location     = "https://doi.org/10.1145/106973.106985", 
  location     = "https://homes.cs.washington.edu/~tom/pubs/interaction.html"
}

@Article{acffca,
  author       = "S.~S. Reddi and E.~A. Feustel",
  title        = "{A} Conceptual Framework for Computer Architecture",
  journal      = surveys,
  year         = 1976,
  volume       = 8,
  number       = 2,
  pages        = "277--300",
  month        = jun,
  keywords     = "computer architecture, framework, architecture composition,
    information flow, physical organization, diverse architecture conceptual
    unification",
  abstract     = "The purpose of this paper is to describe the concepts,
    definitions, and ideas of computer architecture and to suggest that
    architecture can be viewed as composed of three components: physical
    organization; control and flow of information; and representation,
    interpretation and transformation of information.  This framework can
    accommodate diverse architectural concepts such as array processing,
    microprogramming, stack processing and tagged architecture.  Architectures
    of some existing machines are considered and methods of associating
    architectural concepts with the components are established.  Architecture
    design problems and trade-offs are discussed in terms of the proposed
    framework.", 
  location     = "https://doi.org/10.1145/356669.356673"
}

@Article{hbdmsfsp,
  author       = "Gurindar~S. Sohi and Manoj Franklin",
  title        = "High-Bandwidth Data Memory Systems for Superscalar Processors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "53--62",
  month        = apr,
  keywords     = "caches, superscalar instruction issue, bus bandwidth, storage
    hierarchy, cpu performance",
  abstract     = "This paper considers the design of a data memory hierarchy,
    with a level 1 (L1) data cache at the top, to support the data bandwidth
    demands of a future-generation superscalar processor capable of issuing
    about ten instructions per clock cycle.  It introduces the notion of cache
    bandwidth â the bandwidth with which a cache can accept requests from the
    processor â and shows how the bandwidth of a standard, blocking cache, can
    degrade greatly because of its inability to overlap the service of misses.
    To improve the data bandwidth to greater than 1 request per cycle,
    multi-port, interleaved caches are introduced.  Simulation results from a
    cycle-by-cycle simulator, using the MIPS R2000 instruction set, suggest
    that memory hierarchies with blocking L1 caches will be unable to support
    the bandwidth demands of future-generation superscalar processors.
    Multi-port, non-blocking (MPNB) L1 caches introduced in this paper for the
    top of the data memory hierarchy appear to be capable of supporting such
    data bandwidth demands.", 
  location     = "https://doi.org/10.1145/106973.106980",
  location     = "https://minds.wisconsin.edu/handle/1793/59366"
}

@Article{spdc,
  author       = "David Callahan and Ken Kennedy and Allan Porterfield",
  title        = "Software Prefetching",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "40--52",
  month        = apr,
  keywords     = "cache management, performance, latency management",
  abstract     = "We present software prefetching, an approach to reducing
    cache miss latencies.  By providing a nonblocking prefetch instruction that
    brings into cache data at a specified memory address, the compiler can
    overlap the memory latency with other computation.  Our simulations show
    that, even when generated by a simple compiler algorithm, prefetch
    instructions can eliminate nearly all cache misses, while modestly
    increasing data traffic between memory and cache.", 
  location     = "https://doi.org/10.1145/106972.106979"
}

@Article{tfppoassp,
  author       = "Roland~L. Lee and Alex~Y. Kwok and Fay{\' e}~A. Briggs",
  title        = "The Floating-Point Performance of a Superscalar {SPARC} Processor",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "28--37",
  month        = apr,
  keywords     = "superscalar architecture, loop unrolling, software
    pipelining",
  abstract     = "superscalar SPARC processors' floating-point performance is
    evaluated based on empirical data from 12 benchmarks.  This evaluation is
    done in the context of two software instruction scheduling optimization,
    loop unrolling and software pipelining, and for three machine models we
    term, 1-scalar, 2-scalar and 4-scalar.  We also consider the effect of the
    memory system on the performance improvements.  Superscalar hardware alone
    exhibit little performance improvement without software optimization.  Of
    the two scheduling methods we study, software pipelining more effectively
    takes advantage of increased hardware parallelism, and achieves near
    optimal speedup on the 4-scalar machine model.  Loop-unrolling performance
    is restricted by the limited number of floating point registers in the
    SPARC architecture.  Applying both optimization techniques provides best
    performance.  A superscalar SPARC processor can provide improved
    floating-point performance, but with signification software and hardware
    development costs.", 
  location     = "https://doi.org/10.1145/106974.106978"
}

@Article{rtbpbriiadwm,
  author       = "Manolis Katevenis and Nestoras Tzartzanis",
  title        = "Reducing the Branch Penalty by Rearranging Instructions in a Double-Width Memory",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "15--27",
  month        = apr,
  keywords     = "pipelined computer architecture, branch penalty, delayed
    branch, delay slot, rearranging instructions into delay slots, super-scalar
    computer architecture, double-width instruction memory",
  abstract     = "In a pipelined processor with an instruction-fetch throughput
    of two (consecutive) instructions per cycle, one method to reduce the
    branch penalty is to rearrange the code by placing (copies of) instructions
    from both targets of a branch in the double-width fetch stream after that
    branch.  This scheme is of interest e.g.  when the number of fetch cycles
    is large, thus making it hard to fill all the delay slots with instructions
    from before the branch, and when the hardware has super-scalar capabilities
    but the compiler does not find enough instructions for parallel execution
    in the basic block where a b ranch is predicted to go.  We study this
    scheme of rearranging instructions, and we evaluate its performance
    (execution time and code size) in the case where no parallel instructions
    are scheduled in the delay slots.", 
  location     = "https://doi.org/10.1145/106974.106977"
}

@Article{avisettva,
  author       = "Andrew Wolfe and John~P. Shen",
  title        = "{A} Variable Instruction Stream Extension to the {VLIW} Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "2--14",
  month        = apr,
  keywords     = "vliw processors, state-machine models, configurable control
    regimes, sequential programs, barrier synchronization, ",
  abstract     = "A Variable Instruction Stream processor architecture called
    XIMD is proposed.  The XIMD structurally resembles a VLIW and shares many
    of the be@icial characteristics of VLIW; however, the XIMD architecture can
    dynamically partition its resources to support the concurrent execution of
    multiple instruction streams.  The number of streams can vary from cycle to
    cycle to best suit each portion of the application.  The XIMD concept and a
    comparison with other traditional architectures based on stateâ¦machine
    models of control paths are presented.  Several program examples further
    illustrate the capabilities of XIMD.  A brief description of an XIMD
    prototype machine is included; details of this implementation are presented
    in another paper.", 
  location     = "https://doi.org/10.1145/106972.106976"
}

@Article{afatuoipl,
  author       = "George~B. Leeman",
  title        = "{A} Formal Approach to Undo Operations in Programming Languages",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 1,
  pages        = "50--87",
  month        = jan,
  keywords     = "checkpoint, language constructs, preprocessors, recovery,
    reverse execution, undo, human interfaces",
  abstract     = "A framework is presented for adding a general Undo facility
    to programming languages.  A discussion of relevant literature is provided
    to show that the idea of Undoing pervades several areas in computer
    science, and even other disciplines.  A simple model of computation is
    introduced, and it is augmented with a minimal amount of additional
    structure needed for recovery and reversal.  Two different interpretations
    of Undo are motivated with examples.  Then, four primitives are defined in
    a language-independent manner; they are sufficient to support a wide range
    of Undo capability.  Two of these primitives carry out state saving, and
    the others mirror the two versions of the Undo operation.  Properties of
    and relationships between these primitives are explored, and there are some
    preliminary remarks on how one could implement a system based on this
    formalism.  The main conclusion is that the notions of recovery and
    reversal of actions can become part of the programming process.", 
  location     = "https://doi.org/10.1145/5001.5005"
}

@Article{ltls,
  author       = "Jens Peter Alfke",
  title        = "Learning to Love {SOM}",
  journal      = "MacTech Journal",
  year         = 1995,
  volume       = 11,
  number       = 1,
  pages        = "12--16",
  month        = jan,
  keywords     = "system object model, fragile base classes, c++, networked
    objects",
  abstract     = "The System Object Model (SOM) provides the object-oriented
    substrate used by OpenDoc and by future versions of the Macintosh Toolbox.
    SOM is fairly complex, relatively new on the Mac, and competes against
    other proprietary object models.  Itâs not surprising, then, that there is
    some degree of apprehension and misinformation surrounding it.  This
    article is an introduction to SOM.",
  location     = "http://preserve.mactech.com/articles/mactech/Vol.11/11.01/LearningtoLoveSOM/index.html"
}

@Article{djcdeap4,
  author       = "Eric~E. Allen",
  title        = "Diagnosing {Java} Code:  Designing Extensible Applications, Part 4",
  journal      = "IBM developerWorks",
  year         = 2001,
  month        = dec,
  keywords     = "java, s-expressions, extensibility",
  abstract     = "In this installment of Diagnosing Java Code, author Eric
    Allen illustrates how S-expressions -- syntactic representations of lists
    of elements delimited by parentheses -- can be used to provide a useful and
    lightweight form of black box extensibility.  The advantages of using
    S-expressions are discussed in the context of a particular example.  Also,
    the author details the limitations of S-expressions and notes when they may
    not be the best fit for an application.", 
  location     = "https://www.eecis.udel.edu/~decker/courses/280f07/paper/Java%20Ext%204.pdf"
}

@Article{matmfcs,
  author       = "Ira~W. Cotton",
  title        = "Microeconomics and the Market for Computer Services",
  journal      = surveys,
  year         = 1975,
  volume       = 7,
  number       = 2,
  pages        = "95--111",
  month        = jun,
  keywords     = "billing, charge-back, computer services, economics,
    microeconomics, pricing, supply-demand, elasticity, system management",
  abstract     = "Microeconomics has much to offer the computer services
    manager.  This article reviews some of the traditional topics in
    microeconomics and shows how they can be applied to the computer-services
    market.  The topics covered include supply, demand, costs, and pricing.
    The most significant application of microeconomics is in setting prices--so
    much so that microeconomics is frequently called 'price theory.'
    Accordingly, the thrust of the article is towards providing a sound
    framework for computer-services pricing.",
  location     = "https://doi.org/10.1145/356648.356650"
}

@Article{tfdasisoap,
  author       = "Adrian Cronauer",
  title        = "The Fairness Doctrine:  {A} Solution in Search of a Problem",
  journal      = "Federal Communications Law Journal",
  year         = 1994,
  volume       = 47,
  number       = 1,
  keywords     = "content control, fairness doctrine, narrowcasting",
  abstract     = "The 'Fairness Doctrine' refers to a former policy of the
    Federal Communications Commission wherein a broadcast station which
    presented one viewpoint on a controversial public issue had to afford the
    opposing viewpoint an opportunity to be heard.  The FCC ceased to enforce
    the doctrine in 1987, reasoning that the doctrine actually decreased the
    viewpoints heard by discouraging broadcasters from covering controversial
    issues out of fear of censure by the FCC.  The Author explores the
    historical development of the Fairness Doctrine and examines the flaws with
    the different rationales upon which the doctrine is based.  The Author
    concludes that today's marketplace acts as an alternative to the Fairness
    Doctrine by providing numerous media outlets with specialty formats
    catering to particular viewpoints; therefore, the Fairness Doctrine is
    unnecessary and should not be revived.", 
  location     = "https://www.repository.law.indiana.edu/fclj/vol47/iss1/6"
}

@Article{adusboavcs,
  author       = "G.~W.R.~Luderer and H.~Che and J.~P. Haggerty and P.~A. Kirslis and W.~T. Marshall",
  title        = "{A} Distributed " # unix # " System Based on a Virtual Circuit Switch",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "160--168",
  month        = dec,
  keywords     = "distributed computing, datakit, file servers",
  abstract     = "The popular UNIX operating system provides time-sharing
    service on a single computer.  This paper reports on the design and
    implementation of a distributed UNIX system.  The new operating system
    consists of two components: the S-UNIX subsystem provides a complete UNIX
    process environment enhanced by access to remote files; the F-UNIX
    subsystem is specialized to offer remote file service.  A system can be
    configured out of many computers which operate either under the S-UNIX or
    the F-UNIX operating subsystem.  The file servers together present the view
    of a single global file system.  A single-service view is presented to any
    user terminal connected to one of the S-UNIX subsystems.Computers
    communicate with each other through a high-bandwidth virtual circuit
    switch.  Small front-end processors handle the data and control protocol
    for error and flow-controlled virtual circuits.  Terminals may be connected
    directly to the computers or through the switch.Operational since early
    1980, the system has served as a vehicle to explore virtual circuit
    switching as the basis for distributed system design.  The performance of
    the communication software has been a focus of our work.  Performance
    measurement results are presented for user process level and operating
    system driver level data transfer rates, message exchange times, and system
    capacity benchmarks.  The architecture offers reliability and modularly
    growable configurations.  The communication service offered can serve as
    the foundation for different distributed architectures.", 
  location     = "https://doi.org/10.1145/1067627.806604"
}

@Article{lanthrds,
  author       = "G.~Popek and B.~Walker and J.~Chow and D.~Edwards and C.~Kline and G.~Rudisin and G.~Thiel",
  title        = "{LOCUS}: A Network Transparent, High Reliability Distributed System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "169--177",
  month        = dec,
  keywords     = "LOCUS is a distributed operating system that provides a high
    degree of network transparency while at the same time supporting high
    performance and automatic replication of storage.  By network transparency
    we mean that at the system call interface there is no need to mention
    anything network related.  Knowledge of the network and code to interact
    with foreign sites is below this interface and is thus hidden from both
    users and programs under normal conditions.  LOCUS is application code
    compatible with Unix2, and performance compares favorably with standard,
    single system Unix.  LOCUS runs on a high bandwidth, low delay local
    network.  It is designed to permit both a significant degree of local
    autonomy for each site in the network while still providing a network-wide,
    location independent name structure.  Atomic file operations and extensive
    synchronization are supported.Small, slow sites without local mass store
    can coexist in the same network with much larger and more powerful machines
    without larger machines being slowed down through forced interaction with
    slower ones.  Graceful operation during network topology changes is
    supported.", 
  location     = "https://doi.org/10.1145/800216.806605"
}

@Article{gaeidcs,
  author       = "Andrew~D. Birrell and Roy Levin and Roger~M. Needham and Michael~D. Schroeder",
  title        = "Grapevine:  An Exercise in Distributed Computing (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "178--179",
  month        = dec,
  keywords     = "Grapevine is a distributed, replicated system running on a
    large internet within the Xerox research and development community.  The
    internet extends from coast to coast in the USA, to Canada and to Europe,
    and contains more than 50 Ethernet local networks linked by leased
    telephone lines.  Over 1500 computers are attached to the internet.  Most
    computers are used an personal workstations, but some are used as servers
    providing access to shared facilities such as printers, large-scale
    secondary storage, or data bases.  Computers on the internet are uniformly
    addressable using the PUP family of protocols.", 
  location     = "https://doi.org/10.1145/1067627.806606"
}

@Article{baadsfwmvts,
  author       = "Norman Meyrowitz and Margaret Moser",
  title        = "{BRUWIN}:  An Adaptable Design Strategy for Window Manager\slash Virtual Terminal Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "180--189",
  month        = dec,
  keywords     = "window management, device independence",
  abstract     = "With only one process viewable and operational at any moment,
    the standard terminal forces the user to continually switch between
    contexts.  Yet this is unnatural and counter-intuitive to the normal
    working environment of a desk where the worker is able to view and base
    subsequent actions on multiple pieces of information.The window manager is
    an emerging computing paradigm which allows the user to create multiple
    terminals on the same viewing surface and to display and act upon these
    simultaneous processes without loss of context.  Though several research
    efforts in the past decade have introduced window managers, they have been
    based on the design or major overhaul of a language or operating system;
    the window manager becomes a focus ofârather than a tool ofâthe system.
    While many of the existing implementations provide wide functionality, most
    implementations and their associated designs are not readily available for
    common use; extensibility is minimal.This paper describes the design and
    implementation of BRUWIN, the BRown University WINdow manager, stressing
    how such a design can be adapted to a variety of computer systems and
    output devices, ranging from alphanumeric terminals to high-resolution
    raster graphics displays.  The paper first gives a brief overview of the
    general window manager paradigm and existing examples.  Next we present an
    explanation of the user-level functions we have chosen to include in our
    general design.  We then describe the structure and design of a window
    manager, outlining the five important parts in detail.  Finally, we
    describe our current implementation and provide a sample session to
    highlight important features.", 
  location     = "https://doi.org/10.1145/1067627.806607"
}

@Article{aadoag,
  author       = "Kourosh Gharachorloo and Madhu Sharma and Simon Steely and Stephen Van Doren",
  title        = "Architecture and Design of {AlphaServer GS320}",
  journal      = asplos00,
  year         = 2000,
  volume       = 35,
  number       = 11,
  pages        = "13--24",
  month        = nov,
  keywords     = "consistency protocols, directories, snoopy cache, multiprocessors",
  abstract     = "This paper describes the architecture and implementation of
    the AlphaServer GS320, a cache-coherent non-uniform memory access
    multiprocessor developed at Compaq.  The AlphaServer GS320 architecture is
    specifically targeted at medium-scale multiprocessing with 32 to 64
    processors.  Each node in the design consists of four Alpha 21264
    processors, up to 32GB of coherent memory, and an aggressive IO subsystem.
    The current implementation supports up to 8 such nodes for a total of 32
    processors.  While snoopy-based designs have been stretched to medium-scale
    multiprocessors by some vendors, providing sufficient snoop bandwidth
    remains a major challenge especially in systems with aggressive processors.
    At the same time, directory protocols targeted at larger scale designs lead
    to a number of inherent inefficiencies relative to snoopy designs.  A key
    goal of the AlphaServer GS320 architecture has been to achieve the
    best-of-both-worlds, partly by exploiting the bounded scale of the target
    systems.This paper focuses on the unique design features used in the
    AlphaServer GS320 to efficiently implement coherence and consistency.  The
    guiding principle for our directory-based protocol is to address
    correctness issues related to rare protocol races without burdening the
    common transaction flows.  Our protocol exhibits lower occupancy and lower
    message counts compared to previous designs, and provides more efficient
    handling of 3-hop transactions.  Furthermore, our design naturally lends
    itself to elegant solutions for deadlock, livelock, starvation, and
    fairness.  The AlphaServer GS320 architecture also incorporates a couple of
    innovative techniques that extend previous approaches for efficiently
    implementing memory consistency models.  These techniques allow us to
    generate commit events (which are used for ordering purposes) well in
    advance of formulating the reply to a transaction.  Furthermore, the
    separation of the commit event allows time-critical replies to by-pass
    inbound requests without violating ordering properties.  Even though our
    design specifically targets medium-scale servers, many of the same
    techniques can be applied to larger-scale directory-based and smaller-scale
    snoopy-based designs.  Finally, we evaluate the performance impact of some
    of the above optimizations and present a few competitive benchmark
    results.", 
  location     = "https://doi.org/10.1145/356989.356991"
}

@Article{taotes,
  author       = "Edward~D. Lazowska and Henry~M. Levy and Guy~T. Almes and Michael~J. Fischer and Robert~J. Fowler and Stephen~C. Vestal",
  title        = "The Architecture of the {Eden} System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "148--159",
  month        = dec,
  keywords     = "object-based systems, distributed systems, location
    independent objects",
  abstract     = "The University of Washington's Eden project is a five-year
    research effort to design, build and use an âintegrated distributedâ
    computing environment.  The underlying philosophy of Eden involves a fresh
    approach to the tension between these two adjectives.  In briefest form,
    Eden attempts to support both good personal computing and good multi-user
    integration by combining a node machine / local network hardware base with
    a software environment that encourages a high degree of sharing and
    cooperation among its users.The hardware architecture of Eden involves an
    Ethernet local area network interconnecting a number of node machines with
    bit-map displays, based upon the Intel iAPX 432 processor.  The software
    architecture is object-based, allowing each user access to the information
    and resources of the entire system through a simple interface.This paper
    states the philosophy and goals of Eden, describes the programming
    methodology that we have chosen to support, and discusses the hardware and
    kernel architecture of the system.", 
  location     = "https://doi.org/10.1145/800216.806603"
}

@Article{witb,
  author       = "Robert Schmidt",
  title        = "What is the {BIOS}?",
  journal      = "Smart Computing",
  year         = 1994,
  volume       = 5,
  number       = 7,
  month        = jul,
  keywords     = "bios, operating systems, system initialization, libraries"
}

@Article{asofsafl,
  author       = "M.~Satyanarayanan",
  title        = "{A} Study of File Sizes and Functional Lifetimes",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "96--108",
  month        = dec,
  keywords     = "file systems, statistics",
  abstract     = "The performance of a file system depends strongly on the
    characteristics of the files stored in it.  This paper discusses the
    collection, analysis and interpretation of data pertaining to files in the
    computing environment of the Computer Science Department at Carnegie-Mellon
    University (CMU-CSD).  The information gathered from this work will be used
    in a variety of ways:1.  As a data point in the body of information
    available on file systems.2.  As input to a simulation or analytic model of
    a file system for a local network, being designed and imlemented at CMU-CSD
    3. As the basis of implementation decisions and parameters for the file
    system just mentioned.4.  As a step toward understanding how a user
    community creates, maintains and uses files.",  
  location     = "https://doi.org/10.1145/800216.806597",  
  location     = "https://www.cs.cmu.edu/~satya/docdir/satya-sosp-1981.pdf"
}

@Article{htgps,
  author       = "Matt Bishop",
  title        = "Hierarchical Take-Grant Protection Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "109--122",
  month        = dec,
  keywords     = "de jure rules, de facto rules, information flow,
    authorization flow, hierarchical protection",
  abstract     = "The application of the Take-Grant Protection Model to
    hierarchical protection systems is explored.  The proposed model extends
    the results of Wu and applies the results of Bishop and Snyder to obtain
    necessary and sufficient conditions for a hierarchical protection graph to
    be secure.  In addition, restrictions on the take and grant rules are
    developed that ensure the security of all graphs generated by these
    restricted rules.", 
  location     = "https://doi.org/10.1145/800216.806598"
}

@Article{csfisaa,
  author       = "David~K. Gifford",
  title        = "Cryptographic Sealing for Information Secrecy and Authentication (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "123--124",
  month        = dec,
  keywords     = "public-key cryptography, capabilities, access control lists,
    information flow control",
  abstract     = "The problem of computer security can be considered to consist
    of four distinct components: secrecy (ensuring that information is only
    disclosed to authorized users), authentication (ensuring that information
    is not forged), integrity (ensuring that information is not destroyed), and
    availability (ensuring that access to information can not be maliciously
    interrupted).The paper describes a new protection mechanism called
    cryptographic sealing that provides primitives for secrecy and
    authentication.  The mechanism is enforced with a synthesis of classical
    cryptography, public-key cryptography, and a threshold scheme.", 
  location     = "https://doi.org/10.1145/800216.806599"
}

@Article{aumaificiame,
  author       = "George~W. Cox and William~M. Corwin and Konrad~K. Lai and Fred~J. Pollack",
  title        = "{A} Unified Model and Implementation for Interprocess Communication in a Multiprocessor Environment (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "125--126",
  month        = dec,
  keywords     = "ipc, process scheduling, ",
  abstract     = "This paper describes interprocess communication and process
    dispatching on the Intel 432.  The primary assets of the facility are its
    generality and its usefulness in a wide range of applications.  The
    conceptual model, supporting mechanisms, available interfaces, current
    implementations, and absolute and comparative performance are described.", 
  location     = "https://doi.org/10.1145/1067627.806600"
}

@Article{iamosfaobc,
  author       = "Kevin~C. Kahn and William~M. Corwin and T.~Don Dennis and Herman D'Hooge and David~E. Hubka and Linda~A. Hutchins and John~T. Montague and Fred~J. Pollack",
  title        = "{iMAX}:  {A} Multiprocessor Operating System for an Object-Based Computer",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "127--136",
  month        = dec,
  keywords     = "432, ada, architecture design, processor-memory model, system
    configurability, process management, memory management, device independence",
  abstract     = "The Intel iAPX 432 is an object-based microcomputer which,
    together with its operating system iMAX, provides a multiprocessor computer
    system designed around the ideas of data abstraction.  iMAX is implemented
    in Ada and provides, through its interface and facilities, an Ada view of
    the 432 system.  Of paramount concern in this system is the uniformity of
    approach among the architecture, the operating system, and the language.
    Some interesting aspects of both the external and internal views of iMAX
    are discussed to illustrate this uniform approach.", 
  location     = "https://doi.org/10.1145/1067627.806601"
}

@Article{ti4ofs,
  author       = "Fred~J. Pollack and Kevin~C. Kahn and Roy~M. Wilkinson",
  title        = "The {iMAX}-432 Object Filing System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "137--147",
  month        = dec,
  abstract     = "iMAX is the operating system for Intel's iAPX-432 computer
    system. The iAPX-4321 is an object-oriented multiprocessor architecture
    that supports capability-based addressing.  The object filing system is
    that part of iMAX that implements a permanent reliable object store.In this
    paper we describe the key elements of the iMAX object filing system design.
    We first contrast the concept of an object filing system with that of a
    conventional file system.  We then describe the iMAX design paying
    particular attention to five problems that other object filing designs have
    either solved inadequately or failed to address.  Finally, we discuss an
    effect of object filing on the programming semantics of Ada.", 
  keywords     = "type management, object spaces, linkage systems, synchronization", 
  location     = "https://doi.org/10.1145/1067627.806602"
}

@Article{tmattrapl,
  author       = "C.~R. Spooner",
  title        = "The {ML} Approach to the Readable All-Purpose Language",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 2,
  pages        = "215--243",
  month        = apr,
  keywords     = "adaptability, compile-time procedures, context, environment,
    extensibility, language-creation systems, language design, macro expansion,
    readability, textual domains",
  abstract     = "The ideal computer language is seen as one that would be as
    readable as natural language, and so adaptable that it could serve as the
    only language a user need ever know.  An approach to language design has
    emerged that shows promise of allowing one to come much closer to that
    ideal than might reasonably have been expected.  Using this approach, a
    language referred to as ML has been developed, and has been implemented as
    a language-creation system in which user-defined procedures invoked at
    translation time translate the source to some object code.  In this way the
    user can define both the syntax and the semantics of the source language.
    Both language and implementation are capable of further development.  This
    paper describes the approach, the language, and the implementation and
    recommends areas for further work.", 
  location     = "https://doi.org/10.1145/5397.5918"
}

@Article{aroodrfadcs,
  author       = "Liba Svobodova",
  title        = "{A} Reliable Object-Oriented Data Repository for a Distributed Computer System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "47--57",
  month        = dec,
  keywords     = "distributed data storage system, server, atomic update,
    stable storage, optical disk, memory management, crash recovery",
  abstract     = "The repository described in this paper is a component of a
    distributed data storage system for a network of many autonomous machines
    that might run diverse applications.  The repository is a server machine
    that provides very large, very reliable long-term storage for both private
    and shared data objects.  The repository can handle both very small and
    very large data objects, and it supports atomic update of groups of objects
    that might be distributed over several repositories.  Each object is
    represented as a history of its states; in the actual implementation, an
    object is a list of immutable versions.The core of the repository is stable
    append-only storage called Version Storage (VS).  VS contains the histories
    of all data objects in the repository as well as all information needed for
    crash recovery.  To maintain the current versions of objects online, a
    copying scheme was adopted that resembles techniques of real-time garbage
    collection.  VS can be implemented with optical disks.", 
  location     = "https://doi.org/10.1145/1067627.806591"
}

@Article{acotnbfs,
  author       = "James~G. Mitchell and Jeremy Dion",
  title        = "{A} Comparison of Two Network-Based File Servers (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "45--46",
  month        = dec,
  keywords     = "rings, buses, pup, file servers",
  abstract     = "This paper compares two working network-based file servers,
    the Xerox Distributed File System (XDFS) implemented at the Xerox Palo Alto
    Research Center, and the Cambridge File Server (CFS) implemented at the
    Cambridge University Computer Laboratory.  Both servers support concurrent
    random access to files using atomic transactions, both are connected to
    local area networks, and both have been in service long enough to enable us
    to draw lessons from them for future file servers.We compare the servers in
    terms of design goals, implementation issues, performance, and their
    relative successes and failures, and discuss what we would do differently
    next time.", 
  location     = "https://doi.org/10.1145/358468.358475"
}

@Article{otcowsp,
  author       = "Niklaus Wirth",
  title        = "On the Composition of Well-Structured Programs",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 4,
  pages        = "247--259",
  month        = dec,
  keywords     = "programming methods, systematic programming, program schemas,
    goto-free programs, well-structured programs, pascal",
  abstract     = "A professional programmer's know-how used to consist of the
    mastery of a set of techniques applicable to specific problems and to some
    specific problems and to some specific computer.  With the increase of
    computer power, the programmer's tasks grew more complex, and hence the
    need for a systematic approach became evident.  Recently, the subject of
    programming methods, generally applicable rules and patterns of
    development, received considerable attention.  Structured programming is
    the formulation of programs as hierarchical, nested structures of
    statements and objects of computation.  We give brief examples of
    structured programs, show the essence of this approach, discusses its
    relationship with program verification, and comment on the role of
    structured languages.", 
  location     = "https://doi.org/10.1145/356635.356639"
}

@Article{scsian,
  author       = "A.~J. Herbert and R.~M. Needham",
  title        = "Sequencing Computation Steps in a Network",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "59--63",
  month        = dec,
  keywords     = "distributed computations, synchronization, coordination",
  abstract     = "It is sometimes necessary in the course of a distributed
    computation to arrange that a certain set of operations is carried out in
    the correct order and the correct number of times (typically once).  If
    several sets of operations are performed on different machines on the
    network there is no obvious mechanism for enforcing such ordering
    constraints in a fully distributed way.  This lack basically stems from the
    difficulty of preventing copying and repetition of messages by machines and
    from the impossibility of constraining externally the actions of machines
    in response to messages that come into their hands.This paper presents a
    possible method for ensuring the integrity of sequences of operations on
    different machines.  The technique may be thought of as a means of enabling
    machines to ensure that requests made of them are valid and timely, not as
    means of centralized control of services.", 
  location     = "https://doi.org/10.1145/800216.806592"
}

@Article{aaconosk,
  author       = "Richard~F. Rashid and George~G. Robertson",
  title        = "Accent:  {A} Communication Oriented Network Operating System Kernel",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "64--75",
  month        = dec,
  keywords     = "ipc, virtual storage, distributed systems, modularity,
    location transparency, message passing",
  abstract     = "Accent is a communication oriented operating system kernel
    being built at Carnegie-Mellon University to support the distributed
    personal computing project, Spice, and the development of a fault-tolerant
    distributed sensor network (DSN).  Accent is built around a single,
    powerful abstraction of communication between processes, with all kernel
    functions, such as device access and virtual memory management accessible
    through messages and distributable throughout a network.  In this paper,
    specific attention is given to system supplied facilities which support
    transparent network access and fault-tolerant behavior.  Many of these
    facilities are already being provided under a modified version of VAX/UNIX.
    The Accent system itself is currently being implemented on the Three Rivers
    Corp.  PERQ.", 
  location     = "https://doi.org/10.1145/800216.806593"
}

@Article{casbstdpiaalprb,
  author       = "{\" O}zalp {\u B}abaoglu and William Joy",
  title        = "Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Referenced Bits",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "78--86",
  month        = dec,
  keywords     = "page replacement, vaxen, clock page replacement,
    multiprogramming, paging",
  abstract     = "This paper discusses the modifications made to the UNIX
    operating system for the VAX-11/780 to convert it from a swap-based
    segmented system to a paging-based virtual memory system.  Of particular
    interest is that the host machine architecture does not include
    page-referenced bits.  We discuss considerations in the design of
    page-replacement and load-control policies for such an architecture, and
    outline current work in modeling the policies employed by the system.  We
    describe our experience with the chosen algorithms based on
    benchmark-driven studies and production system use.", 
  location     = "https://doi.org/10.1145/800216.806595"
}

@Article{tmpe,
  author       = "Jan Heering and Paul Klint",
  title        = "Towards Monolingual Programming Environments",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 2,
  pages        = "183--213",
  month        = apr,
  keywords     = "monolingual system, language integration, language design,
  debugging languages, type checking, event expectation, side-effect recovery",
  abstract     = "Most programming environments are much too complex.  One way
    of simplifying them is to reduce the number of mode-dependent languages the
    user has to be familiar with.  As a first step towards this end, the
    feasibility of unified command/programming/debugging languages, and the
    concepts on which such languages have to be based, are investigated.  The
    unification process is accomplished in two phases.  First, a unified
    command/programming framework is defined and, second, this framework is
    extended by adding an integrated debugging capability to it.  Strict rules
    are laid down by which to judge language concepts presenting themselves as
    candidates for inclusion in the framework during each phase.  On the basis
    of these rules many of the language design questions that have hitherto
    been resolved this way or that, depending on the taste of the designer,
    lose their vagueness and can be decided in an unambiguous manner.", 
  location     = "https://doi.org/10.1145/3318.3321"
}

@Article{wasaeafvmm,
  author       = "Richard~W. Carr and John~L. Hennessy",
  title        = "{WSClock} --- {A} Simple and Effective Algorithm for Virtual Memory Management",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "87--95",
  month        = dec,
  keywords     = "page replacement algorithms, virtual storage, working set,
    load control, multiprogramming",
  abstract     = "A new virtual memory management algorithm WSCLOCK has been
    synthesized from the local working set (WS) algorithm, the global CLOCK
    algorithm, and a new load control mechanism for auxiliary memory access.
    The new algorithm combines the most useful feature of WSâa natural and
    effective load control that prevents thrashingâwith the simplicity and
    efficiency of CLOCK.  Studies are presented to show that the performance of
    WS and WSCLOCK are equivalent, even if the savings in overhead are
    ignored.", 
  location     = "https://doi.org/10.1145/800216.806596"
}

@Article{asaflscs,
  author       = "Bruno Blanchet and Patrick Cousot and Radhia Cousot and JÃ©rome Feret and Laurent Mauborgne and Antoine MinÃ© and David Monniaux and Xavier Rival",
  title        = "{A} Static Analyzer for Large Safety-Critical Software (Extended Abstract)",
  journal      = notices ,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "196--201",
  month        = may,
  keywords     = "abstract analysis, abstract domains",
  abstract     = "We show that abstract interpretation-based static program
    analysis can be made efficient and precise enough to formally verify a
    class of properties for a family of large programs with few or no false
    alarms.  This is achieved by refinement of a general purpose static
    analyzer and later adaptation to particular programs of the family by the
    end-user through parametrization.  This is applied to the proof of
    soundness of data manipulation operations at the machine level for periodic
    synchronous safety critical embedded software.The main novelties are the
    design principle of static analyzers by refinement and adaptation through
    parametrization (Sect.  3 and 7), the symbolic manipulation of expressions
    to improve the precision of abstract transfer functions (Sect.  6.3), the
    octagon (Sect.  6.2.2), ellipsoid (Sect.  6.2.3), and decision tree (Sect.
    6.2.4) abstract domains, all with sound handling of rounding errors in
    oating point computations, widening strategies (with thresholds: Sect.
    7.1.2, delayed: Sect.  7.1.3) and the automatic determination of the
    parameters (parametrized packing: Sect.  7.2).", 
  location     = "https://doi.org/10.1145/780822.781153"
}

@Article{papdo,
  author       = "Malcolm~P. Atkinson and Ronald Morrison",
  title        = "Procedures as Persistent Data Objects",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 4,
  pages        = "539--559",
  month        = oct,
  keywords     = "persistent storage, first-class functions, scoping,
    polymorphism, closures, partial evaluation, views, separate compilation,
    binding, static and dynamic typing", 
  abstract     = "A persistent programming environment, together with a
    language supporting first class procedures, may be used to provide the
    semantic features of other object modeling languages.  In particular, the
    two concepts may be combined to implement abstract data types, modules,
    separate compilation, views, and data protection.  Furthermore, the ideas
    may be used in system construction and version control, as demonstrated
    here.", 
  location     = "https://doi.org/10.1145/4472.4477"
}

@Article{pasecat,
  author       = "Brad Appleton",
  title        = "Patterns and Software:  Essential Concepts and Terminology",
  journal      = "Object Magazine Online",
  year         = 1997,
  volume       = 3,
  number       = 5,
  month        = may,
  keywords     = "origins, history, pattern types, pattern components,
    qualities", 
  abstract     = "Fundamental to any science or engineering discipline is a
    common vocabulary for expressing its concepts, and a language for relating
    them together.  The goal of patterns within the software community is to
    create a body of literature to help software developers resolve recurring
    problems encountered throughout all of software development.  Patterns help
    create a shared language for communicating insight and experience about
    these problems and their solutions.  Formally codifying these solutions and
    their relationships lets us successfully capture the body of knowledge
    which defines our understanding of good architectures that meet the needs
    of their users.  Forming a common pattern language for conveying the
    structures and mechanisms of our architectures allows us to intelligibly
    reason about them.  The primary focus is not so much on technology as it is
    on creating a culture to document and support sound engineering
    architecture and design.", 
  location     = "https://www.bradapp.net/docs/patterns-intro.html"
}

@TechReport{aqmafd30,
  author       = "Greg White",
  title        = "Active Queue Management Algorithms for {DOCSIS} 3.0",
  subtitle     = "A Simulation Study of CoDel, SfQ-CoDel and PIE in DOCSIS 3.0 Networks",
  institution  = "Access Network Technologies, CableLabs",
  year         = 2013,
  month        = apr,
  keywords     = "latency, buffering, buffer bloat, queue drop algorithms, pie,
    codel, sfq-codel, packet loss, tcp, performance",
  abstract     = "This paper describes the results of a simulation study of
    three active queue management algorithms applied to the upstream
    transmission buffer in a DOCSIS 3.  cable modem.  This paper is a follow-on
    to an earlier study which examined the Controlled Delay (CoDel) active
    queue management algorithm in a simulated DOCSIS 3.  cable modem.  This
    expanded study looks at CoDel in more depth, and compares it to two other
    promising active queue management algorithms, Stochastic Flow Queue - CoDel
    (SFQ- CoDel) and Proportional Integral Enhanced (PIE).  These three queue
    management algorithms are compared to existing (tail drop) buffering
    implementations that exist in current cable modems across a range of
    latency-sensitive applications.  It is demonstrated that current cable
    modem implementations result in severe degradation of user experience for
    latency-sensitive applications in situations where the user is
    simultaneously uploading a file via TCP.  The goal of the active queue
    managers in this study is to prevent the degradation of latency sensitive
    applications, while not impacting the TCP upload performance.  The
    Stochastic Flow Queue - Controlled Delay active queue manager displays
    extremely good performance in most traffic scenarios, enabling up to 2x
    reduction in latency for gaming traffic, x reduction in web page load time,
    and pristine VoIP quality, all while minimally impacting TCP upload
    performance.  The Proportional Integral Enhanced active queue manager
    similarly provided good performance, and is optimized for efficient
    implementation in existing cable modems.", 
  location     = "https://www.cablelabs.com/wp-content/uploads/2014/05/Active_Queue_Management_Algorithms_DOCSIS_3_0.pdf"
}

@TechReport{apfdsifcip,
  author       = "Joel Moses",
  title        = "{A} Program for Drilling Students in Freshman Calculus Integration Problems",
  institution  = "Artificial Intelligence Project, Project MAC, " # mit,
  year         = 1968,
  type         = "Memo",
  number       = 158,
  address      = cma,
  month        = mar,
  keywords     = "integration, problem solving, testing",
  abstract     = "The SARGE program is a prototype of a program which is
    intended to be used as an adjacent to regular classroom work in freshman
    calculus.  Using SARGE, students can type their step-by-step solution to an
    indefinite integration problem, and can have the correctness of their
    solution determined by the system.  The syntax for these steps comes quite
    close to normal mathematical notation, given the limitations of typewriter
    input.  The methods of solution is pretty much unrestricted as long as no
    mistakes are made along the way.  If a mistake is made, SARGE will catch it
    and yield an error message.  The student may modify the incorrect step, or
    he may ask the program for advice on how the mistake arose by typing
    'help'.  At present the program is weak in generating explanations for
    mistakes.  Sometimes the 'help' mechanisms will just yield a response which
    will indicate the way in which the erroneous step can be corrected.  In
    order to improve the explanation mechanism one would need a sophisticated
    analysis of students solutions to homework or quiz problems.  Experience
    with the behavior of students with SARGE, which is nil at present, should
    also help in accomplishing this goal.  SARGE is available as SARGE SAVED in
    T302 2517.", 
  location     = "https://dspace.mit.edu/bitstream/handle/1721.1/6163/AIM-158.pdf"
}

@TechReport{atsij,
  author       = "Christian Heinlein",
  title        = "Advanced Thread Synchronization in {Java}",
  institution  = "Dept. of Computer Structures, University of Ulm",
  year         = 2002,
  address      = "Ulm, Germany",
  keywords     = "bounded buffers, interaction expressions, readers-writers
    problem, synchronization, threads, java",
  abstract     = "Thread synchronization in Java using synchronized methods or
    statements is simple and straightforward as long as mutual exclusion of
    threads is sufficient for an application.  Things become less
    straightforward when wait() andnotify() have to be employed to realize more
    flexible synchronization schemes.  Using two well-known examples, the
    bounded buffer and the readers and writers problem, the traps and snares of
    hand-coded synchronization code and its entanglement with the actual
    application code are illustrated.  Following that, interaction expressions
    are introduced as a completely different approach where synchronization
    problems are solved in a declarative way by simply specifying permissible
    execution sequences of methods.  Their integration into the Java
    programming language using a simple precompiler and the basic ideas to
    enforce at run time the synchronization constraints specified that way are
    described.", 
  location     = "https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.1007%2F3-540-36557-5_25"
}

@TechReport{agifc,
  author       = "Lorenzo Alvisi and Fred~B. Schneider",
  title        = "{A} Graphical Interface for {CHIP}",
  institution  = dcs # "Cornell University",
  year         = 1996,
  number       = 1591,
  address      = itny,
  month        = "6 " # jun,
  keywords     = "chip, debugging, hypothetical processors",
  abstract     = "CHIP (Cornell Hypothetical Instructional Processor) [BBDS83]
    is a computer system designed as an educational tool for teaching
    undergraduate courses in operating system and machine architecture.  This
    document describes CHIP's graphical interface and covers in a tutorial how
    the interface is used to debug and execute CHIP programs.  A Graphical
    Interface for CHIP Lorenzo Alvisi The University of Texas at Austin
    Department of Computer Sciences Austin, TX Fred B.  Schneider Cornell
    University Department of Computer Science Ithaca, NY 6 June 1996 Abstract
    CHIP (Cornell Hypothetical Instructional Processor) is a computer system
    designed for use in teaching undergraduate courses in operating system and
    machine architecture.  This document describes CHIP's graphical interface
    and contains a tutorial describing how the interface is used to debug and
    execute CHIP programs.", 
  location     = "https://www.cs.cornell.edu/fbs/publications/96-1591.pdf"
}

@TechReport{rovhcm,
  author       = "Daniel~C. Hyde",
  title        = "Realization of {Verilog HDL} Computation Model",
  institution  = dcs # "Bucknell University",
  year         = 1997,
  month        = oct,
  keywords     = "vhdl, computational model, digital logic circuits"
}

@TechReport{s04d,
  author       = "Michael Dales",
  title        = "{SWARM} 0.44 Documentation",
  institution  = dcs # "University of Glasgow",
  year         = 2000,
  address      = "Glasgow, Scotland",
  month        = nov,
  keywords     = "architecture, compilation",
  location     = "This document gives a brief explanation of the design and
    implementation of SWARM --- the Software ARM.  It explains what SWARM is,
    and what it isn't, along with the design philosophy."
}

@InProceedings{cmultsygbtr,
  author       = "Stanley~P. Hanks",
  title        = "Creating {MAN}s using {LAN} Technology:  Sometimes You Gotta Break the Rules",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "439--451",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "man, lans, bridging, security, encapsulation, fddi, ethernet,
    vpn, standards",
  abstract     = "Commercially available, off-the-shelf internetworking
    products provide good mechanisms for the creation of limited-throughput
    metropolitan and wide area networks.  However, for many applications either
    the throughput obtained from T1 or slower communication circuits is
    inadequate, or the expense of obtaining high throughput using multiple T1
    or whole DS-3 circuits is prohibitive.  Proposed service offerings such as
    802.6, SMDS, frame relay, or SONET promise adequate speed metropolitan area
    network connectivity at a reasonable price.  Today these services arc not
    available, or where they are available are limited to T1 speeds.
    Metropolitan Fiber Systems owns over 17,000 miles of fiber optic cable in
    12 cities.  Most of this capacity is currently devoted to providing leased
    T1 and DS-3 circuits, and a significant portion of those circuits are for
    data transmission.  This provided a unique opportunity to address the
    question of how to provide LAN speed connectivity in and between
    metropolitan areas using commercially available products.  This paper
    discusses the development of the high speed MAN connectivity service
    offerings based on FDDI provided by MFS.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ahotcosalu1tj1,
  author       = "Alan~E. Kaplan",
  title        = "{A} History of the {COSNIX} Operating System: Assembly Language " # unix # " 1971 to {July}, 1991",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "429--437",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "history, fortran, disk performance, database systems",
  abstract     = "From 1971 until July, 1991 a variant of the assembly language
    version of the Unix operating system (Unix-A) was used to run a transaction
    processing system called COSMOS (Computer System for Mainframe Operations)
    in the Regional Bell Operating Companies.  At one time about seven hundred
    such systems were running on PDP 11//45 and PDP 11/70 computers.  This talk
    describes the history and development of that Unix operating system
    variant, called COSNIX, and explains some of the reasons for its success.
    I hope, also, that it gives a feeling of the challenges involved in
    producing a viable system during the days when computing resources were
    much more severely limited than they are today.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{fsfhs,
  author       = "Henry Spencer",
  title        = "Faster String Function",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "419--428",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "string functions, optimization, performance, parallel execution",
  abstract     = "The string functions provided by ANSI C and by traditional
    Unix C libraries are usually not as well-optimized as they could be.
    Careful tuning of inner loops is common, and on some processors it is
    profitable to rewrite them in assembler to exploit special instructions,
    but on most systems operations are still done a character at a time.  Given
    fairly lenient assumptions about the architecture, versions that operate a
    word at a time are possible.  Word-at-a-time processing is superficially
    difficult for C strings, since they arc terminated by a single null that is
    awkward to detect within a word.  However, carefully chosen combinations of
    logical and arithmetic operations can do such detection at a cost of 3-6
    operations per word, depending on data constraints and architecture,
    without relying on any architecture-specific specialized instructions or
    data paths.  This technique has been around as occasionally-heard folklore
    for some time, but does not appear to have been investigated in detail.
    The resulting word-at-a-time string functions are conspicuously faster than
    the usual ones for long strings.  The crossover point is typically 20-30
    characters, and the asymptotic speed advantage can be as much as a factor
    of 5, although a factor of 2-3 is more typical on 100-character operands.
    For specialized requirements where customized interfaces and customized
    code are permissible, rather higher factors are possible.  Certain problems
    occur, notably higher startup overhead, difficulties with unaligned
    strings, and the prevalence of relatively short strings as operands to some
    string functions.  The case for the fast functions is mixed, and an
    adaptive algorithm is needed to maximize overall performance.  It would
    also be useful to package the algorithms for use in custom string code,
    although this is somewhat challenging.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{rsis50,
  author       = "Sandeep Khanna and Michael Sebree and John Zolnowsky",
  title        = "Realtime Scheduling in {SunOS} 5.0",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "375--390",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time, scheduling, kernel threads, interrupts",
  abstract     = "We describe the fundamental mechanisms in SunOS 5.0 to
    provide realtime scheduling functionality.  Our primary goal was to provide
    bounded behavior for dispatching or blocking threads.  To achieve this goal
    we have modified the kernel to be fully preemptive, guaranteeing dispatch
    after both synchronous and asynchronous wakeups.  We have also worked
    toward controlling priority inversion in the kernel.  The result is a
    kernel capable of delivering realtime scheduling and bounded response to a
    large class of user level applications.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cancpmtppl,
  author       = "Sharon Hopkins",
  title        = "Camels and Needles: Computer Poetry Meets the {Perl} Programming Language",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "391--404",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "perl, poetry, free verse",
  abstract     = "Although various forms of literature have been created with
    the assistance of a computer, and even been generated by computer programs,
    it is only recently that literary works have actually been written in a
    computer language.  A computer-language poem need not necessarily produce
    any output, it may succeed merely by fooling the parser into thinking it is
    an ordinary program.  'Ihe Perl programming language has proved well-suited
    to the creation of computer-language poetry.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{3atofs,
  author       = "W.~D. Roome",
  title        = "{3DFS}: {A} Time-Oriented File Server",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "405--418",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, archiving, nfs, backups, optical storage, caching",
  abstract     = "3DFS is a network file server that provides time-oriented
    access to files and directories.  3DFS allows a user to read the version of
    a file as it existed on a particular day in the past, or to list the files
    in a directory on some prior date.  3DFS saves the daily incremental
    backups from other file systems (Sun file servers, Vaxen,...), and creates
    an on-line file system from these dumps.  3DFS uses optical disks in an
    automated jukebox, so no operator intervention is required.  3DFS uses the
    Sun NFSâ¢ protocol, and looks like any other NFS server.  Any UNIXÂ® command
    can read files in 3DFS, and users mount 3DFS just like any file server.
    Because 3DFS provides on-line access to old versions, users can access
    those versions in-place, without copying them to magnetic disk.  This paper
    describes 3DFS, its implementation, and our experience with it.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ntbpnm,
  author       = "Matt Blaze",
  title        = "{NFS} Tracing by Passive Network Monitoring",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "333--343",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "nfs, filesystem performance, network monitoring",
  abstract     = "Traces of filesystem activity have proven to be useful for a
    wide variety of purposes, ranging from quantitative analysis of system
    behavior to trace-driven simulation of filesystem algorithms.  Such traces
    can be difficult to obtain, however, usually entailing modification of the
    filesystems to be monitored and runtime overhead for the period of the
    trace.  Largely because of these difficulties, a surprisingly small number
    of filesystem traces have been conducted, and few sample workloads are
    available to filesystem researchers.  This paper describes a portable
    toolkit for deriving approximate traces of NFS activity by non-intrusively
    monitoring the Ethernet traffic to and from the file server.  The toolkit
    uses a promiscuous Ethernet listener interface (such as the Packetfilter)
    to read and reconstruct NFS-related RPC packets intended for the server.
    It produces traces of the NFS activity as well as a plausible set of
    corresponding client system calls.  The tool is currently in use at
    Princeton and other sites, and is available via anonymous ftp.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ccfcsius,
  author       = "Joseph~L. Hellerstein",
  title        = "Control Considerations for {CPU} Scheduling in " # unix # " Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "359--374",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "scheduling, priority scheduling, compute-bound jobs",
  abstract     = "Managing UNIX systems often involves setting service rate
    objectives, such as specifying that application A should receive 50% of the
    central processing unit (CPU).  In most UNIX systems, the only way to
    control CPU usage is by adjusting nice values; unfortunately, the
    relationship between nice values and process service rates has been poorly
    understood.  This paper develops an analytic model that relates service
    rate objectives for compute-bound processes to nice values and three
    scheduler parameters: R (the rate at which priority increases for each
    quantum of CPU consumed), D (the decay factor), and T (the number of quanta
    that expire before CPU usages are decayed); the model is evaluated using
    measurements of a workstation running IBM's Advanced Interactive Executive
    (AIX) 3.1 Operating System.  Based on the model, we develop an algorithm
    that calculates nice values that achieve service rate objectives for
    compute-bound processes.  Experiments conducted on a production AIX 3.1
    system suggest that our algorithm works well in practice.  In addition, we
    use the model to obtain insights into the control implications of parameter
    settings.  For example, we show that the nice mechanism is often less
    effective on faster processors since T tends to increase with processor
    speed; this increases the fraction of time during which processes with
    larger nice values execute, and hence limits the extent to which their
    service rates can be controlled.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{atdaonaaciads,
  author       = "Ken~W. Shirriff and John~K. Ousterhout 
",
  title        = "{A} Trace-Driven Analysis of Name and Attribute Caching in a Distributed System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "315--330",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "caching, network file system, name look-up, performance,
    server load, cache coherence, process migration, trace-driven simulation,
    sprite",
  abstract     = "This paper presents the results of simulating file name and
    attribute caching on client machines in a distributed file system.  The
    simulation used trace data gathered on a network of about 40 workstations.
    Caching was found to be advantageous: a cache on each client containing
    just 10 directories had a 91% hit rate on name lookups.  Entry-based name
    caches (holding individual directory entries) had poorer performance for
    several reasons, resulting in a maximum hit rate of about 83%.  File
    attribute caching obtained a 90% hit rate with a cache on each machine of
    the attributes for 30 files.  The simulations show that maintaining cache
    consistency between machines is not a significant problem; only 1 in 400
    name component lookups required invalidation of a remotely cached entry.
    Process migration to remote machines had little effect on caching.  Caching
    was less successful in heavily shared and modified directories such as
    /tmp, but there werenât enough references to /tmp overall to affect the
    results significantly.  We estimate that adding name and attribute caching
    to the Sprite operating system could reduce server load by 36% and the
    number of network packets by 30%.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/nameUsenix92.ps.Z"
}

@InProceedings{toatfmaaosj,
  author       = "Matt~W. Mutka and Philip~K. McKinley ",
  title        = "The {OPENSIM} Approach:  Tools for Management and Analysis of Simulation Jobs",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "291--304",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, workstation clusters, scheduling, work sharing, guis,
    simulations",
  abstract     = "This paper presents the design, implementation, and usage of
    OpenSim.  OpenSim provides new tools and integrates existing tools into an
    environment in order to establish a comprehensive facility for performing
    simulation work.  First, OpenSim provides a graphical user interface to
    users for creating input files for simulations and managing output files
    produced from simulations.  Second, tools are provided to help a user
    easily generate plots from sets of output files assocated with a simulation
    project.  Third, OpenSim addresses a common problem for many simulation
    users, namely, lack of computing capacity to serve the jobs.  In order to
    solve this problem, OpenSim integrates Condor, an existing system that
    clusters idle workstations into a processor bank, into its environment so
    that users have access to a large amount of computing capacity without
    interfering with the local usage of workstations by their owners.  Finally,
    since users often plan their schedules according to the deadlines required
    for their jobs, OpenSim enhances Condor so that users can request jobs to
    be scheduled within a deadline.  Therefore, a user can expect that the
    amount of computing capacity required for a simulation project will be
    available before a specified deadline.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mlcidfs,
  author       = "D.~Muntz and Peter Honeyman",
  title        = "Multi-level Caching in Distributed File Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "305--313",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "intermediate caches, distributed file systems, performance,
    host caches, hit rates, trace-driven simulations",
  abstract     = "We are investigating the potential for a hierarchy of
    intermediate file servers to address scaling problems in increasingly large
    distributed file systems.  To this end, we have run trace-driven
    simulations based on data from DEC-SRC and our own data collection to
    determine the potential of caching-only intermediate servers.  The degree
    of sharing among clients is central to the effectiveness of an intermediate
    server.  This turns out to be quite low in the traces available to us.  All
    told, fewer than 10% of block accesses are to files shared by more than one
    file system client.  Our simulations show that even with an infinite cache
    at an intermediate server, cache hit rates are disappointingly low.  For
    client caches as small as 20M, we observe hit rates under 19%.  As client
    cache sizes increase, the hit rate at an intermediate server approaches the
    degree of sharing among all clients.  On the other hand, the intermediate
    server does appear to be effective in boosting the performance and
    scalability of upstream file servers by substantially reducing the request
    rate presented to them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "http://www.citi.umich.edu/techreports/reports/citi-tr-91-3.pdf"
}

@InProceedings{obf,
  author       = "Mitch Bradley",
  title        = "{Open Boot} Firmwear",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "223--235",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "forth, firmwear, boot-time configuration",
  abstract     = "Open Boot is a software architecture for the firmware that
    controls a computer before the operating system has begun execution.  The
    Open Boot firmware design is based on a machine-independent interactive
    programming language (Forth).  Open Boot includes features for
    self-identifying plug-in devices with device-resident boot drivers, support
    for disk, tape, and network booting, hardware configuration reporting, and
    debugging tools for hardware, software, and firmware.  Open Boot is the
    basis for the device identification and booting capabilities of SBus.  An
    IEEE standards effort for boot firmware based on Open Boot is underway.
    The Futurebus+ and VME-D bus standards include support for Open Boot.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{scapmotuk,
  author       = "Michael Litzkow and Marvin Solomon ",
  title        = "Supporting Checkpointing and Process Migration Outside the " # Unix # " Kernel",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "281--290",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, checkpointing, process migration, kernel
    interception, state transfer, coredumps",
  abstract     = "We have implemented both checkpointing and migration of
    processes under UNIX as a part of the Condor package.  Checkpointing,
    remote execution, and process migration are different, but closely related
    ideas; the relationship between these ideas is explored.  A unique feature
    of the Condor implementation of these items is that they are accomplished
    entirely at user level.  Costs and benefits of implementing these features
    without kernel support are presented.  Portability issues, and the
    mechanisms we have devised to deal with these issues, are discussed in
    concrete terms.  The limitations of our implementation, and possible
    avenues to relieve some of these limitations, are presented.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcacidce,
  author       = "Douglas Rosenthal and Wayne Allen and Kenneth Fiduk",
  title        = "Process Control and Communication in Distributed {CAD} Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "271--281",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "ipc, load balancing, process control, message passing,
    distributed computing",
  abstract     = "The MCC Computer-Aided Design (CAD) Framework Process Control
    System (PCS) provides distributed process control and communication
    services in heterogeneous network environments.  The PCS also provides
    network-wide load balancing via an efficient and flexible process placement
    mechanism.  The PCS services enable design tools and CAD framework
    components to leverage the resources of distributed computing networks,
    while supporting various degrees of interaction through distributed,
    real-time communication.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lasodc,
  author       = "Robert~M. English and Alexander~A. Stepanov",
  title        = "Loge:  {A} Self-Organizing Disk Controller",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "237--251",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "While the task of organizing data on the disk has
    traditionally been performed by the file system, the disk controller is in
    many respects better suited to the task.  In this paper, we describe Loge,
    a disk controller that uses internal indirection, accurate physical
    information, and reliable metadata storage to improve I/O performance.  Our
    simulations show that Loge improves overall disk performance, doubles write
    performance, and can, in some cases, improve read performance.  The Loge
    disk controller operates through standard device interfaces, enabling it to
    be used on standard systems without software modification.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{hawsibtifn,
  author       = "Bruce Nelson and Yu-Ping Cheng",
  title        = "How and Why {SCSI} is Better than {IPI} for {NFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "253--270",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "disk controllers, traffic patterns",
  abstract     = "Disk drives are often dismissed as mundane devices, but they
    are actually interesting, complicated, and misunderstood.  In traditional
    Unix servers, disk storage subsystems are usually optimized for
    sequential-transfer performance.  Perhaps counter-intuitively, however, NFS
    file servers exhibit marked random-access disk traffic.  This report
    investigates this apparent contradiction and shows that disk-drive
    concurrency not disk transfer rateâis the important factor in disk storage
    performance for most NFS network servers.  The investigation begins with a
    concrete and detailed comparison of both performance-oriented and
    nonperformance-oriented technical specifications of both SCSI and IPI drive
    and interface types.  It offers a thorough empirical evaluation of SCSI
    disk drive performance, varying parameters such as synchronous or
    asynchronous bus transfers, random and sequential access patterns, and
    multiplicity of drives per SCSI channel.  It discusses (nonempirically)
    similar characteristics for IPI-2 drives.  The report concludes with
    benchmarked comparisons of NFS file servers using SCSI-based disk arrays
    and IPI-2 subsystems.  The results show that NFS heavy-load throughput
    using SCSI disk arrays scales linearly with extra drives, whereas IPI-2
    throughput scales less than proportionally with extra drives.  This SCSI
    scalability advantage, combined with SCSIâs appealing price-performance and
    price-capacity, make SCSI disks a superior choice for NFS servers.  IPI-2
    drives, with their optional high transfer rates, remain an excellent choice
    for compute-oriented servers executing large-file applications where high
    sequential throughput is essential.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{iiiocas,
  author       = "Murthy Devarakonda and Arup Mukherjee",
  title        = "Issues in Implementation of Cache-Affinity Scheduling",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "345--357",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "affinity scheduling, cache management, thread scheduling,
    performance", 
  abstract     = "In a shared memory multiprocessor, a thread may have an
    affinity to a processor because of the data remaining in the processorâs
    cache from a previous dispatch.  We show that two basic problems should be
    addressed in a Unix-like system to exploit cache affinity for improved
    performance: First, the limitation of the Unix dispatcher model (âprocessor
    seeking a threadâ); Second, pseudo-affinity caused by low-cost waiting
    techniques used in a threads package such as C Threads.  We demonstrate
    that the affinity scheduling is most effective when used in a threads
    package that supports multiplexing of user threads on kernel threads.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{at,
  author       = "Jay Littman",
  title        = "Applying Threads",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "209--221",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multithreading, synchronization, deadlock, performance",
  abstract     = "Multithreading components of a software system can increase
    performance, but it can also increase complexity.  At Hewlett-Packard, we
    have developed a workstation based medical product, called the Monitoring
    Full Disclosure Review Station, or M1251A, that uses multithreading to
    achieve performance requirements.  The M1251A continuously acquires
    physiological waveforms and arrhythmia information for presentation to a
    clinician in an intensive care unit.  This paper describes the benefits the
    M1251A gains from multithreading, identifies the problems the development
    team had with multithreading, and explains how those problems were
    resolved.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aspmap,
  author       = "Bernhard Wagner and Bruce~K. Haddon",
  title        = "Application Software:  Product Management and Privileges",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "system administration, portability, standards",
  abstract     = "Application programs for UNIX are increasingly making greater
    demands upon the system structure, are adhering less well to admittedly
    implicit guidelines, or, are being inexactly transliterated from the
    paradigms of other systems.  These influences add to the administrative
    problems and load, and, in some cases, are exacerbating security risks.
    The administrative problems and corresponding solutions are presented here
    in a twofold manner: Firstly, by the description of our use of methods that
    separate administration of the programs and files that make up an
    application suite both from system administration and from eacli other.  We
    argue that thus a sort ol modularity takes place in the software
    administration.  The goal is the lack of need for super user privilege, so
    that this separation improves the overall security of the system.
    Secondly, we list a number of features that we support as being essential,
    a list of requirements to be fulfilled by all application programs written
    for UNIX systems.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcsr,
  author       = "Spencer Rugaber",
  title        = "Program Comprehension",
  booktitle    = "Encyclopedia of Computer Science and Technology",
  year         = 1995,
  editor       = "Allen Kent and James~G. Williams",
  pages        = "341--368",
  publisher    = "Marcel Dekker",
  address      = nyny,
  keywords     = "program comprehension, cognitive models"
}

@InProceedings{rwpfm,
  author       = "Robin Schaufler",
  title        = "Realtime Workstation Performance for {MIDI}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "139--",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time systems, midi, system performance, benchmarking",
  abstract     = "MIDI studio applications require 1 millisecond accuracy in
    timing transmission and receipt of MIDI messages.  Past MIDI
    implementations on UNIXâ¢ have either used the Roland MPU-401 coprocessor to
    do accurate timing, or have not had timing tests published for them.
    Timing MIDI I/O on the host processor allows for more flexible scheduling
    policies than the MPU-401, but many people expressed skepticism that it
    could be done with sufficient accuracy and efficiency because of UNIXâ¢
    virtual memory and pre-emptive scheduling.  This paper describes studies we
    did on providing millisecond accuracy on the host processor of a Silicon
    Graphics Iris Indigo running IRIX, the Silicon Graphics version of UNIX.
    Our measurements show that millisecond accuracy is feasible on IRIXâ¢
    without modifying the kernel.  The paper goes on to describe how the
    studies relate to other time based media.  With a small set of real time
    features, UNIX can really sing.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aafapmt,
  author       = "Sun Wu and Udi Manber",
  title        = "{\tt agrep} --- Fast Approximate Pattern-Matching Tool",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "153--162",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "approximate string matching, boyer-more, knuth-morris-pratt",
  abstract     = "Searching for a pattern in a text file is a common
    operation in many applications ranging from text editors and databases to
    applications in molecular biology.  In many instances the pattern does not
    appear in the text exaedy.  Errors in the text or in the query can result
    from misspelling or from experimental errors (e.g., when the text is a DNA
    sequence).  The use of such approximate pattern matching has been limited
    until now to specific applications.  Most text editors and searching
    programs do not support searching with errors because of the complexity
    involved in implementing it.  In this paper we describe a new tool, called
    agrep, for approximate pattern matching.  Agrep is based on a new efficient
    and flexible algorithm for approximate string matching.  Agrep is also
    competitive with other tools for exact string matching; it include many
    options that make searching more powerful and convenient.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aewbiwacileas,
  author       = "Bill Cheswick",
  title        = "An Evening with {Berferd} In Which a Cracker is Lured, Endured, and Studied",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "163--173",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "cracking, jails, honeypots, law",
  abstract     = "On 7 January 1991 a cracker, believing he had discovered the
    famous sendmail DEBUG hole in our Internet gateway machine, attempted to
    obtain a copy of our password file.  I sent him one.  For several months we
    led this cracker on a merry chase in order to trace his location and learn
    his techniques.  This paper is a chronicle of the crackerâs 'successes' and
    disappointments, the bait and traps used to lure and detect him, and the
    chroot 'Jail' we built to watch his activities.  We concluded that our
    cracker had a lot of time and persistence, and a good list of security
    holes to use once he obtained a login on a machine.  With these holes he
    could often subvert the uucp and bin accounts in short order, and then
    root.  Our cracker was interested in military targets and new machines to
    help launder his connections.  This is a draft of a paper accepted for the
    January 1992 San Francisco Usenix.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{haph,
  author       = "Peter Honeyman and L.~B. Huston and M.~T. Stolarchuk",
  title        = "Hijacking {AFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "175--181",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "network security, mitm attack, afs, network protocols,
    challenge/response oracle",
  abstract     = "We have identified several techniques that allow uncontrolled
    access to files managed by AFS 3.0.  One method relies on administrative
    (or root) access to a userâs workstation.  Defending against this sort of
    attack is difficult.  Another class of attacks comes from promiscuous
    access to the physical network.  Stronger cryptographic protocols, such as
    those employed by AFS 3.1, obviate this problem.  These exercises help us
    understand vulnerabilities in the distributed systems that we employ (and
    deploy), and offer guidelines for securing them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aibaflsdse,
  author       = "Dale Skeen",
  title        = "An Information Bus Architecture for Large-Scale, Decision-Support Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information bus, publish-subscribe",
  abstract     = "Some of the promising industries for commercializing UNIX are
    those requiring real-time decision support, such as trading rooms, factory
    automation, process control, and network management.  These large-scale,
    real-time environments present challenging technical problems of high data
    volumes, split-second response times, and high availability.  Moreover,
    these environments demand flexible architectures that can support a rapidly
    changing set of application requirements.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{xwbstfu,
  author       = "Doug Blewett and Scott Anderson and Meg Kilduff and Susan Udovic and Mike Wish",
  title        = "{X Widget}-Based Software Tools for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "111--123",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "x resources, xtent, ipc, control flow, guis, user interface
    development, x windows",
  abstract     = "This paper describes a small language and IPC protocol that
    can be used for specifying UNIX style, X Toolkit based, graphics software
    tools.  The language is unusual in that it integrates the X Toolkit widget
    world and the UNIX philosophy of creating applications from collections of
    small reusable filters.  Filters can be constructed from old Xt based
    graphics processes or specified directly in the small language.  The system
    is based on an easily reproducible macro interpreter and IPC system that
    can be used with any collection of widgets.  A multi-process application
    builder constructed with the system is used as an example of how the
    software tools philosophy can be effectively used to construct graphics
    applications.  We present data on the use of the system by both research
    organizations and development groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pfdomlaae,
  author       = "Reed Hastings and Bob Joyce",
  title        = "Purify: Fast Detection of Memory Leaks and Access Errors",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "125--137",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "storage-access errors, storage leaks, detecting storage
    leaks, object-code rewriting",
  abstract     = "This paper describes Purifyâ¢, a software testing and quality
    assurance tool that detects memory leaks and access errors.  Purify inserts
    additional checking instructions directly into the object code produced by
    existing compilers.  These instructions check every memory read and write
    performed by the program-under-test and detect several types of access
    errors, such as reading uninitialized memory or writing to freed memory.
    Purify inserts checking logic into all of the code in a program, including
    third-party and vendor object-code libraries, and verifies system call
    interfaces.  In addition, Purify tracks memory usage and identifies
    individual memory leaks using a novel adaptation of garbage collection
    techniques.  Purify produces standard executable files compatible with
    existing debuggers, and currently runs on Sun Microsystemsâ SPARC family of
    workstations.  Purifyâs nearly-comprehensive memory access checking slows
    the target program down typically by less than a factor of three and has
    resulted in significantly more reliable software for several development
    groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aaedsfti,
  author       = "Alan Emtage and Peter Deutsch",
  title        = "{archie} --- An Electronic Directory Service for the {Internet}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "93--110",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information retrieval, internet crawling, ftp, indexing",
  abstract     = {The huge size and continued rapid growth of the Internet
    offers a particular challenge to systems designers and service providers in
    this new environment.  Before a user can effectively exploit any of the
    services offered by the Internet community or access any information
    provided by such services, that user must be aware of both the existence of
    the service and the host or hosts on which it is available.  Adequately
    addressing this âresource discovery problemâ is a central challenge for
    both service providers and users wishing to capitalize on the possibilities
    of the Internet.  This paper describes archie, our attempt at an on-line
    resource directory service for an internetworked environment.  The current
    implementation of archie automatically indexes and makes available all
    filenames stored at known anonymous FTP sites.  The filename information is
    updated automatically ensuring users access to authoritative information.
    The system also makes available the names and descriptions of several
    thousand packages found on the Internet.}, 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{tefs,
  author       = "Sailesh Chutani and Owen~T. Anderson and Michael~L. Kazar and Bruce~W.  Leverett and W.~Anthony Mason and Robert~N. Sidebotham",
  title        = "The {Episode} File System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "43--60",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, filesets, metadata logging, integrity checking,
    performance",
  abstract     = "We describe the design of Episode,â¢ a highly portable
    POSIX-compliant file system.  Episode is designed to utilize the disk
    bandwidth efficiently, and to scale well with improvements in disk capacity
    and speed.  It utilizes logging of meta-data to obtain good performance,
    and to restart quickly after a crash.  Episode uses a layered architecture
    and a generalization of files called containers to implement fileseis.  A
    fileset is a logical file system representing a connected subtree.
    Filesets are the unit of administration, replication, and backup in
    Episode.  The system works well, both as a standalone file system and as a
    distributed file system integrated with the OSF's Distributed Computing
    Environment (DCE).  Episode will be shipped with the DCE as the Local File
    System component, and is also exportable by NFS.  As for performance,
    Episode meta-data operations are significantly faster than typical UNIX
    Berkeley Fast File System implementations due to Episode's use of logging,
    while normal I/O operations run near disk capacity.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aiolffbu,
  author       = "Dave Shaver and Eric Schnoebelen and George Bier",
  title        = "An Implementation of Large Files for {BSD} " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "61--68",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, portability, software maintenance, 64-bit
    address spaces",
  abstract     = "The design of the ConvexOS 1 filesystem, based on the BSD
    Fast File System, allows for a theoretical maximum file size of about 4402G
    2 with a 4K filesystem block size (or about 64T with 8K blocks.)
    Unfortunately, the actual limit of the CONVEX filesystem has been 2G-1
    because key kernel values and file offset pointers are 32-bits in size.
    This is a problem shared by many other UNIX 3 vendors.  This paper
    describes the path CONVEX has taken to implement files and filesystems
    larger than 2G.  The implementation is based on a new set of 64-bit system
    calls and new library interfaces; it requires no changes to the on-disk
    i-node representation.  The large file programming models and the kernel
    and utilities changes are described.  Measurements of read and write I/O
    rates are presented and show that there is little performance penalty for
    manipulating large files using the chosen implementation.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{serf,
  author       = "Walter~A. Burkhard and Petar~D. Stojadinovi{\' c}",
  title        = "Storage-Efficient Reliable Files",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "69--77",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "n-of-m redundancy, reliability, file systems, file dispersal,
    fault tolerance, performance",
  abstract     = "The File Dispersal Shell is a storage-efficient reliable data
    storage prototype facility for local area networks.  Rabin's information
    dispersal algorithm provides an attractive data organization scheme which
    potentially uses less physical storage space than replication while
    obtaining excellent data reliability and access times comparable to those
    obtained for a single disk.  We have constructed Rabin's information
    dispersal algorithm within a UNIX system shell that provides almost all the
    traditional shell facilities augmented with two additional commands to
    create and delete dispersed files.  We present analytical
    mean-time-to-data-loss results, storage requirements, together with our
    prototype implementation and preliminary access-time measurements.  For
    practical purposes, dispersed files are invisible to the user except for
    the improved reliability at modest disk space cost.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mmftbu,
  author       = "Nathaniel~S. Borenstein",
  title        = "Multimedia Mail From the Bottom Up",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "79--91",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multimedia, mail, extensions, outboard processing,
    configuration files, usability",
  abstract     = "Multimedia mail systems have exhibited great potential, but
    the widespread use of multimedia mail has so far been inhibited by the lack
    of interchange standards and the heterogeneity of mail-reading software.
    This paper describes a new approach that seeks to break the existing
    log-jam and make multimedia mail a practical reality.  The paper begins
    with a brief summary of the state of the art in multimedia mail systems.
    It then outlines the new, 'bottom-up' approach, and describes the
    configuration mechanism that is central to its operation.  Next, it
    describes a prototype implementation and its deployment on top of over a
    dozen different mailreading programs at Bellcore and elsewhere.  Finally,
    problems in the prototype installation are discussed, along with future
    prospects for multimedia mail using this approach.  The paper ends by
    outlining a vision of a new and better 'lowest common denominator' for
    electronic mail.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lpmtfu,
  author       = "Margo Seltzer and Michael Olson",
  title        = "{LIBTP}: Portable, Modular Transactions for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "9--25",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "transactions, shared storage, user-space libraries, two-phase
    commit, crash recovery, lock management",
  abstract     = "Transactions provide a useful programming paradigm for
    maintaining logical consistency, arbitrating concurrent access, and
    managing recovery.  In traditional UNIX systems, the only easy way of using
    transactions is to purchase a database system.  Such systems are often
    slow, costly, and may not provide the exact functionality desired.  This
    paper presents the design, implementation, and performance of LIBTP, a
    simple, non-proprietary transaction library using the 4.4BSD database
    access routines (db(3)).  On a conventional transaction processing style
    benchmark, its performance is approximately 85% that of the database access
    routines without transaction protection, 200% that of using fsync(2) to
    commit modifications to disk, and 125% that of a commercial relational
    database system.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/1925.html"
}

@InProceedings{etaomffsio,
  author       = "Orran Krieger and Michael Stumm and Ron Unrau",
  title        = "Exploiting the Advantages of Mapped Files for Stream {I}/{O}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "27--42",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file-mapped io, std-io, stream io",
  abstract     = "A new approach for providing user level support for fast
    stream I/O is motivated by four factors common to most modern systems: 1)
    the capability of the operating system to support mapped files, 2) the
    increasing number of applications that use threads, 3) the increasing
    discrepancy between processor speed and disk latency, and 4) the increasing
    amount of available main memory.  In this paper, we first describe the
    advantages and disadvantages of using mapped files to support stream access
    to files, and then describe a new interface, the Alloc Stream Interface
    (ASI), that allows for improved performance over existing stream
    interfaces.  A library that supports ASI has been implemented on several
    systems (including IRIX and SunOS).  In addition, the Stdio library has
    been re-implemented to use ASI.  Significant performance advantages are
    demonstrated for Stdio applications that are linked to this new library and
    particularly for applications that are modified to use ASI directly.  For
    example, on typical Unix platforms, some standard I/O intensive utilities
    are shown to run up to twice as fast when re-linked to use this library and
    up to three times as fast when converted to use ASI.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cco,
  author       = "Eduardo Krell and Balachander Krishnamurthy",
  title        = "{COLA}: Customied Overlaying",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "3--7",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "",
  abstract     = "System calls are the basic building blocks for writing
    programs in the UNIX operating system.  From the canonical read, write,
    open, close, seek, ...  to the more obscure ones, programs have been
    written to use system calls in a variety of ways.  Often there is a need to
    intercept a few system calls to perform some special task.  Given that it
    is hard to go below the level of system calls and still write portable
    programs, it is easy to see the need for intercepts at the system call
    level.  A simple example of a useful interception facility is a library
    that watches for file creation and modifications.  In this paper we
    describe COLA, an elegant, customizable and dynamic facility to overlay a
    variety of system call intercepts.  With COLA, users can specify an
    arbitrary number of system call filters, each of which may intercept
    different system calls and perform different actions upon interception.
    The set of overlaying filters can be modified at any time during the
    session.  A program run under COLA will have any of the filtered system
    calls processed at each layer before control is passed on to the next
    layer.  The final layer always is the standard UNIX system call layer.
    System calls not intercepted by any of the overlaying filters will execute
    transparently.  No recompilation of programs or static relinking is
    necessary.  It should be noted that this facility depends on availability
    of shared libraries.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{abp,
  author       = "Susan~L. Graham and Steven Lucco and Robert Wahbe",
  title        = "Adaptable Binary Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "315--325",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "disassembly, control-flow, relocation, register management",
  abstract     = "To accurately and comprehensively monitor a program's
    behavior, many performance measurement tools transform the program's
    executable representation or binary.  By instrumenting binary programs to
    monitor program events, tools can precisely analyze compiler optimization
    effectiveness, memory system performance, pipeline interlocking, and other
    dynamic program characteristics that are fully exposed only at this level.
    Binary transformation has also been used to support software-enforced fault
    isolation, debugging, machine re-targeting, and machine-dependent
    optimization.At present, binary transformation applications face a
    difficult trade-off.  Previous approaches to implementing robust
    transformations result in significant disk space and run-time overhead.  To
    improve efficiency, some current systems sacrifice robustness, relying on
    heuristic assumptions about the program and recognition of
    compiler-dependent code generation idioms.  In this paper we begin by
    investigating the run-time and disk space overhead of transformation
    strategies that do not require assumptions about the program's control flow
    or register usage.  We then detail simple information about the binary
    program that can significantly reduce this overhead.  For each type of
    information, we show how it enables a corresponding type of binary
    transformation.  We call binary programs that contain such enabling
    information adaptable binaries.  Because adaptable binary information is
    simple, any compiler can generate it.  Despite its simplicity, adaptable
    binary information has the necessary and sufficient expressive power to
    support a rich set of binary transformations.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/adaptable-binary-programs"
}

@InProceedings{aafifbhppat95,
  author       = "Eustace, Alan and Srivastava, Amitabh",
  title        = "{ATOM}, {A} Flexible Interface for Building High-Peformance Program Analysis Tools",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "303--314",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cache simulator, instruction profiling, analysis tools,
    compiler auditing",
  abstract     = "ATOM (Analysis Tools with OM) is a single framework for
    building a wide range of customized program analysis tools.  It provides
    the common infrastructure present in all code-instrumenting tools; this is
    the difficult and time-consuming part.  The user simply defines the
    tool-specific details in instrumentation and analysis routines.  Building a
    basic block counting tool like Pixie with ATOM requires only a page of
    code.ATOM, using OM link-time technology, organizes the final executable
    such that the application program and user's analysis routines run in the
    same address space.  Information is directly passed from the application
    program to the analysis routines through simple procedure calls instead of
    inter-process communication or files on disk.  ATOM takes care that
    analysis routines do not interfere with the program's execution, and
    precise information about the program is presented to the analysis routines
    at all times.  ATOM uses no simulation or interpretation.  ATOM has been
    implemented on the Alpha AXP under OSF/1.  It is efficient and has been
    used to build a diverse set of tools for basic block counting, profiling,
    dynamic memory recording, instruction and data cache simulation, pipeline
    simulation, evaluating branch prediction, and instruction scheduling.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/atom-flexible-interface-building-high-performance",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-44.html"
}

@InProceedings{ltcuu,
  author       = "James~S. Plank and Micah Beck and Gerry Kingsley and Kai Li",
  title        = "Libckpt:  Transparent Checkpointing under " # unix,
  booktitle    = usenix95,
  year         = 1995,
  pages        = "213--224",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "checkpointing, optimizations, fault tolerance",
  abstract     = "Checkpointing is a simple technique for rollback recovery:
    the state of an executing program is periodically saved to a disk file from
    which it can be recovered after a failure.  While recent research has
    developed a collection of powerful techniques for minimizing the overhead
    of writing checkpoint files, checkpointing remains unavailable to most
    application developers.  In this paper we describe libckpt, a portable
    checkpointing tool for Unix that implements all applicable performance
    optimizations which are reported in the literature.  While libckpt can be
    used in a mode which is almost totally transparent to the programmer, it
    also supports the incorporation of user directives into the creation of
    checkpoints.  This user-directed checkpointing is an innovation which is
    unique to our work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/libckpt-transparent-checkpointing-under-unix"
}

@InProceedings{otpodlp,
  author       = "W.~Wilson Ho and Wei-Chau Chang and Lilian~H. Leung",
  title        = "Optimizing the Performance of Dynamically-Linked Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "225--233",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indirect addressing, data structures, procedure
    repositioning",
  abstract     = "Dynamically-linked programs in general do not perform as well
    as statically-linked programs.  This paper identifies three main areas that
    account for the performance loss.  First, symbols are referenced indirectly
    and thus extra instructions are required.  Second, the overhead in run-time
    symbol resolution is significant.  Third, poor locality of functions in
    shared libraries and data structures maintained by the run-time linker may
    result in poor memory utilization.  This paper presents new optimization
    techniques we developed that address these three areas and significantly
    improve the performance of dynamically-linked programs.  Also, we provide
    measurements of the performance improvement achieved.  Most importantly, we
    show that all desirable features of shared libraries can be achieved
    without sacrificing performance.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267430"
}

@InProceedings{dalfbprda,
  author       = "David~M. Arnow",
  title        = "{DP}:  {A} Library for Building Portable, Reliable Distributed Applications",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "235--247",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed programming, libraries, process management,
    communication",
  abstract     = "DP is a library of process management and communication tools
    for writing portable, reliable distributed applications.  It provides
    support for a flexible set of message operations as well as process
    creation and management.  It has been successfully used in developing
    distributed Monte Carlo, disjunctive programming and integer goal
    programming codes.It differs from PVM and similar libraries in its support
    for lightweight, unreliable messages, as well as asynchronous delivery of
    interrupt-generating messages.  In addition, DP supports the development of
    long-running distributed applications tolerant to the failure or loss of a
    subset of its processors.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/arnow.html"
}

@InProceedings{fslvcapc,
  author       = "Margo Seltzer and Keith~A. Smith and Hari Balakrishnan and Jacqueline Chang and Sara McMains and Venkata Padmanabhan",
  title        = "File System Logging Versus Clustering:  {A} Performance Comparison",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "249--264",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "fast file system, log structured file system, i-o
    performance",
  abstract     = "The Log-structured File System (LFS), introduced in 1991 [8],
    has received much attention for its potential order-of-magnitude
    improvement in file system performance.  Early research results [9] showed
    that small file performance could scale with processor speed and that
    cleaning costs could be kept low, allowing LFS to write at an effective
    bandwidth of 62 to 83% of the maximum.  Later work showed that the presence
    of synchronous disk operations could degrade performance by as much as 62%
    and that cleaning overhead could become prohibitive in transaction
    processing workloads, reducing performance by as much as 40% [10].  The
    same work showed that the addition of clustered reads and writes in the
    Berkeley Fast File System [6] (FFS) made it competitive with LFS in
    large-file handling and software development environments as approximated
    by the Andrew benchmark [4].These seemingly inconsistent results have
    caused confusion in the file system research community.  This paper
    presents a detailed performance comparison of the 4.4BSD Log-structured
    File System and the 4.4BSD Fast File System.  Ignoring cleaner overhead,
    our results show that the order-of-magnitude improvement in performance
    claimed for LFS applies only to meta-data intensive activities,
    specifically the creation of files one-kilobyte or less and deletion of
    files 64 kilobytes or less.For small files, both systems provide comparable
    read performance, but LFS offers superior performance on writes.  For large
    files (one megabyte and larger), the performance of the two file systems is
    comparable.  When FFS is tuned for writing, its large-file write
    performance is approximately 15% better than LFS, but its read performance
    is 25% worse.  When FFS is optimized for reading, its large-file read and
    write performance is comparable to LFS.Both LFS and FFS can suffer
    performance degradation, due to cleaning and disk fragmentation
    respectively.  We find that active FFS file systems function at
    approximately 85-95% of their maximum performance after two to three years.
    We examine LFS cleaner performance in a transaction processing environment
    and find that cleaner overhead reduces LFS performance by more than 33%
    when the disk is 50% full.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267432"
}

@InProceedings{mlians,
  author       = "Uresh Vahalia and Cary~G. Gray and Dennis Ting",
  title        = "Metadata Logging in an {NFS} Server",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "265--276",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "log-structured file systems, nfs, file-system metadata, fault
    tolerance, crash recovery, ",
  abstract     = "Over the last few years, there have been several efforts to
    use logging to improve performance, reliability, and recovery times of file
    systems.  The two major techniques are metadata logging, where the log
    records metadata changes and is a supplement to the on-disk file system,
    and log-structured file systems, whose log is their only on-disk
    representation.  When the file system is mainly or wholly accessed through
    the Network File System (NFS) protocol, it adds new considerations to the
    suitability of the logging technique.  NFS requires that all operations be
    updated to stable storage before returning.  As a result, file system
    implementations that were effective for local access may perform poorly on
    an NFS server.  This paper analyzes the issues regarding the use of logging
    on an NFS server, and describes an implementation of a BSD Fast File System
    (FFS) with metadata logging that performs effectively for a dedicated NFS
    server.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267433", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/metadata-logging-nfs-server"
}

@InProceedings{hcailsfs,
  author       = "Trevor Blackwell and Jeffrey Harris and Margo Seltzer",
  title        = "Heuristic Cleaning Algorithms in Log-Structured File Systems",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "277--288",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "garbage collection, log-structured file systems,
    benchmarking, performance, trace-driven analysis, scheduling",
  abstract     = "Research results show that while Log-Structured File Systems
    (LFS) offer the potential for dramatically improved file system
    performance, the cleaner can seriously degrade performance, by as much as
    40% in transaction processing workloads [9].  Our goal is to examine trace
    data from live file systems and use those to derive simple heuristics that
    will permit the cleaner to run without interfering with normal file access.
    Our results show that trivial heuristics perform very well, allowing 97% of
    all cleaning on the most heavily loaded system we studied to be done in the
    background.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/blackwell.html", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267434"
}

@InProceedings{muuogoos,
  author       = "J.~Mark Stevenson and Daniel~P. Julin",
  title        = "Mach-{US}: " # unix # " on Generic {OS} Object Servers",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "119--130",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "unix, apis, mach, object-oriented design, ipc",
  abstract     = "This paper examines the Mach-US operating system, its unique 
    architecture, and the lessons demonstrated through its implementation.
    Mach-US is an object-oriented multi-server OS which runs on the Mach3.0
    kernel.  Mach-US has a set of separate servers supplying orthogonal OS
    services and a library which is loaded into each user process.  This
    library uses the services to generate the semantics of the Mach2.5/4.3BSD
    application programmers interface (API).  This architecture makes Mach-US a
    flexible research platform and a powerful tool for developing and examining
    various OS service options.  We will briefly describe Mach-US, the
    motivations for its design choices, and its demonstrated strengths and
    weaknesses.  We will then discuss the insights that we've acquired in the
    areas of multi-server architecture, OS remote method invocation, Object
    Oriented technology for OS implementation, API independent OS services,
    UNIX API re-implementation, and smart user-space API emulation libraries.", 
  location     = "https://www.researchgate.net/publication/2811091_Mach-US_UNIX_on_generic_OS_object_servers"
}

@InProceedings{eiarbds,
  author       = "Jim Waldo and Ann Wollrath and Geoff Wyant and Samuel~C. Kendall",
  title        = "Events in an {RPC} Based Distributed System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "131--142",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "events, publish-subscribe, ipc, corba, interface
    polymorphism, third-party servers",
  abstract     = "We show how to build a distributed system allowing objects to
    register interest in and receive notifications of events in other objects.
    The system is built on top of a pair of interfaces that are interesting
    only in their extreme simplicity.  We then present a simple and efï¬cient
    implementation of these interfaces.  We then show how more complex
    functionality can be introduced to the system by adding third-party
    services.  These services can be added without changing the simple
    interfaces, and without changing the objects in the system that do not need
    the functionality of those services.  Finally, we note a number of open
    issues that remain, and attempt to draw some conclusions based on the
    work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/events-rpc-based-distributed-system"
}

@InProceedings{ttaosiamco,
  author       = "Jacques Talbot",
  title        = "Turning the {AIX} Operating System into an {MP}-capable {OS}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "143--153",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "powerscale, hardware architecture, atomic operations, caches,
  interrupt handling, locks, deadlocks, debugging, affinity scheduling",
  abstract     = "This paper describes those MP features that Bull and IBM
    together introduced into the AIX operating system to support the Symmetric
    Multiprocessor machine marketed by Bull under the Escala name and by IBM
    under the RS/6000 Models G30, J30 and R30 names.  The PowerPC architecture
    and the AIX operating system present some specific challenges.  We present
    the major problems encountered and how they were solved.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/turning-aix-operating-system-mp-capable-os"
}

@InProceedings{afmbfs,
  author       = "Atsuo Kawaguchi and Shingo Nishioka and Hiroshi Motoda",
  title        = "{A} Flash-Memory Based File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "155--164",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "flash memory, file systems, ",
  abstract     = "A flash memory device driver that supports a conventional
    UNIX file system transparently was designed.  To avoid the limitations due
    to flash memory's restricted number of write cycles and its inability to be
    overwritten, this driver writes data to the flash memory system
    sequentially as a Log-structured File System (LFS) does and uses a cleaner
    to collect valid data blocks and reclaim invalid ones by erasing the
    corresponding flash memory regions.  Measurements showed that the overhead
    of the cleaner has little effect on the performance of the prototype when
    utilization is low but that the effect becomes critical as the utilization
    gets higher, reducing the random write throughput from 222 Kbytes/s at 30%
    utilization to 40 Kbytes/s at 90% utilization.  The performance of the
    prototype in the Andrew Benchmark test is roughly equivalent to that of the
    4.4BSD Pageable Memory based File System (MFS).", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flash-memory-based-file-system"
}

@InProceedings{tpsfpftuos,
  author       = "Andrew Berman and Virgil Bourassa and Erik Selberg",
  title        = "{TRON}:  Process-Specific File Protection for the " # unix # " Operating System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "165--175",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "capabilities, protection domains",
  abstract     = "The file protection mechanism provided in UNIX is
    insufficient for current computing environments.  While the UNIX file
    protection system attempts to protect users from attacks by other users, it
    does not directly address the agents of destruction-executing processes.
    As computing environments become more interconnected and interdependent,
    there is increasing pressure and opportunity for users to acquire and test
    non-secure, and possibly malicious, software.We introduce TRON, a
    process-level discretionary access control system for UNIX.  TRON allows
    users to specify capabilities for a process' access to individual files,
    directories, and directory trees.  These capabilities are enforced by
    system call wrappers compiled into the operating system kernel.  No
    privileged system calls, special files, system administrator intervention,
    or changes to the file system are required.  Existing UNIX programs can be
    run without recompilation under TRON-enhanced UNIX.  Thus, TRON improves
    UNIX security while maintaining current standards of flexibility and
    openness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/tron-process-specific-file-protection-unix-operating"
}

@InProceedings{satfwaid,
  author       = "Tak Yan and Hector Garcia-Molina",
  title        = "{SIFT} --- {A} Tool for Wide-Area Information Dissemination",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "177--186",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indexing, user interest, information filtering",
  abstract     = "The dissemination model is becoming increasingly important in 
    wide-area information system.  In this model, the user subscribes to an
    information dissemination service by submitting profiles that describe his
    interests.  He then passively receives new, filtered information.  The
    Stanford Information Filtering Tool (SIFT) is a tool to help provide such
    service.  It supports full-text filtering using well-known information
    retrieval models.  The SIFT filtering engine implements novel indexing
    techniques, capable of processing large volumes of information against a
    large number of profiles.  It runs on several major Unix platforms and is
    freely available to the public.  In this paper we present SIFT's approach
    to user interest modeling and user-server communication.  We demonstrate
    the processing capability of SIFT by describing a running server that
    disseminates USENET News.  We present an empirical study of SIFT's
    performance, examining its main memory requirement and ability to scale
    with information volume and user population.", 
  location     = "http://ilpubs.stanford.edu/73/1/1994-7.pdf"
}

@InProceedings{tfsbitk,
  author       = "Brent Welch",
  title        = "The File System Belongs in the Kernel",
  booktitle    = pot # "Second USENIX Mach Symposium",
  year         = 1991,
  pages        = "233--250",
  address      = "Monterey, " # CA,
  month        = "20--22 " # nov,
  keywords     = "os architecture, file systems, sprite, mach, naming, name
  spaces, microkernels",
  abstract     = "This paper argues that a shared, distributed name space and
    I/O interface should be implemented inside the operating system kernel.
    The grounding for the argument is a comparison between the Sprite network
    operating system and the Mach microkernel.  Sprite optimizes the common
    case of file and device access, both local and remote, by providing a
    kernel-level implementation.  Sprite also allows for user-level
    extensibility by letting a user-level process implement the naming and I/O
    interfaces of the file system.  Mach, in contrast, provide general
    interprocess communication and does not define a file system protocol in
    the kernel.", 
  location     = "http://ftp.dk.netbsd.org/pub/doc/OS/Sprite/welch.filesys.ps.Z"
}

@InProceedings{piomps,
  author       = "Jeffrey~C. Mogul and Joel~F. Bartlett and Robert~N. Mayo and Amitabh Srivastava",
  title        = "Performance Implications of Multiple Pointer Sizes",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "187--200",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "performance, address spaces, tlb",
  abstract     = "Many users need 64-bit architectures: 32-bit systems cannot
    support the largest applications, and 64-bit systems perform better for
    some applications.  However, performance on some other applications can
    suffer from the use of large pointers; large pointers can also constrain
    feasible problem size.  Such applications are best served by a 64-bit
    machine that supports the use of both 32-bit and 64-bit pointer variables.
    This paper analyzes several programs and programming techniques to
    understand the performance implications of different pointer sizes.  Many
    (but not all) programs show small but definite performance consequences,
    primarily due to cache and paging effects.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/performance-implications-multiple-pointer-sizes"
}

@InProceedings{dcswmbs,
  author       = "Steven~W. Schlosser and John Linwood Griffin and David~F. Nagle and Gregory~R. Ganger",
  title        = "Designing Computer Systems with {MEMS}-based Storage",
  booktitle    = asplos00,
  year         = 2000,
  pages        = "1--12",
  address      = boma,
  month        = "9--13" # oct,
  keywords     = "mems, non-volatile storage, semi-conductor technology",
  abstract     = "For decades the RAM-to-disk memory hierarchy gap has plagued
    computer architects.  An exciting new storage technology based on
    microelectromechanical systems (MEMS) is poised to fill a large portion of
    this performance gap, significantly reduce system power consumption, and
    enable many new applications.  This paper explores the system-level
    implications of integrating MEMS-based storage into the memory hierarchy.
    Results show that standalone MEMS-based storage reduces I/O stall times by
    4-74X over disks and improves overall application runtimes by 1.9-4.4X.
    When used as on-board caches for disks, MEMS-based storage improves I/O
    response time by up to 3.5X.  Further, the energy consumption of MEMS-based
    storage is 10-54X less than that of state-of-the-art low-power disk drives.
    The combination of the high-level physical characteristics of MEMS-based
    storage (small footprints, high shock tolerance) and the ability to
    directly integrate MEMS-based storage with processing leads to such new
    applications as portable gigabit storage systems and ubiquitous active
    storage nodes.",  
  location     = "https://doi.org/10.1145/378993.378996"
}

@InProceedings{iins95,
  author       = "Richard Golding and Peter Bosch and Carl Staelin and Tim Sullivan and John Wilkes",
  title        = "Idleness is not Sloth",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "201--212",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cycle harvesting, idle-time processing, process dispatch,
    idle detection",
  abstract     = "Many people have observed that computer systems spend much of
    their time idle, and various schemes have been proposed to use this idle
    time productively.  The commonest approach is to off-load activity from
    busy periods to less-busy ones in order to improve system responsiveness.
    In addition, speculative work can be performed in idle periods in the hopes
    that it will be needed later at times of higher utilization, or
    non-renewable resource like battery power can be conserved by disabling
    unused resources.We found opportunities to exploit idle time in our work on
    storage systems, and after a few attempts to tackle specific instances of
    it in ad hoc ways, began to investigate general mechanisms that could be
    applied to this problem.  Our results include a taxonomy of idle-time
    detection algorithms, metrics for evaluating them, and an evaluation of a
    number of idleness predictors that we generated from our taxonomy.", 
  location     = "https://john.e-wilkes.com/papers/idleness.pdf"
}

@InProceedings{eodafacfs,
  author       = "Murthy Devarakonda and Ajay Mohindra and Jill Simoneaux and William~H. Tetzlaff",
  title        = "Evaluation of Design Alternatives for a Cluster File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "35--46",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cluster computing, token-based consistency, disk sharing,
    file-system sharing, journaling file systems",
  abstract     = "Based on implementation experience and measurements, this
    paper presents an evaluation of design alternatives for a cluster file
    system.  The file system is targeted for IBM cluster systems, Scalable
    POWERparallel and AIX HACMP/6000.  We considered a shared disk approach
    where serialized, multiple instances of a single-system file system
    directly access file data as disk blocks, and a shared system approach
    which is the conventional method of distributing file system function
    between client and server.  We conclude the shard disk approach suffers
    form the difficulties serializing metadata, poor write-sharing performance,
    and low read throughput.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/evaluation-design-alternatives-cluster-file-system"
}

@InProceedings{mraaaims,
  author       = "Jonathan~S. Goldick and Kathy Benninger and Christopher Kirby and Christopher Maher and Bill Zumach",
  title        = "Multi-Resident {AFS}: An Adventure in Mass Storage",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "47--58",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed file systems, mass storage, afs",
  abstract     = "The Pittsburgh Supercomputing Center has been working to
    integrate distributed file system technology with hierarchical mass
    storage.  We produced a system utilizing the Andrew File System that can be
    interfaced to many mass storage systems.  We retained the semantics of AFS
    and compatibility with standard clients and servers.  The architecture has
    a logical separation between the facility that provides the user interface
    and access semantics and the management of the storage systems that contain
    user data.  Support for file level replication is provided for high
    availability to data in a fashion that is transparent to users.  This
    system is called Multi-Resident AFS.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/multi-resident-afs-adventure-mass-storage", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267416"
}

@InProceedings{reatahbmpfs,
  author       = "Ethan~L. Miller and Randy~H. Katz",
  title        = "{RAMA}:  Easy Access to a High-Bandwidth Massively Parallel File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "59--69",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "",
  abstract     = "Massively parallel file systems must provide high bandwidth
    file access to programs running on their machines.  Most accomplish this
    goal by striping files across arrays of disks attached to a few specialized
    I/O nodes in the massively parallel processor (MPP).  This arrangement
    requires programmers to give the file system many hints on how their data
    is to be laid out on disk if they want to achieve good performance.
    Additionally, the custom interface makes massively parallel file systems
    hard for programmers to use and difficult to seamlessly integrate into an
    environment with workstations and tertiary storage.The RAMA file system
    addresses these problems by providing a massively parallel file system that
    does not need user hints to provide good performance.  RAMA takes advantage
    of the recent decrease in physical disk size by assuming that each
    processor in an MPP has one or more disks attached to it.  Hashing is then
    used to pseudo-randomly distribute data to all of these disks, insuring
    high bandwidth regardless of access pattern.  Since MPP programs often have
    many nodes accessing a single file in parallel, the file system must allow
    access to different parts of the file without relying on a particular node.
    In RAMA, a file request involves only two nodes -- the node making the
    request and the node on whose disk the data is stored.  Thus, RAMA scales
    well to hundreds of processors.  Since RAMA needs no layout hints from
    applications, it fits well into systems where users cannot (or will not)
    provide such hints.  Fortunately, this flexibility does not cause a large
    loss of performance.  RAMA's simulated performance is within 10-15% of the
    optimum performance of a similarly-sized striped file system, and is a
    factor of 4 or more better than a striped file system with poorly laid out
    data.",
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/rama-easy-access-high-bandwidth-massively-parallel-file",
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267417"
}

@InProceedings{irtpfpus,
  author       = "Ian Wakeman and Atanu Ghosh and Jon Crowcroft and Van Jacobson and Sally Floyd",
  title        = "Implementing Real-Time Packet Forwarding Policies Using {Streams}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "71--82",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "link sharing, diff serve, queue scheduling, streams, packet
    forwarding",
  abstract     = "This paper describes an implementation of the class based
    queueing (CBQ) mechanisms proposed by Sally Floyd and Van Jacobson to
    provide real time policies for packet forwarding.  CBQ allows the traffic
    flows sharing a data link to be guaranteed a share of the bandwidth when
    the link is congested, yet allows flexible sharing of the unused bandwidth
    when the link is unloaded.  In addition, CBQ provides mechanisms which give
    flows requiring low delay priority over other flows.  In this way, links
    can be shared by multiple flows yet still meet the policy and Quality of
    Service (QoS) requirements of the flows.We present a brief description of
    the implementation and some preliminary preformance measurements.  The
    problems of packet classification are addressed in a flexible and
    extensible, yet efficient manner, and whilst the Streams implementation
    cannot cope with very high speed interfaces, it can cope with the serial
    link speeds that are likely to be loaded.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/implementing-real-time-packet-forwarding-policies-using", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267418"
}

@InProceedings{stwotckaptplsa,
  author       = "Jeffrey I. Schiller and Derek Atkins",
  title        = "Scaling the {Web of Trust}:  Combining {Kerberos} and {PGP} to Provide Large Scale Authentication",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "83--94",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "e-mail, pgp, public key cryptography, kerberos, web of trust,
    authentication",
  abstract     = "Internet Security has become more important recently as the
    Internet grows exponentially and security breaches become more publicized.
    An important area of concern for many Internet users is the privacy and
    integrity of their electronic files and messages.  Phil Zimmermann's Pretty
    Good Privacy (PGP) provides a general purpose utility for file and message
    protection.  However PGP requires that communicating users be 'introduced'
    to each other.  This paper describes a scheme that permits an enterprise
    using Kerberos to create an automated introducer called the PGP Key Signer
    Service.  Using this service people in the enterprise who have no common
    acquaintances to act as introducers can be introduced through the Key
    Signer.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/scaling-web-trust-combining-kerberos-and-pgp-provide"
}

@InProceedings{fasrofc,
  author       = "Puneet Kumar and M.~Satyanarayanan",
  title        = "Flexible and Safe Resolution of File Conflicts",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "95--106",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "coda file system, conflict resolution, consistency",
  abstract     = "In this paper we describe the support provided by the Coda
    File System for transparent resolution of conflicts arising from concurrent
    updates to a file in different network partitions.  Such partitions often
    occur in mobile computing environments.  Coda provides a framework for
    invoking customized pieces of code called application-specific resolvers
    (asrs) that encapsulate the knowledge needed for file resolution.  If
    resolution succeeds, the user notices nothing more than a slight
    performance delay.  Only if resolution fails does the user have to resort
    to manual repair.  Our design combines a rule-based approach to ASR
    selection with transactional encapsulation of ASR execution.  This paper
    shows how such an approach leads to flexible and efficient file resolution
    without loss of security or robustness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flexible-and-safe-resolution-file-conflicts", 
  location     = "https://dl.acm.org/citation.cfm?id=1267419"
}

@InProceedings{oacfftodce,
  author       = "John Dilley",
  title        = "{OODCE}: {A} {C}++ Framework for the {OSF Distributed Computing Environment}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "107--118",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "dce, distributed programming",
  abstract     = "This paper presents a method for developing object-oriented
    distributed applications using the C++ and DCE technologies.  The core of
    this package is a DCE IDL-to-C++ compiler and a set of C++ classes
    providing easy access to DCE functionality.  Using this approach we were
    able to develop more object-oriented distributed applications, and saw a
    significant decrease in application code size.  This contributed to an
    increase in developer productivity and code maintainability.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/oodce-c-framework-osf-distributed-computing-environment"
}

@InProceedings{umi44l,
  author       = "Jan-Simon Pendry and Marshall Kirk McKusick",
  title        = "Union Mounts in 4.{4BSD}-Lite",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "25--33",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "file systems, union file mounts, vnodes",
  abstract     = "This paper describes the design and rationale behind union
    mounts, a new filesystem-namespace management tool available in
    4.4BSD-Lite.  Unlike a traditional mount that hides the contents of the
    directory on which it is placed, a union mount presents a view of a merger
    of the two directories.  Although only the filesystem at the top of the
    union stack can be modified, the union filesystem gives the appearance of
    allowing any- thing to be deleted or modified.  Files in the lower layer
    may be deleted with whiteout in the top layer.  Files to be modified are
    automatically copied to the top layer.  This new functionality makes
    possible several new applications including the ability to apply patches to
    a CD-ROM and eliminate symbolic links generated by an automounter.  Also
    possible is the provision of per- user views of the filesystem, allowing
    private views of a shared work area, or local builds from a centrally
    shared read-only source tree.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/mckusick.html"
}

@InProceedings{pi44,
  author       = "W.~Richard Stevens and Jan-Simon Pendry",
  title        = "Portals in {4.4BSD}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "1--10",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "remote access, portals",
  abstract     = "Portals were added to 4.4BSD as an experimental feature and
    are in the publicly available 4.4BSD-Lite distribution.  Portals provide
    access to alternate file types or devices using names in the normal
    filesystem that a process just opens.  For example, an open of
    /p/tcp/foo.com/smtp returns a TCP socket descriptor to the calling process
    that is connected to the SMTP server on the specified host.  By providing
    access through the normal filesystem, the calling process need not be aware
    of the special functions necessary to create a TCP socket and establish a
    TCP connection.  This makes TCP connections, for example, available to
    programs such as Awk, Tcl, and shell scripts.  This paper describes the
    implementation of portals in 4.4BSD as another type of filesystem and
    provides some examples.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/portals-44bsd"
}

@InProceedings{dvdai,
  author       = "Aju John",
  title        = "Dynamic {Vnodes} --- Design and Implementation",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "11--23",
  organization = "USENIX Association",
  address      = nola,
  month        = "11--23 " # jan,
  keywords     = "osf, dynamic storage management, coordinated timeouts",
  abstract     = "Dynamic vnodes make the UNIX kernel responsive to a varying
    demand for vnodes, without a need to rebuild the kernel.  It also optimizes
    the usage of memory by deallocating excess vnodes.  This paper describes
    the design and implementation of dynamic vnodes in DEC OSF/1 V3.0.  The
    focus is on the vnode deallocation logic in a Symmetric Multi-Processing
    environment.Deallocation of vnodes differs from the familiar concept of
    dynamically allocated data structures in the following ways: the legacy
    name-cache design implicitly assumes that vnodes are never deallocated, and
    the vnode free-list needs to cache unused vnodes effectively.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/dynamic-vnodes-design-and-implementation"
}

@InProceedings{pitccl,
  author       = "Vijay Saraswat and Radha Jagadeesan and Vinheet Gupta",
  title        = "Programming in Timed Concurrent Constraint Languages",
  booktitle    = "Constraint Programming",
  year         = 1994,
  editor       = "B.~Mayoh and E.~Tyugu and J.~Penjam",
  series       = "NATO ASI Series (Series F: Computer and Systems Sciences)",
  volume       = 131,
  publisher    = sv,
  address      = begr,
  keywords     = "synchronous programming, operational semantics, constraint
    systems, negative information, parallel composition",
  abstract     = "The areas of Qualitative Reasoning about physical systems
    (Weld and de Kleer 1989), reasoning about action and state change (Ginsberg
    1987), reactive, realtime computing (Real-time systems 1991) and concurrent
    programming languages (Milner 1980; Hoare 1985) are areas of inquiry that
    are fundamentally about the same subject matter â the representation,
    design and analysis of continuous and discrete dynamical systems.", 
  location     = "https://doi.org/10.1007/978-3-642-85983-0_15"
}

@InProceedings{srmc,
  author       = "Diomidis Spinellis",
  title        = "Software Reliability: Modern Challenges",
  booktitle    = "Proceedings ESRL '99 --- The Tenth European Conference on Safety and Reliability",
  year         = 1999,
  editor       = "G.~I. Schu{\" e}ller and P.~Kafka",
  pages        = "589--592",
  address      = "Munich-Garching, Germany",
  month        = sep,
  keywords     = "hardware, operating systems, software system architecture,
    programming languages, software development",
  abstract     = "The evolution of computer technology is creating for
    safety-critical systems new challenges and different types of failure
    modes.  Modern computer processors are often delivered with errors, while
    intelligent hardware subsystems may exhibit nondeterministic behaviour.
    Operating systems and programming languages are becoming increasingly
    complicated and their implementations less trustworthy.  In addition,
    component-based multi-tier software system architectures exponentially
    increase the number of failure modes, while Internet connectivity exposes
    systems to malicious attacks.  Finally, IT outsourcing and blind reliance
    on standards can provide developers with a false sense of security.
    Planning in advance for the new challenges is as important as embracing the
    new technology.", 
  location     = "https://www2.dmst.aueb.gr/dds/pubs/conf/1999-ESREL-SoftRel/html/chal.html"
}

@InProceedings{daomflfc,
  author       = "Gokul~V. Subramaniam and Eric~J. Byrne",
  title        = "Deriving an Object Model from Legacy {Fortran} Code",
  booktitle    = pot # "1996 International Conference on Software Maintenance (ICSM '96)",
  year         = 1996,
  pages        = "3--12",
  publisher    = "IEEE Press",
  address      = moncal,
  month        = "4--8 " # nov,
  keywords     = "re-engineering, object models, object-oriented design",
  abstract     = "The practice of software development continues to shift
    towards the use of object-oriented approaches.  The motivation for this
    trend is the benefits attributed to object-oriented software, including
    improved maintainability.  As organizations develop new object-oriented
    software, they face the problem of maintaining their older software.  How
    can existing non-objected-oriented software benefit from this new software
    engineering technology? This paper presents a nine step process for
    deriving an object model from existing unstructured FORTRAN source code.
    Both top-down and bottom-up approaches are used to derive objects, classes,
    class attributes and methods, and relationships among classes.  This
    process can be used within a reengineering project to convert legacy
    FORTRAN code into a new object-oriented implementation written in a
    language such as C++.  Experience with using this process is also
    described.", 
  location     = "https://dl.acm.org/doi/10.5555/645544.655855"
}

@InProceedings{aaotdfpfcimc,
  author       = "Geoffrey~H. Kuenning and Gerald~J. Popek and Peter~L. Reiher",
  title        = "An Analysis of Trace Data for Predictive File Caching in Mobile Computing",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "291--303",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file caching, off-line execution, workloads, trace analysis,
    user behavior",
  abstract     = "One way to provide mobile computers with access to the resources of a network, even in the absence of communication, is to predict which information will be used during disconnection and cache the appropriate data while still connected.  To determine the feasibility of this approach, traces of file-access activity for three diverse application domains were collected for periods of over two months.  Analysis of these traces using traditional and new measures reveals that user working sets tend to be small compared to modern disk sizes, that users tend to reference the same files for several days or even weeks at a time, and that different users do not tend to write to the same file except in highly constrained circumstances.  These factors encourage the conclusion that an automated caching system can be built for a wide variety of environments.",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267277",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/analysis-trace-data-predictive-file-caching"
}

@InProceedings{sscrfmi,
  author       = "Trevor Blackwell and Kee Chan and Koling Chang and Thomas Charuhas and James Gwertzman and Brad Karp and H.~T. Kung and W.~David Li and Dong Lin and Robert Morris and Robert Polansky and Diane Tang and Cliff Young and John Zao",
  title        = "Secure Short-Cut Routing for Mobile {IP}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "305--316",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "ip, mobility, routing, security, encapsulation, forwarding,
    short-cut routing",
  abstract     = "This paper describes the architecture and implementation of a mobile IP system.  It allows mobile hosts to roam between cells implemented with 2-Mbps radio base stations, while maintaining Internet connectivity.  The system is being developed as part of a course on wireless networks at Harvard and has been operational since March 1994.The architecture scales well, both geographically and in the number of mobile hosts supported.  It supports secure short-cut routing to mobile hosts using the existing Internet routing system without change.  The implementation demonstrates a robust, low complexity realization of the architecture, and provides trade-off opportunities between efficiency and cost.Measured performance of the mobile system is generally excellent.  The system can handle a high rate of location updates, and routes packets almost as efficiently for mobile hosts as the Internet does for stationary hosts.  We observe reasonable TCP behavior during hand-offs.",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/secure-short-cut-routing-mobile-ip",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267278"
}

@InProceedings{prtpopwtl,
  author       = "Arthur Bernstein and Paul~K. Harter",
  title        = "Proving Real-Time Properties of Programs with Temporal Logic",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "1--11",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "temporal logic, proof systems, real-time systems",
  abstract     = "Wirth [Wi77] categorized programs into three classes.  The
    most difficult type of program to understand and write is a real-time
    program.  Much work has been done in the formal verification of sequential
    programs, but much remains to be done for concurrent and real-time
    programs.  The critical nature of typical real-time applications makes the
    validity problem for real-time programs particularly important.  Owicki and
    Lamport [OL80] present a relatively new method for verifying concurrent
    programs using temporal logic.  This paper presents an extension of their
    work to the area of real-time programs.  A model and proof system are
    presented and their use demonstrated using examples from the literature.", 
  location     = "https://doi.org/10.1145/1067627.806585"
}

@InProceedings{davoss,
  author       = "J.~M. Rushby",
  title        = "Design and Verifiation of Secure Systems",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "12--21",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed systems, isolation, security, mechanism",
  abstract     = "This paper reviews some difficulties arising when verifying
    kernelized secure systems and suggests new techniques for their resolution.
    It proposes that secure systems be conceived as distributed systems in
    which security is achieved partly through the physical separation of its
    individual components and partly through the mediation of trusted functions
    performed within some of those components. The security kernel lets such a
    'distributed' system run within a single processor; policy enforcement is
    not the concern of a security kernel. This approach decouples component
    verification, which perform trusted functions, from security kernel
    verification.  This latter task may be accomplished by a new verification
    technique called 'proof of separability' which explicitly addresses the
    security relevant aspects of interrupt handling and other issues ignored by
    present methods.", 
  location     = "https://doi.org/10.1145/1067627.806586"
}

@InProceedings{ankjfb,
  author       = "Joel~F. Bartlett",
  title        = "{A} {NonStop} Kernel",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "22--29",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "redundancy, fault models, processes, message passing, error
    recovery", 
  abstract     = "The Tandem NonStop System is a fault-tolerant, expandable,
    and distributed computer system designed for online transaction processing.
    This paper describes the operating system kernel's key primitives.  The
    first section describes the basic hardware building blocks and introduces
    their software analogs: processes and messages.  Using these primitives, a
    mechanism allowing fault-tolerant resource access, the process-pair, is
    described.  The paper concludes with some observations on this type of
    system structure, and on system use.",
  location     = "https://doi.org/10.1145/1067627.806587",
  location     = "https://www.hpl.hp.com/techreports/tandem/TR-81.4.pdf"
}

@InProceedings{ootdoaos,
  author       = "Hugh~C. Lauer",
  title        = "Observations on the Development of an Operating System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "30--36",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "software development, the five-to-seven-year model, os
    classification, pilot, mesa, software management",
  abstract     = "The development of Pilot, an operating system for a personal
    computer, is reviewed, including a brief history and some of the problems
    and lessons encountered during this development.  As part of understanding
    how Pilot and other operating systems come about, an hypothesis is
    presented that systems can be classified into five kinds according to the
    style and direction of their development, independent of their structure.
    A further hypothesis is presented that systems such as Pilot, and many
    others in widespread use, take about five to seven years to reach maturity,
    independent of the quality and quantity of the talent applied to their
    development.  The pressures, constraints, and problems of producing Pilot
    are discussed in the context of these hypotheses.",
  location     = "https://doi.org/10.1145/800216.806588"
}

@InProceedings{tffsmf,
  author       = "Marek Fridrich and William~J. Older",
  title        = "The {Felix} File Server",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "37--44",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "consistency, file sets, transactions, resiliency",
  abstract     = "This paper describes Felix - a File Server for an
    experimental distributed multicomputer system.  Felix is designed to
    support a variety of file systems, virtual memory, and database
    applications with access being provided by a local area network.  Its
    interface combines block oriented data access with a high degree of crash
    resistance and a comprehensive set of primitives for controlling data
    sharing and consistency.  An extended set of access modes allows increased
    concurrency over conventional systems.", 
  location     = "https://doi.org/10.1145/800216.806589"
}

@Manual{ai43ict,
  title        = "An Introductory 4.{3BSD} Interprocess Communiation Tutorial",
  author       = "Stuart Sechrest",
  organization = csd # ucb,
  address      = beca,
  keywords     = "pipes, socket pairs",
  location     = "https://docs.freebsd.org/44doc/psd/20.ipctut/paper.pdf"
}

@Manual{prapirpca,
  title        = "{PI}-{RPC}:  {A} Platform-Independent Remote Procedure Call Architecture",
  author       = "Randy~J. Ray",
  year         = 2000,
  keywords     = "transport model, message architecture, rpc",
  abstract     = "This introduction of remote procedure call (RPC) services
    using XML and the encoding scheme simplified what was once a daunting
    aspect of distributed computing.  However, the current work stands in a
    frozen state for the sake of compliance with existing software
    implementations This proposal outlines a new model that builds on the
    existing basis of XML-RPC in a modular fashion, maintaining compatibility
    with the existing specification.", 
  location     = "http://www.blackperl.com/xml/PI-RPC.html"
}

@Manual{dcwlmtldhohs,
  title        = "Deplate --- Convert Wiki-like Markup to Latex, {Docbook, HTML}, or ``{HTML} Slides''",
  author       = "Thomas Link",
  year         = 2004,
  month        = aug,
  keywords     = "latex, html, docbook",
  location     = "http://deplate.sourceforge.net/"
}

@Misc{tfmcdp,
  author       = "Gopalan Suresh Raj",
  OPTtitle     = "The Factory Method (Creational) Design Pattern",
  howpublished = "Web page",
  year         = "2000",
  keywords     = "design patterns, factory, instance creation",
  location     = "http://gsraj.tripod.com/design/creational/factory/factory.html"
}

% Local Variables:
% eval: (set-register ?b "  booktitle    = usenixs94,\n  year         = 1994,\n  pages        = \"--\",\n  organization = \"USENIX Association\",\n  address      = boma,\n  month        = \"6--10 \" # jun,\n")
% End:
