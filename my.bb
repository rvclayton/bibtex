.so bibtex.header

@string{asplos91 = sigplan # " (" # pot # "Fourth International Conference on " # asplos # ", ASPLOS IV)"}
@string{asplos00 = sigplan # " (" # pot # "Ninth International Conference on " # asplos # ", ASPLOS IX)"}
@string{asplos92 = sigplan # " (" # pot # "Fifth International Conference on " # asplos # ", ASPLOS V)"}
@string{icfp02 = sigplan # " (" # pot # "Seventh ACM SIGPLAN International Conference on Functional Programming, ICFP '02)" }
@string{osdi96 = osr # " (" # pot # "Second USENIX Symposium on Operating Systems Design and Implementation, OSDI '96)"}
@string{ppeals88 = sigplan # " (" # pot # "ACM\slash SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems, PPEALS '88)"}
@string{pldi03 = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming
Language Design and Implementation, PLDI '03)"}
@string{sosp81    = osr # " (" # pot # "Eighth" # sosp # ", SOSP '81)"}
@string{usenixw92 = pot # "Winter 1992 USENIX Conference"}
@string{usenixs94 = pot # "USENIX Summer 1994 Technical Conference"}
@string{usenix95  = pot # "1995 USENIX Technical Conference"}
		  
		  
@Book{tcwekm,
  author       = "Ella~K. Maillart",
  title        = "The Cruel Way",
  subtitle     = "Switzerland to Afghanistan in a Ford, 1939",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "travel, middle east",
  location     = "DS 352.M18"
}

@Book{taeojl,
  author       = "Yves Beauchemin",
  title        = "The Accidental Education of Jerome Lupien",
  publisher    = "House of Anansi Press",
  year         = 2018,
  address      = "Canada",
  keywords     = "political machinations, lobbyists, con games",
  location     = "PS 8553.E172 E4813"
}

@Book{himt,
  author       = "Matt Taibbi",
  title        = "Hate Inc.",
  publisher    = "OR Books",
  year         = 2019,
  address      = nyny,
  keywords     = "media, propaganda, chomsky, maddow, journalism, the profit motive",
  location     = "9781949017250"
}

@Book{eolp,
  author       = "Christopher John Hogger",
  title        = "Essentials of Logic Programming",
  publisher    = oup,
  year         = 1990,
  series       = "Graduate Texts in Computer Science",
  address      = nyny,
  keywords     = "logic programming, first-order logic, causal-form logic,
    herbrand domain, sld resolution, definite program semantics, finite
    failure, program verification",
  location     = "QA 76.63 H64"
}

@Book{tmjc,
  author       = "Jorge Comensal",
  title        = "The Mutations",
  publisher    = fsg,
  year         = 2019,
  address      = nyny,
  keywords     = "cancer, survivorship, psychotherapy",
  location     = "PQ 7298.413.O438 M8813 "
}

@Book{ojhhdb,
  author       = "John~H. Halpern and David Blistein",
  title        = "Opium",
  subtitle     = "How an Ancient Flower Shaped and Poisoned Our World",
  publisher    = "Hachette",
  year         = 2019,
  address      = nyny,
  keywords     = "opium, addiction, poppies, drugs, medicine",
  location     = "HV 5816.H35"
}

@Book{tlpmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Laughing Policeman",
  publisher    = "Vintage",
  year         = 1977,
  price        = "$1.65",
  address      = nyny,
  keywords     = "murrdaar, doggedness, lone wolves",
  location     = "PT 9876.29.J63"
}

@Book{topsad,
  author       = "Edward Yourdon",
  title        = "Techniques of Program Structure and Design",
  publisher    = ph,
  year         = 1975,
  address      = ecnj,
  keywords     = "computer programs, top-down design, modular programming,
    structured programming, antidebugging, program testing, debugging,
    programming style",
  location     = "QA 76.6.6.Y68"
}

@Book{natb,
  author       = "Lars Iyer",
  title        = "Nietzsche and the Burbs",
  publisher    = "Melville House",
  year         = 2019,
  address      = "Brooklyn, N.Y.",
  keywords     = "suburban life, wasted youth",
  location     = ""
}

@Book{hwsts,
  author       = "Thomas Hockey",
  title        = "How We See the Sky",
  subtitle     = "A Naked-Eye Tour of Day and Night",
  publisher    = ucp,
  year         = 2011,
  address      = chil,
  keywords     = "the night sky, stars, planets, the moon",
  location     = "QB 44.3 H635"
}

@Book{htrm,
  author       = "Randall Munroe",
  title        = "How To",
  subtitle     = "Absurd Scientific Advice for Common Real-World Problems",
  publisher    = "Riverhead Books",
  year         = 2019,
  address      = nyny,
  keywords     = "jumping, pool parties, digging holes, piano playing,
    emergency landings, crossing rivers, moving, stability, lava moats,
    throwing, football, weather prediction, physics, algebra, modeling.",
  location     = ""
}

@Book{tnbcw,
  author       = "Colson Whitehead",
  title        = "The Nickel Boys",
  publisher    = "Doubleday",
  year         = 2019,
  address      = nyny,
  keywords     = "racism, florida, juvenile detention, the past",
  location     = "PS 3573.H4768 N53"
}

@Book{ystb,
  author       = "Wendy Lesser",
  title        = "You Say to Brick",
  subtitle     = "The Life of Louis Kahn",
  publisher    = fsg,
  year         = 2017,
  address      = nyny,
  keywords     = "louis kahn, american architecture",
  location     = "NA 737.K32 L48"
}

@Book{cnast,
  author       = "Andrew~S. Tanenbaum and David~J. Wetherall",
  title        = "Computer Networks",
  publisher    = ph,
  year         = 2011,
  address      = boma,
  keywords     = "iso osi stack, network security",
  location     = "TK 5105.5.T36"
}

@Book{tjs,
  author       = "John Suchet",
  title        = "Tchaikovsky",
  title        = "The Man Revealed",
  publisher    = "Pegasus Books",
  year         = 2018,
  address      = nyny,
  keywords     = "p.i. tchaikovsky, russian romanticism, music composition",
  location     = "ML 410.C4 S8"
}

@Book{bleh,
  author       = "Elizabeth Hand",
  title        = "Blacklight",
  publisher    = "HarperColins",
  year         = 1999,
  address      = nyny,
  keywords     = "occult, small-town life, into the weird",
  location     = "PS 3558.A4619 B53"
}

@Book{qsr,
  author       = "Salman Rushdie",
  title        = "Quichotte",
  publisher    = "Random House",
  year         = 2019,
  address      = nyny,
  keywords     = "quests, the near future, metafiction",
  location     = "PR 6068,U757 Q53"
}

@Book{gase,
  author       = "Shimon Even",
  title        = "Graph Algorithms",
  publisher    = "Computer Science Press",
  year         = 1979,
  address      = "Potomac, Maryland",
  keywords     = "graph theory, paths, trees, depth-first search, ordered
    trees, maximum flow, planar graphs, graph planarity tests, np completeness",
  location     = "QA 166.E93"
}

@Book{ckmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "Cop Killer",
  publisher    = "Vintage",
  year         = 1975,
  address      = nyny,
  price        = "$1.75",
  keywords     = "murrdaar, coincidences",
  location     = "PT 9876.29.J63"
}

@Book{teor,
  author       = "Stephen~P. Kershaw",
  title        = "The Enemies of Rome",
  subtitle     = "The Barbarian Rebellion Against the Roman Empire",
  publisher    = "Pegasus Books",
  year         = 2019,
  address      = nyny,
  keywords     = "ancient rome, warfare, barbarians",
  location     = ""
}

@Book{ldods,
  author       = "Arthur~D. Friedman",
  title        = "Logical Design of Digital Systems",
  publisher    = "Computer Science Press",
  year         = 1975,
  address      = "Woodland Hills, California",
  keywords     = "number systems, nondecimal arithmetic, codes, combinational
    circuits, sequential circuits, circuit design",
  location     = "TK 7868.S9 F74"
}

@Book{tcdtrhpcp,
  author       = "Peter Buse",
  title        = "The Camera Does the Rest",
  subtitle     = "How Polaroid Changed Photography",
  publisher    = ucp,
  year         = 2016,
  address      = chil,
  keywords     = "photography, polaroid, technology, history, marketing",
  location     = "TR 269.B87"
}

@Book{tlabt,
  author       = "Barbara Taylor",
  title        = "The Last Asylum",
  subtitle     = "A Memoir of Madness in Our Times",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "mental health, psychotherapy",
  location     = "RC 451.4.W6 T38"
}

@Book{awg,
  author       = "William Gibson",
  title        = "Agency",
  publisher    = "Berkley",
  year         = 2020,
  address      = nyny,
  keywords     = "travel broadens the mind, temporal meddling",
  location     = "PS 3557.I2264 A34"
}

@Book{dswj,
  author       = "John~R. Hubbard",
  title        = "Data Structures with {Java}",
  publisher    = "Schaum's Outline Series",
  year         = 2007,
  address      = nyny,
  keywords     = "java, data structures, oo programming, arrays, linked data
    structures, java collection framework, stacks, queues, lists, hash tables,
    recursion, trees, binary trees, search trees, heaps and priority queues,
    sorting, graphs, mathematics",
  location     = "QA 76.73.J38 H82"
}

@Book{gjg,
  author       = "James Gleick",
  title        = "Genius",
  subtitle     = "The Life and Science of Richard Feynman",
  publisher    = "Vintage Books",
  year         = 1993,
  address      = nyny,
  keywords     = "richard feynman, 2nd-half 20th century atomic physics, biography",
  location     = "QC 16.F49 G55"
}

@Book{hotd,
  author       = "Joe Meno",
  title        = "Hairstyles of the Damned",
  publisher    = "Punk Planet Books",
  year         = 2004,
  address      = chil,
  keywords     = "teenage wasteland, lurv",
  location     = "PS 3563.E53 H35"
}

@Book{afane,
  author       = "David~C. Korten",
  title        = "Agenda for a New Economy",
  subtitle     = "From Phantom Wealth to Real Wealth",
  publisher    = "Berrett-Koehler Publishers",
  year         = 2010,
  address      = sfca,
  keywords     = "economics, development, society, finance, ",
  location     = "HC 106.83.K67"
}

@Book{esaaf,
  author       = "\AE leen Frisch",
  title        = "Essential System Administration",
  publisher    = "O'Reilly \& Associates, Inc.",
  year         = 1991,
  address      = seca,
  keywords     = "unix, system administration, startup, shutdown, user
    accounts, security, automation, managing system resources, filesystems,
    disks, backup, restore, terminals, modems, printers, spooling, tcp/ip
    network management, accounting, bourne shell programming",
  location     = "QA 76.76.O63 F782"
}

@Book{ttcjd,
  author       = "Jared Diamond",
  title        = "The Third Chimpanzee",
  subtitle     = "The Evolution and Future of the Human Animal",
  publisher    = "Harper Perennial",
  year         = 1992,
  address      = nyny,
  keywords     = "human development, language, warfare, sex, evolution",
  location     = "GN 281.D53"
}

@Book{aese,
  author       = "William~C. Deitz",
  title        = "At Empire's Edge",
  publisher    = "Ace Books",
  year         = 2009,
  address      = nyny,
  keywords     = "empath cops, empath killer shape shifters, intergalactic
    corruption", 
  location     = "PS 3554.I388 A84"
}

@Book{ltzbee,
  author       = "Bret Easton Ellis",
  title        = "Less Than Zero",
  publisher    = "Vintage",
  year         = 1985,
  address      = nyny,
  keywords     = "sex, drugs, rock and roll, ennui",
  location     = "PS 3555.L5937 L4"
}

@Book{epekb,
  author       = "Kent Beck",
  title        = "Extreme Programming Explained",
  publisher    = aw,
  year         = 2000,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, economics, design",
  location     = "QA 76.76.D47 B434"
}

@Book{ttmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Terrorists",
  publisher    = "Vintage",
  year         = 1976,
  address      = nyny,
  keywords     = "terrorism",
  location     = "PT 9876.29.J63"
}

@Book{ttatb,
  author       = "David~P. Billington",
  title        = "The Tower and the Bridge",
  subtitle     = "The New Art of Structural Engineering",
  publisher    = pup,
  year         = 1983,
  address      = prnj,
  keywords     = "structural engineering, structural art",
  location     = "TA 636.B54"
}

@Book{lgew,
  author       = "Elizabeth Wilson",
  title        = "Love Game",
  subtitle     = "A History of Tennis from Victorian Pastime to Global Phenomenon",
  publisher    = ucp,
  year         = 2014,
  address      = chil,
  keywords     = "tennis, commerce, amateurism, professionalism",
  location     = "GV 992.W558"
}

@Book{waacf,
  author       = "Daryl Gregory",
  title        = "We are All Completely Fine",
  publisher    = "Tachyon",
  year         = 2014,
  address      = sfca,
  keywords     = "monsters, the talking cure",
  location     = "978-1-61696-173-5"
}

@Book{epirj,
  author       = "Ron Jeffries and Ann Anderson and Chet Hendrickson",
  title        = "Extreme Programming Installed",
  publisher    = aw,
  year         = 2001,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, extreme programming, experience, practice",
  location     = "QA 76.76 D47 J44"
}

@Book{epewcw,
  author       = "William~C. Wake",
  title        = "Extreme Programming Explored",
  publisher    = aw,
  year         = 2002,
  series       = "The XP Series",
  address      = boma,
  keywords     = "extreme programming, software development, programming,
    teamwork, process",
  location     = "QA 76.76 D47 W34"
}

@Book{lmbfwy,
  author       = "Richard Ford",
  title        = "Let Me Be Frank With You",
  publisher    = "Ecco",
  year         = 2014,
  address      = nyny,
  keywords     = "new jersey, super storm sandy, real estate, disaster,
    divorce, murderous passions, death-bed confessions",
  location     = "PS 3556.O713 L48"
}

@Book{butl,
  author       = "Tanith Lee",
  title        = "Black Unicorn",
  publisher    = "Atheneum",
  year         = 1991,
  address      = nyny,
  keywords     = "quests, unicorns",
  location     = "PZ 7.L5149 Bl"
}

@Book{okotew,
  author       = "Bertrand Russell",
  title        = "Our Knowledge of the External World",
  publisher    = "The New American Library",
  year         = 1960,
  address      = nyny,
  month        = sep,
  price        = "$0.60",
  keywords     = "philosophy, science, logic, infinity, free will,
    epistemology, continuity, causality",
  location     = "B 1649.R93 O8"
}

@Book{cgfjp,
  author       = "Leen Ammeraal and Kang Zhang",
  title        = "Computer Graphics for Java Programmers",
  publisher    = "Springer Science+Business Media",
  year         = 2017,
  address      = nyny,
  edition      = "third",
  keywords     = "geometry, transformations, 2d algorithms, perspective, 3d
    algorithms, hidden line removal, hidden face removal, color, texture,
    shading, fractals",
  location     = "T 385.A488"
}

@Book{tpwmdg,
  author       = "Michael~D. Gordin",
  title        = "The Pseudoscience Wars",
  subtitle     = "Immanuel Velikovsky and the Birth of the Modern Fringe",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "psuedoscience, astronomy, history, catastrophism, bible
    interpretation",
  location     = "Q 172.5.P77 G674"
}

@Book{mdsacg,
  author       = "Kurt Mehlhorn",
  title        = "Multi-Dimensional Searching and Computational Geometry",
  publisher    = sv,
  year         = 1984,
  volume       = 3,
  series       = "EACTS Monographs on Theoretical Computer Science",
  address      = bege,
  keywords     = "multidimensional data structures, computational geometry,
    algorithms",
  location     = "0-387-13642-8"
}

@Book{msbtm,
  author       = "Thomas McMahon",
  title        = "McKay's Bees",
  publisher    = ucp,
  year         = 1979,
  address      = chil,
  keywords     = "frontier and pioneer life, bee culture, beekeepers, kansas",
  location     = "PS 3563.A3188 M39"
}

@Book{pldab,
  author       = "Arnold Businger",
  title        = "{PORTAL} Language Description",
  publisher    = sv,
  year         = 1988,
  volume       = "198",
  series       = lncs,
  address      = bege,
  edition      = "second",
  keywords     = "real-time systems, language definition",
  location     = "QA 76.73.P66 B87 "
}

@Book{ltdpl,
  author       = "William~W. Wadage and Edward~A. Ashcroft",
  title        = "Lucid, the Dataflow Programming Language",
  publisher    = "Academic Press",
  year         = 1985,
  volume       = "22",
  series       = "APIC Studies in Data Processing",
  address      = loen,
  keywords     = "iswim, luswim, iterations, program transformations",
  location     = "QA 76.7"
}

@Book{laofcm,
  author       = "Francisis~G. McCabe",
  title        = "Logic and Objects",
  publisher    = phi,
  year         = 1992,
  address      = "Hertfordshire, U.K.",
  keywords     = "logic programming, scheduler, semantics, implementation",
  location     = "QA 76.63.M42"
}

@Book{ncehm,
  author       = "Edward~H. Miller",
  title        = "Nut Country",
  subtitle     = "Right-Wing Dallas and the Birth of the Southern Strategy",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "american political history, dallas texas, republicans,
    ultraconservatives, the southern strategy, racism",
  location     = "JK 2359.D35 M55"
}

@Book{gmmem,
  author       = "Michael~E. Martenson",
  title        = "Geometric Modeling",
  publisher    = "John Wiley \& Sons",
  year         = 1985,
  address      = nyny,
  keywords     = "curves, surfaces, solids, analytic properties, relational
    properties, intersections, transformations, solid modeling fundamentals,
    solid model construction, global properties, computer graphics, cad/cam",
  location     = "QA 447.M62"
}

@Book{itor,
  author       = "Frederick~S. Hillier and Gerald~J. Lieberman",
  title        = "Introduction to Operations Research",
  publisher    = "McGraw-Hill",
  year         = 2001,
  address      = boma,
  keywords     = "modeling, linear programming, simplex method, duality theory,
    sensitivity analysis, transportation problems, assignment problems, network
    optimization, project management, pert/cpm, dynamic programming, integer
    programming, nonlinear programming, game theory, decision analysis, markov
    chains, queueing theory, inventory theory, forecasting, markov decision
    processes, simulation",
  location     = "T 57.6 M53"
}

@Book{skr,
  author       = "Karen Russell",
  title        = "Swamplandia!",
  publisher    = "Vintage",
  year         = 2011,
  address      = nyny,
  keywords     = "florida, gator rasslin', ",
  location     = "PS 3618.U755 S39"
}

@Book{canpjdc,
  author       = "Jeffery~D. Clements",
  title        = "Corporations are not People",
  publisher    = "Berrett-Koehler",
  year         = 2014,
  address      = sfca,
  edition      = "second",
  keywords     = "corporate governance, constitutional law, the judicial
    system, the powell doctrine",
  location     = "JK 467.C55"
}

@Book{bhobc,
  author       = "David Crystal",
  title        = "By Hook or by Crook",
  subtitle     = "A Journey in Search of English",
  publisher    = "The Overlook Press",
  year         = 2007,
  address      = "Woodstock, " # ny,
  keywords     = "english as she is spoke",
  location     = "PE 1711.C79"
}

@Book{afgaip,
  author       = "Theo Pavlidis",
  title        = "Algorithms for Graphics and Image Processing",
  publisher    = "Computer Science Press",
  year         = 1982,
  keywords     = "digitization, gray-scale images, segmentation, data
    structures, bilevel pictures, projections, contour filling, thinning
    algorithms, curve fitting, curve displaying, splines, curve approximation,
    surface fitting, surface displaying, mathematics, polygon clipping, 2d
    graphics, 3d graphics"
}

@Book{pfai,
  author       = "Randall Jarrell",
  title        = "Pictures from an Institution",
  publisher    = ucp,
  year         = 1986,
  address      = chil,
  keywords     = "college life, writers, character studies",
  location     = "PS 3519.A86 P5"
}

@Book{mttp,
  author       = "Thierry Poibeau",
  title        = "Machine Translation",
  publisher    = mitp,
  year         = 2017,
  address      = cma,
  keywords     = "automatic translation, linguistics, statistics, deep
    learning, evaluation",
  location     = "P 308.P65"
}

@Book{pjb95,
  author       = "Julia Barrett",
  title        = "Presumption",
  subtitle     = "An Entertainment",
  publisher    = ucp,
  year         = 1995,
  address      = chil,
  keywords     = "the bennets, pride and prejudice",
  location     = "PS 3552.A73463 P74"
}

@Book{sptml,
  author       = "Mike Loukides",
  title        = "System Performance Tuning",
  publisher    = "O'Reilly \& Associates",
  year         = 1991,
  address      = seca,
  keywords     = "performance, monitoring, workload, storage, disk, network,
    terminal, kernel, real-time processes, tuning",
  location     = "QA 76.76.O63 L66"
}

@Book{hose,
  title        = "Handbook of Software Engineering",
  publisher    = "Van Nostrand Reinhold",
  year         = 1984,
  editor       = "C.~R. Vick and C.~V. Ramamoorthy",
  address      = nyny,
  keywords     = "software engineering, concurrency control, testing, formal
    verification, reliability, performance, fault tolerance, costing, life
    cycle factors, requirements, process design, applicative programming, array
    machines",
  location     = "QA 76.6 H3335"
}

@Book{mfmmms,
  author       = "Mark McKenney and Markus Schneider",
  title        = "Map Framework",
  subtitle     = "A Formal Model of Maps as a Fundamental Data Type in Information Systems",
  publisher    = sv,
  year         = 2916,
  address      = nyny,
  keywords     = "maps, formal models, points and lines, map operations,
  discrete map models, 2d maps",
  location     = "978-3-319-46764-1"
}

@Book{ttetc,
  author       = "David Rapp",
  title        = "Tinkers to Evers to Chance",
  subtitle     = "The Chicago Cubs and the Dawn of Modern America",
  publisher    = ucp,
  year         = 2018,
  address      = chil,
  keywords     = "baseball, america, chicago sports, chicago cubs",
  location     = "GV 875.C6 R36"
}

@Book{tboem,
  author       = "Caroline Chute",
  title        = "The Beans of Egypt, Maine",
  publisher    = "Warner Books",
  year         = 1985,
  address      = nyny,
  keywords     = "maine, rural poverty",
  location     = "PS 3553.H87 B4"
}

@Book{tpfows,
  author       = "Aaron Brown",
  title        = "The Poker Face of Wall Street",
  publisher    = "John Wiley \& Sons",
  year         = 2006,
  address      = honj,
  keywords     = "poker, finance, risk management",
  location     = "HG 4661.B766"
}

@Book{ttllr,
  author       = "Laurence Ralph",
  title        = "The Torture Letters",
  subtitle     = "Reckoning with Police Violence",
  publisher    = ucp,
  year         = 2020,
  address      = chil,
  keywords     = "police, chicago, law enforcement, torture",
  location     = "HV 8148.C52 R35"
}

@Book{viusax,
  title        = "Visualizing Information Using {SVG} and {X3D}",
  publisher    = sv,
  year         = 2005,
  editor       = "Vladimir Geroimenko and Chaomei Chen",
  address      = loen,
  keywords     = "sgv, x3d, semantic web, user interfaces, visualization",
  location     = "T 385.V597"
}

@Book{trosh,
  author       = "Arthur Conan Doyle",
  title        = "The Return of Sherlock Holmes",
  subtitle     = "Including the Hounds of the Baskervilles",
  publisher    = "Bramhall House",
  year         = 1976,
  address      = nyny,
  keywords     = "sherlock homes",
  location     = "PR 4622.R48"
}

@Book{taumg,
  author       = "Martin Gardner",
  title        = "The Ambidextrous Universe",
  subtitle     = "Left, Right, and the Fall of Parity",
  publisher    = "New American Library",
  year         = 1969,
  address      = nyny,
  price        = "$1.25",
  keywords     = "symmetry, asymmetry, physics, parity, mirrors",
  location     = "QC 793.3.S9 G37"
}

@Book{caccf,
  author       = "Caxton~C. Foster",
  title        = "Computer Architecture",
  publisher    = "Van Nostrand Reinhold",
  year         = 1976,
  address      = nyny,
  edition      = "second",
  keywords     = "information representation, gates, elementary logic, storage,
    addressing, input, output",
  location     = "QA 76.9 A73 F67"
}

@Book{xdbld,
  author       = "Don Brutzman and Loenard Daly",
  title        = "{X3D}",
  subtitle     = "Extensible 3d Graphics for Web Authors",
  publisher    = mk,
  year         = 2007,
  address      = sfca,
  keywords     = "geometry, nodes, navigation, textures, polygons, event
    animation, interpolation, interactivity, scripting, lighting, environment,
    sensors, sound, quadrilaterals",
  location     = "TR 897.7.B796"
}

@Book{itpcmg,
  author       = "Charles~M. Grinstead and J.~Laurie Snell",
  title        = "Introduction to Probability",
  publisher    = "American Mathematical Society",
  year         = 1997,
  address      = "Providence, Rhode Island",
  keywords     = "discrete probability, continuous probability, distributions,
    random walks",
  location     = "QA 273.S668"
}

@Book{dafew,
  author       = "Evelyn Waugh",
  title        = "Decline and Fall",
  publisher    = "Knopf",
  year         = 1993,
  address      = nyny,
  keywords     = "wrong place, wrong time, the knock-about life, satire",
  location     = "PR 6045.A97 D4"
}

@Book{vbew,
  author       = "Evelyn Waugh",
  title        = "Vile Bodies",
  publisher    = "Chapman \& Hall",
  year         = 1930,
  address      = loen,
  keywords     = "bright young things, gossip, dissolution",
  location     = "PZ 3.W356"
}

@Book{ahod,
  author       = "Evelyn Waugh",
  title        = "{A} Handful of Dust",
  publisher    = "Chapman \& Hall",
  year         = 1934,
  address      = loen,
  keywords     = "marriage, divorce, the explorin' life",
  location     = "PR 6045.A97"
}

@Book{avqfi,
  author       = "Simon Goldhill",
  title        = "{A} Very Queer Family Indeed",
  subtitle     = "Sex, Religion, and the Bensons in Victorian Britain",
  publisher    = ucp,
  year         = 2016,
  address      = chil,
  keywords     = "the benson family, victorian england, edwardian england,
    homosexuality, religion, anglicans, catholics, family life",
  location     = "DA 562.G65"
}

@Book{wwp,
  author       = "Jefferson~D. Bates",
  title        = "Writing With Precision",
  subtitle     = "How To Write So That You Cannot Possibly Be Misunderstood",
  publisher    = "Acropolis Books",
  year         = 1978,
  address      = wdc,
  keywords     = "writing, grammar, rhetoric, english, exposition",
  location     = "PE 1429.B35"		  
}

@Book{cowdls,
  author       = "Dorothy~L. Sayers",
  title        = "Clouds of Witness",
  publisher    = "Avon",
  year         = 1966,
  address      = nyny,
  keywords     = "class warfare, eat the rich, the honey trap",
  location     = "PR 6037.A95 C5"
}

@Book{hndc,
  author       = "Douglas Coupland",
  title        = "Hey Nostradamus!",
  publisher    = "Bloomsbury",
  year         = 2003,
  address      = nyny,
  keywords     = "school shootings, trauma, religion, religious fundamentalists, sex",
  location     = "PS 3553.O855 H49"
}

@Book{sew77,
  author       = "Evelyn Waugh",
  title        = "Scoop",
  publisher    = "Little, Brown and Co.",
  year         = 1977,
  address      = boma,
  keywords     = "war correspondents, newspaper media, revolutionaries",
  location     = "PZ 3.W356 Sc"
}

@Book{hacpc,
  author       = "Patrick Carey",
  title        = "{HTML} and {CSS}",
  publisher    = "Cengage Course Technology",
  year         = 2012,
  address      = boma,
  keywords     = "html 5, page layouts, css, tables, web forms, multimedia,
    xhtml, javascript, colors",
  location     = "978-1-111-52644-3"
}

@Book{ccjekpw,
  author       = "Eli K.~P. William",
  title        = "Cash Crash Jubilee",
  subtitle     = "Book One of the Jubilee Cycle",
  publisher    = "Talos Press",
  year         = 2015,
  address      = nyny,
  keywords     = "distopias, neoliberalism, tokyo, augmented reality",
  location     = "PR 9199.4.W5433 C37"
}

@Book{tfsd,
  title        = "\TeX\ for Scientific Documentation",
  subtitle     = pot # "Second European Conference",
  publisher    = sv,
  year         = 1986,
  editor       = "Jacques D{\' e}sarm{\' e}nien",
  volume       = 236,
  series       = lncs,
  address      = bege,
  keywords     = "document processing, multi-lingual documents, document
    preparation systems, font design",
  location     = "Z 253.4.T47 T49"
}

@Book{lopym,
  author       = "Yann Mantel",
  title        = "Life of Pi",
  publisher    = "Harcourt",
  year         = 2001,
  price        = "$14.00",
  address      = "Orlando, Flordia",
  keywords     = "survival, religion, ship wreck",
  location     = "PR 9199.2.M3855 L54"
}

@Book{lll86,
  author       = "Leslie Lamport",
  title        = "\LaTeX",
  subtitle     = "A Document Preparation System",
  publisher    = aw,
  year         = 1986,
  address      = rma,
  keywords     = "document preparation, tex, document formatting",
  location     = "Z 253.4.L38 L35"
}

@Book{pampk,
  author       = "Philip Kraft",
  title        = "Programmers and Managers",
  subtitle     = "The Routinization of Computer Programming in the United States",
  publisher    = sv,
  year         = 1977,
  address      = nyny,
  keywords     = "computer programmers, personnel management, industrial
    relations, deskilling, fragmentation, professionalism",
  location     = "HD 8039.D37 K7"
}

@Article{famw,
  author       = "Charles~P. Thacker and Lawrence~C. Stewart",
  title        = "Firefly:  {A} Multiprocessor Workstation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "164--172",
  month        = oct,
  keywords     = "multiprocessor architecture, vax, caching, trade-offs,
    simulation",
  abstract     = "Firefly is a shared-memory multiprocessor workstation that
    contains from one to seven MicroVAX 78032 processors, each with a floating
    point unit and a sixteen kilobyte cache.  The caches are coherent, so that
    all processors see a consistent view of main memory.  A system may contain
    from four to sixteen megabytes of storage.  Input-output is done via a
    standard DEC QBus.  Input-output devices are an Ethernet controller, fixed
    disks, and a monochrome 1024 x 768 display with keyboard and mouse.
    Optional hardware includes a high resolution color display and a controller
    for high capacity disks.  Figure 1 is a system block diagram.The Firefly
    runs a software system that emulates the Ultrix system call interface.  It
    also supports medium- and coarse-grained multiprocessing through multiple
    threads of control in a single address space.  Communications are
    implemented uniformly through the use of remote procedure calls.This paper
    describes the goals, architecture, implementation and performance analysis
    of the Firefly.  It then presents some measurements of hardware
    performance, and discusses the degree to which SRC has been successful in
    producing software to take advantage of multiprocessing.", 
  location     = "https://doi.org/10.1145/36177.36199", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-23.html"
}

@Article{paritv8p,
  author       = "Douglas~W. Clark",
  title        = "Pipelining and Performance in the {VAX} 8800 Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "173--177",
  month        = oct,
  keywords     = "pipelining, microcode, micropipelining, instruction set
    architecture, performance", 
  abstract     = "The VAX 8800 family (models 8800, 8700, 8550), currently the
    fastest computers in the VAX product line, achieve their speed through a
    combination of fast cycle time and deep pipelining.  Rather than pipeline
    highly variable VAX instructions as such, the 8800 design pipelines uniform
    microinstructions whose addresses are generated by instruction unit
    hardware.  This design approach helps achieve a fast cycle time, which is
    the prime determinan of performance.  Some preliminary measurements of
    cycles per average instruction are reported.", 
  location     = "https://doi.org/10.1145/36177.36200"
}

@Article{avafatsc,
  author       = "Robert~P. Colwell and Robert~P. Nix and John~J. O'Donnell and David~B. Papworth and Paul~K. Rodman",
  title        = "{A} {VLIW} Architecture for a Trace Scheduling Compiler",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "180--192",
  month        = oct,
  keywords     = "vliw architecture, pipelining, compilation, trace scheduling,
    scientific computation, instruction set architecture, branching",
  abstract     = "Very Long Instruction Word (VLIW) architectures were promised
    to deliver far more than the factor of two or three that current
    architectures achieve from overlapped execution.  Using a new type of
    compiler which compacts ordinary sequential code into long instruction
    words, a VLIW machine was expected to provide from ten to thirty times the
    performance of a more conventional machine built of the same implementation
    technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE
    along with its companion Trace Scheduling compacting compiler.  This new
    machine has fulfilled the performance promises that were made. Using many
    fast functional units in parallel, this machine extends some of the basic
    Reduced-Instruction-Set precepts: the architecture is load/store, the
    microarchitecture is exposed to the compiler, there is no microcode, and
    there is almost no hardware devoted to synchronization, arbitration, or
    interlocking of any kind (the compiler has sole responsibility for runtime 
    resource usage).  This paper discusses the design of this machine and
    presents some initial performance results.", 
  location     = "https://doi.org/10.1145/36206.36201"
}

@Article{dttstcplitcm,
  author       = "David~R. Ditzel and Hubert~R. McLellan and Alan~D. Berenbaum",
  title        = "Design Tradeoffs to Support the {C} Programming Language in the {CRISP} Microprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "158--162",
  month        = oct,
  keywords     = "system architecture, language support, context switching,
    code density, stack cache, compiler support, code analysis, path length",
  abstract     = "The CRISP Microprocessor contains a number of new
    architectural features to achieve high performance and support the C
    programming language.  The instruction set was designed to be independent
    of architectural tradeoffs used in any single implementation.  This paper
    describes the particular tradeoffs used in the implementation of a 172,163
    transistor 32-bit single chip microprocessor.  Many tradeoffs were used in
    the design of CRISP, this paper tries to focus on those particular to C. ", 
  location     = "https://doi.org/10.1145/36177.36198"
}

@Article{arafsc,
  author       = "Richard~B. Kieburtz",
  title        = "{A} {RISC} Architecture for Symbolic Computation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "146--155",
  month        = oct,
  keywords     = "graph reduction, combinators, g-machines, tagged data types,
    pipelining",
  abstract     = "The G-machine is a language-directed processor architecture
    designed to support graph reduction as a model of computation.  It can
    carry out lazy evaluation of functional language programs and can evaluate
    programs in which logical variables are used.  To support these language
    features, the abstract machine requires tagged memory and executes some
    rather complex instructions, such as to evaluate a function
    application.This paper explores an implementation of the G-machine as a
    high performance RISC architecture.  Complex instructions can be
    represented by RISC code without experiencing a large expansion of code
    volume.  The instruction pipeline is discussed in some detail.  The
    processor is intended to be integrated into a standard, 32-bit memory
    architecture.  Tagged memory is supported by aggregating data with tags in
    a cache.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{rvcfpacs,
  author       = "Gaetano Borriello and Andrew~R. Cherenson and Peter Bernard Danzig and Michael Newell Nelson",
  title        = "{RISCs} vs. {CISCs} for {Prolog}: {A} Case Study",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "136--145",
  month        = oct,
  keywords     = "prolog, logic programming, abstract machines, compilation,
    spur, tagged data types, unification, backtracking, coprocessors",
  abstract     = "This paper compares the performance of executing compiled
    Prolog code on two different architectures under development at
    U. C. Berkeley.  The first is the PLM, a special-purpose CISC architecture
    intended as a coprocessor for a host machine.  The second is SPUR, a
    general-purpose RISC architecture that supports tagged data.  Fourteen
    standard benchmark programs were run on both the PLM and SPUR simulators.
    The compiled code for SPUR was obtained by simple macro-expansion of PLM
    code generated by the PLM Prolog compiler.  The two implementations are
    compared with regard to static and dynamic program size, execution speed,
    and memory system performance.  On average, the macrocoded SPUR
    implementation has a static code size 14 times larger than the PLM,
    executes 16 times more instructions, yet requires only 2.3 times the number
    of machine cycles (or has the performance of 0.43 PLMs).  When memory
    system performance is taken into account, SPUR is equivalent to 0.29 PLMs.
    Optimizations of the macro-expanded code and minor architectural changes to
    SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks.
    Thus a tagged RISC architecture can execute Prolog at least half as fast as
    a special-purpose CISC architecture for Prolog.", 
  location     = "https://doi.org/10.1145/36177.36196"
}

@Article{tmeuailatmd,
  author       = "David~W. Wall and Michael~L. Powell",
  title        = "The {Mahler} Experience:  Using an Intermediate Language as the Machine Description",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "100--104",
  month        = oct,
  keywords     = "intermediate language, machine independence, compilers,
    global optimizations, link-time optimizations, instruction scheduling,
    pipelining, abstract machines",
  abstract     = "Division of a compiler into a front end and a back end that 
    communicate via an intermediate language is a well-known technique.  We go
    farther and use the intermediate language as the official description of a
    family of machines with simple instruction sets and addressing
    capabilities, hiding some of the inconvenient details of the real machine
    from the users and the front end compilers.To do this credibly, we have had
    to hide not only the existence of the details but also the performance
    consequences of hiding them.  The back end that compiles and links the
    intermediate language tries to produce code that does not suffer a
    performance penalty because of the details that were hidden from the front
    end compiler.  To accomplish this, we have used a number of link-time
    optimizations, including instruction scheduling and interprocedural
    register allocation, to hide the existence of such idiosyncracies as
    delayed branches and non-infinite register sets.  For the most part we have
    been successful.", 
  location     = "https://doi.org/10.1145/36177.36190",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-1.pdf"
}

@Article{asosctfps,
  author       = "Shlomo Weiss and James~E. Smith",
  title        = "{A} Study of Scalar Compilation Techniques for Pipelined Supercomputers",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "cray computers, loop unrolling, software pipelining,
    performance, scientific computing, optimization, machine architectures,
    register files, vectorization",
  abstract     = "This paper studies two compilation techniques for enhancing
    scalar performance in high-speed scientific processors: software pipelining
    and loop unrolling.  We study the impact of the architecture (size of the
    register file) and of the hardware (size of instruction buffer) on the
    efficiency of loop unrolling.  We also develop a methodology for
    classifying software pipelining techniques.  For loop unrolling, a
    straightforward scheduling algorithm is shown to produce near-optimal
    results when not inhibited by recurrences or memory hazards.  Our study
    indicates that the performance produced with a modified CRAY-1S scalar
    architecture and a code scheduler utilizing loop unrolling is comparable to
    the performance achieved by the CRAY-1S with a vector unit and the CFT
    vectorizing compiler.", 
  location     = "https://doi.org/10.1145/79505.79508"
}

@Article{cs8tar,
  author       = "William~R. Bush and A.~Dain Samples and David Ungar and Paul~N. Hilfinger",
  title        = "Compiling {Smalltalk-80} to a {RISC}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "risc architecture, bytecode, compilation, register windows,
    soar, dynamic languages, caching, performance",
  abstract     = "The Smalltalk On A RISC project at U.  C.  Berkeley proves
    that a high-level object-oriented language can attain high performance on a
    modified reduced instruction set architecture.  The single most important
    optimization is the removal of a layer of interpretation, compiling the
    bytecoded virtual machine instructions into low-level, register-based,
    hardware instructions.  This paper describes the compiler and how it was
    affected by SOAR architectural features.  The compiler generates code of
    reasonable density and speed.  Because of Smalltalk-80's semantics,
    relatively few optimizations are possible, but hardware and software
    mechanisms at runtime offset these limitations.  Register allocation for an
    architecture with register windows comprises the major task of the
    compiler.  Performance analysis suggests that SOAR is not simple enough;
    several hardware features could be efficiently replaced by instruction
    sequences constructed by the compiler.", 
  location     = "https://doi.org/10.1145/36177.36192"
}

@Article{hmamae,
  author       = "F.~Chow and S.~Correll and M.~Himelstein and E.~Killian and L.~Weber",
  title        = "How Many Addressing Modes are Enough?",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "117--121",
  month        = oct,
  keywords     = "risc, addressing modes, addressing architecture, offset
    indexing, optimizations, performance, simplicity",
  abstract     = "Programs naturally require a variety of memory-addressing
    modes.  It isn't necessary to provide them in hardware, however, if a
    compiler can synthesize them from a few primitive modes.  This not only
    simplifies the hardware, but also permits the compiler to use its
    understanding of the program to economize on the modes which it uses.  We
    present some compilation techniques that allow the compiler to deal
    effectively with a single addressing mode in a target RISC processor.  We
    also give measurements to show the benefits of such techniques, and to
    support our assertion that a single addressing mode is adequate for a
    general purpose processor, provided that mode incorporates both a pointer
    and an offset.", 
  location     = "https://doi.org/10.1145/36177.36193"
}

@Article{salatsp,
  author       = "Henry Massalin",
  title        = "Superoptimizer --- {A} Look at the Smallest Program",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "122--126",
  month        = oct,
  keywords     = "optimization, exhaustive search, probabilistic testing,
    assembly code",
  abstract     = "Given an instruction set, the superoptimizer finds the
    shortest program to compute a function.  Startling programs have been
    generated, many of them engaging in convoluted bit-fiddling bearing little
    resemblance to the source programs which defined the functions.  The key
    idea in the superoptimizer is a probabilistic test that makes exhaustive
    searches practical for programs of useful size.  The search space is
    defined by the processor's instruction set, which may include the whole
    set, but it is typically restricted to a subset.  By constraining the
    instructions and observing the effect on the output program, one can gain
    insight into the design of instruction sets.  In addition, superoptimized
    programs may be used by peephole optimizers to improve the quality of
    generated code, or by assembly language programmers to improve manually
    written code.", 
  location     = "https://doi.org/10.1145/36177.36194"
}

@Article{paaeotpm,
  author       = "Kazuo Taki and Katsuto Nakajima and Hiroshi Nakashima and Morihiro Ikeda",
  title        = "Performance and Architectural Evaluation of the {PSI Machine}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "128--135",
  month        = oct,
  keywords     = "kl0, prolog, logic programming, caching, branching, psi machine",
  abstract     = "We evaluated a Prolog machine PSI (Personal Sequential
    Inference machine) for the purpose of improving and redesigning it.  In
    this evaluation, we measured the execution speed and the dynamic
    characteristics of cache memory, register file, and branching hardware
    introduced for high-speed execution of Prolog programs.Execution speed of
    the PSI firmware interpreter was found to be comparable to that of the
    DEC-10 Prolog compiled code on the DEC-2060.  It was also found that PSI
    was faster than DEC for executing programs containing much unification and
    backtracking that require runtime processing.With the cache memory, the hit
    ratio for application programs was found higher than 96%; this demonstrates
    that the Prolog execution has much memory access locality.  The memory
    access frequency and the appearance ratio between Read and Write command
    were also investigated.Concerning the register file, use rate of each
    dedicated access mode was measured and effect of each mode was discussed.
    In the branching function we confirmed a high appearance rate of
    conditional branches and multi-way branches based on tag values.", 
  location     = "https://doi.org/10.1145/36177.36195"
}

@Article{cbatccaisohci,
  author       = "Sam Wineburg and Susan Mosborg and Dan Porat and Ariel Duncan",
  title        = "Common Belief and the Cultural Curriculum:  An Intergenerational Study of Historical Consciousness",
  journal      = "American Education Research Journal",
  year         = 2007,
  volume       = 44,
  number       = 1,
  pages        = "40--76",
  month        = mar,
  keywords     = "history instruction, collective memory, vietnam war, united
    states history, curricula, veterans, soldiers, student protests, political
    protests, textbooks, forest gump",
  abstract     = "How is historical knowledge transmitted across generations?
    What is the role of schooling in that transmission? The authors address
    these questions by reporting on a thirty-month longitudinal study into how
    home, school, and larger society served as contexts for the development of
    historical consciousness among adolescents.  Fifteen families drawn from
    three different school communities participated.  By adopting an
    intergenerational approach, the authors sought to understand how the
    defining moments of one generation-its lived history'-becomes the available
    history to the next.  In this article, the authors focus on what parents
    and children shared about one of the most formative historical events in
    parents' lives: the Vietnam War.  Drawing on notions of collective memory,
    as articulated by the French sociologist Maurice Halbwachs, the authors
    sought to understand which stories, archived in historical memory and
    available to the disciplinary community, are remembered and used by those
    beyond its borders.  In contrast, which stories are no longer widely
    shared, eclipsed by time's passage and unable to cross the bridge
    separating generation from generation? The authors conclude by discussing
    the forces that act to historicize today's youth and suggest how educators
    might marshal these forces-rather than spurning or simply ignoring them-to
    advance young people's historical understanding.",
  location     = "https://www.jstor.org/stable/30069471?sa=X&ved=2ahUKEwiOxfHPx__mAhVph-AKHfibCVcQFjAAegQICBAB"
}

@Article{imadothpa,
  author       = "Daniel~J. Magenheimer and Liz Peters and Karl Pettis and Dan Zuras",
  title        = "Integer Multiplication and Division on the {HP Precision Architecture}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "90--99",
  month        = oct,
  keywords     = "multiplication, division, approximations",
  abstract     = "In recent years, many architectural design efforts have
    focused on maximizing performance for frequently executed, simple
    instructions.  Although these efforts have resulted in machines with better
    average price/performance ratios, certain complex instructions and, thus,
    certain classes of programs which heavily depend on these instructions may
    suffer by comparison.  Integer multiplication and division are one such set
    of complex instructions.  This paper describes how a small set of primitive
    instructions combined with careful frequency analysis and clever
    programming allows the Hewlett-Packard Precision Architecture integer
    multiplication and division implementation to provide adequate performance
    at little or no hardware cost.", 
  location     = "https://doi.org/10.1145/36206.36189"
}

@Article{aecfipooai4,
  author       = "Christos John Georgiou and Stewart~L. Palmer and P.~L. Rosenfeld",
  title        = "An Experimental Coprocessor for Implementing Persistent Objects on an {IBM} 4381",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "84--87",
  month        = oct,
  keywords     = "coprocessors, persistent objects, system architecture",
  abstract     = "In this paper we describe an experimental coprocessor for an
    IBM 4381 that is designed to facilitate the exploration of persistent objects.",
  location     = "https://doi.org/10.1145/36177.36188"
}

@Article{chsfsdap,
  author       = "Thomas~A. Cargill and Burt~N. Locanthi",
  title        = "Cheap Hardware Support for Software Debugging and Profiling",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "82--83",
  month        = oct,
  keywords     = "debugging, watch points, reverse execution",
  abstract     = "We wish to determine the effectiveness of some simple
    hardware for debugging and profiling compiled programs on a conventional
    processor. The hardware cost is small -- a counter decremented on each
    instruction that raises an exception when its value becomes zero. With the
    counter a debugger can provide data watchpoints and reverse execution: a
    profiler can measure the total instruction cost of a code segment and
    sample the program counter accurately. Such a counter has been included on
    a single-board MC68020 workstation, for which system software is currently
    being written. We will report our progress at the symposium.", 
  location     = "https://doi.org/10.1145/36177.36187"
}

@Article{cotplictp,
  author       = "Pei~Jyun Leu and Bharat Bhargava",
  title        = "Clarification of Two-Phase Locking in Concurrent Transaction Processing",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "122--125",
  month        = jan,
  keywords     = "two-phase locking, atomic operations, transactions, read
    locks, relaxation, serializability, write locks, logs",
  abstract     = "The authors propose a formal definition of the two-phase
    locking class derived from the semantic description of the two-phase
    locking protocol, and prove that this definition is equivalent to that
    given by C.H.  Papadimitriou (1979).  They present: (1) a precise
    definition of the two phase locking; (2) a clarification of the occurrence
    and the order ofall events such as lock points, unlock points, read
    operations, and write operations of conflicting transactions; and (3) by
    relaxing some conditions in the given definition, the derivation of a new
    class called restricted-non-two-phase locking (RN2PL), which is a superset
    of the class two-phase locking (2PL) but a subset of the class
    D-serializable (DSR) given by Papadimitriou.",
  location     = "https://doi.org/10.1109/32.4629"
}

@Article{cfmvac,
  author       = "James~R. Goodman",
  title        = "Coherency for Multiprocessor Virtual Address Caches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "72--80",
  month        = oct,
  keywords     = "caches, coherency, virtual storage, virtual addressing, tlbs,
    snooping",
  abstract     = "A multiprocessor cache memory system is described that
    supplies data to the processor based on virtual addresses, but maintains
    consistency in the main memory, both across caches and across virtual
    address spaces.  Pages in the same or different address spaces may be
    mapped to share a single physical page.  The same hardware is used for
    maintaining consistency both among caches and among virtual addresses.
    Three different notions of a cache block are defined: (1) the unit for
    transferring data to/from main storage, (2) the unit over which tag
    information is maintained, and (3) the unit over which consistency is
    maintained.  The relation among these block sizes is explored, and it is
    shown that they can be optimized independently.  It is shown that the use
    of large address blocks results in low overhead for the virtual address
    cache.", 
  location     = "https://doi.org/10.1145/36177.36186"
}

@Article{tdprra,
  author       = "Russell~R. Atkinson and Edward~M. McCreight",
  title        = "The {Dragon} Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "65--69",
  month        = oct,
  keywords     = "processor design, instruction sets, procedure calls, dorado,
    instruction density, chip packaging",
  abstract     = "The Xerox PARC Dragon is a VLSI research computer that uses
    several techniques to achieve dense code and fast procedure calls in a
    system that can support multiple processors on a central high bandwidth
    memory bus.",
  location     = "https://doi.org/10.1145/36205.36185"
}

@Article{teoiscopsamp,
  author       = "Jack~W. Davidson and Richard~A. Vaughan",
  title        = "The Effect of Instruction Set Complexity on Program Size and Memory Performance",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "60--64",
  month        = oct,
  keywords     = "instruction set design, risc, cisc, memory pressure, code
    size, portable compilation",
  abstract     = "One potential disadvantage a machines with a simple
    instruction set is object-program size may be substantially larger than
    those for a machine with a complex instruction set.  Groups of simple
    instructions are required to implement the same functions performed by a
    single instruction from a complex instruction set.  In addition, the
    tendency of simple instructions to be fixed length with a few instruction
    formats also increases object-program size.  Larger object-program size
    could adversely affect memory performance and bus traffic.  This paper
    reports the results of experiments to isolate and determine the effect of
    instruction set complexity on cache memory performance and bus traffic.
    Three high-level language compilers were constructed for machines with
    instruction sets of varying complexity.  Using a set of benchmark programs,
    we evaluated the effect of instruction set complexity had on program size.
    Five of the programs were used to perform a set of trace-driven simulations
    to study each machine's cache and bus performance.  While we found that the
    miss ratio is affected by object program size, it appears that this can be
    corrected by increasing the cache size.  Our measurements of bus traffic,
    however, show that even with large caches, machines with simple instruction
    sets can expect substantially more main memory reads than machines with
    complex instruction sets.",  
  location     = "https://doi.org/10.1145/36205.36184"
}

@Article{aafdfdpd,
  author       = "Mike Adler",
  title        = "An Algebra for Data Flow Diagram Process Decomposition",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "169--183",
  month        = feb,
  keywords     = "algebra, automatic process decomposition, data flow diagram,
    dfd, directed acyclic graph, graph-based grammar, software engineering,
    structured analysis",
  abstract     = "Data flow diagram process decomposition, as applied in the
    analysis phase of software engineering, is a top-down method that takes a
    process, and its input and output data flows, and logically implements the
    process as a network of smaller processes.  The decomposition is generally
    performed in an ad hoc manner by an analyst applying heuristics, expertise,
    and knowledge to the problem.  An algebra that formalizes process
    decomposition is presented using the De Marco representation scheme.  In
    this algebra, the analyst relates the disjoint input and output sets of a
    single process by specifying the elements of an input/output connectivity
    matrix.  A directed acyclic graph is constructed from the matrix and is the
    decomposition of the process.  The graph basis, grammar matrix, and graph
    interpretations, and the operators of the algebra are discussed.  A
    decomposition procedure for applying the algebra, prototype, and production
    tools and outlook are also discussed.", 
  location     = "https://doi.org/10.1109/32.4636"
}

@Article{tatcilhasa,
  author       = "Peter Steenkiste and John Hennessy",
  title        = "Tags and Type Checking in {LISP}:  Hardware and Software Approaches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "50--59",
  month        = oct,
  keywords     = "lisp, type checking, generic operations, tagged
    architectures, performance",
  abstract     = "One  major factor distinguishing LISP from other languages
    (e.g., Pascal, C, Fortran) is the need for run-time type checking.  Run-time
    type checking is implemented by adding to each data     object a tag that
    encodes type information.  Tags must be compared for type compatibility,
    removed when using the data, and inserted when new data items are created.
    This tag manipulation, together with other work related to dynamic type
    checking and generic operations, constitutes a significant component of the
    execution time of LISP programs.  This has led both to the development of
    LISP machines that support tag checking in hardware and to the avoidance of
    type checking by users running on stock hardware.  To understand the role
    and necessity of special-purpose hardware for tag handling, we first
    measure the cost of type checking operations for a group of LISP programs.
    We then examine hardware and software implementations of tag operations and
    estimate the cost of tag handling with the different tag implementation
    schemes.  The data shows that minimal levels of support provide most of the
    benefits, and that tag operations can be relatively inexpensive, even when
    no special hardware support is present.",  
  location     = "https://doi.org/10.1145/36206.36183"
}

@Article{aaftdeotfpl,
  author       = "John~R. Hayes and Martin~E. Fraeman and Robert~L. Williams and Thomas Zaremba",
  title        = "An Architecture for the Direct Execution of the {Forth} Programming Language",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "42--49",
  month        = oct,
  keywords     = "zero-address languages, stack management, embedded systems,
    instruction set architecture, data paths",
  abstract     = "We have developed a simple direct execution architecture for
    a 32-bit Forth microprocessor.  The processor can directly access a linear
    address space of over 4 gigawords.  Two instruction types are defined; a
    subroutine call, and a user defined microcode instruction.  On-chip stack
    caches allow most Forth primitives to execute in a single cycle.", 
  location     = "https://doi.org/10.1145/36206.36182"
}

@Article{asfmppohs,
  author       = "Roberto Bisiani and Alessandro Forin",
  title        = "Architectural Support for Multilanguage Parallel Programming on Heterogeneous Systems",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "21--30",
  month        = oct,
  keywords     = "common runtimes, shared storage, remote operations, mach",
  abstract     = "We have designed and implemented a software facility, called
    Agora, that supports the development of parallel applications written in
    multiple languages.  At the core of Agora there is a mechanism that allows
    concurrent computations to share data structures independently of the
    computer architecture they are executed on.  Concurrent computations
    exchange control information by using a pattern-directed technique.  This
    paper describes the Agora shared memory and its software implementation on
    both tightly and loosely-coupled architectures.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{mivmmfpuama,
  author       = "Richard Rashid and Avadis Tevanian and Michael Young and David Golub and Robert Baron and David Black and William Bolosky and Jonathan Chew",
  title        = "Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "31--39",
  month        = oct,
  keywords     = "mach, virtual storage, machine independence, portability,
    shared storage, address mapping, paging",
  abstract     = "This paper describes the design and implementation of virtual
    memory management within the CMU Mach Operating System and the experiences
    gained by the Mach kernel group in porting that system to a variety of
    architectures.  As of this writing, Mach runs on more than half a dozen
    uniprocessors and multiprocessors including the VAX family of uniprocessors
    and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the
    Sequent Balance 21000 and several experimental computers.  Although these
    systems vary considerably in the kind of hardware support for memory
    management they provide, the machine-dependent portion of Mach virtual
    memory consists of a single code module and its related header file.  This
    separation of software memory management from hardware support has been
    accomplished without sacrificing system performance.  In addition to
    improving portability, it makes possible a relatively unbiased examination
    of the pros and cons of various hardware memory management schemes,
    especially as they apply to the support of multiprocessors.", 
  location     = "https://doi.org/10.1145/36177.36181"
}

@Article{vafam,
  author       = "Bob Beck and Bob Kasten and Shreekant Thakkar",
  title        = "{VLSI} Assist for a Multiprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "10--20",
  month        = oct,
  keywords     = "caching, co-processors, ipc, synchronization, buses,
    interrupt handling, system configuration",
  abstract     = "Multiprocessors have long been of interest to computer
    community.  They provide the potential for accelerating applications through
    parallelism and increased throughput for large multi-user system.  Three
    factors have limited the commercial success of multiprocessor systems;
    entry cost, range of performance, and ease of application.  Advances in
    large scale integration (VLSI) and in computer aided design (CAD) have
    removed these limitations, making possible a new class of multiprocessor
    systems based on VLSI components.  A set of requirements for building an
    efficient shared multiprocessor system are discussed, including: low-level
    mutual exclusion, interrupt distribution, inter-processor signaling,
    process dispatching, caching, and system configuration.  A system that
    meets these requirements is described and evaluated.", 
  location     = "https://doi.org/10.1145/36177.36179"
}

@Article{clir,
  author       = "Matthew Flatt",
  title        = "Creating Languages in {Racket}",
  journal      = "ACM Queue",
  year         = 2011,
  volume       = 9,
  number       = 11,
  month        = nov,
  keywords     = "racket, language design, macros, dsl, modules",
  abstract     = "Choosing the right tool for a simple job is easy: a
    screwdriver is usually the best option when you need to change the battery
    in a toy, and grep is the obvious choice to check for a word in a text
    document.  For more complex tasks, the choice of tool is rarely so
    straightforward—all the more so for a programming task, where programmers
    have an unparalleled ability to construct their own tools.  Programmers
    frequently solve programming problems by creating new tool programs, such
    as scripts that generate source code from tables of data.", 
  location     = "https://doi.org/10.1145/2063166.2068896", 
  location     = "https://queue.acm.org/detail.cfm?id=2068896"
}

@Article{hafplaplfha,
  author       = "Niklaus Wirth",
  title        = "Hardware Architecture for Programming Languages and Programming Languages for Hardware Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "2--8",
  month        = oct,
  keywords     = "abstractions, hardware design, complexity, mathematical
    formalisms, system state, correctness reasoning, programming languages",
  abstract     = "Programming Languages and Operating Systems introduce
    abstractions which allow the programmer to ignore details of an
    implementation.  Support of an abstraction must not only concentrate on
    promoting the efficiency of an implementation, but also on providing the
    necessary guards against violations of the abstractions.  In the frantic
    drive for efficiency the second goal has been neglected.  There are
    indications that recent designs which are claimed to be both simple and
    powerful, achieve efficiency by shifting the complex issues of code
    generation and of appropriate guards onto compilers.Complexity has become
    the common hallmark of software as well as hardware designs.  It cannot be
    mastered by the common practices of testing and simulation.  Hardware
    design may profit from developments in programming methodology by adopting
    proof techniques similar to those used in programming.", 
  location     = "https://doi.org/10.1145/36177.36178"
}

@Article{atosigpdcs,
  author       = "Thomas~L. Casavant and Jon~G. Kuhl",
  title        = "{A} Taxonomy of Scheduling in General-Purpose Distributed Computing Systems",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "141--154",
  month        = feb,
  keywords     = "distributed operating systems, distributed resource
    management, general-purpose distributed computing systems, scheduling, task
    allocation, taxonomy",
  abstract     = "One measure of the usefulness of a general-purpose
    distributed computing system is the system's ability to provide a level of
    performance commensurate to the degree of multiplicity of resources present
    in the system.  A taxonomy of approaches to the resource management problem
    is presented in an attempt to provide a common terminology and
    classification mechanism necessary in addressing this problem.  The
    taxonomy, while presented and discussed in terms of distributed scheduling,
    is also applicable to most types of resource management.", 
  location     = "https://doi.org/10.1109/32.4634"
}

@Article{idfaa,
  author       = "Barbara~G. Ryder and Marvin~C. Paull",
  title        = "Incremental Data-Flow Algorithm Algorithms",
  journal      = toplas,
  year         = 1988,
  volume       = 10,
  number       = 1,
  pages        = "1--50",
  month        = jan,
  keywords     = "incremental algorithms, data-flow equations, interval
    analysis",
  abstract     = "An incremental update algorithm modifies the solution of a
    problem that has been changed, rather than re-solving the entire problem.
    ACINCF and ACINCB are incremental update algorithms for forward and
    backward data-flow analysis, respectively, based on our equations model of
    Allen-Cocke interval analysis.  In addition, we have studied their
    performance on a “nontoy” structured programming language L.  Given a set
    of localized program changes in a program written in L, we identify a
    priori the nodes in its flow graph whose corresponding data-flow equations
    may be affected by the changes.  We characterize these possibly affected
    nodes by their corresponding program structures and their relation to the
    original change sites, and do so without actually performing the
    incremental updates.  Our results can be refined to characterize the
    reduced equations possibly affected if structured loop exit mechanisms are
    used, either singly or together, thereby relating richness of programming
    language usage to the ease of incremental updating.", 
  location     = "https://doi.org/10.1145/42192.42193"
}

@Article{mmiaes,
  author       = "Christopher Rosebrugh and Eng-Kee Kwang",
  title        = "Multiple Microcontrollers in an Embedded System",
  journal      = ddj,
  year         = 1992,
  volume       = 17,
  number       = 1,
  pages        = "48--57",
  month        = jan,
  keywords     = "hardware design, ",
  abstract     = "A case study in system architecture and embedded hardware design"
}

@Article{j14pjsacp1,
  author       = "Wm. Paul Rogers",
  title        = "{J2SE} 1.4 Premieres {Java}'s Assertion Capabilities, Part 1",
  journal      = "Java World",
  year         = 2001,
  month        = November,
  keywords     = "java, assertions",
  location     = "https://www.javaworld.com/article/2075803/j2se-1-4-premieres-java-s-assertion-capabilities--part-1.html"
}

@Article{mmocoaiafoed,
  author       = "Ketabchi, Mohammad~A. and Berzins, Valdis",
  title        = "Mathematical Model of Composite Objects and Its Application for Organizing Engineering Databases",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "71--84",
  month        = jan,
  keywords     = "database partitioning, composite objects, engineering
    databases, clustering concept, component aggregation, assemblies,
    equivalent objects, equivalence classes, Boolean algebra, minterms, stored
    views, relational database, design data, frequent access patterns", 
  abstract     = "The authors introduce a clustering concept called component
    aggregation which considers assemblies having the same types of parts as
    equivalent objects.  The notion of equivalent objects is used to develop a
    mathematical model of composite objects.  It is shown that the set of
    equivalence classes of objects form a Boolean algebra whose minterms
    represent the objects that are not considered composite at the current
    viewing level.  The algebraic structure of composite objects serves as a
    basis for developing a technique for organizing composite objects and
    supporting materialization of explosion views.  The technique provides a
    clustering mechanism which partitions the database into meaningful and
    application-oriented clusters, and allows any desired explosion view to be
    materialized using a minimal set of stored views.  A simplified relational
    database for design data and a set of frequent access patterns in design
    applications are outlined and used to demonstrate the benefits of database
    organizations based on the mathematical model of composite objects.", 
  location     = "https://doi.org/10.1109/32.4624"
}

@Article{elsfpsi,
  author       = "Deepinder~P. Sidhu and Carole~S. Crall",
  title        = "Executable Logic Specifications for Protocol Service Interfaces",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "98--121",
  month        = jan,
  keywords     = "automated development tools, formal description technique,
    formal modeling, protocol specification, protocol verification, state
    transitions, prolog, service specification, iso osi model, tcp, tp2, tp4",
  abstract     = "A general, formal modeling technique for protocol service
    interfaces is discussed.  An executable description of the model using a
    logic-programming-based language, Prolog, is presented.  The specification
    of protocol layers consists of two parts, the specification of the protocol
    interfaces and the specification of entities within the protocol layer.
    The specification of protocol interfaces forms the standard against which
    protocols are verified.  When a protocol has been implemented, the
    correctness of its implementation can be tested using the sequences of
    events generated at the service interface.  If the behavior of the protocol
    implementation is consistent with the behavior at the service interface,
    the implementation conforms to its standard.  To illustrate how it works,
    the model is applied to the service interfaces of protocol standards
    developed for the transport layer of the ISO/OSI architecture.  The results
    indicate that Prolog is a useful formal language for specifying
    protocol interfaces.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4626"
}

@Article{fabfpe,
  author       = "Nazim~H. Madhavji",
  title        = "Fragtypes:  {A} Basis for Programming Environments",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "85--97",
  month        = jan,
  keywords     = "modula-2, mupe-2, program fragments, programming
    environments, programming in the all, program composition, structured manipulation",
  abstract     = "The author introduces a novel basis for programming
    environments that encourages development of software in fragments of
    various types, called fragtypes.  Fragtypes range from a simple expression
    type to a complete subsystem type.  As a result, they are suited to the
    development of software in an enlarged scope that includes both programming
    in the small and programming in the large.  The author shows how proposed
    operations on fragtypes can achieve unusual effects on the software
    development process.  Fragtypes and their associated construction rules
    form the basis of the programming environment MUPE-2, which is currently
    under development at McGill University.  The target and the implementation
    language of this environment is the programming language Modula-2.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4625"
}

@Article{aaomasisuotsb,
  author       = "Robert~F. Cmelik and Shing~I. Kong and David~R. Ditzel and Edmund~J. Kelly",
  title        = "An Analysis of {MIPS} and {SPARC} Instruction Set Utiliation on the {SPEC} Benchmarks",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "290--302",
  month        = apr,
  keywords     = "instruction sets, sparc, mips, risc, autoincrement,
    compilers, delay slots",
  abstract     = "The dynamic instruction counts of MIPS and SPARC are compared
    using the SPEC benchmarks.  MIPS typically executes more user-level
    instructions than SPARC.  This difference can be accounted for by
    architectural differences, compiler differences, and library differences.
    The most significant differences are that SPARC�S double-precision floating
    point load/store is an architectural advantage in the SPEC floating point
    benchmarks while MIPS�s compare-and-branch instruction is an architectural
    advantage in the SPEC integer benchmarks.  After the differences in the two
    architectures are isolated, it appears that although MIPS and SPARC each
    have strengths and weaknesses in their compilers and library routines, the
    combined effect of compilers and library routines does not give either MIPS
    or SPARC a clear advantage in these areas.", 
  location     = "https://doi.org/10.1145/106974.107001"
}

@Article{pcoafotirs6,
  author       = "C.~Brian Hall and Kevin O'Brien",
  title        = "Performance Characteristics of Architectural Features of the {IBM RISC System/6000}",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "303--308",
  month        = apr,
  keywords     = "instruction sets, count registers, branching, performance,
    condition codes",
  abstract     = "The IBM RISC System/6000 has a number of architectural
    features that, are not usually found on RISC machines.  Among these are
    pre-increment and decrement forms of memory referencing instructions, a
    special purpose count register that can be used as a loop counter, and
    eight independent sets of condition code bits.  This paper examines the
    performance gained on a number of industry sta.udard benchmarks through the
    use of each of these features.", 
  location     = "https://doi.org/10.1145/106974.107002"
}

@Article{pfacaraacws,
  author       = "Dileep Bhandarkar and Douglas~W. Clark",
  title        = "Performance from Architecture:  Comparing a {RISC} and a {CISC} with Similar Hardware Organization",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "310--319",
  month        = apr,
  keywords     = "performance, mips, vax, pipelining, benchmarking, cache
    behavior, system architecture",
  abstract     = "Performance comparisons across different computer
    architectures cannot usually separate the architectural contribution from
    various implementation and technology contributions to performance.  This
    paper compares an example implementation from the RISC and CISC
    architectural schools (a MIPS M/2000 and a Digital VAX 8700) on nine of the
    ten SPEC benchmarks.  The organizational similarity of these machines
    provides an opportunity to examine the purely architectural advantages of
    RISC.  The RISC approach offers, compared with VAX, many fewer cycles per
    instruction but somewhat more instructions per program.  Using results from
    a software monitor on the MIPS machine and a hardware monitor on the VAX,
    this paper shows that the resulting advantage in cycles per program ranges
    from slightly under a factor of 2 to almost a factor of 4, with a geometric
    mean of 2.7.  It also demonstrates the correlation between cycles per
    instruction and relative instruction count.  Various reasons for this
    correlation, and for the consistent net advantage of RISC, are discussed.", 
  location     = "https://doi.org/10.1145/106972.107003"
}

@Article{pcwfai,
  author       = "Eric Freudenthal and Allan Gottlieb",
  title        = "Process Coordination with Fetch-and-Increment",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "260--268",
  month        = apr,
  keywords     = "barrier synchronization, bottleneck-free algorithms,
    fetch-and-add, fetch-and-increment, parallel access queues, process
    coordination, readers-writers problem",
  abstract     = "The fetch-and-add (F&A) operation has been used effectively
    in a number of process coordination algorithms.  In this paper we assess
    the power of fetch-and-increment (F&I) and fetch-and-decrement (F&D), which
    we view as restricted forms of F&A in which the only addends permitted are
    ±1.  F&A-based algorithms that use only unit addends are thus trivially
    expressed with just F&I and F&D.  Our primary contributions are new F&I/F&D
    algorithms for readers/writers coordination and barrier synchronization for
    dynamically-sized groups.  We also restructure an existing F&A-based
    algorithm for queues-with-multiplicity to obtain an algorithm using just
    F&I and F&D.  When executed on certain hardware architectures, most of
    these algorithms are free of serial bottlenecks.  We also discuss a general
    technique for implementing F&A using F&I/F&D at a cost logarithmic in the
    number of processors.", 
  location     = "https://doi.org/10.1145/106974.106998", 
  location     = "https://nyuscholars.nyu.edu/en/publications/process-coordination-with-fetch-and-increment"
}

@Article{tcfarb,
  author       = "Douglas Johnson",
  title        = "The Case for a Read Barrier",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "279--287",
  month        = apr,
  keywords     = "lisp, garbage collection, generational gc, temporal gc,
    virtual memory, paging",
  abstract     = "This paper looks at the performance of two different garbage
    collection algorithms on a large and long running Lips application.
    Both algorithms use write barriers for generational collection.  Only one
    algorithm uses a rad barrier for incremental collection.  The results show
    little difference in the two algorithm's ability to collect garbage and
    some difference in memory size.  Any differences in CPU usage were too
    small to be visible with the measuring techniques used.  However, there
    were major differences in paging behavior with a read barrier permitted the
    garbage collector to work with the virtual memory manager instead of
    independently.", 
  location     = "https://doi.org/10.1145/106974.107000"
}

@Article{swcjmmc,
  author       = "John~M. Mellor-Crummey and Michael~L. Scott",
  title        = "Synchroniation Without Contention",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "269--278",
  month        = apr,
  keywords     = "bbn butterfly multiprocessor and busy-wait synchronization
    and contention-free mutual exclusion and dance hall machines and exploit
    local access and fetch_and_X instructions and large shared-memory
    multiprocessor and local access and memory consistency  and performance and
    reader-writer control and sequent symmetry and special-purpose hardware
    support", 
  abstract     = "Conventional wisdom holds that contention due to busy-wait
    synchronization is a major obstacle to scalability and acceptable
    performance in large shared-memory multiprocessors.  We argue the contrary,
    and present fast, simple algorithms for contention-free mutual exclusion,
    reader-writer control, and barrier synchronization.  These algorithms,
    based on widely available fetch_and_phi instructions, exploit local access
    to shared memory to avoid contention.  We compare our algorithms to
    previous approaches in both qualitative and quantitative terms, presenting
    their performance on the Sequent Symmetry and BBN Butterfly
    multiprocessors.  Our results highlight the importance of local access to
    shared memory, provide a case against the construction of so-called 'dance
    hall' machines, and suggest that special-purpose hardware support for
    synchronization is unlikely to be cost effective on machines with
    sequentially consistent memory.", 
  location     = "https://doi.org/10.1145/106975.106999"
}

@Article{aecbaads,
  author       = "Sang~L. Min and Jong-Deok Choi",
  title        = "An Efficient Cache-based Access Anomaly Detection Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "235--244",
  month        = apr,
  keywords     = "cache coherence, access anomalies, processor scheduling,
    cache-coherence protocols",
  abstract     = "One of the important issues in parallel program debugging is
    an efficient detection of access anomalies caused by uncoordinated access
    to shared variables.  On-the-fly detection of access anomalies has the
    major advantage that it reports only actual anomalies during execution
    while static analysis methods report all the potential anomalies, many of
    which cannot actually materialize during execution.  It also has the
    advantage that shorter traces are produced for post-mortem analysis
    purposes if an anomaly is detected.  The reason for this is that after an
    anomaly occurs, further trace information is of dubious value because the
    first anomaly may have affected subsequent program behavior.  So, once the
    first anomaly occurs, no further trace information need be generated.
    Existing methods for on-the-fly access anomaly detection suffer from
    performance penalties since the execution of the program being debugged has
    to be interrupted on every access to shared variables.  In this paper, we
    propose an efficient cache-based access anomaly detection scheme that
    piggybacks on the overhead already paid by the underlying cache coherence
    protocol.", 
  location     = "https://doi.org/10.1145/106973.106996"
}

@Article{peomcmfsmm,
  author       = "Kourosh Gharachorloo and Anoop Gupta and John Hennessy",
  title        = "Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "245--257",
  month        = apr,
  keywords     = "The memory consistency model supported by a multiprocessor
    architecture determines the amount of buffering and pipelining that may be
    used to hide or reduce the latency of memory accesses.  Several different
    consistency models have been proposed.  These range from sequential
    consistency on one end, allowing limited buffering, to release
    consistency on the other end, allowing extensive buffering and pipelining.
    The processor consistency and weak consistency models fall in between.  The
    advantage of the less strict models is increased performance potential.
    The disadvantage is increased hardware complexity and a more complex
    programming model.  To make an informed decision on the above trade-off
    requires performance data for the various models.  This paper addresses the
    issue of performance benefits from the above four consistency models.  Our
    results are based on simulation studies done for three applications.  The
    results show that in an environment where processor reads are blocking and
    writes are buffered, a significant performance increase is achieved from
    allowing reads to bypass previous writes.  Pipelining of writes, which
    determines the rate at which writes are retired from the write buffer, is
    of secondary importance.  As a result, we show that the sequential
    consistency model performs poorly relative to all other models, while the
    processor consistency model provides most of the benefits of the weak and
    release consistency models.", 
  location     = "https://doi.org/10.1145/106973.106997"
}

@Article{pacup,
  author       = "Jacques Cohen and Timothy~J. Hickey",
  title        = "Parsing and Compiling Using Prolog",
  journal      = toplas,
  year         = 1987,
  volume       = 9,
  number       = 2,
  pages        = "125--163",
  month        = apr,
  keywords     = "code generation, grammar properties, optimization, parsing,
    prolog, backtracking, definite-clause grammars, difference lists",
  abstract     = "This paper presents the material needed for exposing the
    reader to the advantages of using Prolog as a language for describing
    succinctly most of the algorithms needed in prototyping and implementing
    compilers or producing tools that facilitate this task.  The available
    published material on the subject describes one particular approach in
    implementing compilers using Prolog.  It consists of coupling actions to
    recursive descent parsers to produce syntax-trees which are subsequently
    utilized in guiding the generation of assembly language code.  Although
    this remains a worthwhile approach, there is a host of possibilities for
    Prolog usage in compiler construction.  The primary aim of this paper is to
    demonstrate the use of Prolog in parsing and compiling.  A second, but
    equally important, goal of this paper is to show that Prolog is a
    labor-saving tool in prototyping and implementing many non-numerical
    algorithms which arise in compiling, and whose description using Prolog is
    not available in the literature.  The paper discusses the use of
    unification and nondeterminism in compiler writing as well as means to
    bypass these (costly) features when they are deemed unnecessary.  Topics
    covered include bottom-up and top-down parsers, syntax-directed
    translation, grammar properties, parser generation, code generation, and
    optimizations.  Newly proposed features that are useful in compiler
    construction are also discussed.  A knowledge of Prolog is assumed.", 
  location     = "https://doi.org/10.1145/22719.22946"
}

@Article{pcoppida,
  author       = "Edward~K. Lee and Randy~H. Katz",
  title        = "Performance Consequences of Parity Placement in Disk Arrays",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "190--199",
  month        = apr,
  keywords     = "raid, error detection, parity, data mapping, performance",
  abstract     = "Due to recent advances in CPU and memory system performance,
    I/O systems are increasingly limiting the performance of modern computer
    systems.  Redundant Arrays of Inexpensive Disks (RAID) have been proposed
    by Patterson et. al. to meet the impending I/O crisis.  RAIDs substitute
    many small inexpensive disks for a few large expensive disks to provide
    higher performance (both transfer rate and I/O rate), smaller footprints
    and lower power consumption at a lower cost than the large expensive disks
    they replace, Unfortunately, with so many small disks, media availability
    becomes a serious problem.  RAIDs provide high availability by using parity
    encoding of data to survive disk failures.  As will be shown by this paper,
    the way parity is distributed in a RAID has significant consequences for
    performance.  In particular, we show that for relatively large request
    sizes of hundreds of kilobytes, the choice of parity placement
    significantly affects performance (up to 20-30 percent for the typical disk
    array configurations that are common today) and propose properties that are
    generally desirable of parity placements.",  
  location     = "https://doi.org/10.1145/106974.106992"
}

@Article{ctcocacfatlf,
  author       = "Vincent Cate and Thomas Gross",
  title        = "Combining the Concepts of Compression and Caching for a Two-Level Filesystem",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "200--211",
  month        = apr,
  keywords     = "compression algorithms, file migration, caching, temporal
    locality, lru, file migration, storage hierarchies",
  abstract     = "Caching Storage systems have always attempted to use
    properties of the files that are stored in the system to optimize access
    time, capacity, and/or cost.  Compression exploits patterns within files,
    and file migration and file caching exploit file access patterns, but the
    combination of these concepts has not been reported on before.  We discuss
    here the effectiveness of a filesystem that integrates caching and
    compression to provide two levels of file storage on disks.  This
    investigation is based on measurements that were collected on nine
    computers at three different sites.  The data indicate that automatic
    compression of least recently used files doubles the amount of data that
    can be stored on a given disk system, while incurring only a slight
    performance cost.", 
  location     = "https://doi.org/10.1145/106973.106993"
}

@Article{npatrtma,
  author       = "William~J. Bolosky and Michael~L. Scott and Robert~P. Fitzgerald and Robert~J. Fowler and Alan~L. Cox",
  title        = "{NUMA} Policies and Their Relation to Memory Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "212--221",
  month        = apr,
  keywords     = "trace-based analysis, global storage, programming style, ",
  abstract     = "Multiprocessor memory reference traces provide a wealth of
    information on the behavior of parallel programs.  We have used this
    information to explore the relationship between kernel-based NUMA
    management policies and multiprocessor memory architecture.  Our trace
    analysis techniques employ an off-line, optimal cost policy as a baseline
    against which to compare on-line policies, and as a policy-insensitive tool
    for evaluating architectural design alternatives.  We compare the
    performance of our optimal policy with that of three implementable policies
    (two of which appear in a previous work), on a variety of applications,
    with varying relative speeds for page moves and local, global, and remote
    memory references.  Our results indicate that a good NUMA policy must be
    chosen to match its machine, and confirm that such policies can be both
    simple and effective.  They also indicate that programs for NUMA machine
    must be written with care to obtain the best performance.", 
  location     = "https://doi.org/10.1145/106973.106994"
}

@Article{ldasccs,
  author       = "David Chaiken and John Kubiatowicz and Anant Agarwal",
  title        = "{LimitLESS} Directories:  A Scalable Cache Coherence Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "224--234",
  month        = apr,
  keywords     = "alewife machine, cache coherence, cache coherence protocol,
    interprocessor interrupt, performance",
  abstract     = "Caches enhance the performance of multiprocessors by reducing
    network traffic and average memory access latency.  However, cache-based
    systems must address the problem of cache coherence.  We propose the
    LimitLESS directory protocol to solve this problem.  The LimitLESS scheme
    uses a combination of hardware and software techniques to realize the
    performance of a full-map directory with the memory overhead of a limited
    directory.  This protocol is supported by Alewife, a large-scale
    multiprocessor.  We describe the architectural interfaces needed to
    implement the LimitLESS directory, and evaluate its performance through
    simulations of the Alewife machine.", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a237629.pdf",
  location     = "https://doi.org/10.1145/106973.106995"
}

@Article{iraaisfr,
  author       = "David~G. Bradlee and Susan~J. Eggers and Robert~R. Henry",
  title        = "Integrating Register Allocation and Instruction Scheduling for {RISC}s",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "122--131",
  month        = apr,
  keywords     = "instruction scheduling, register allocation, risc, pass
    phasing, code generation",
  abstract     = "To achieve high performance in uniprocessor RISC systems,
    compilers must perform both register allocation to reduce memory references
    and instruction scheduling to avoid pipeline hazards.  Compilers that
    separate the two functions should perform poorly on uniprocessor RISCS that
    support multi-cycle operations, particularly on computation-intensive
    workloads.  This is because the lack of coordination between register
    allocation and instruction scheduling results in poor use of the register
    set.  In this paper we compare three code generation strategies on three
    RISC processors that support multi-cycle operations.  The first strategy
    completely separates register allocation and instruction scheduling; the
    second logically separates the two phases, but uses a heuristic to force
    the instruction scheduler, which runs first, to adhere to the same
    restrictions as the register allocator; the third performs pre-scheduling
    to calculate schedule cost estimates that enable the register allocator to
    find a balance between using registers to avoid pipeline delays and using
    them to reduce memory references.  Our results show that separating
    register allocation and code scheduling produces inefficient code.
    However, a technique as complex as the third alternative brings little
    added benefit over the second.", 
  location     = "https://doi.org/10.1145/106972.106986"
}

@Article{cgfsaaem,
  author       = "Manuel~E. Benitez and Jack~W. Davidson",
  title        = "Code Generation for Streaming:  an Access\slash Execute Mechanism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "132--141",
  month        = apr,
  keywords     = "wm architecture, code generation, recurrence detection, ",
  abstract     = "Access/execute architectures have several advantages over
    more traditional architectures.  Because address generation and memory
    access are decoupled from operand use, memory latencies are tolerated
    better, there is more potential for concurrent operation, and it permits
    the use of specialized hardware to facilitate fast address generation.
    This paper describes the code generation and optimization algorithms that
    are used in an optimizing compiler for an architecture that contains
    explicit hardware support for the access/execute model of computation.  Of
    particular interest is the novel approach that the compiler uses to detect
    recurrence relations in programs and to generate code for them.  Because
    these relations are often used in problem domains that require significant
    computational resources, detecting and handling them can result in
    significant reductions in execution time.  While the tectilques discussed
    were originally targeted for one specific architecture, many of the
    techniques are applicable to commonly available microprocessors.  The paper
    describes the algorithms as well as our experience with using them on a
    number of machines.", 
  location     = "https://doi.org/10.1145/106975.106987"
}

@Article{eiohlpp,
  author       = "Rajive Bagrodia and Sharad Mathur",
  title        = "Efficient Implementation of High-Level Parallel Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "142--151",
  month        = apr,
  keywords     = "uc, data mapping, connection machine, copying, reducing,
    permuting, folding, matrix computations",
  abstract     = "The efficiency of a parallel program is related to the
    implementation of its data structures on the distributed (or shared) memory
    of a specific architecture.  This paper describes a declarative approach
    that may be used to modify the mapping of the program data on a specific
    architecture.  The ideas are developed in the context of a new language
    called UC and its implementation on the Connection Machine.  The paper also
    contains measurements on sample programs to illustrate the effectiveness of
    data mappings in improving the execution efficiency of example programs.", 
  location     = "https://doi.org/10.1145/106972.376053"
}

@Article{vrdfpvs,
  author       = "William Mangione-Smith and Santosh~G. Abraham and Edward~S. Davidson",
  title        = "Vector Register Design for Polycyclic Vector Scheduling",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "154--163",
  month        = apr,
  keywords     = "instruction scheduling, register naming, memory latency",
  abstract     = "Most vector compilers use a scheduling technique known as
    simple vector scheduling (SVS).  With SVS, all instructions of one loop
    iteration are issued before any succeeding iteration begins.  Long vector
    registers is the primary mechanism for increasing pipeline utilization.
    Some vector compilers use a newer technique, polycyclic vector scheduling
    (PVS), that executes multiple loop iterations concurrently.  Furthermore,
    chaining is not required for optimal performance using PVS.  While PVS code
    schedules typically perform as well as or better than SVS schedules, they
    also tend to require more vector registers.  This limits the applicability
    of PVS for current vector machines.  This paper studies how the register
    requirements of PVS code are related to machine architecture.  An
    architecture similar to the Cray-2 has been used for a series of scheduling
    experiments that cover a wide range of vector register lengths and memory
    latencies.  The results of these experiments indicate that the critical
    machine parameter for a PVS compiler is the number of available vector
    registers.  Little advantage has been found for using a vector register
    length of more than sixteen elements.", 
  location     = "https://doi.org/10.1145/106972.328664"
}

@Article{fgpwmhsacctam,
  author       = "David~E. Culler and Anurag Sah and Klaus~E. Schauser and Thorsten von Eicken and John Wawrzynek",
  title        = "Fine-grain Parallelism with Minimal Hardware Support:  {A} Compiler-Controlled Threaded Abstract Machine",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "164--175",
  month        = apr,
  keywords     = "activations, threads, quanta, id",
  abstract     = "In this paper, we present a relatively primitive execution
    model for fine-grain parallelism, in which all synchronization, scheduling,
    and storage management is explicit and under compiler control.  This is
    defined by a threaded abstract machine (TAM) with a multilevel scheduling
    hierarchy.  Considerable temporal locality of logically related threads is
    demonstrated, providing an avenue for effective register use under
    quasidynamic scheduling.  A prototype TAM instruction set, TLO, has been
    developed, along with a translator to a variety of existing sequential and
    parallel machines.  Compilation of Id, an extended functional language
    requiring fine-grain synchronization, under this model yields performance
    approaching that of conventional languages on current uniprocessors.
    Measurements suggest that the net cost of synchronization on conventional
    multiprocessors can be reduced to within a small factor of that on machines
    with elaborate hardware support, such aa proposed dataflow architectures.
    This brings into question whether tolerance to latency and inexpensive
    synchronization require specific hardware support or merely an appropriate
    compilation strategy and program representation.", 
  location     = "https://doi.org/10.1145/106972.106990"
}

@Article{loilp,
  author       = "David~W. Wall",
  title        = "Limits of Instruction-Level Parallelism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "176--188",
  month        = apr,
  keywords     = "basic blocks, branch and jump prediction, loop unrolling,
    alias analysis, register renaming, ",
  abstract     = "Growing interest in ambitious multiple-issue machines and
    heavily pipelined machines requires a careful examination of how much
    instruction level parallelism exists in typical programs.  Such an
    examination is complicated by the wide variety of hardware and software
    techniques for increasing the parallelism that can be exploited, including
    branch prediction, register renaming, and alias analysis.  By performing
    simulations based on instruction traces, we can model techniques at the
    limits of feasibility and even beyond.  Our study shows a striking
    difference between assuming that the techniques we use are perfect and
    merely assuming that they are impossibly good.  Even with impossibly good
    techniques, average parallelism rarely exceeds 7, with 5 more common.", 
  location     = "https://doi.org/10.1145/106974.106991",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-15.pdf"
}

@Article{tcpaooba,
  author       = "Monica~D. Lam and Edward~E. Rothberg and Michael~E. Wolf",
  title        = "The Cache Performance and Optimizations of Blocked Algorithms",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "63–-74",
  month        = apr,
  keywords     = "blocking, cache performance, matrix computations, data
    locality, analytic models, data access patterns",
  abstract     = "Blocking is a well-known optimization technique for improving
    the effectiveness of memory hierarchies.  Instead of operating on entire
    rows or columns of an array, blocked algorithms operate on submatrices or
    blocks, so that data loaded into the faster levels of the memory hierarchy
    are reused.  This paper presents cache performance data for blocked
    programs and evaluates several optimization to improve this performance.
    The data is obtained by a theoretical model of data conflicts in the cache,
    which has been validated by large amounts of simulation.  We show that the
    degree of cache interference is highly sensitive to the stride of data
    accesses and the size of the blocks, and can cause wide variations in
    machine performance for different matrix sizes.  The conventional wisdom of
    frying to use the entire cache, or even a fixed fraction of the cache, is
    incorrect.  If a fixed block size is used for a given cache size, the block
    size that minimizes the expected number of cache misses is very small.
    Tailoring the block size according to the matrix size and cache parameters
    can improve the average performance and reduce the variance in performance
    for different matrix sizes.  Finally, whenever possible, it is beneficial
    to copy non-contiguous reused data into consecutive locations.", 
  location     = "https://doi.org/10.1145/106972.106981"
}

@Article{teocsocp,
  author       = "Jeffrey~C. Mogul and Anita Borg",
  title        = "The Effect of Context Switches on Cache Performance",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "75--84",
  month        = apr,
  keywords     = "scheduling, context switching, trace analysis, cache
    simulations, performance",
  abstract     = "The sustained performance of fast processors is critically
    dependent on cache performance.  Cache performance in turn depends on
    locality of reference.  When an operating system switches contexts, the
    assumption of locality may be violated because the instructions and data of
    the newly-scheduled process may no longer be in the cache(s).
    Context-switching thus has a cost above that associated with that of the
    operations performed by the kernel.  We fed address traces of the processes
    running on a multi-tasking operating system through a cache simulator, to
    compute accurate cache-hit rates over short intervals.  By marking the
    output of such a simulation whenever a context switch occurs, and then
    aggregating the post-context-switch results of a large number of context
    switches, it is possible to estimate the cache performance reduction caused
    by a switch.  Depending on cache parameters the net cost of a context
    switch appears to be in the thousands of cycles, or tens to hundreds of
    microseconds.", 
  location     = "https://dl.acm.org/doi/10.1145/106974.106982"
}

@Article{apifotfism,
  author       = "David Keppel",
  title        = "{A} Portable Interface for On-The-Fly Instruction Space Modification",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "86--95",
  month        = apr,
  keywords     = "multiprocessors, protection, coherence, virtual machines,
    portability, address-space management, ",
  abstract     = "Applications such as incremental linking must modify
    instruction space during program execution.  Whenever instruction space is
    modified, machine-dependent systems issues such as instruction caching must
    be dealt with properly.  However, there are no standard idioms for
    signaling a change to the instruction space.  This paper discusses issues
    for instruction space allocation and coherence, describes a portable
    interface for modifying instruction space, and examines the details of
    several implementations of the interface.", 
  location     = "https://doi.org/10.1145/106973.106983"
}

@Article{vmpfup,
  author       = "Andrew~W. Appel and Kai Li",
  title        = "Virtual Memory Primatives for User Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "96--107",
  month        = apr,
  keywords     = "virtual memory, concurrent garbage collection, shared virtual
    storage, concurrent checkpointing, general garbage collection, persistent
    stores, extending addressability, data-compression paging, heap overflow
    detection, performance, tlb consistency, page size, ",
  abstract     = "Memory Management Units (MMUs) are traditionally used by
    operating systems to implement disk-paged virtual memory.  Some operating
    systems allow user programs to specify the protection level (inaccessible,
    read-only, read-write) of pages, and allow user programs to handle
    protection violations, but these mechanisms are not always robust,
    efficient, or well-matched to the needs of applications.  We survey several
    user-level algorithms that make use of page-protection techniques, and
    analyze their common characteristics, in an attempt to answer the question,
    'What virtual-memory primitives should the operating system provide to user
    processes, and how do today's operating systems provide them?'.", 
  location     = "https://doi.org/10.1145/106973.106984", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-276-90"
}

@Article{tioaaosd,
  author       = "Thomas~E. Anderson and Henry~M. Levy and Brian~N. Bershad and Edward~D. Lazowska",
  title        = "The Interaction of Architecture and Operating System Design",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "108--120",
  month        = apr,
  keywords     = "ipc, cross-machine communication, local communication, system
    calls, interrupt handling, virtual memory, translation buffers",
  abstract     = "Today's high-performance RISC microprocessors have been
    highly tuned for integer and floating point application performance.  These
    architectures have paid less attention to operating system requirements.
    At the same time, new operating system designs often have overlooked modern
    architectural trends which may unavoidably change the relative cost of
    certain primitive operations.  The result is that operating system
    performance is well below application code performance on contemporary
    RISCs.  This paper examines recent directions in computer architecture and
    operating systems, and the implications of changes in each domain for the
    other.  The requirements of three components of operating system design are
    discussed in detail: interprocess communication, virtual memory, and thread
    management.  For each component, we relate operating system functional and
    performance needs to the mechanisms available on commercial RISC
    architectures such as the MIPS R2000 and R3000, SUN SPARC, IBM RS6000,
    Motorola 88000, and Intel i860.  Our analysis reveals a number of specific
    reasons why the performance of oeprating system primitives on RISCs has not
    scaled with integer performance.  In addition, we identify areas in which
    architectures could better (and cost-effectively) accommodate operating
    system needs, and areas in which operating system design could accommodate
    certain necessary characteristics of cost-effective high-performance
    microprocessors.", 
  location     = "https://doi.org/10.1145/106973.106985", 
  location     = "https://homes.cs.washington.edu/~tom/pubs/interaction.html"
}

@Article{acffca,
  author       = "S.~S. Reddi and E.~A. Feustel",
  title        = "{A} Conceptual Framework for Computer Architecture",
  journal      = surveys,
  year         = 1976,
  volume       = 8,
  number       = 2,
  pages        = "277--300",
  month        = jun,
  keywords     = "computer architecture, framework, architecture composition,
    information flow, physical organization, diverse architecture conceptual
    unification",
  abstract     = "The purpose of this paper is to describe the concepts,
    definitions, and ideas of computer architecture and to suggest that
    architecture can be viewed as composed of three components: physical
    organization; control and flow of information; and representation,
    interpretation and transformation of information.  This framework can
    accommodate diverse architectural concepts such as array processing,
    microprogramming, stack processing and tagged architecture.  Architectures
    of some existing machines are considered and methods of associating
    architectural concepts with the components are established.  Architecture
    design problems and trade-offs are discussed in terms of the proposed
    framework.", 
  location     = "https://doi.org/10.1145/356669.356673"
}

@Article{hbdmsfsp,
  author       = "Gurindar~S. Sohi and Manoj Franklin",
  title        = "High-Bandwidth Data Memory Systems for Superscalar Processors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "53--62",
  month        = apr,
  keywords     = "caches, superscalar instruction issue, bus bandwidth, storage
    hierarchy, cpu performance",
  abstract     = "This paper considers the design of a data memory hierarchy,
    with a level 1 (L1) data cache at the top, to support the data bandwidth
    demands of a future-generation superscalar processor capable of issuing
    about ten instructions per clock cycle.  It introduces the notion of cache
    bandwidth — the bandwidth with which a cache can accept requests from the
    processor — and shows how the bandwidth of a standard, blocking cache, can
    degrade greatly because of its inability to overlap the service of misses.
    To improve the data bandwidth to greater than 1 request per cycle,
    multi-port, interleaved caches are introduced.  Simulation results from a
    cycle-by-cycle simulator, using the MIPS R2000 instruction set, suggest
    that memory hierarchies with blocking L1 caches will be unable to support
    the bandwidth demands of future-generation superscalar processors.
    Multi-port, non-blocking (MPNB) L1 caches introduced in this paper for the
    top of the data memory hierarchy appear to be capable of supporting such
    data bandwidth demands.", 
  location     = "https://doi.org/10.1145/106973.106980",
  location     = "https://minds.wisconsin.edu/handle/1793/59366"
}

@Article{spdc,
  author       = "David Callahan and Ken Kennedy and Allan Porterfield",
  title        = "Software Prefetching",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "40--52",
  month        = apr,
  keywords     = "cache management, performance, latency management",
  abstract     = "We present software prefetching, an approach to reducing
    cache miss latencies.  By providing a nonblocking prefetch instruction that
    brings into cache data at a specified memory address, the compiler can
    overlap the memory latency with other computation.  Our simulations show
    that, even when generated by a simple compiler algorithm, prefetch
    instructions can eliminate nearly all cache misses, while modestly
    increasing data traffic between memory and cache.", 
  location     = "https://doi.org/10.1145/106972.106979"
}

@Article{tfppoassp,
  author       = "Roland~L. Lee and Alex~Y. Kwok and Fay{\' e}~A. Briggs",
  title        = "The Floating-Point Performance of a Superscalar {SPARC} Processor",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "28--37",
  month        = apr,
  keywords     = "superscalar architecture, loop unrolling, software
    pipelining",
  abstract     = "superscalar SPARC processors' floating-point performance is
    evaluated based on empirical data from 12 benchmarks.  This evaluation is
    done in the context of two software instruction scheduling optimization,
    loop unrolling and software pipelining, and for three machine models we
    term, 1-scalar, 2-scalar and 4-scalar.  We also consider the effect of the
    memory system on the performance improvements.  Superscalar hardware alone
    exhibit little performance improvement without software optimization.  Of
    the two scheduling methods we study, software pipelining more effectively
    takes advantage of increased hardware parallelism, and achieves near
    optimal speedup on the 4-scalar machine model.  Loop-unrolling performance
    is restricted by the limited number of floating point registers in the
    SPARC architecture.  Applying both optimization techniques provides best
    performance.  A superscalar SPARC processor can provide improved
    floating-point performance, but with signification software and hardware
    development costs.", 
  location     = "https://doi.org/10.1145/106974.106978"
}

@Article{rtbpbriiadwm,
  author       = "Manolis Katevenis and Nestoras Tzartzanis",
  title        = "Reducing the Branch Penalty by Rearranging Instructions in a Double-Width Memory",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "15--27",
  month        = apr,
  keywords     = "pipelined computer architecture, branch penalty, delayed
    branch, delay slot, rearranging instructions into delay slots, super-scalar
    computer architecture, double-width instruction memory",
  abstract     = "In a pipelined processor with an instruction-fetch throughput
    of two (consecutive) instructions per cycle, one method to reduce the
    branch penalty is to rearrange the code by placing (copies of) instructions
    from both targets of a branch in the double-width fetch stream after that
    branch.  This scheme is of interest e.g.  when the number of fetch cycles
    is large, thus making it hard to fill all the delay slots with instructions
    from before the branch, and when the hardware has super-scalar capabilities
    but the compiler does not find enough instructions for parallel execution
    in the basic block where a b ranch is predicted to go.  We study this
    scheme of rearranging instructions, and we evaluate its performance
    (execution time and code size) in the case where no parallel instructions
    are scheduled in the delay slots.", 
  location     = "https://doi.org/10.1145/106974.106977"
}

@Article{avisettva,
  author       = "Andrew Wolfe and John~P. Shen",
  title        = "{A} Variable Instruction Stream Extension to the {VLIW} Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "2--14",
  month        = apr,
  keywords     = "vliw processors, state-machine models, configurable control
    regimes, sequential programs, barrier synchronization, ",
  abstract     = "A Variable Instruction Stream processor architecture called
    XIMD is proposed.  The XIMD structurally resembles a VLIW and shares many
    of the be@icial characteristics of VLIW; however, the XIMD architecture can
    dynamically partition its resources to support the concurrent execution of
    multiple instruction streams.  The number of streams can vary from cycle to
    cycle to best suit each portion of the application.  The XIMD concept and a
    comparison with other traditional architectures based on state…machine
    models of control paths are presented.  Several program examples further
    illustrate the capabilities of XIMD.  A brief description of an XIMD
    prototype machine is included; details of this implementation are presented
    in another paper.", 
  location     = "https://doi.org/10.1145/106972.106976"
}

@Article{afatuoipl,
  author       = "George~B. Leeman",
  title        = "{A} Formal Approach to Undo Operations in Programming Languages",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 1,
  pages        = "50--87",
  month        = jan,
  keywords     = "checkpoint, language constructs, preprocessors, recovery,
    reverse execution, undo, human interfaces",
  abstract     = "A framework is presented for adding a general Undo facility
    to programming languages.  A discussion of relevant literature is provided
    to show that the idea of Undoing pervades several areas in computer
    science, and even other disciplines.  A simple model of computation is
    introduced, and it is augmented with a minimal amount of additional
    structure needed for recovery and reversal.  Two different interpretations
    of Undo are motivated with examples.  Then, four primitives are defined in
    a language-independent manner; they are sufficient to support a wide range
    of Undo capability.  Two of these primitives carry out state saving, and
    the others mirror the two versions of the Undo operation.  Properties of
    and relationships between these primitives are explored, and there are some
    preliminary remarks on how one could implement a system based on this
    formalism.  The main conclusion is that the notions of recovery and
    reversal of actions can become part of the programming process.", 
  location     = "https://doi.org/10.1145/5001.5005"
}

@Article{ltls,
  author       = "Jens Peter Alfke",
  title        = "Learning to Love {SOM}",
  journal      = "MacTech Journal",
  year         = 1995,
  volume       = 11,
  number       = 1,
  pages        = "12--16",
  month        = jan,
  keywords     = "system object model, fragile base classes, c++, networked
    objects",
  abstract     = "The System Object Model (SOM) provides the object-oriented
    substrate used by OpenDoc and by future versions of the Macintosh Toolbox.
    SOM is fairly complex, relatively new on the Mac, and competes against
    other proprietary object models.  It’s not surprising, then, that there is
    some degree of apprehension and misinformation surrounding it.  This
    article is an introduction to SOM.",
  location     = "http://preserve.mactech.com/articles/mactech/Vol.11/11.01/LearningtoLoveSOM/index.html"
}

@Article{djcdeap4,
  author       = "Eric~E. Allen",
  title        = "Diagnosing {Java} Code:  Designing Extensible Applications, Part 4",
  journal      = "IBM developerWorks",
  year         = 2001,
  month        = dec,
  keywords     = "java, s-expressions, extensibility",
  abstract     = "In this installment of Diagnosing Java Code, author Eric
    Allen illustrates how S-expressions -- syntactic representations of lists
    of elements delimited by parentheses -- can be used to provide a useful and
    lightweight form of black box extensibility.  The advantages of using
    S-expressions are discussed in the context of a particular example.  Also,
    the author details the limitations of S-expressions and notes when they may
    not be the best fit for an application.", 
  location     = "https://www.eecis.udel.edu/~decker/courses/280f07/paper/Java%20Ext%204.pdf"
}

@Article{matmfcs,
  author       = "Ira~W. Cotton",
  title        = "Microeconomics and the Market for Computer Services",
  journal      = surveys,
  year         = 1975,
  volume       = 7,
  number       = 2,
  pages        = "95--111",
  month        = jun,
  keywords     = "billing, charge-back, computer services, economics,
    microeconomics, pricing, supply-demand, elasticity, system management",
  abstract     = "Microeconomics has much to offer the computer services
    manager.  This article reviews some of the traditional topics in
    microeconomics and shows how they can be applied to the computer-services
    market.  The topics covered include supply, demand, costs, and pricing.
    The most significant application of microeconomics is in setting prices--so
    much so that microeconomics is frequently called 'price theory.'
    Accordingly, the thrust of the article is towards providing a sound
    framework for computer-services pricing.",
  location     = "https://doi.org/10.1145/356648.356650"
}

@Article{tfdasisoap,
  author       = "Adrian Cronauer",
  title        = "The Fairness Doctrine:  {A} Solution in Search of a Problem",
  journal      = "Federal Communications Law Journal",
  year         = 1994,
  volume       = 47,
  number       = 1,
  keywords     = "content control, fairness doctrine, narrowcasting",
  abstract     = "The 'Fairness Doctrine' refers to a former policy of the
    Federal Communications Commission wherein a broadcast station which
    presented one viewpoint on a controversial public issue had to afford the
    opposing viewpoint an opportunity to be heard.  The FCC ceased to enforce
    the doctrine in 1987, reasoning that the doctrine actually decreased the
    viewpoints heard by discouraging broadcasters from covering controversial
    issues out of fear of censure by the FCC.  The Author explores the
    historical development of the Fairness Doctrine and examines the flaws with
    the different rationales upon which the doctrine is based.  The Author
    concludes that today's marketplace acts as an alternative to the Fairness
    Doctrine by providing numerous media outlets with specialty formats
    catering to particular viewpoints; therefore, the Fairness Doctrine is
    unnecessary and should not be revived.", 
  location     = "https://www.repository.law.indiana.edu/fclj/vol47/iss1/6"
}

@Article{adusboavcs,
  author       = "G.~W.R.~Luderer and H.~Che and J.~P. Haggerty and P.~A. Kirslis and W.~T. Marshall",
  title        = "{A} Distributed " # unix # " System Based on a Virtual Circuit Switch",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "160--168",
  month        = dec,
  keywords     = "distributed computing, datakit, file servers",
  abstract     = "The popular UNIX operating system provides time-sharing
    service on a single computer.  This paper reports on the design and
    implementation of a distributed UNIX system.  The new operating system
    consists of two components: the S-UNIX subsystem provides a complete UNIX
    process environment enhanced by access to remote files; the F-UNIX
    subsystem is specialized to offer remote file service.  A system can be
    configured out of many computers which operate either under the S-UNIX or
    the F-UNIX operating subsystem.  The file servers together present the view
    of a single global file system.  A single-service view is presented to any
    user terminal connected to one of the S-UNIX subsystems.Computers
    communicate with each other through a high-bandwidth virtual circuit
    switch.  Small front-end processors handle the data and control protocol
    for error and flow-controlled virtual circuits.  Terminals may be connected
    directly to the computers or through the switch.Operational since early
    1980, the system has served as a vehicle to explore virtual circuit
    switching as the basis for distributed system design.  The performance of
    the communication software has been a focus of our work.  Performance
    measurement results are presented for user process level and operating
    system driver level data transfer rates, message exchange times, and system
    capacity benchmarks.  The architecture offers reliability and modularly
    growable configurations.  The communication service offered can serve as
    the foundation for different distributed architectures.", 
  location     = "https://doi.org/10.1145/1067627.806604"
}

@Article{lanthrds,
  author       = "G.~Popek and B.~Walker and J.~Chow and D.~Edwards and C.~Kline and G.~Rudisin and G.~Thiel",
  title        = "{LOCUS}: A Network Transparent, High Reliability Distributed System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "169--177",
  month        = dec,
  keywords     = "LOCUS is a distributed operating system that provides a high
    degree of network transparency while at the same time supporting high
    performance and automatic replication of storage.  By network transparency
    we mean that at the system call interface there is no need to mention
    anything network related.  Knowledge of the network and code to interact
    with foreign sites is below this interface and is thus hidden from both
    users and programs under normal conditions.  LOCUS is application code
    compatible with Unix2, and performance compares favorably with standard,
    single system Unix.  LOCUS runs on a high bandwidth, low delay local
    network.  It is designed to permit both a significant degree of local
    autonomy for each site in the network while still providing a network-wide,
    location independent name structure.  Atomic file operations and extensive
    synchronization are supported.Small, slow sites without local mass store
    can coexist in the same network with much larger and more powerful machines
    without larger machines being slowed down through forced interaction with
    slower ones.  Graceful operation during network topology changes is
    supported.", 
  location     = "https://doi.org/10.1145/800216.806605"
}

@Article{gaeidcs,
  author       = "Andrew~D. Birrell and Roy Levin and Roger~M. Needham and Michael~D. Schroeder",
  title        = "Grapevine:  An Exercise in Distributed Computing (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "178--179",
  month        = dec,
  keywords     = "Grapevine is a distributed, replicated system running on a
    large internet within the Xerox research and development community.  The
    internet extends from coast to coast in the USA, to Canada and to Europe,
    and contains more than 50 Ethernet local networks linked by leased
    telephone lines.  Over 1500 computers are attached to the internet.  Most
    computers are used an personal workstations, but some are used as servers
    providing access to shared facilities such as printers, large-scale
    secondary storage, or data bases.  Computers on the internet are uniformly
    addressable using the PUP family of protocols.", 
  location     = "https://doi.org/10.1145/1067627.806606"
}

@Article{baadsfwmvts,
  author       = "Norman Meyrowitz and Margaret Moser",
  title        = "{BRUWIN}:  An Adaptable Design Strategy for Window Manager\slash Virtual Terminal Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "180--189",
  month        = dec,
  keywords     = "window management, device independence",
  abstract     = "With only one process viewable and operational at any moment,
    the standard terminal forces the user to continually switch between
    contexts.  Yet this is unnatural and counter-intuitive to the normal
    working environment of a desk where the worker is able to view and base
    subsequent actions on multiple pieces of information.The window manager is
    an emerging computing paradigm which allows the user to create multiple
    terminals on the same viewing surface and to display and act upon these
    simultaneous processes without loss of context.  Though several research
    efforts in the past decade have introduced window managers, they have been
    based on the design or major overhaul of a language or operating system;
    the window manager becomes a focus of—rather than a tool of—the system.
    While many of the existing implementations provide wide functionality, most
    implementations and their associated designs are not readily available for
    common use; extensibility is minimal.This paper describes the design and
    implementation of BRUWIN, the BRown University WINdow manager, stressing
    how such a design can be adapted to a variety of computer systems and
    output devices, ranging from alphanumeric terminals to high-resolution
    raster graphics displays.  The paper first gives a brief overview of the
    general window manager paradigm and existing examples.  Next we present an
    explanation of the user-level functions we have chosen to include in our
    general design.  We then describe the structure and design of a window
    manager, outlining the five important parts in detail.  Finally, we
    describe our current implementation and provide a sample session to
    highlight important features.", 
  location     = "https://doi.org/10.1145/1067627.806607"
}

@Article{aadoag,
  author       = "Kourosh Gharachorloo and Madhu Sharma and Simon Steely and Stephen Van Doren",
  title        = "Architecture and Design of {AlphaServer GS320}",
  journal      = asplos00,
  year         = 2000,
  volume       = 35,
  number       = 11,
  pages        = "13--24",
  month        = nov,
  keywords     = "consistency protocols, directories, snoopy cache, multiprocessors",
  abstract     = "This paper describes the architecture and implementation of
    the AlphaServer GS320, a cache-coherent non-uniform memory access
    multiprocessor developed at Compaq.  The AlphaServer GS320 architecture is
    specifically targeted at medium-scale multiprocessing with 32 to 64
    processors.  Each node in the design consists of four Alpha 21264
    processors, up to 32GB of coherent memory, and an aggressive IO subsystem.
    The current implementation supports up to 8 such nodes for a total of 32
    processors.  While snoopy-based designs have been stretched to medium-scale
    multiprocessors by some vendors, providing sufficient snoop bandwidth
    remains a major challenge especially in systems with aggressive processors.
    At the same time, directory protocols targeted at larger scale designs lead
    to a number of inherent inefficiencies relative to snoopy designs.  A key
    goal of the AlphaServer GS320 architecture has been to achieve the
    best-of-both-worlds, partly by exploiting the bounded scale of the target
    systems.This paper focuses on the unique design features used in the
    AlphaServer GS320 to efficiently implement coherence and consistency.  The
    guiding principle for our directory-based protocol is to address
    correctness issues related to rare protocol races without burdening the
    common transaction flows.  Our protocol exhibits lower occupancy and lower
    message counts compared to previous designs, and provides more efficient
    handling of 3-hop transactions.  Furthermore, our design naturally lends
    itself to elegant solutions for deadlock, livelock, starvation, and
    fairness.  The AlphaServer GS320 architecture also incorporates a couple of
    innovative techniques that extend previous approaches for efficiently
    implementing memory consistency models.  These techniques allow us to
    generate commit events (which are used for ordering purposes) well in
    advance of formulating the reply to a transaction.  Furthermore, the
    separation of the commit event allows time-critical replies to by-pass
    inbound requests without violating ordering properties.  Even though our
    design specifically targets medium-scale servers, many of the same
    techniques can be applied to larger-scale directory-based and smaller-scale
    snoopy-based designs.  Finally, we evaluate the performance impact of some
    of the above optimizations and present a few competitive benchmark
    results.", 
  location     = "https://doi.org/10.1145/356989.356991"
}

@Article{taotes,
  author       = "Edward~D. Lazowska and Henry~M. Levy and Guy~T. Almes and Michael~J. Fischer and Robert~J. Fowler and Stephen~C. Vestal",
  title        = "The Architecture of the {Eden} System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "148--159",
  month        = dec,
  keywords     = "object-based systems, distributed systems, location
    independent objects",
  abstract     = "The University of Washington's Eden project is a five-year
    research effort to design, build and use an “integrated distributed”
    computing environment.  The underlying philosophy of Eden involves a fresh
    approach to the tension between these two adjectives.  In briefest form,
    Eden attempts to support both good personal computing and good multi-user
    integration by combining a node machine / local network hardware base with
    a software environment that encourages a high degree of sharing and
    cooperation among its users.The hardware architecture of Eden involves an
    Ethernet local area network interconnecting a number of node machines with
    bit-map displays, based upon the Intel iAPX 432 processor.  The software
    architecture is object-based, allowing each user access to the information
    and resources of the entire system through a simple interface.This paper
    states the philosophy and goals of Eden, describes the programming
    methodology that we have chosen to support, and discusses the hardware and
    kernel architecture of the system.", 
  location     = "https://doi.org/10.1145/800216.806603"
}

@Article{witb,
  author       = "Robert Schmidt",
  title        = "What is the {BIOS}?",
  journal      = "Smart Computing",
  year         = 1994,
  volume       = 5,
  number       = 7,
  month        = jul,
  keywords     = "bios, operating systems, system initialization, libraries"
}

@Article{asofsafl,
  author       = "M.~Satyanarayanan",
  title        = "{A} Study of File Sizes and Functional Lifetimes",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "96--108",
  month        = dec,
  keywords     = "file systems, statistics",
  abstract     = "The performance of a file system depends strongly on the
    characteristics of the files stored in it.  This paper discusses the
    collection, analysis and interpretation of data pertaining to files in the
    computing environment of the Computer Science Department at Carnegie-Mellon
    University (CMU-CSD).  The information gathered from this work will be used
    in a variety of ways:1.  As a data point in the body of information
    available on file systems.2.  As input to a simulation or analytic model of
    a file system for a local network, being designed and imlemented at CMU-CSD
    3. As the basis of implementation decisions and parameters for the file
    system just mentioned.4.  As a step toward understanding how a user
    community creates, maintains and uses files.",  
  location     = "https://doi.org/10.1145/800216.806597",  
  location     = "https://www.cs.cmu.edu/~satya/docdir/satya-sosp-1981.pdf"
}

@Article{htgps,
  author       = "Matt Bishop",
  title        = "Hierarchical Take-Grant Protection Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "109--122",
  month        = dec,
  keywords     = "de jure rules, de facto rules, information flow,
    authorization flow, hierarchical protection",
  abstract     = "The application of the Take-Grant Protection Model to
    hierarchical protection systems is explored.  The proposed model extends
    the results of Wu and applies the results of Bishop and Snyder to obtain
    necessary and sufficient conditions for a hierarchical protection graph to
    be secure.  In addition, restrictions on the take and grant rules are
    developed that ensure the security of all graphs generated by these
    restricted rules.", 
  location     = "https://doi.org/10.1145/800216.806598"
}

@Article{csfisaa,
  author       = "David~K. Gifford",
  title        = "Cryptographic Sealing for Information Secrecy and Authentication (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "123--124",
  month        = dec,
  keywords     = "public-key cryptography, capabilities, access control lists,
    information flow control",
  abstract     = "The problem of computer security can be considered to consist
    of four distinct components: secrecy (ensuring that information is only
    disclosed to authorized users), authentication (ensuring that information
    is not forged), integrity (ensuring that information is not destroyed), and
    availability (ensuring that access to information can not be maliciously
    interrupted).The paper describes a new protection mechanism called
    cryptographic sealing that provides primitives for secrecy and
    authentication.  The mechanism is enforced with a synthesis of classical
    cryptography, public-key cryptography, and a threshold scheme.", 
  location     = "https://doi.org/10.1145/800216.806599"
}

@Article{aumaificiame,
  author       = "George~W. Cox and William~M. Corwin and Konrad~K. Lai and Fred~J. Pollack",
  title        = "{A} Unified Model and Implementation for Interprocess Communication in a Multiprocessor Environment (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "125--126",
  month        = dec,
  keywords     = "ipc, process scheduling, ",
  abstract     = "This paper describes interprocess communication and process
    dispatching on the Intel 432.  The primary assets of the facility are its
    generality and its usefulness in a wide range of applications.  The
    conceptual model, supporting mechanisms, available interfaces, current
    implementations, and absolute and comparative performance are described.", 
  location     = "https://doi.org/10.1145/1067627.806600"
}

@Article{iamosfaobc,
  author       = "Kevin~C. Kahn and William~M. Corwin and T.~Don Dennis and Herman D'Hooge and David~E. Hubka and Linda~A. Hutchins and John~T. Montague and Fred~J. Pollack",
  title        = "{iMAX}:  {A} Multiprocessor Operating System for an Object-Based Computer",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "127--136",
  month        = dec,
  keywords     = "432, ada, architecture design, processor-memory model, system
    configurability, process management, memory management, device independence",
  abstract     = "The Intel iAPX 432 is an object-based microcomputer which,
    together with its operating system iMAX, provides a multiprocessor computer
    system designed around the ideas of data abstraction.  iMAX is implemented
    in Ada and provides, through its interface and facilities, an Ada view of
    the 432 system.  Of paramount concern in this system is the uniformity of
    approach among the architecture, the operating system, and the language.
    Some interesting aspects of both the external and internal views of iMAX
    are discussed to illustrate this uniform approach.", 
  location     = "https://doi.org/10.1145/1067627.806601"
}

@Article{ti4ofs,
  author       = "Fred~J. Pollack and Kevin~C. Kahn and Roy~M. Wilkinson",
  title        = "The {iMAX}-432 Object Filing System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "137--147",
  month        = dec,
  abstract     = "iMAX is the operating system for Intel's iAPX-432 computer
    system. The iAPX-4321 is an object-oriented multiprocessor architecture
    that supports capability-based addressing.  The object filing system is
    that part of iMAX that implements a permanent reliable object store.In this
    paper we describe the key elements of the iMAX object filing system design.
    We first contrast the concept of an object filing system with that of a
    conventional file system.  We then describe the iMAX design paying
    particular attention to five problems that other object filing designs have
    either solved inadequately or failed to address.  Finally, we discuss an
    effect of object filing on the programming semantics of Ada.", 
  keywords     = "type management, object spaces, linkage systems, synchronization", 
  location     = "https://doi.org/10.1145/1067627.806602"
}

@Article{tmattrapl,
  author       = "C.~R. Spooner",
  title        = "The {ML} Approach to the Readable All-Purpose Language",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 2,
  pages        = "215--243",
  month        = apr,
  keywords     = "adaptability, compile-time procedures, context, environment,
    extensibility, language-creation systems, language design, macro expansion,
    readability, textual domains",
  abstract     = "The ideal computer language is seen as one that would be as
    readable as natural language, and so adaptable that it could serve as the
    only language a user need ever know.  An approach to language design has
    emerged that shows promise of allowing one to come much closer to that
    ideal than might reasonably have been expected.  Using this approach, a
    language referred to as ML has been developed, and has been implemented as
    a language-creation system in which user-defined procedures invoked at
    translation time translate the source to some object code.  In this way the
    user can define both the syntax and the semantics of the source language.
    Both language and implementation are capable of further development.  This
    paper describes the approach, the language, and the implementation and
    recommends areas for further work.", 
  location     = "https://doi.org/10.1145/5397.5918"
}

@Article{aroodrfadcs,
  author       = "Liba Svobodova",
  title        = "{A} Reliable Object-Oriented Data Repository for a Distributed Computer System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "47--57",
  month        = dec,
  keywords     = "distributed data storage system, server, atomic update,
    stable storage, optical disk, memory management, crash recovery",
  abstract     = "The repository described in this paper is a component of a
    distributed data storage system for a network of many autonomous machines
    that might run diverse applications.  The repository is a server machine
    that provides very large, very reliable long-term storage for both private
    and shared data objects.  The repository can handle both very small and
    very large data objects, and it supports atomic update of groups of objects
    that might be distributed over several repositories.  Each object is
    represented as a history of its states; in the actual implementation, an
    object is a list of immutable versions.The core of the repository is stable
    append-only storage called Version Storage (VS).  VS contains the histories
    of all data objects in the repository as well as all information needed for
    crash recovery.  To maintain the current versions of objects online, a
    copying scheme was adopted that resembles techniques of real-time garbage
    collection.  VS can be implemented with optical disks.", 
  location     = "https://doi.org/10.1145/1067627.806591"
}

@Article{acotnbfs,
  author       = "James~G. Mitchell and Jeremy Dion",
  title        = "{A} Comparison of Two Network-Based File Servers (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "45--46",
  month        = dec,
  keywords     = "rings, buses, pup, file servers",
  abstract     = "This paper compares two working network-based file servers,
    the Xerox Distributed File System (XDFS) implemented at the Xerox Palo Alto
    Research Center, and the Cambridge File Server (CFS) implemented at the
    Cambridge University Computer Laboratory.  Both servers support concurrent
    random access to files using atomic transactions, both are connected to
    local area networks, and both have been in service long enough to enable us
    to draw lessons from them for future file servers.We compare the servers in
    terms of design goals, implementation issues, performance, and their
    relative successes and failures, and discuss what we would do differently
    next time.", 
  location     = "https://doi.org/10.1145/358468.358475"
}

@Article{otcowsp,
  author       = "Niklaus Wirth",
  title        = "On the Composition of Well-Structured Programs",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 4,
  pages        = "247--259",
  month        = dec,
  keywords     = "programming methods, systematic programming, program schemas,
    goto-free programs, well-structured programs, pascal",
  abstract     = "A professional programmer's know-how used to consist of the
    mastery of a set of techniques applicable to specific problems and to some
    specific problems and to some specific computer.  With the increase of
    computer power, the programmer's tasks grew more complex, and hence the
    need for a systematic approach became evident.  Recently, the subject of
    programming methods, generally applicable rules and patterns of
    development, received considerable attention.  Structured programming is
    the formulation of programs as hierarchical, nested structures of
    statements and objects of computation.  We give brief examples of
    structured programs, show the essence of this approach, discusses its
    relationship with program verification, and comment on the role of
    structured languages.", 
  location     = "https://doi.org/10.1145/356635.356639"
}

@Article{scsian,
  author       = "A.~J. Herbert and R.~M. Needham",
  title        = "Sequencing Computation Steps in a Network",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "59--63",
  month        = dec,
  keywords     = "distributed computations, synchronization, coordination",
  abstract     = "It is sometimes necessary in the course of a distributed
    computation to arrange that a certain set of operations is carried out in
    the correct order and the correct number of times (typically once).  If
    several sets of operations are performed on different machines on the
    network there is no obvious mechanism for enforcing such ordering
    constraints in a fully distributed way.  This lack basically stems from the
    difficulty of preventing copying and repetition of messages by machines and
    from the impossibility of constraining externally the actions of machines
    in response to messages that come into their hands.This paper presents a
    possible method for ensuring the integrity of sequences of operations on
    different machines.  The technique may be thought of as a means of enabling
    machines to ensure that requests made of them are valid and timely, not as
    means of centralized control of services.", 
  location     = "https://doi.org/10.1145/800216.806592"
}

@Article{aaconosk,
  author       = "Richard~F. Rashid and George~G. Robertson",
  title        = "Accent:  {A} Communication Oriented Network Operating System Kernel",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "64--75",
  month        = dec,
  keywords     = "ipc, virtual storage, distributed systems, modularity,
    location transparency, message passing",
  abstract     = "Accent is a communication oriented operating system kernel
    being built at Carnegie-Mellon University to support the distributed
    personal computing project, Spice, and the development of a fault-tolerant
    distributed sensor network (DSN).  Accent is built around a single,
    powerful abstraction of communication between processes, with all kernel
    functions, such as device access and virtual memory management accessible
    through messages and distributable throughout a network.  In this paper,
    specific attention is given to system supplied facilities which support
    transparent network access and fault-tolerant behavior.  Many of these
    facilities are already being provided under a modified version of VAX/UNIX.
    The Accent system itself is currently being implemented on the Three Rivers
    Corp.  PERQ.", 
  location     = "https://doi.org/10.1145/800216.806593"
}

@Article{casbstdpiaalprb,
  author       = "{\" O}zalp {\u B}abaoglu and William Joy",
  title        = "Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Referenced Bits",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "78--86",
  month        = dec,
  keywords     = "page replacement, vaxen, clock page replacement,
    multiprogramming, paging",
  abstract     = "This paper discusses the modifications made to the UNIX
    operating system for the VAX-11/780 to convert it from a swap-based
    segmented system to a paging-based virtual memory system.  Of particular
    interest is that the host machine architecture does not include
    page-referenced bits.  We discuss considerations in the design of
    page-replacement and load-control policies for such an architecture, and
    outline current work in modeling the policies employed by the system.  We
    describe our experience with the chosen algorithms based on
    benchmark-driven studies and production system use.", 
  location     = "https://doi.org/10.1145/800216.806595"
}

@Article{tmpe,
  author       = "Jan Heering and Paul Klint",
  title        = "Towards Monolingual Programming Environments",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 2,
  pages        = "183--213",
  month        = apr,
  keywords     = "monolingual system, language integration, language design,
  debugging languages, type checking, event expectation, side-effect recovery",
  abstract     = "Most programming environments are much too complex.  One way
    of simplifying them is to reduce the number of mode-dependent languages the
    user has to be familiar with.  As a first step towards this end, the
    feasibility of unified command/programming/debugging languages, and the
    concepts on which such languages have to be based, are investigated.  The
    unification process is accomplished in two phases.  First, a unified
    command/programming framework is defined and, second, this framework is
    extended by adding an integrated debugging capability to it.  Strict rules
    are laid down by which to judge language concepts presenting themselves as
    candidates for inclusion in the framework during each phase.  On the basis
    of these rules many of the language design questions that have hitherto
    been resolved this way or that, depending on the taste of the designer,
    lose their vagueness and can be decided in an unambiguous manner.", 
  location     = "https://doi.org/10.1145/3318.3321"
}

@Article{wasaeafvmm,
  author       = "Richard~W. Carr and John~L. Hennessy",
  title        = "{WSClock} --- {A} Simple and Effective Algorithm for Virtual Memory Management",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "87--95",
  month        = dec,
  keywords     = "page replacement algorithms, virtual storage, working set,
    load control, multiprogramming",
  abstract     = "A new virtual memory management algorithm WSCLOCK has been
    synthesized from the local working set (WS) algorithm, the global CLOCK
    algorithm, and a new load control mechanism for auxiliary memory access.
    The new algorithm combines the most useful feature of WS—a natural and
    effective load control that prevents thrashing—with the simplicity and
    efficiency of CLOCK.  Studies are presented to show that the performance of
    WS and WSCLOCK are equivalent, even if the savings in overhead are
    ignored.", 
  location     = "https://doi.org/10.1145/800216.806596"
}

@Article{asaflscs,
  author       = "Bruno Blanchet and Patrick Cousot and Radhia Cousot and Jérome Feret and Laurent Mauborgne and Antoine Miné and David Monniaux and Xavier Rival",
  title        = "{A} Static Analyzer for Large Safety-Critical Software (Extended Abstract)",
  journal      = notices ,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "196--201",
  month        = may,
  keywords     = "abstract analysis, abstract domains",
  abstract     = "We show that abstract interpretation-based static program
    analysis can be made efficient and precise enough to formally verify a
    class of properties for a family of large programs with few or no false
    alarms.  This is achieved by refinement of a general purpose static
    analyzer and later adaptation to particular programs of the family by the
    end-user through parametrization.  This is applied to the proof of
    soundness of data manipulation operations at the machine level for periodic
    synchronous safety critical embedded software.The main novelties are the
    design principle of static analyzers by refinement and adaptation through
    parametrization (Sect.  3 and 7), the symbolic manipulation of expressions
    to improve the precision of abstract transfer functions (Sect.  6.3), the
    octagon (Sect.  6.2.2), ellipsoid (Sect.  6.2.3), and decision tree (Sect.
    6.2.4) abstract domains, all with sound handling of rounding errors in
    oating point computations, widening strategies (with thresholds: Sect.
    7.1.2, delayed: Sect.  7.1.3) and the automatic determination of the
    parameters (parametrized packing: Sect.  7.2).", 
  location     = "https://doi.org/10.1145/780822.781153"
}

@Article{papdo,
  author       = "Malcolm~P. Atkinson and Ronald Morrison",
  title        = "Procedures as Persistent Data Objects",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 4,
  pages        = "539--559",
  month        = oct,
  keywords     = "persistent storage, first-class functions, scoping,
    polymorphism, closures, partial evaluation, views, separate compilation,
    binding, static and dynamic typing", 
  abstract     = "A persistent programming environment, together with a
    language supporting first class procedures, may be used to provide the
    semantic features of other object modeling languages.  In particular, the
    two concepts may be combined to implement abstract data types, modules,
    separate compilation, views, and data protection.  Furthermore, the ideas
    may be used in system construction and version control, as demonstrated
    here.", 
  location     = "https://doi.org/10.1145/4472.4477"
}

@Article{pasecat,
  author       = "Brad Appleton",
  title        = "Patterns and Software:  Essential Concepts and Terminology",
  journal      = "Object Magazine Online",
  year         = 1997,
  volume       = 3,
  number       = 5,
  month        = may,
  keywords     = "origins, history, pattern types, pattern components,
    qualities", 
  abstract     = "Fundamental to any science or engineering discipline is a
    common vocabulary for expressing its concepts, and a language for relating
    them together.  The goal of patterns within the software community is to
    create a body of literature to help software developers resolve recurring
    problems encountered throughout all of software development.  Patterns help
    create a shared language for communicating insight and experience about
    these problems and their solutions.  Formally codifying these solutions and
    their relationships lets us successfully capture the body of knowledge
    which defines our understanding of good architectures that meet the needs
    of their users.  Forming a common pattern language for conveying the
    structures and mechanisms of our architectures allows us to intelligibly
    reason about them.  The primary focus is not so much on technology as it is
    on creating a culture to document and support sound engineering
    architecture and design.", 
  location     = "https://www.bradapp.net/docs/patterns-intro.html"
}

@Article{cmfiaaitdoftcs,
  author       = "Aadrew~M. Tyrrell and Geof~F. Carpenter",
  title        = "{CSP} Methods for Identifying Atomic Actions in the Design of Fault Tolerant Concurrent Systems",
  journal      = tse,
  year         = 1995,
  volume       = 21,
  number       = 7,
  pages        = "629--639",
  month        = jul,
  keywords     = "CSP methods, atomic actions, fault tolerant concurrent
    systems design, error propagation, error recovery, fault tolerant parallel
    processing systems, fault tolerance mechanisms, underlying atomic actions,
    explicit trace evaluation, interprocess communications, CSP descriptions,
    structural arguments, full trace sets, communicating sequential processes", 
  abstract     = "Limiting the extent of error propagation when faults occur
    and localizing the subsequent error recovery are common concerns in the
    design of fault tolerant parallel processing systems.  Both activities are
    made easier if the designer associates fault tolerance mechanisms with the
    underlying atomic actions of the system.  With this in mind, the paper has
    investigated two methods for the identification of atomic actions in
    parallel processing systems described using CSP.  Explicit trace evaluation
    forms the basis of the first algorithm, which enables a designer to analyze
    interprocess communications and thereby locate atomic action boundaries in
    a hierarchical fashion.  The second method takes CSP descriptions of the
    parallel processes and uses structural arguments to infer the atomic action
    boundaries.  This method avoids the difficulties involved with producing
    full trace sets, but does incur the penalty of a more complex algorithm.", 
  location     = "https://doi.org/10.1109/32.392983"
}

@Article{abntir,
  author       = "Robert Fung and Brendan Del Favero",
  title        = "Applying Bayesian Networks to Information Retrieval",
  journal      = cacm,
  year         = 1995,
  volume       = 38,
  number       = 3,
  pages        = "42--57",
  month        = mar,
  keywords     = "information retrieval, baysian networks, search",
  abstract     = "Information retrieval (IR) is the identification of documents
    or other units of information in a collection that are relevant to a
    particular information need.  An information need is a set of questions to
    which someone would like to find an answer.  Here are some examples of IR
    tasks: finding articles in the New York Times that discuss the Iran-Contra
    affair; searching the recent postings in a Usenet newsgroup for references
    to a particular model of personal computer; finding the entries referring
    to butterflies in an online CD-ROM encyclopedia.", 
  location     = "https://doi.org/10.1145/203330.203340"
}

@Article{anlrfnc,
  author       = "Wei Li and Keshav Pingali",
  title        = "Access Normalization:  Loop Resturcturing for {NUMA} Compilers",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "285--295",
  month        = sep,
  keywords     = "numa compilation, data access matrices, loop transformations,
    data dependencies, numa code generation",
  abstract     = "In scalable parallel machines, processors can make local
    memory accesses much faster than they can make remote memory accesses.  In
    addition, when a number of remote accesses must be made, it is usually more
    efficient to use block transfers of data rather than to use many small
    messages.  To run well on such machines, software must exploit these
    features.  We believe it is too onerous for a programmer to do this by
    hand, so we have been exploring the use of restructuring compiler technology
    for this purpose.  In this paper, we start with a language like FORTRAN-D
    with user-specified data distribution and develop a systematic loop
    transformation strategy called access normalization that restructures loop
    nests to exploit locality and block transfers.  We demonstrate the power of
    our techniques using routines from the BLAS (Basic Linear Algebra
    Subprograms) library.  An important feature of our approach is that we
    model loop transformations using invertible matrices and integer lattice
    theory, thereby generalizing Banerjee's framework of unimodular matrices.", 
  location     = "https://doi.org/10.1145/143371.143541"
}

@Article{ctwofimmt,
  author       = "John Kubiatowicz and David Chaiken and Anant Agarwal",
  title        = "Closing the Window of Fulnerability in Multiphase Memory Transactions",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "274--284",
  month        = sep,
  keywords     = "deadlock, livelock, caching, context switching, cache coherence",
  abstract     = "Multiprocessor architects have begun to explore several
    mechanisms such as prefetching, context-switching and software-assisted
    dynamic cache-coherence, which transform single-phase memory transactions
    in conventional memory systems into multiphase operations.  Multiphase
    operations introduce a window of vulnerability in which data can be
    invalidated before it is used.  Losing data due to invalidations introduces
    damaging livelock situations.  This paper discusses the origins of the
    window of vulnerability and proposes an architectural framework that closes
    it.  The framework is implemented in Alewife, a large-scale multi-processor
    being built at MIT.", 
  location     = "https://doi.org/10.1145/143371.143540", 
  location     = "http://groups.csail.mit.edu/cag/papers/pdf/window-of-vulnerability.pdf"
}

@Article{csmsahfsm,
  author       = "Mark~D. Hill and James~R. Larus and Steven~K. Reinhardt and David~A. Wood",
  title        = "Cooperative Shared Memory:  Software and Hardware for Scalable Multiprocessors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "262--273",
  month        = sep,
  keywords     = "annotations, check-in/check-out, synchronization",
  abstract     = "We believe the paucity of massively parallel, shared-memory
    machines follows from the lack of a shared-memory programming performance
    model that can inform programmers of the cost of operations (so they can
    avoid expensive ones) and can tell hardware designers which cases are
    common (so they can build simple hardware to optimize them).  Cooperative
    shared memory, our approach to shared-memory design, addresses this
    problem.Our initial implementation of cooperative shared memory uses a
    simple programming model, called Check-In/Check-Out (CICO), in conjunction
    with even simpler hardware, called Dir1SW.  In CICO, programs bracket uses
    of shared data with a check_in directive terminating the expected use of
    the data.  A cooperative prefetch directive helps hide communication
    latency.  Dir1SW is a minimal directory protocol that adds little
    complexity to message-passing hardware, but efficiently supports programs
    written within the CICO model.", 
  location     = "https://doi.org/10.1145/161541.161544", 
  location     = "ftp://ftp.cs.wisc.edu/wwt/tocs93_csm.pdf"
}

@Article{esptb,
  author       = "Michael~D. Smith and Mark Horowitz and Monica~S. Lam",
  title        = "Efficient Superscalar Perfformance Through Boosting",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "248--259",
  month        = sep,
  keywords     = "speculative execution, instruction reordering, boosting,
    instruction scheduling, trace scheduling, hardware support, superscalar
    architectures",
  abstract     = "The foremost goal of superscalar processor design is to
    increase performance through the exploitation of instruction-level
    parallelism (ILP).  Previous studies have shown that speculative execution
    is required for high instruction per cycle (IPC) rates in non-numerical
    applications.  The general trend has been toward supporting speculative
    execution in complicated, dynamically-scheduled processors.  Performance,
    though, is more than just a high IPC rate; it also depends upon instruction
    count and cycle time.  Boosting is an architectural technique that supports
    general speculative execution in simpler, statically-scheduled processors.
    Boosting labels speculative instructions with their control dependence
    information.  This labelling eliminates control dependence constraints on
    instruction scheduling while still providng full dependence information to
    the hardware.  We have incorporated boosting into a trace-based, global
    scheduling algorithm that exploits ILP without adversely affecting the
    instruction count of a program.  We use this algorithm and estimates of the
    boosting hardware involved to evaluate how much speculative execution
    support is really necessary to achieve good performance.  We find that a
    statically-scheduled superscalar processor using a minimal implementation
    of boosting can easily reach the performance of a much more complex
    dynamically-scheduled superscalar processor.", 
  location     = "https://doi.org/10.1145/143371.143534"
}

@Article{ssfvasp,
  author       = "Scott~A. Mahlke and William~Y. Chen and Wen-mei~W. Hwu and B.~Ramakrishna Rau and Michael~S. Schlansker",
  title        = "Sentinel Scheduling for {VLIW} and Superscalar Processors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "238--247",
  month        = sep,
  keywords     = "superblock scheduling, percolation scheduling, instruction
    boosting, exception handling, sentinel scheduling, speculative stores",
  abstract     = "Speculative execution is an important source of parallelism
    for VLIW and superscalar processors.  A serious challenge with
    compiler-controlled speculative execution is to accurately detect and
    report all program execution errors at the time of occurrence.  In this
    paper, a set of architectural features and compile-time scheduling support
    referred to as sentinel scheduling is introduced.  Sentinel scheduling
    provides an effective framework for compiler-controlled speculative
    execution that accurately detects and reports all exceptions.  Sentinel
    scheduling also supports speculative execution of store instructions by
    providing a store buffer which allows probationary entries.  Experimental
    results show that sentinel scheduling is highly effective for a wide range
    of VLIW and superscalar processors.", 
  location     = "https://doi.org/10.1145/143371.143529"
}

@Article{fmefu,
  author       = "Brian~N. Bershad and David~D. Redell and John~R. Ellis",
  title        = "Fast Mutual Exclusion for Uniprocessors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "223--233",
  month        = sep,
  keywords     = "mutual exclusion, memory interlocking, restartable
    instructions",
  abstract     = "In this paper we describe restartable atomic sequences, an
    optimistic mechanism for implementing simple atomic operations (such as
    Test-And-Set) on a uniprocessor.  A thread that is suspended within a
    restartable atomic sequence is resumed by the operating system at the
    beginning of the sequence, rather than at the point of suspension.  This
    guarantees that the thread eventually executes the sequence atomically.  A
    restartable atomic sequence has significantly less overhead than other
    software-based synchronization mechanisms, such as kernel emulation or
    software reservation.  Consequently, it is an attractive alternative for
    use on uniprocessors that do no support atomic operations.  Even on
    processors that do support atomic operations in hardware, restartable
    atomic sequences can have lower overhead.We describe different
    implementations of restartable atomic sequences for the Mach 3.0 and Taos
    operating systems.  These systems' thread management packages rely on
    atomic operations to implement higher-level mutual exclusion facilities.
    We show that improving the performance of low-level atomic operations, and
    therefore mutual exclusion mechanisms, improves application performance.", 
  location     = "https://doi.org/10.1145/143371.143523"
}

@Article{acoeamdo,
  author       = "Kamen Yotov and Xiaoming Li and Gang Ren and Michael Cibulskis and Gerald DeJong and Maria Garzaran and David Padua and Keshav Pingali and Paul Stodghill and Peng Wu",
  title        = "{A} Comparison of Empirical and Model-Deiven Optimization",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation, PLDAI '03)",
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "63--76",
  month        = may,
  keywords     = "compilers, memory hierarchy, tiling, blocking, unrolling,
    program transformation, code generation, empirical optimization, model-driven
    optimization, blas",
  abstract     = "Empirical program optimizers estimate the values of key
    optimization parameters by generating different program versions and
    running them on the actual hardware to determine which values give the best
    performance.  In contrast, conventional compilers use models of programs
    and machines to choose these parameters.  It is widely believed that
    model-driven optimization does not compete with empirical optimization, but
    few quantitative comparisons have been done to date.  To make such a
    comparison, we replaced the empirical optimization engine in ATLAS (a
    system for generating a dense numerical linear algebra library called the
    BLAS) with a model-driven optimization engine that used detailed models to
    estimate values for optimization parameters, and then measured the relative
    performance of the two systems on three different hardware platforms.  Our
    experiments show that model-driven optimization can be surprisingly
    effective, and can generate code whose performance is comparable to that of
    code generated by empirical optimizers for the BLAS.", 
  location     = "https://dl.acm.org/doi/10.1145/781131.781140"
}

@Article{maccforvoct,
  author       = "Kristy Andrews and Duane Sand",
  title        = "Migrating a {CISC} Computer Family onto {RISC} via Object Code Translation",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "213--222",
  month        = sep,
  keywords     = "portability, interpretation",
  abstract     = "A minicomputer/mainframe family (Tandem NonStop Systems) and
    all of its machine-dependent vendor and user software has been moved from a
    proprietary CISC instruction set onto a generic RISC instruction set
    Translation of programs' CISC object code into optimized RISC object code
    is a migration path that is easy, gives greatly improved performance,and
    provides all the benefits of traditional object code compatibility.  These
    benefits include no reprogramming, no retraining, fast time to market, and
    debugging of optimized programs as if they were still running on the CISC
    platform.  This paper shares our experience in implementing this migration
    scheme, with measurements of the resulting performance and code size.", 
  location     = "https://doi.org/10.1145/143371.143520",
  location     = "https://www.hpl.hp.com/techreports/tandem/TR-92.1.pdf"
}

@Article{edb,
  author       = "Robert Wahbe",
  title        = "Efficient Data Breakpoints",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "200--212",
  month        = sep,
  keywords     = "debugging, data breakpoints, virtual memory protection",
  abstract     = "Breakpoints are user-specified rules that trigger debugging
    actions when certain conditions arise in an executing program.  To support
    source-level debugging, programmers should be able to specify breakpoints
    conditions in terms of programming language control and data abstractions.
    Support for breakpoints specified in terms of control conditions, known as
    control breakpoints, is ubiquitous.  The analogous data breakpoints, a
    breakpoints specified in terms of a data condition, is difficult to
    implement efficiently and has only limited support in most current
    debuggers.  A number of authors have speculated that efficient data
    breakpoints require hardware support.  In this paper we examine hardware
    and software strategies for implementing data breakpoints.  We use a
    simulation experiment to estimate the performance of four representative
    implementations.  We conclude that while hardware-based solutions are able
    to deliver the best overall performance, they are expensive and can
    simultaneously support only a limited number of breakpoints.  In contrast,
    a software solution based on modifying the code of the program being
    debugged to monitor all instructions that might affect the data breakpoints
    condition is simple and portable, and provides for any number of
    breakpoints.  Further, we show that its expected performance is acceptable
    for most debugging applications.", 
  location     = "https://doi.org/10.1145/143371.143518"
}

@Article{acpmuepcm,
  author       = "Kieran Harty and David~R. Cheriton",
  title        = "Application-Controlled Physical Memory using External Page-Cache Management",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "187--197",
  month        = sep,
  keywords     = "storage management, v++, external page-cache management, ",
  abstract     = "Next generation computer systems will have gigabytes of
    physical memory and processors in the 100 MIPS range or higher.  Contrary
    to some conjectures, this trend requires more sophisticated memory
    management support for memory-bound computations such as scientific
    simulations and systems such as large-scale database systems, even though
    memory management for most programs will be less of a concern.  We describe
    the design, implementation and evaluation of a virtual memory system that
    provides application control of physical memory using external page-cache
    management.  In this approach, a sophisticated application is able to
    monitor and control the amount of physical memory it has available for
    execution, the exact contents of this memory, and the scheduling and nature
    of page-in and page-out using the abstraction of a physical page cache
    provided by the kernel.  We claim that this approach can significantly
    improve performance for many memory-bound applications while reducing
    kernel complexity, yet does not complicate other applications or reduce
    their performance.", 
  location     = "https://doi.org/10.1145/143365.143511",
  location     = "http://i.stanford.edu/pub/cstr/reports/cs/tr/91/1394/CS-TR-91-1394.pdf"
}

@Article{atcpni,
  author       = "Dana~S. Henry and Christopher~F. Joerg",
  title        = "{A} Tightly-Coupled Processor-Network Interface",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "111--122",
  month        = sep,
  keywords     = "nics, register access, message-passing architectures, memory
    mapping, performance, optimizations",
  abstract     = "Careful design of the processor-network interface can
    dramatically reduce the software overhead of interprocessor communication.
    Our interface architecture reduces communication overhead five fold in our
    benchmarks.  Most of our performance gain comes from simple, low cost
    hardware mechanisms for fast dispatching on, forwarding of, and replying to
    messages.  The remaining improvement can be gained by implementing the
    network interface as part of the processor's register file.  For example,
    using our hardware mechanisms a register-mapped interface can receive,
    process, and reply to a remote read request in a total of two RISC
    instructions.  We have implemented an RTL model of an off-chip
    memory-mapped interface which provides our hardware mechanisms.  Our
    industrial partner, Motorola, is implementing a similar network interface
    on-chip in an experimental version of the 88110 processor.", 
  location     = "https://doi.org/10.1145/143371.143497",
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-342/memo-342.pdf"
}

@Article{cmfvic,
  author       = "Bob Wheeler and Brian~N. Bershad",
  title        = "Consistency Management for Virtually Indexed Caches",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "124--136",
  month        = sep,
  keywords     = "aliasing, virtual-physical mapping, state machines,
    consistency models, dma",
  abstract     = "A virtually indexed cache can improve performance by allowing
    cache lookup and address translation to occur in parallel, thus reducing
    processor cycle time.  Unlike physically indexed caches, virtually indexed
    caches create consistency problems because a physical address may be
    represented in more than one cache line when it has been accessed through
    more than one virtual address.  Write-back virtually indexed caches create
    additional inconsistencies because memory may become stale with respect…to
    the cache.  In this paper we examine the problem of consistency management
    for a virtually indexed write-back cache.  We assume that the hardware does
    not support intracache consistency.  We present model and software
    implementation strategy for maintaining consistency with virtually indexed
    caches.  We present measurements from an implementation of this model on
    the HP 9000 Series 7000 in the context of the Mach operating system.  Our
    measurements show that a virtually indexed cache can be managed with nearly
    the same cost as the required to manage a physically indexed one, even when
    used by a virtual memory system that encourages and exploits sharing.",
  location     = "https://doi.org/10.1145/143371.143499"
}

@Article{etatbfpac,
  author       = "Tzi-cker Chiueh and Randy~H. Katz",
  title        = "Eliminating the Address Translation Bottleneck for Physical Address Cache",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "137--148",
  month        = sep,
  keywords     = "cache architecture, lazy address translation, pipelining",
  abstract     = "Two architectural techniques are presented and analyzed in
    this paper that aim at eliminating the Translation Lcookaside Buffer (TLB)
    access delay from the critical path of physical address cache-based scalar
    processors.  The first technique, parallel address translation, masks the
    TLB access delay by using a set-associative virtual memory map to extend
    the cache size beyond the product of the cache associativity and the
    virtual memory page size.  The second technique, lazy address translation,
    bypasses the TLB access completely by using the base register and offset in
    a memory reference as a caching mechanism for its corresponding physical
    page.  Consequently the TLB access is needed only when this caching scheme
    fails.  A trace-driven simulation study is conducted and the experimental
    results show that under the given workload the parallel address translation
    scheme works best when the virtual memory is 16-way set associative, and
    the penalty on the average cycle-per-instruction due to lazy address
    translation is less than 1.3%.", 
  location     = "https://doi.org/10.1145/143371.143501"
}

@Article{acothsa,
  author       = "Ivan~E. Sutherland and Robert~F. Sproull and Robert~A. Schumacker",
  title        = "{A} Characterization of Ten Hidden-Surface Algorithms",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 1,
  pages        = "1--55",
  month        = mar,
  keywords     = "sorting, graphics rendering, hidden-line elimination,
    hidden-surface elimination, coherence, computer graphics, raster graphics,
    perspective transformation, algorithm analysis",
  abstract     = "The paper asserts that the hidden-surface problem is mainly
    one of sorting.  The various surfaces of an object to be shown in
    hidden-surface or hidden-line form must be sorted to find out which ones
    are visible at various places on the screen.  Surfaces may be sorted by
    lateral position in the picture (XY), by depth (Z), or by other criteria.
    The paper shows that the order of sorting and the types of sorting used
    form differences among the existing hidden-surface algorithms.  To reduce
    the work of sorting, each algorithm capitalizes on some coherence property
    of the objects represented.  'San-line coherence,' the fact that one TV
    scan line of output is likely to be nearly the same as the previous TV scan
    line, is one commonly used kind of coherence.  'Frame coherence,' the fact
    that the entire picture does not change very much between successive frames
    of a motion picture can be very helpful if it is applicable.  By
    systematically looking for additional kinds of coherence and untried
    sorting orders and sorting types, the paper is able to suggest two
    promising new approaches to the hidden-surface problem.  The first, a
    combination of three existing algorithms, is promising because it would
    capitalize on the frame and scan-line coherence.  The second new approach
    would sort in order Y, Z, X, the only sorting order for which an existing
    algorithm could not be found.", 
  location     = "https://dl.acm.org/doi/abs/10.1145/356625.356626"
}

@Article{apeoohccp,
  author       = "Jack~E. Veenstra and Robert~J. Fowler",
  title        = "{A} Performance Evaluation of Optimal Hybrid Cache Coherency Protocols",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "149--160",
  month        = sep,
  keywords     = "coherency protocols, cost models, trace-driven simulations",
  abstract     = "The caches within a multiprocessor typically use either a 
    write-invalidate protocol or a write-update protocol to maintain
    consistency.  The recently introduced MIPS R4000 processor allows operating
    system software to select, on a per-page basis, which multiprocessor cache
    coherence protocol (write-invalidate versus write-update) the hardware will
    use.  The availability of the R4000 and the prospect of even more flexible
    hardware motivated us to examine the potential performance advantages of
    allowing user-level control over the choice of coherence protocol on a
    per-page basis and to ask whether more powerful hybrid protocols provide
    substantially more benefit.  We examine the potential benefits of three
    classes of hybrid protocols: (1) hybrid protocols that choose statically,
    at the beginning of the program, between write-invalidate (WI) or
    write-update (WU) on a per-page basis, (2) hybrid protocols that choose
    statically between WI or WU for each cache block, and (3) dynamic hybrid
    protocols that can choose between WI or WU at each write.  In order to
    determine how much potential benefit could be obtained by each of these
    protocol classes, we used trace-driven simulations to evaluate the optimal
    off-line protocol for each class.  We found that the use of a hybrid
    protocol can substantially reduce the cost of memory references for most of
    the programs studied.  A few programs can also realize large additional
    benefits from a per-block static hybrid protocol compared to a per-page
    static hybrid protocol.  None of the programs, however, receive a
    significant additional benefit from using a dynamic hybrid protocol
    compared to the per-block static hybrid protocol, unless cache block sizes
    are larger than 16 words (64 bytes).", 
  location     = "https://doi.org/10.1145/143371.143503"
}

@Article{ctcaspoamos,
  author       = "Josep Torrellas and Anoop Gupta and John Hennessy",
  title        = "Charactrizing the Caching and Synchronization Performance of a Multiprocessor Operating System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "162--174",
  month        = sep,
  keywords     = "workloads, cache behavior, os cache activity, cache misses",
  abstract     = "Good cache memory performance is essential to achieving high
    CPU utilization in shared memory multiprocessors.  While the performance of
    caches is determined by both application and operating system (OS)
    references, most research has focused on the cache performance of the OS is
    largely unknown.  In this paper we characterize the cache performance of a
    commercial System V UNIX running on a four-CPU multiprocessor.  The related
    issue of the performance impact of the OS synchronization activity is also
    studied.  For our study, we use a hardware monitor that records the cache
    misses in the machine without perturbing it.  We study three multiprocessor
    workloads: a parallel compile, a multiprogrammed load, and a commercial
    database.  Our results show that OS misses occur frequently enough to stall
    CPUs for 17--21% of their non-idle time.  Further, if we include
    application misses induced by OS interference in the cache, then the stall
    time reaches 25%.  A detailed analysis reveals three major sources of OS
    misses: instruction fetches, process migration and data accesses in block
    operations.  As for synchronization behavior, we find that OS
    synchronization has low overhead if supported correctly and that OS locks
    show good locality and low contention.", 
  location     = "https://doi.org/10.1145/143371.143506"
}

@Article{hsssflan,
  author       = "Thomas~E. Anderson and Susan~S. Owicki and James~B. Saxe and Charles~P. Thacker",
  title        = "High-Speed Switch Scheduling for Local-Area Networks",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "98--110",
  month        = sep,
  keywords     = "scheduling, data forwarding, port buffering, cross-bar
    switches, optical networks, high-speed networking, bipartite-graph
    matching, fabric scheduling",
  abstract     = "Current technology trends make it possible to build
    communication networks that can support high-performance distributed
    computing.  This paper describes issues in the design of a prototype switch
    for an arbitrary topology point-to-point network with link speeds of up to
    1 Gbit/s.  The switch deals in fixed-length ATM-style cells, which it can
    process at a rate of 37 million cells per second.  It provides high
    bandwidth and low latency for datagram traffic.  In addition, it supports
    real-time traffic by providing bandwidth reservations with guaranteed
    latency bounds.  The key to the switch's operation is a technique called
    parallel iterative matching, which can quickly identify a set of
    conflict-free cells for transmission in a time slot.  Bandwidth
    reservations are accommodated in the switch by building a fixed schedule
    for transporting cells from reserved flows across the switch; parallel
    iterative matching can fill unused slots with datagram traffic.  Finally,
    we note that parallel iterative matching may not allocate bandwidth fairly
    among flows of datagram traffic.  We describe a technique called
    statistical matching, which can be used to ensure fairness at the switch
    and to support applications with rapidly changing needs for guaranteed
    bandwidth.", 
  location     = "https://doi.org/10.1145/161541.161736", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-99.pdf"
}

@Article{lfosipac,
  author       = "Jan Newmarch",
  title        = "Lessons from Open Source:  Intellectual Property and Courseware",
  journal      = "First Monday",
  year         = 2001,
  volume       = 6,
  number       = 6,
  month        = apr,
  keywords     = "intellectual property, secrecy, courseware, the commons,
    copyright",
  abstract     = "In this competitive age, universities are seeking ways to
    protect their intellectual property, for fear that it might be stolen or
    used by others without financial benefit coming back to the university.
    Increasingly, universities are using mechanisms of secrecy to secure their
    property.  This paper argues that this approach is wrong on both moral and
    business grounds, and that a better model can be found in the Open Source
    movement of the software industry.", 
  location     = ""
}

@Article{asfsasos,
  author       = "Eric~J. Koldinger and Jeffrey~S. Chase and Susan~J. Eggers",
  title        = "Architectural Support for Single Address Space Operating System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "175--186",
  month        = sep,
  keywords     = "single address space operating systems, single global virtual
    address space, memory system architecture, protection, domains, system
    organization, sharing, wide-address architectures, 32-bit address spaces,
    virtually indexed caches, protection lookaside buffer", 
  abstract     = "Recent microprocessor announcements show a trend toward
    wide-address computers: architectures that support 64 bits of virtual
    address space.  Such architectures facilitate fundamentally new operating
    system organizations that promote efficient data sharing and cooperation,
    both between complex applications and between parts of the operating system
    itself.  One such organization is the single address space operating
    system, in which all processes run within a single global virtual address
    space; protection is provided not through conventional address space
    boundaries, but through protection domains that dictate which pages of the
    global address space a process can reference.  This paper focuses on the
    architectural implications of single address space operating systems,
    specifically the interaction between the memory system architecture and the
    operating system's use of addressing and protection.  Our purpose is to
    explore certain architectural opportunities created by single address space
    by evaluating two protection models that support them.  The first provides
    protection on a per-page, per-domain basis; we define the protection
    lookaside buffer, a hardware structure that implements this model.  The
    second provides protection on a page-group basis; this model is implemented
    in the Hewlett-Packard PA-RISC architecture.", 
  location     = "https://doi.org/10.1145/143371.143508"
}

@Article{nvmffrfs,
  author       = "Mary Baker and Satoshi Asami and Etienne Deprit and John Ouseterhout and Margo Seltzer",
  title        = "Non-Volatile Memory for Fast, Reliable File Systems",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "10--22",
  month        = sep,
  keywords     = "nvram, write file-system performance, client-side caching,
    server-side caching, log-structured file systems",
  abstract     = "Given the decreasing cost of non-volatile RAM (NVRAM), by the
    late 1990’s it will be feasible for most workstations to include a megabyte
    or more of NVRAM, enabling the design of higher-performance, more reliable
    systems.  We present the trace-driven simulation and analysis of two uses
    of NVRAM to improve I/O performance in distributed file systems:
    non-volatile file caches on client workstations to reduce write traffic to
    file servers, and write buffers for write-optimized file systems to reduce
    server disk accesses.  Our results show that a megabyte of NVRAM on
    diskless clients reduces the amount of file data written to the server by
    40 to 50%.  Increasing the amount of NVRAM shows rapidly diminishing
    returns, and the particular NVRAM block replacement policy makes little
    difference to write traffic.  Closely integrating the NVRAM with the
    volatile cache provides the best total traffic reduction.  At today’s
    prices, volatile memory provides a better performance improvement per
    dollar than NVRAM for client caching, but as volatile cache sizes increase
    and NVRAM becomes cheaper, NVRAM will become cost effective.  On the server
    side, providing a one-half megabyte write-buffer per file system reduces
    disk accesses by about 20% on most of the measured logstructured file
    systems (LFS), and by 90% on one heavilyused file system that includes
    transaction-processing workloads.", 
  location     = "https://doi.org/10.1145/143371.143380"
}

@Article{pdfcoirda,
  author       = "Mark Holland and Garth~A. Gibson",
  title        = "Parity Declustering for Continuous Operation in Redundant Disk Arrays",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "23--35",
  month        = sep,
  keywords     = "raid disks, error recovery, declustering, data layout, data
    reconstruction",
  abstract     = "We describe and evaluate a strategy for declustering the
    parity encoding in a redundant disk array.  This declustered parity
    organization balances cost against data reliability and performance during
    failure recovery.  It is targeted at highly-available parity-based arrays
    for use in continuousoperation systems.  It improves on standard parity
    organizations by reducing the additional load on surviving disks during the
    reconstruction of a failed disk’s contents.  This yields higher user
    throughput during recovery, and/or shorter recovery time.  We first address
    the generalized parity layout problem, basing our solution on balanced
    incomplete and complete block designs.  A software implementation of
    declustering is then evaluated using a disk array simulator under a highly
    concurrent workload comprised of small user accesses.  We show that
    declustered parity penalizes user response time while a disk is being
    repaired (before and during its recovery) less than comparable
    non-declustered (RAID5) organizations without any penalty to user response
    time in the fault-free state.  We then show that previously proposed
    modifications to a simple, single-sweep reconstruction algorithm further
    decrease user response times during recovery, but, contrary to previous
    suggestions, the inclusion of these modifications may, for many
    configurations, also slow the reconstruction process.  This result arises
    from the simple model of disk access performance used in previous work,
    which did not consider throughput variations due to positioning delays.", 
  location     = "https://doi.org/10.1145/143371.143383",
  location     = "http://www.pdl.cmu.edu/PDL-FTP/Declustering/ASPLOS.pdf"
}

@Article{ssfsl,
  author       = "Anne Rogers and Kai Li",
  title        = "Software Support for Speculative Loads",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "38--50",
  month        = sep,
  keywords     = "speculative loads, instruction lifting, loop unrolling,
    basic-block lifting",
  abstract     = "This paper describes a very simple mechanism and related
    compiler support for software--controlled speculative loads.  The compiler
    issues speculative load instructions based on anticipated data references
    and the ability of the memory system to hide memory latency in
    high--performance processors.  The architectural support for such a
    mechanism is simple and minimal, yet handles faults gracefully.  We have
    simulated the speculative load mechanism based on a MIPS processor and a
    detailed memory system.  The results of scientific kernel loops indicate
    that the speculative load techniques are effective approaches to hiding
    memory latency.", 
  location     = "https://doi.org/10.1145/143365.143484"
}

@Article{aupocp,
  author       = "Jeremy Jacob",
  title        = "{A} Uniform Presentation of Confidentiality Properties",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1186--1194",
  month        = nov,
  keywords     = "theoretical foundations of security, security models,
    information flow, formal methods, measures of confidentiality, shared
    systems, program correctness, program specification",
  abstract     = "Security (in the sense of confidentiality) properties are
    properties of shared systems.  A suitable model of shared systems, in which
    one can formally define the term security property and then proceed to
    catalog several security properties, is presented.  The purpose is to
    present various information-flow properties in a manner that exposes their
    differences and similarities.  Abstraction is the main tool, and everything
    that is not central to the purpose is discarded.  The presentation is
    generic in the model of computation.  The abstraction lays bare a regular
    structure into which many interesting information-flow properties fall.  A
    shared system is represented by a relation.  How this model lets one reason
    about information flow is discussed and the term information flow property
    is formally defined.  Various information-flow properties are described.
    Composability and probabilistic security properties are addressed.", 
  location     = "https://doi.org/10.1109/32.106973"
}

@Article{rmlvnbapc,
  author       = "Tien-Fu Chen and Jean-Loup Baer",
  title        = "Reducing Memory Latency via Non-Blocking and Prefetching Caches",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "51--61",
  month        = sep,
  keywords     = "non-blocking caches, prefetching caches, memory models,
    architectural variations, latency, compiler manipulations",
  abstract     = "Non-blocking caches and prefetching caches are two techniques
    for hiding memory latency by exploiting the overlap of processor
    computations with data accesses.  A non-blocking cache allows execution to
    proceed concurrently with cache misses as long as dependency constraints
    are observed, thus exploiting post-miss operations.  A prefetching cache
    generates prefetch requests to bring data in the cache before it is
    actually needed, thus allowing overlap with pre-miss computations.  In this
    paper, we evaluate the effectiveness of these two hardware-based schemes.
    We propose a hybrid design based on the combination of these approaches.
    We also consider compiler-based optimizations to enhance the effectiveness
    of non-blocking caches.  Results from instruction level simulations on the
    SPEC benchmarks show that the hardware prefetching caches generally
    outperform non-blocking caches.  Also, the relative effectiveness of
    non-blocking caches is more adversely affected by an increase in memory
    latency than that of prefetching caches.  However, the performance of
    non-blocking caches can be improved substantially by compiler optimizations
    such as instruction scheduling and register renaming.  The hybrid design
    can be effective in reducing the memory latency penalty for many applications.", 
  location     = "https://doi.org/10.1145/143371.143486",
  location     = "https://dada.cs.washington.edu/research/tr/1992/06/UW-CSE-92-06-03.pdf"
}

@Article{daeoacafp,
  author       = "Todd~C. Mowry and Monica~S. Lam and Anoop Gupta",
  title        = "Design and Evaluation of a Compiler Algorithm for Prefetching",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "62--73",
  month        = sep,
  keywords     = "memory hierarchy, access optimization, prefetching, locality
    analysis, loop splitting, prefetch scheduling, software pipelining, cpu
    stalls",
  abstract     = "Software-controlled data prefetching is a promising technique
    for improving the performance of the memory subsystem to match today’s
    high-performance processors.  While prefetching is useful in hiding the
    latency, issuing prefetches incurs an instruction overhead and can increase
    the load on the memory subsystem.  As a result, care must be taken to
    ensure that such overheads do not exceed the benefits.  This paper proposes
    a compiler algorithm to insert prefetch instructions into code that
    operates on dense matrices.  Our algorithm identifies those references that
    are likely to be cache misses, and issues prefetches only for them.  We
    have implemented our algorithm in the SUIF (Stanford University
    Intermediate Form) optimizing compiler.  By generating fully functional
    code, we have been able to measure not only the improvements in cache miss
    rates, but also the overall performance of a simulated system.  We show
    that our algorithm significantly improves the execution speed of our
    benchmark programs—some of the programs improve by as much as a factor of
    two.  When compared to an algorithm that indiscriminately prefetches all
    array accesses, our algorithm can eliminate many of the unnecessary
    prefetches without any significant decrease in the coverage of the cache
    misses.", 
  location     = "https://doi.org/10.1145/143371.143488",
  location     = "https://suif.stanford.edu/papers/mowry92.pdf"
}

@Article{itaodbpubc,
  author       = "Shien-Tai Pan and Kimming So and Joseph~T. Rahmeh",
  title        = "Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "76--84",
  month        = sep,
  keywords     = "dynamic branch prediction, counter-based prediction,
    correlation-based prediction",
  abstract     = "Long branch delay is a well–known problem in today’s high
    performance superscalar and supetpipeline processor designs.  A common
    technique used to alleviate this problem is to predict the direction of
    branches during the instruction fetch.  Counter-based branch prediction, in
    particular, has been reported as an effective scheme for predicting the
    direction of branches.  However, its accuracy is generally limited by
    branches whose future behavior is also dependent upon the history of other
    branches.  To enhance branch prediction accuracy with a minimum increase in
    hardware cost, we propose a correlation-based scheme and show how the
    prediction accuracy can be improved by incorporating information, not only
    from the history of a specific branch, but also from the history of other
    branches.  Specifically, we use the information provided by a proper
    subhistory of a branch to predict the outcome of that branch.  The proper
    subhistory is selected based on the outcomes of the most recently executed
    M branches.  The new scheme is evaluated using traces collected from
    running the spec benchmark suite on an IBM RISC System/60000 workstation.
    The results show that, as compared with the 2-bit counter-based prediction
    scheme, the correlation-based branch prediction achieves up to 11%
    additional accuracy at the extra hardware cost of one shift register.  The
    results also show that the accuracy of the new scheme surpasses that of the
    counter-based branch prediction at saturation.", 
  location     = "https://doi.org/10.1145/143371.143490"
}

@Article{pcbdfproap,
  author       = "Joseph~A. Fisher and Stefan~M. Freudenberger",
  title        = "Prediction Conditional Branch Directions from Previous Runs of a Program",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "85--95",
  month        = sep,
  keywords     = "conditional branch predictions, instructions per mispredicted
    branch, instruction-level parallelism",
  abstract     = "There are several important reasons for predicting which way
    the flow of control of a program is going to go: first, in
    instruction-level parallel architectures, code motions can produce more
    data-ready candidate instructions at once than there are resources to
    execute them.  Some of these are speculative (executed ahead of a
    conditional branch that might otherwise have prevented their execution), so
    one must sensibly pick among them, and one must avoid issuing low
    probability speculative instructions when the system overhead associated
    with canceling them most of the time outweighs the gain of their infrequent
    success; second, important classes of compiler optimizations depend upon
    this information; and finally, branch prediction can help optimize
    pipelined fetch and execute, icache fill, etc.  If substantial code motions
    are desired, it is probably impractical to expect the hardware to make
    them, and a compiler must instead.  Thus, the compiler must have access to
    branch predictions made before the program runs.  In this paper we consider
    the question of how predictable branches are when previous runs of a
    program are used to feed back information to the compiler.  We propose new
    measures which we believe more clearly capture the predictability of
    branches in programs.  We find that even code with a complex flow of
    control, including systems utilities and and language processors written in
    C, are dominated by branches which go in one way, and that this direction
    usually varies little when one changes the data used as the predictor and
    target.", 
  location     = "https://doi.org/10.1145/143371.143493",
  location     = "https://www.hpl.hp.com/techreports/92/HPL-92-98.html"
}

@Article{ttinp,
  author       = "Lal George and Matthias Blume",
  title        = "Taming the {IXP} Network Processor",
  journal      = pldi03,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "26--34",
  month        = may,
  keywords     = "network processors, intel ixa, integer linear programming,
    register allocation, bank assignment, programming languages, code generation",
  abstract     = "We compile Nova, a new language designed for writing network
    processing applications, using a back end based on integer-linear
    programming (ILP) for register allocation, optimal bank assignment, and
    spills.  The compiler's optimizer employs CPS as its intermediate
    representation; some of the invariants that this IR guarantees are
    essential for the formulation of a practical ILP model.Appel and George
    used a similar ILP-based technique for the IA32 to decide which variables
    reside in registers but deferred the actual assignment of colors to a later
    phase.  We demonstrate how to carry over their idea to an architecture with
    many more banks, register aggregates, variables with multiple simultaneous
    register assignments, and, very importantly, one where bank- and
    register-assignment cannot be done in isolation from each other.  Our
    approach performs well in practise---without causing an explosion in size
    or solve time of the generated integer linear programs.", 
  location     = "https://doi.org/10.1145/781131.781135"
}

@Article{cpcpbh,
  author       = "Per Brinch Henson",
  title        = "Concurrent Programming Concepts",
  journal      = surveys,
  year         = 1973,
  volume       = 5,
  number       = 4,
  pages        = "223--245",
  month        = dec,
  keywords     = "structured multiprogramming, programming languages, operating
    systems, programming errors, resource protection, compile-time checking,
    correctness proofs, sequential and concurrent processes, synchronizing
    events, semaphores, shared data, mutual exclusion, critical regions, monitors",
  abstract     = "This paper describes the evolution of language features for
    multiprogramming from event queues and semaphores to critical regions and
    monitors.  It suggests that the choice of language concepts should be
    guided by two simple principles: First, it should be possible to understand
    a concurrent program in time-independent terms by an effort proportional to
    its size; secondly, it should be possible to state assumptions about
    invarmnt relationships among program components and have these assumptions
    checked automatically.  The central problems of multiprogramming are
    illustrated by annotated algorithms written in a weU-structured programming
    language.", 
  location     = "https://doi.org/10.1145/356622.356624"
}

@Article{oldcialsfs,
  author       = "Michael Burrows and Charles Jerian and Butler Lampson and Timothy Mann",
  title        = "On-Line Data Compression in an Log-Structured File System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "2--9",
  month        = sep,
  keywords     = "sprite, log-structured file systems, data compression,
    hardware compression, write-log management, i-o performance",
  abstract     = "We have incorporated on-line data compression into the low
    levels of a log-structured file system (Rosenblum’s Sprite LFS).  Each
    block of data or meta-data is compressed as it is written to the disk and
    decompressed as it is read.  The log-structuring overcomes the problems of
    allocation and fragmentation for variable-sized blocks.  We observe
    compression factors ranging from 1.6 to 2.2, using algorithms running from
    1.7 to 0.4 MBytes per second in software on a DECstation 5000/200.  System
    performance is degraded by a few percent for normal activities (such as
    compiling or editing), and as much as a factor of 1.6 for file system
    intensive operations (such as copying multi-megabyte files).  Hardware
    compression devices mesh well with this design.  Chips are already
    available that operate at speeds exceeding disk transfer rates, which
    indicates that hardware compression would not only remove the performance
    degradation we observed, but might well increase the effective disk
    transfer rate beyond that obtainable from a system without compression.",
  location     = "https://www.hpl.hp.com/techreports/92/HPL-92-85.html"
}

@Article{laaoosp,
  author       = "Andrew~A. Lamb and William Thies and Saman Amarasinghe",
  title        = "Linear Analysis and Optimization of Stream Programs",
  journal      = pldi03,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "26--34",
  month        = may,
  keywords     = "stream programming, streamit, optimization, embedded systems,
    linear systems, algebraic simplification, dsp, fft",
  abstract     = "As more complex DSP algorithms are realized in practice,
    there is an increasing need for high-level stream abstractions that can be
    compiled without sacrificing efficiency.  Toward this end, we present a set
    of aggressive optimizations that target linear sections of a stream
    program.  Our input language is StreamIt, which represents programs as a
    hierarchical graph of autonomous filters.  A filter is linear if each of
    its outputs can be represented as an affine combination of its inputs.
    Linearity is common in DSP components; examples include FIR filters,
    expanders, compressors, FFTs and DCTs.We demonstrate that several
    algorithmic transformations, traditionally hand-tuned by DSP experts, can
    be completely automated by the compiler.  First, we present a linear
    extraction analysis that automatically detects linear filters from the
    C-like code in their work function.  Then, we give a procedure for
    combining adjacent linear filters into a single filter, as well as for
    translating a linear filter to operate in the frequency domain.  We also
    present an optimization selection algorithm, which finds the sequence of
    combination and frequency transformations that will give the maximal
    benefit.We have completed a fully-automatic implementation of the above
    techniques as part of the StreamIt compiler, and we demonstrate a 450%
    performance improvement over our benchmark suite.", 
  location     = "https://doi.org/10.1145/780822.781134", 
  location     = "https://dspace.mit.edu/handle/1721.1/29668"
}

@Article{cpocptieftl,
  author       = "Marty Ossefort",
  title        = "Correctness Proofs of Communicating Processes:  Three Illustrative Examples from the Literature",
  journal      = toplas,
  year         = 1983,
  volume       = 5,
  number       = 4,
  pages        = "620--640",
  month        = oct,
  keywords     = "message-passing system, program proofs, rebound sorting,
    communicating sequential processes, csp, pre- and post-conditions,
    hierarchy, local and global reasoning",
  abstract     = "The proof method for process networks proposed by Misra and
    Chandy is applied to three examples from the literature.  The proof method
    is easy to use, preserves process autonomy in the network proof, and
    conforms naturally to the hierarchical structure of the network.  Two very
    large-scale integration algorithms and a sorting network are presented in a
    variant Hoare's communicating sequential processes model, specified
    completely, and formally proved.",
  location     = "https://doi.org/10.1145/69575.2083"
}

@Article{adofssfid,
  author       = "Kanth Miriyala and Mehdi~T. Harandi",
  title        = "Automatic Derivation of Formal Software Specifications from Informal Descriptions",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 10,
  pages        = "1126--1142",
  month        = oct,
  keywords     = "approximate analogy, formal specification, informal
    specification, difference-based reasoning, schemas, specification assistance",
  abstract     = "SPECIFIER, an interactive system which derives formal
    specifications of data types and programs from their informal descriptions,
    is described.  The process of deriving formal specifications is viewed as a
    problem-solving process.  The system uses common problem-solving techniques
    such as schemas, analogy, and difference-based reasoning to derive formal
    specifications.  If an informal description is a commonly occurring
    operation for which the system has a schema, then the formal specification
    is derived by instantiating the schema.  If there is a no such schema,
    SPECIFIER tries to find a previously solved problem which is analogous to
    the current problem.  If the problem found is directly analogous to the
    current problem, it applies an analogy mapping to obtain a formal
    specification.  On the other hand, if the analogy found is only
    approximate, it solves the directly analogous part of the problem by
    analogy and performs difference-based reasoning using the remaining
    (unmatched) parts to transform the formal specification obtained by analogy
    to a formal specification for the entire original problem.",
  location     = "https://doi.org/10.1109/32.99198"
}

@Article{arotvvsm,
  author       = "Paul~A. Karger and Mary Ellen Zurko and Douglas~W. Bonin and Andrew~H. Mason and Clifford~E. Kahn",
  title        = "{A} Retrospective on the {VAX} {VMM} Security Model",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1147--1165",
  month        = nov,
  keywords     = "computer security, virtual machines, covert channels,
    mandatory security, discretionary security, layered design, security kernels,
    protection rights",
  abstract     = "The development of a virtual-machine monitor (VMM) security
    kernel for the VAX architecture is described.  The focus is on how the
    system's hardware, microcode, and software are aimed at meeting A1-level
    security requirements while maintaining the standard interfaces and
    applications of the VMS and ULTRIX-32 operating systems.  The VAX security
    kernel supports multiple concurrent virtual machines on a single VAX
    system, providing isolation and controlled sharing of sensitive data.
    Rigorous engineering standards were applied during development to comply
    with the assurance requirements for verification and configuration
    management.  The VAX security kernel has been developed with a heavy
    emphasis on performance and system management tools.  The kernel performs
    sufficiently well that much of its development was carried out in virtual
    machines running on the kernel itself, rather than in a conventional
    time-sharing system.", 
  location     = ""
}

@TechReport{aqmafd30,
  author       = "Greg White",
  title        = "Active Queue Management Algorithms for {DOCSIS} 3.0",
  subtitle     = "A Simulation Study of CoDel, SfQ-CoDel and PIE in DOCSIS 3.0 Networks",
  institution  = "Access Network Technologies, CableLabs",
  year         = 2013,
  month        = apr,
  keywords     = "latency, buffering, buffer bloat, queue drop algorithms, pie,
    codel, sfq-codel, packet loss, tcp, performance",
  abstract     = "This paper describes the results of a simulation study of
    three active queue management algorithms applied to the upstream
    transmission buffer in a DOCSIS 3.  cable modem.  This paper is a follow-on
    to an earlier study which examined the Controlled Delay (CoDel) active
    queue management algorithm in a simulated DOCSIS 3.  cable modem.  This
    expanded study looks at CoDel in more depth, and compares it to two other
    promising active queue management algorithms, Stochastic Flow Queue - CoDel
    (SFQ- CoDel) and Proportional Integral Enhanced (PIE).  These three queue
    management algorithms are compared to existing (tail drop) buffering
    implementations that exist in current cable modems across a range of
    latency-sensitive applications.  It is demonstrated that current cable
    modem implementations result in severe degradation of user experience for
    latency-sensitive applications in situations where the user is
    simultaneously uploading a file via TCP.  The goal of the active queue
    managers in this study is to prevent the degradation of latency sensitive
    applications, while not impacting the TCP upload performance.  The
    Stochastic Flow Queue - Controlled Delay active queue manager displays
    extremely good performance in most traffic scenarios, enabling up to 2x
    reduction in latency for gaming traffic, x reduction in web page load time,
    and pristine VoIP quality, all while minimally impacting TCP upload
    performance.  The Proportional Integral Enhanced active queue manager
    similarly provided good performance, and is optimized for efficient
    implementation in existing cable modems.", 
  location     = "https://www.cablelabs.com/wp-content/uploads/2014/05/Active_Queue_Management_Algorithms_DOCSIS_3_0.pdf"
}

@TechReport{apfdsifcip,
  author       = "Joel Moses",
  title        = "{A} Program for Drilling Students in Freshman Calculus Integration Problems",
  institution  = "Artificial Intelligence Project, Project MAC, " # mit,
  year         = 1968,
  type         = "Memo",
  number       = 158,
  address      = cma,
  month        = mar,
  keywords     = "integration, problem solving, testing",
  abstract     = "The SARGE program is a prototype of a program which is
    intended to be used as an adjacent to regular classroom work in freshman
    calculus.  Using SARGE, students can type their step-by-step solution to an
    indefinite integration problem, and can have the correctness of their
    solution determined by the system.  The syntax for these steps comes quite
    close to normal mathematical notation, given the limitations of typewriter
    input.  The methods of solution is pretty much unrestricted as long as no
    mistakes are made along the way.  If a mistake is made, SARGE will catch it
    and yield an error message.  The student may modify the incorrect step, or
    he may ask the program for advice on how the mistake arose by typing
    'help'.  At present the program is weak in generating explanations for
    mistakes.  Sometimes the 'help' mechanisms will just yield a response which
    will indicate the way in which the erroneous step can be corrected.  In
    order to improve the explanation mechanism one would need a sophisticated
    analysis of students solutions to homework or quiz problems.  Experience
    with the behavior of students with SARGE, which is nil at present, should
    also help in accomplishing this goal.  SARGE is available as SARGE SAVED in
    T302 2517.", 
  location     = "https://dspace.mit.edu/bitstream/handle/1721.1/6163/AIM-158.pdf"
}

@TechReport{atsij,
  author       = "Christian Heinlein",
  title        = "Advanced Thread Synchronization in {Java}",
  institution  = "Dept. of Computer Structures, University of Ulm",
  year         = 2002,
  address      = "Ulm, Germany",
  keywords     = "bounded buffers, interaction expressions, readers-writers
    problem, synchronization, threads, java",
  abstract     = "Thread synchronization in Java using synchronized methods or
    statements is simple and straightforward as long as mutual exclusion of
    threads is sufficient for an application.  Things become less
    straightforward when wait() andnotify() have to be employed to realize more
    flexible synchronization schemes.  Using two well-known examples, the
    bounded buffer and the readers and writers problem, the traps and snares of
    hand-coded synchronization code and its entanglement with the actual
    application code are illustrated.  Following that, interaction expressions
    are introduced as a completely different approach where synchronization
    problems are solved in a declarative way by simply specifying permissible
    execution sequences of methods.  Their integration into the Java
    programming language using a simple precompiler and the basic ideas to
    enforce at run time the synchronization constraints specified that way are
    described.", 
  location     = "https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.1007%2F3-540-36557-5_25"
}

@TechReport{agifc,
  author       = "Lorenzo Alvisi and Fred~B. Schneider",
  title        = "{A} Graphical Interface for {CHIP}",
  institution  = dcs # "Cornell University",
  year         = 1996,
  number       = 1591,
  address      = itny,
  month        = "6 " # jun,
  keywords     = "chip, debugging, hypothetical processors",
  abstract     = "CHIP (Cornell Hypothetical Instructional Processor) [BBDS83]
    is a computer system designed as an educational tool for teaching
    undergraduate courses in operating system and machine architecture.  This
    document describes CHIP's graphical interface and covers in a tutorial how
    the interface is used to debug and execute CHIP programs.  A Graphical
    Interface for CHIP Lorenzo Alvisi The University of Texas at Austin
    Department of Computer Sciences Austin, TX Fred B.  Schneider Cornell
    University Department of Computer Science Ithaca, NY 6 June 1996 Abstract
    CHIP (Cornell Hypothetical Instructional Processor) is a computer system
    designed for use in teaching undergraduate courses in operating system and
    machine architecture.  This document describes CHIP's graphical interface
    and contains a tutorial describing how the interface is used to debug and
    execute CHIP programs.", 
  location     = "https://www.cs.cornell.edu/fbs/publications/96-1591.pdf"
}

@TechReport{rovhcm,
  author       = "Daniel~C. Hyde",
  title        = "Realization of {Verilog HDL} Computation Model",
  institution  = dcs # "Bucknell University",
  year         = 1997,
  month        = oct,
  keywords     = "vhdl, computational model, digital logic circuits"
}

@TechReport{s04d,
  author       = "Michael Dales",
  title        = "{SWARM} 0.44 Documentation",
  institution  = dcs # "University of Glasgow",
  year         = 2000,
  address      = "Glasgow, Scotland",
  month        = nov,
  keywords     = "architecture, compilation",
  location     = "This document gives a brief explanation of the design and
    implementation of SWARM --- the Software ARM.  It explains what SWARM is,
    and what it isn't, along with the design philosophy."
}

@TechReport{soovfs,
  author       = "Timo Lehtinen",
  title        = "Store --- Object-Oriented Virtual File System",
  institution  = "Stream Technologies Inc.",
  year         = 1993,
  address      = "Espoo, Finland",
  month        = "29 " # jul,
  keywords     = "global name spaces, class store, support class"
}

@TechReport{hvavcveaeispp,
  author       = "Paul Hudak and Mark~P. Jones",
  title        = "Haskell vs. {Ada} vs. " # cpp # " vs. \ldots\ An Experiment in Software Prototyping Productivity",
  institution  = dcs # "Yale University",
  year         = 1994,
  number       = 1049,
  address      = nhco,
  month        = "4 " # jul,
  keywords     = "software engineering, software prototyping, software
    development, programming languages",
  abstract     = "We describe the results of an experiment in which several
    conventional programming languages, together with the functional language
    Haskell, were used to prototype a Naval Surface Warfare Center (NSWC)
    requirement for a Geometric Region Server.  The resulting programs and
    development metrics were reviewed by a committee chosen by the Navy.  The
    results indicate that the Haskell prototype took significantly less time to
    develop and was considerably more concise and easier to understand than the
    corresponding prototypes written in several different imperative languages,
    including Ada and C++.", 
  location     = "http://www.cvc.yale.edu/publications/techreports/tr1049.pdf"
}

@TechReport{tbp,
  author       = "Claus Brabrand and Anders Møller and Michael~I. Schwartzbach",
  title        = "The {\tt <bigwig>} Project",
  institution  = dcs # "University of Aarhus",
  year         = 2002,
  number       = "BRICS-RS-00-42",
  address      = "Aarhus, Denmark",
  month        = jan,
  keywords     = "web services, language design, state, session, concurrency
    control, dynamic html, security",
  abstract     = "We present the results of the project, which aims to design
    and implement a high-level domain-specific language for programming
    interactive Web services.  A fundamental aspect of the development of the
    World Wide Web during the last decade is the gradual change from static to
    dynamic generation of Web pages.  Generating Web pages dynamically in
    dialogue with the client has the advantage of providing up-to-date and
    tailor-made information.  The development of systems for constructing such
    dynamic Web services has emerged as a whole new research area.  The
    language is designed by analyzing its application domain and identifying
    fundamental aspects of Web services inspired by problems and solutions in
    existing Web service development languages.  The core of the design
    consists of a session-centered service model together with a flexible
    template-based mechanism for dynamic Web page construction.  Using
    specialized program analyses, certain Web specific properties are verified
    at compile-time, for instance that only valid HTML 4.01 is ever shown to
    the clients.  In addition, the design provides high-level solutions to form
    field validation, caching of dynamic pages, and temporal-logic based
    concurrency control, and it proposes syntax macros for making highly
    domain-specific languages.  The language is implemented via widely
    available Web technologies, such as Apache on the server-side and
    JavaScript and Java Applets on the client-side.  We conclude with
    experience and evaluation of the project", 
  location     = "https://www.brics.dk/RS/02/1/index.html"
}

@TechReport{tppcin,
  author       = "K.~V. Nori and U.~Ammann and K.~Jensen and H.~H. N{\" a}geli",
  title        = "The {PASCAL} `{P}' Compiler:  Implementation Notes",
  institution  = "Institut f{\" u}r Informatik, Technische Hochschule Z{\" u}ich",
  year         = 1974,
  number       = 10,
  address      = "Z{\" u}rich, Switzerland",
  month        = dec,
  keywords     = "pascal, compilation, p-code, code generation",
  abstract     = "The PASCAL 'P' compiler is a portable compiler for a subset
    of 'Standard PASCAL'.  This compiler is written using exactly the subset it
    processes and it generates object code for a hypothetical stack computer.
    This report is a documentation of the stack computer and of the compiler.
    The latter part of the documentation proved to be useful to one of the 
    authors (K. V. Nori) when informally verifying the compiler.", 
  location     = "http://www.standardpascaline.org/p4.html"
}

@TechReport{sttaaas,
  author       = "Philip Greenspun",
  title        = "Scalability, Three-Tiered Architectures, and Application Servers",
  institution  = "ArsDigita Systems Journal",
  keywords     = "three-tiered architectures, scalability, application servers,
    software development",
  abstract     = "Application servers for Web publishing are generally systems
    that let you write database-backed Web pages in Java.  The first problem
    with this idea is that Java, because it must be compiled, is usually a bad
    choice of programming language for Web services.  The second problem is if
    what you really want to do is write some Java code that talks to data in
    your database, you can execute Java in your RDBMS (Oracle 8.1, Informix
    9.x).  Java executing inside the database server's process is always going
    to have faster access to table data than Java running as a client.  In
    fact, at least with Oracle on a Unix box, you could bind Port 80 to a
    program that would call a Java program running in the Oracle RDBMS.  You
    don't even need a Web server, much less an application server.  It is
    possible that you'll get higher performance and easier development by
    adding a thin-layer Web server like AOLserver or Microsoft's IIS/ASP, but
    certainly you can't get higher reliability by adding a bunch of extra
    programs and computers to a system that need only rely on one program and
    one computer.  This document works through some of these issues in greater
    detail, pointing out the grievous flaws in Netscape Application Server
    (formerly 'Kiva') and explaining the situations in which Oracle Application
    Server is useful.", 
  location     = "http://www.eveandersson.com/arsdigita/asj/application-servers"
}

@TechReport{tra,
  author       = "Dale Green",
  title        = "The Reflection {API}",
  institution  = "Oracle",
  year         = 2019,
  keywords     = "java, reflection",
  abstract     = "Reflection is commonly used by programs which require the
    ability to examine or modify the runtime behavior of applications running
    in the Java virtual machine.  This is a relatively advanced feature and
    should be used only by developers who have a strong grasp of the
    fundamentals of the language.  With that caveat in mind, reflection is a
    powerful technique and can enable applications to perform operations which
    would otherwise be impossible.",
  location     = "https://docs.oracle.com/javase/tutorial/reflect/index.html"
}

@TechReport{tfoiw,
  author       = "Jose Nazario and Jeremy Anderson and Rick Wash and Chris Connelly",
  title        = "The Future of Internet Worms",
  institution  = "Crimelabs Research",
  year         = 2001,
  month        = "20 " # jul,
  keywords     = "worms, internet security, ",
  abstract     = "Network worms, simple slang terminology for automated
    intrusion agents, represent a persistent threat to a growing Internet in an
    increasingly networked world.  However, their evolution has been somewhat
    limited, and they still rely on the same basic paradigms, which contain
    fundamental flaws.  We analyze the basic components of a worm and apply
    this analysis to three worms found in the wild on the Internet.  We then
    proceed to analyze the limiting factors of existing worm paradigms and
    outline new ideas which we expect to become prevalent.  These new worms
    will prove to be more diffcult to identify and eradicate.  It is our
    intention in sharing this knowledge to stimulate the development of
    strategies to detect and counteract the threat of smarter network worms.", 
  location     = "http://www.blackhat.com/presentations/bh-usa-01/JoseNazario/bh-usa-01-Joes-Nazario.pdf"
}

@InProceedings{cmultsygbtr,
  author       = "Stanley~P. Hanks",
  title        = "Creating {MAN}s using {LAN} Technology:  Sometimes You Gotta Break the Rules",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "439--451",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "man, lans, bridging, security, encapsulation, fddi, ethernet,
    vpn, standards",
  abstract     = "Commercially available, off-the-shelf internetworking
    products provide good mechanisms for the creation of limited-throughput
    metropolitan and wide area networks.  However, for many applications either
    the throughput obtained from T1 or slower communication circuits is
    inadequate, or the expense of obtaining high throughput using multiple T1
    or whole DS-3 circuits is prohibitive.  Proposed service offerings such as
    802.6, SMDS, frame relay, or SONET promise adequate speed metropolitan area
    network connectivity at a reasonable price.  Today these services arc not
    available, or where they are available are limited to T1 speeds.
    Metropolitan Fiber Systems owns over 17,000 miles of fiber optic cable in
    12 cities.  Most of this capacity is currently devoted to providing leased
    T1 and DS-3 circuits, and a significant portion of those circuits are for
    data transmission.  This provided a unique opportunity to address the
    question of how to provide LAN speed connectivity in and between
    metropolitan areas using commercially available products.  This paper
    discusses the development of the high speed MAN connectivity service
    offerings based on FDDI provided by MFS.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ahotcosalu1tj1,
  author       = "Alan~E. Kaplan",
  title        = "{A} History of the {COSNIX} Operating System: Assembly Language " # unix # " 1971 to {July}, 1991",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "429--437",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "history, fortran, disk performance, database systems",
  abstract     = "From 1971 until July, 1991 a variant of the assembly language
    version of the Unix operating system (Unix-A) was used to run a transaction
    processing system called COSMOS (Computer System for Mainframe Operations)
    in the Regional Bell Operating Companies.  At one time about seven hundred
    such systems were running on PDP 11//45 and PDP 11/70 computers.  This talk
    describes the history and development of that Unix operating system
    variant, called COSNIX, and explains some of the reasons for its success.
    I hope, also, that it gives a feeling of the challenges involved in
    producing a viable system during the days when computing resources were
    much more severely limited than they are today.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{fsfhs,
  author       = "Henry Spencer",
  title        = "Faster String Function",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "419--428",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "string functions, optimization, performance, parallel execution",
  abstract     = "The string functions provided by ANSI C and by traditional
    Unix C libraries are usually not as well-optimized as they could be.
    Careful tuning of inner loops is common, and on some processors it is
    profitable to rewrite them in assembler to exploit special instructions,
    but on most systems operations are still done a character at a time.  Given
    fairly lenient assumptions about the architecture, versions that operate a
    word at a time are possible.  Word-at-a-time processing is superficially
    difficult for C strings, since they arc terminated by a single null that is
    awkward to detect within a word.  However, carefully chosen combinations of
    logical and arithmetic operations can do such detection at a cost of 3-6
    operations per word, depending on data constraints and architecture,
    without relying on any architecture-specific specialized instructions or
    data paths.  This technique has been around as occasionally-heard folklore
    for some time, but does not appear to have been investigated in detail.
    The resulting word-at-a-time string functions are conspicuously faster than
    the usual ones for long strings.  The crossover point is typically 20-30
    characters, and the asymptotic speed advantage can be as much as a factor
    of 5, although a factor of 2-3 is more typical on 100-character operands.
    For specialized requirements where customized interfaces and customized
    code are permissible, rather higher factors are possible.  Certain problems
    occur, notably higher startup overhead, difficulties with unaligned
    strings, and the prevalence of relatively short strings as operands to some
    string functions.  The case for the fast functions is mixed, and an
    adaptive algorithm is needed to maximize overall performance.  It would
    also be useful to package the algorithms for use in custom string code,
    although this is somewhat challenging.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{rsis50,
  author       = "Sandeep Khanna and Michael Sebree and John Zolnowsky",
  title        = "Realtime Scheduling in {SunOS} 5.0",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "375--390",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time, scheduling, kernel threads, interrupts",
  abstract     = "We describe the fundamental mechanisms in SunOS 5.0 to
    provide realtime scheduling functionality.  Our primary goal was to provide
    bounded behavior for dispatching or blocking threads.  To achieve this goal
    we have modified the kernel to be fully preemptive, guaranteeing dispatch
    after both synchronous and asynchronous wakeups.  We have also worked
    toward controlling priority inversion in the kernel.  The result is a
    kernel capable of delivering realtime scheduling and bounded response to a
    large class of user level applications.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cancpmtppl,
  author       = "Sharon Hopkins",
  title        = "Camels and Needles: Computer Poetry Meets the {Perl} Programming Language",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "391--404",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "perl, poetry, free verse",
  abstract     = "Although various forms of literature have been created with
    the assistance of a computer, and even been generated by computer programs,
    it is only recently that literary works have actually been written in a
    computer language.  A computer-language poem need not necessarily produce
    any output, it may succeed merely by fooling the parser into thinking it is
    an ordinary program.  'Ihe Perl programming language has proved well-suited
    to the creation of computer-language poetry.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{3atofs,
  author       = "W.~D. Roome",
  title        = "{3DFS}: {A} Time-Oriented File Server",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "405--418",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, archiving, nfs, backups, optical storage, caching",
  abstract     = "3DFS is a network file server that provides time-oriented
    access to files and directories.  3DFS allows a user to read the version of
    a file as it existed on a particular day in the past, or to list the files
    in a directory on some prior date.  3DFS saves the daily incremental
    backups from other file systems (Sun file servers, Vaxen,...), and creates
    an on-line file system from these dumps.  3DFS uses optical disks in an
    automated jukebox, so no operator intervention is required.  3DFS uses the
    Sun NFS™ protocol, and looks like any other NFS server.  Any UNIX® command
    can read files in 3DFS, and users mount 3DFS just like any file server.
    Because 3DFS provides on-line access to old versions, users can access
    those versions in-place, without copying them to magnetic disk.  This paper
    describes 3DFS, its implementation, and our experience with it.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ntbpnm,
  author       = "Matt Blaze",
  title        = "{NFS} Tracing by Passive Network Monitoring",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "333--343",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "nfs, filesystem performance, network monitoring",
  abstract     = "Traces of filesystem activity have proven to be useful for a
    wide variety of purposes, ranging from quantitative analysis of system
    behavior to trace-driven simulation of filesystem algorithms.  Such traces
    can be difficult to obtain, however, usually entailing modification of the
    filesystems to be monitored and runtime overhead for the period of the
    trace.  Largely because of these difficulties, a surprisingly small number
    of filesystem traces have been conducted, and few sample workloads are
    available to filesystem researchers.  This paper describes a portable
    toolkit for deriving approximate traces of NFS activity by non-intrusively
    monitoring the Ethernet traffic to and from the file server.  The toolkit
    uses a promiscuous Ethernet listener interface (such as the Packetfilter)
    to read and reconstruct NFS-related RPC packets intended for the server.
    It produces traces of the NFS activity as well as a plausible set of
    corresponding client system calls.  The tool is currently in use at
    Princeton and other sites, and is available via anonymous ftp.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ccfcsius,
  author       = "Joseph~L. Hellerstein",
  title        = "Control Considerations for {CPU} Scheduling in " # unix # " Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "359--374",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "scheduling, priority scheduling, compute-bound jobs",
  abstract     = "Managing UNIX systems often involves setting service rate
    objectives, such as specifying that application A should receive 50% of the
    central processing unit (CPU).  In most UNIX systems, the only way to
    control CPU usage is by adjusting nice values; unfortunately, the
    relationship between nice values and process service rates has been poorly
    understood.  This paper develops an analytic model that relates service
    rate objectives for compute-bound processes to nice values and three
    scheduler parameters: R (the rate at which priority increases for each
    quantum of CPU consumed), D (the decay factor), and T (the number of quanta
    that expire before CPU usages are decayed); the model is evaluated using
    measurements of a workstation running IBM's Advanced Interactive Executive
    (AIX) 3.1 Operating System.  Based on the model, we develop an algorithm
    that calculates nice values that achieve service rate objectives for
    compute-bound processes.  Experiments conducted on a production AIX 3.1
    system suggest that our algorithm works well in practice.  In addition, we
    use the model to obtain insights into the control implications of parameter
    settings.  For example, we show that the nice mechanism is often less
    effective on faster processors since T tends to increase with processor
    speed; this increases the fraction of time during which processes with
    larger nice values execute, and hence limits the extent to which their
    service rates can be controlled.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{atdaonaaciads,
  author       = "Ken~W. Shirriff and John~K. Ousterhout 
",
  title        = "{A} Trace-Driven Analysis of Name and Attribute Caching in a Distributed System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "315--330",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "caching, network file system, name look-up, performance,
    server load, cache coherence, process migration, trace-driven simulation,
    sprite",
  abstract     = "This paper presents the results of simulating file name and
    attribute caching on client machines in a distributed file system.  The
    simulation used trace data gathered on a network of about 40 workstations.
    Caching was found to be advantageous: a cache on each client containing
    just 10 directories had a 91% hit rate on name lookups.  Entry-based name
    caches (holding individual directory entries) had poorer performance for
    several reasons, resulting in a maximum hit rate of about 83%.  File
    attribute caching obtained a 90% hit rate with a cache on each machine of
    the attributes for 30 files.  The simulations show that maintaining cache
    consistency between machines is not a significant problem; only 1 in 400
    name component lookups required invalidation of a remotely cached entry.
    Process migration to remote machines had little effect on caching.  Caching
    was less successful in heavily shared and modified directories such as
    /tmp, but there weren’t enough references to /tmp overall to affect the
    results significantly.  We estimate that adding name and attribute caching
    to the Sprite operating system could reduce server load by 36% and the
    number of network packets by 30%.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/nameUsenix92.ps.Z"
}

@InProceedings{toatfmaaosj,
  author       = "Matt~W. Mutka and Philip~K. McKinley ",
  title        = "The {OPENSIM} Approach:  Tools for Management and Analysis of Simulation Jobs",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "291--304",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, workstation clusters, scheduling, work sharing, guis,
    simulations",
  abstract     = "This paper presents the design, implementation, and usage of
    OpenSim.  OpenSim provides new tools and integrates existing tools into an
    environment in order to establish a comprehensive facility for performing
    simulation work.  First, OpenSim provides a graphical user interface to
    users for creating input files for simulations and managing output files
    produced from simulations.  Second, tools are provided to help a user
    easily generate plots from sets of output files assocated with a simulation
    project.  Third, OpenSim addresses a common problem for many simulation
    users, namely, lack of computing capacity to serve the jobs.  In order to
    solve this problem, OpenSim integrates Condor, an existing system that
    clusters idle workstations into a processor bank, into its environment so
    that users have access to a large amount of computing capacity without
    interfering with the local usage of workstations by their owners.  Finally,
    since users often plan their schedules according to the deadlines required
    for their jobs, OpenSim enhances Condor so that users can request jobs to
    be scheduled within a deadline.  Therefore, a user can expect that the
    amount of computing capacity required for a simulation project will be
    available before a specified deadline.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mlcidfs,
  author       = "D.~Muntz and Peter Honeyman",
  title        = "Multi-level Caching in Distributed File Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "305--313",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "intermediate caches, distributed file systems, performance,
    host caches, hit rates, trace-driven simulations",
  abstract     = "We are investigating the potential for a hierarchy of
    intermediate file servers to address scaling problems in increasingly large
    distributed file systems.  To this end, we have run trace-driven
    simulations based on data from DEC-SRC and our own data collection to
    determine the potential of caching-only intermediate servers.  The degree
    of sharing among clients is central to the effectiveness of an intermediate
    server.  This turns out to be quite low in the traces available to us.  All
    told, fewer than 10% of block accesses are to files shared by more than one
    file system client.  Our simulations show that even with an infinite cache
    at an intermediate server, cache hit rates are disappointingly low.  For
    client caches as small as 20M, we observe hit rates under 19%.  As client
    cache sizes increase, the hit rate at an intermediate server approaches the
    degree of sharing among all clients.  On the other hand, the intermediate
    server does appear to be effective in boosting the performance and
    scalability of upstream file servers by substantially reducing the request
    rate presented to them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "http://www.citi.umich.edu/techreports/reports/citi-tr-91-3.pdf"
}

@InProceedings{obf,
  author       = "Mitch Bradley",
  title        = "{Open Boot} Firmwear",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "223--235",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "forth, firmwear, boot-time configuration",
  abstract     = "Open Boot is a software architecture for the firmware that
    controls a computer before the operating system has begun execution.  The
    Open Boot firmware design is based on a machine-independent interactive
    programming language (Forth).  Open Boot includes features for
    self-identifying plug-in devices with device-resident boot drivers, support
    for disk, tape, and network booting, hardware configuration reporting, and
    debugging tools for hardware, software, and firmware.  Open Boot is the
    basis for the device identification and booting capabilities of SBus.  An
    IEEE standards effort for boot firmware based on Open Boot is underway.
    The Futurebus+ and VME-D bus standards include support for Open Boot.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{scapmotuk,
  author       = "Michael Litzkow and Marvin Solomon ",
  title        = "Supporting Checkpointing and Process Migration Outside the " # Unix # " Kernel",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "281--290",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, checkpointing, process migration, kernel
    interception, state transfer, coredumps",
  abstract     = "We have implemented both checkpointing and migration of
    processes under UNIX as a part of the Condor package.  Checkpointing,
    remote execution, and process migration are different, but closely related
    ideas; the relationship between these ideas is explored.  A unique feature
    of the Condor implementation of these items is that they are accomplished
    entirely at user level.  Costs and benefits of implementing these features
    without kernel support are presented.  Portability issues, and the
    mechanisms we have devised to deal with these issues, are discussed in
    concrete terms.  The limitations of our implementation, and possible
    avenues to relieve some of these limitations, are presented.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcacidce,
  author       = "Douglas Rosenthal and Wayne Allen and Kenneth Fiduk",
  title        = "Process Control and Communication in Distributed {CAD} Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "271--281",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "ipc, load balancing, process control, message passing,
    distributed computing",
  abstract     = "The MCC Computer-Aided Design (CAD) Framework Process Control
    System (PCS) provides distributed process control and communication
    services in heterogeneous network environments.  The PCS also provides
    network-wide load balancing via an efficient and flexible process placement
    mechanism.  The PCS services enable design tools and CAD framework
    components to leverage the resources of distributed computing networks,
    while supporting various degrees of interaction through distributed,
    real-time communication.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lasodc,
  author       = "Robert~M. English and Alexander~A. Stepanov",
  title        = "Loge:  {A} Self-Organizing Disk Controller",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "237--251",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "While the task of organizing data on the disk has
    traditionally been performed by the file system, the disk controller is in
    many respects better suited to the task.  In this paper, we describe Loge,
    a disk controller that uses internal indirection, accurate physical
    information, and reliable metadata storage to improve I/O performance.  Our
    simulations show that Loge improves overall disk performance, doubles write
    performance, and can, in some cases, improve read performance.  The Loge
    disk controller operates through standard device interfaces, enabling it to
    be used on standard systems without software modification.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{hawsibtifn,
  author       = "Bruce Nelson and Yu-Ping Cheng",
  title        = "How and Why {SCSI} is Better than {IPI} for {NFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "253--270",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "disk controllers, traffic patterns",
  abstract     = "Disk drives are often dismissed as mundane devices, but they
    are actually interesting, complicated, and misunderstood.  In traditional
    Unix servers, disk storage subsystems are usually optimized for
    sequential-transfer performance.  Perhaps counter-intuitively, however, NFS
    file servers exhibit marked random-access disk traffic.  This report
    investigates this apparent contradiction and shows that disk-drive
    concurrency not disk transfer rate—is the important factor in disk storage
    performance for most NFS network servers.  The investigation begins with a
    concrete and detailed comparison of both performance-oriented and
    nonperformance-oriented technical specifications of both SCSI and IPI drive
    and interface types.  It offers a thorough empirical evaluation of SCSI
    disk drive performance, varying parameters such as synchronous or
    asynchronous bus transfers, random and sequential access patterns, and
    multiplicity of drives per SCSI channel.  It discusses (nonempirically)
    similar characteristics for IPI-2 drives.  The report concludes with
    benchmarked comparisons of NFS file servers using SCSI-based disk arrays
    and IPI-2 subsystems.  The results show that NFS heavy-load throughput
    using SCSI disk arrays scales linearly with extra drives, whereas IPI-2
    throughput scales less than proportionally with extra drives.  This SCSI
    scalability advantage, combined with SCSI’s appealing price-performance and
    price-capacity, make SCSI disks a superior choice for NFS servers.  IPI-2
    drives, with their optional high transfer rates, remain an excellent choice
    for compute-oriented servers executing large-file applications where high
    sequential throughput is essential.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{iiiocas,
  author       = "Murthy Devarakonda and Arup Mukherjee",
  title        = "Issues in Implementation of Cache-Affinity Scheduling",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "345--357",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "affinity scheduling, cache management, thread scheduling,
    performance", 
  abstract     = "In a shared memory multiprocessor, a thread may have an
    affinity to a processor because of the data remaining in the processor’s
    cache from a previous dispatch.  We show that two basic problems should be
    addressed in a Unix-like system to exploit cache affinity for improved
    performance: First, the limitation of the Unix dispatcher model (“processor
    seeking a thread”); Second, pseudo-affinity caused by low-cost waiting
    techniques used in a threads package such as C Threads.  We demonstrate
    that the affinity scheduling is most effective when used in a threads
    package that supports multiplexing of user threads on kernel threads.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{at,
  author       = "Jay Littman",
  title        = "Applying Threads",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "209--221",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multithreading, synchronization, deadlock, performance",
  abstract     = "Multithreading components of a software system can increase
    performance, but it can also increase complexity.  At Hewlett-Packard, we
    have developed a workstation based medical product, called the Monitoring
    Full Disclosure Review Station, or M1251A, that uses multithreading to
    achieve performance requirements.  The M1251A continuously acquires
    physiological waveforms and arrhythmia information for presentation to a
    clinician in an intensive care unit.  This paper describes the benefits the
    M1251A gains from multithreading, identifies the problems the development
    team had with multithreading, and explains how those problems were
    resolved.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aspmap,
  author       = "Bernhard Wagner and Bruce~K. Haddon",
  title        = "Application Software:  Product Management and Privileges",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "system administration, portability, standards",
  abstract     = "Application programs for UNIX are increasingly making greater
    demands upon the system structure, are adhering less well to admittedly
    implicit guidelines, or, are being inexactly transliterated from the
    paradigms of other systems.  These influences add to the administrative
    problems and load, and, in some cases, are exacerbating security risks.
    The administrative problems and corresponding solutions are presented here
    in a twofold manner: Firstly, by the description of our use of methods that
    separate administration of the programs and files that make up an
    application suite both from system administration and from eacli other.  We
    argue that thus a sort ol modularity takes place in the software
    administration.  The goal is the lack of need for super user privilege, so
    that this separation improves the overall security of the system.
    Secondly, we list a number of features that we support as being essential,
    a list of requirements to be fulfilled by all application programs written
    for UNIX systems.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcsr,
  author       = "Spencer Rugaber",
  title        = "Program Comprehension",
  booktitle    = "Encyclopedia of Computer Science and Technology",
  year         = 1995,
  editor       = "Allen Kent and James~G. Williams",
  pages        = "341--368",
  publisher    = "Marcel Dekker",
  address      = nyny,
  keywords     = "program comprehension, cognitive models"
}

@InProceedings{rwpfm,
  author       = "Robin Schaufler",
  title        = "Realtime Workstation Performance for {MIDI}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "139--",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time systems, midi, system performance, benchmarking",
  abstract     = "MIDI studio applications require 1 millisecond accuracy in
    timing transmission and receipt of MIDI messages.  Past MIDI
    implementations on UNIX™ have either used the Roland MPU-401 coprocessor to
    do accurate timing, or have not had timing tests published for them.
    Timing MIDI I/O on the host processor allows for more flexible scheduling
    policies than the MPU-401, but many people expressed skepticism that it
    could be done with sufficient accuracy and efficiency because of UNIX™
    virtual memory and pre-emptive scheduling.  This paper describes studies we
    did on providing millisecond accuracy on the host processor of a Silicon
    Graphics Iris Indigo running IRIX, the Silicon Graphics version of UNIX.
    Our measurements show that millisecond accuracy is feasible on IRIX™
    without modifying the kernel.  The paper goes on to describe how the
    studies relate to other time based media.  With a small set of real time
    features, UNIX can really sing.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aafapmt,
  author       = "Sun Wu and Udi Manber",
  title        = "{\tt agrep} --- Fast Approximate Pattern-Matching Tool",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "153--162",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "approximate string matching, boyer-more, knuth-morris-pratt",
  abstract     = "Searching for a pattern in a text file is a common
    operation in many applications ranging from text editors and databases to
    applications in molecular biology.  In many instances the pattern does not
    appear in the text exaedy.  Errors in the text or in the query can result
    from misspelling or from experimental errors (e.g., when the text is a DNA
    sequence).  The use of such approximate pattern matching has been limited
    until now to specific applications.  Most text editors and searching
    programs do not support searching with errors because of the complexity
    involved in implementing it.  In this paper we describe a new tool, called
    agrep, for approximate pattern matching.  Agrep is based on a new efficient
    and flexible algorithm for approximate string matching.  Agrep is also
    competitive with other tools for exact string matching; it include many
    options that make searching more powerful and convenient.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aewbiwacileas,
  author       = "Bill Cheswick",
  title        = "An Evening with {Berferd} In Which a Cracker is Lured, Endured, and Studied",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "163--173",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "cracking, jails, honeypots, law",
  abstract     = "On 7 January 1991 a cracker, believing he had discovered the
    famous sendmail DEBUG hole in our Internet gateway machine, attempted to
    obtain a copy of our password file.  I sent him one.  For several months we
    led this cracker on a merry chase in order to trace his location and learn
    his techniques.  This paper is a chronicle of the cracker’s 'successes' and
    disappointments, the bait and traps used to lure and detect him, and the
    chroot 'Jail' we built to watch his activities.  We concluded that our
    cracker had a lot of time and persistence, and a good list of security
    holes to use once he obtained a login on a machine.  With these holes he
    could often subvert the uucp and bin accounts in short order, and then
    root.  Our cracker was interested in military targets and new machines to
    help launder his connections.  This is a draft of a paper accepted for the
    January 1992 San Francisco Usenix.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{haph,
  author       = "Peter Honeyman and L.~B. Huston and M.~T. Stolarchuk",
  title        = "Hijacking {AFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "175--181",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "network security, mitm attack, afs, network protocols,
    challenge/response oracle",
  abstract     = "We have identified several techniques that allow uncontrolled
    access to files managed by AFS 3.0.  One method relies on administrative
    (or root) access to a user’s workstation.  Defending against this sort of
    attack is difficult.  Another class of attacks comes from promiscuous
    access to the physical network.  Stronger cryptographic protocols, such as
    those employed by AFS 3.1, obviate this problem.  These exercises help us
    understand vulnerabilities in the distributed systems that we employ (and
    deploy), and offer guidelines for securing them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aibaflsdse,
  author       = "Dale Skeen",
  title        = "An Information Bus Architecture for Large-Scale, Decision-Support Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information bus, publish-subscribe",
  abstract     = "Some of the promising industries for commercializing UNIX are
    those requiring real-time decision support, such as trading rooms, factory
    automation, process control, and network management.  These large-scale,
    real-time environments present challenging technical problems of high data
    volumes, split-second response times, and high availability.  Moreover,
    these environments demand flexible architectures that can support a rapidly
    changing set of application requirements.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{xwbstfu,
  author       = "Doug Blewett and Scott Anderson and Meg Kilduff and Susan Udovic and Mike Wish",
  title        = "{X Widget}-Based Software Tools for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "111--123",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "x resources, xtent, ipc, control flow, guis, user interface
    development, x windows",
  abstract     = "This paper describes a small language and IPC protocol that
    can be used for specifying UNIX style, X Toolkit based, graphics software
    tools.  The language is unusual in that it integrates the X Toolkit widget
    world and the UNIX philosophy of creating applications from collections of
    small reusable filters.  Filters can be constructed from old Xt based
    graphics processes or specified directly in the small language.  The system
    is based on an easily reproducible macro interpreter and IPC system that
    can be used with any collection of widgets.  A multi-process application
    builder constructed with the system is used as an example of how the
    software tools philosophy can be effectively used to construct graphics
    applications.  We present data on the use of the system by both research
    organizations and development groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pfdomlaae,
  author       = "Reed Hastings and Bob Joyce",
  title        = "Purify: Fast Detection of Memory Leaks and Access Errors",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "125--137",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "storage-access errors, storage leaks, detecting storage
    leaks, object-code rewriting",
  abstract     = "This paper describes Purify™, a software testing and quality
    assurance tool that detects memory leaks and access errors.  Purify inserts
    additional checking instructions directly into the object code produced by
    existing compilers.  These instructions check every memory read and write
    performed by the program-under-test and detect several types of access
    errors, such as reading uninitialized memory or writing to freed memory.
    Purify inserts checking logic into all of the code in a program, including
    third-party and vendor object-code libraries, and verifies system call
    interfaces.  In addition, Purify tracks memory usage and identifies
    individual memory leaks using a novel adaptation of garbage collection
    techniques.  Purify produces standard executable files compatible with
    existing debuggers, and currently runs on Sun Microsystems’ SPARC family of
    workstations.  Purify’s nearly-comprehensive memory access checking slows
    the target program down typically by less than a factor of three and has
    resulted in significantly more reliable software for several development
    groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aaedsfti,
  author       = "Alan Emtage and Peter Deutsch",
  title        = "{archie} --- An Electronic Directory Service for the {Internet}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "93--110",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information retrieval, internet crawling, ftp, indexing",
  abstract     = {The huge size and continued rapid growth of the Internet
    offers a particular challenge to systems designers and service providers in
    this new environment.  Before a user can effectively exploit any of the
    services offered by the Internet community or access any information
    provided by such services, that user must be aware of both the existence of
    the service and the host or hosts on which it is available.  Adequately
    addressing this “resource discovery problem” is a central challenge for
    both service providers and users wishing to capitalize on the possibilities
    of the Internet.  This paper describes archie, our attempt at an on-line
    resource directory service for an internetworked environment.  The current
    implementation of archie automatically indexes and makes available all
    filenames stored at known anonymous FTP sites.  The filename information is
    updated automatically ensuring users access to authoritative information.
    The system also makes available the names and descriptions of several
    thousand packages found on the Internet.}, 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{tefs,
  author       = "Sailesh Chutani and Owen~T. Anderson and Michael~L. Kazar and Bruce~W.  Leverett and W.~Anthony Mason and Robert~N. Sidebotham",
  title        = "The {Episode} File System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "43--60",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, filesets, metadata logging, integrity checking,
    performance",
  abstract     = "We describe the design of Episode,™ a highly portable
    POSIX-compliant file system.  Episode is designed to utilize the disk
    bandwidth efficiently, and to scale well with improvements in disk capacity
    and speed.  It utilizes logging of meta-data to obtain good performance,
    and to restart quickly after a crash.  Episode uses a layered architecture
    and a generalization of files called containers to implement fileseis.  A
    fileset is a logical file system representing a connected subtree.
    Filesets are the unit of administration, replication, and backup in
    Episode.  The system works well, both as a standalone file system and as a
    distributed file system integrated with the OSF's Distributed Computing
    Environment (DCE).  Episode will be shipped with the DCE as the Local File
    System component, and is also exportable by NFS.  As for performance,
    Episode meta-data operations are significantly faster than typical UNIX
    Berkeley Fast File System implementations due to Episode's use of logging,
    while normal I/O operations run near disk capacity.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aiolffbu,
  author       = "Dave Shaver and Eric Schnoebelen and George Bier",
  title        = "An Implementation of Large Files for {BSD} " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "61--68",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, portability, software maintenance, 64-bit
    address spaces",
  abstract     = "The design of the ConvexOS 1 filesystem, based on the BSD
    Fast File System, allows for a theoretical maximum file size of about 4402G
    2 with a 4K filesystem block size (or about 64T with 8K blocks.)
    Unfortunately, the actual limit of the CONVEX filesystem has been 2G-1
    because key kernel values and file offset pointers are 32-bits in size.
    This is a problem shared by many other UNIX 3 vendors.  This paper
    describes the path CONVEX has taken to implement files and filesystems
    larger than 2G.  The implementation is based on a new set of 64-bit system
    calls and new library interfaces; it requires no changes to the on-disk
    i-node representation.  The large file programming models and the kernel
    and utilities changes are described.  Measurements of read and write I/O
    rates are presented and show that there is little performance penalty for
    manipulating large files using the chosen implementation.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{serf,
  author       = "Walter~A. Burkhard and Petar~D. Stojadinovi{\' c}",
  title        = "Storage-Efficient Reliable Files",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "69--77",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "n-of-m redundancy, reliability, file systems, file dispersal,
    fault tolerance, performance",
  abstract     = "The File Dispersal Shell is a storage-efficient reliable data
    storage prototype facility for local area networks.  Rabin's information
    dispersal algorithm provides an attractive data organization scheme which
    potentially uses less physical storage space than replication while
    obtaining excellent data reliability and access times comparable to those
    obtained for a single disk.  We have constructed Rabin's information
    dispersal algorithm within a UNIX system shell that provides almost all the
    traditional shell facilities augmented with two additional commands to
    create and delete dispersed files.  We present analytical
    mean-time-to-data-loss results, storage requirements, together with our
    prototype implementation and preliminary access-time measurements.  For
    practical purposes, dispersed files are invisible to the user except for
    the improved reliability at modest disk space cost.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mmftbu,
  author       = "Nathaniel~S. Borenstein",
  title        = "Multimedia Mail From the Bottom Up",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "79--91",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multimedia, mail, extensions, outboard processing,
    configuration files, usability",
  abstract     = "Multimedia mail systems have exhibited great potential, but
    the widespread use of multimedia mail has so far been inhibited by the lack
    of interchange standards and the heterogeneity of mail-reading software.
    This paper describes a new approach that seeks to break the existing
    log-jam and make multimedia mail a practical reality.  The paper begins
    with a brief summary of the state of the art in multimedia mail systems.
    It then outlines the new, 'bottom-up' approach, and describes the
    configuration mechanism that is central to its operation.  Next, it
    describes a prototype implementation and its deployment on top of over a
    dozen different mailreading programs at Bellcore and elsewhere.  Finally,
    problems in the prototype installation are discussed, along with future
    prospects for multimedia mail using this approach.  The paper ends by
    outlining a vision of a new and better 'lowest common denominator' for
    electronic mail.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lpmtfu,
  author       = "Margo Seltzer and Michael Olson",
  title        = "{LIBTP}: Portable, Modular Transactions for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "9--25",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "transactions, shared storage, user-space libraries, two-phase
    commit, crash recovery, lock management",
  abstract     = "Transactions provide a useful programming paradigm for
    maintaining logical consistency, arbitrating concurrent access, and
    managing recovery.  In traditional UNIX systems, the only easy way of using
    transactions is to purchase a database system.  Such systems are often
    slow, costly, and may not provide the exact functionality desired.  This
    paper presents the design, implementation, and performance of LIBTP, a
    simple, non-proprietary transaction library using the 4.4BSD database
    access routines (db(3)).  On a conventional transaction processing style
    benchmark, its performance is approximately 85% that of the database access
    routines without transaction protection, 200% that of using fsync(2) to
    commit modifications to disk, and 125% that of a commercial relational
    database system.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/1925.html"
}

@InProceedings{etaomffsio,
  author       = "Orran Krieger and Michael Stumm and Ron Unrau",
  title        = "Exploiting the Advantages of Mapped Files for Stream {I}/{O}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "27--42",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file-mapped io, std-io, stream io",
  abstract     = "A new approach for providing user level support for fast
    stream I/O is motivated by four factors common to most modern systems: 1)
    the capability of the operating system to support mapped files, 2) the
    increasing number of applications that use threads, 3) the increasing
    discrepancy between processor speed and disk latency, and 4) the increasing
    amount of available main memory.  In this paper, we first describe the
    advantages and disadvantages of using mapped files to support stream access
    to files, and then describe a new interface, the Alloc Stream Interface
    (ASI), that allows for improved performance over existing stream
    interfaces.  A library that supports ASI has been implemented on several
    systems (including IRIX and SunOS).  In addition, the Stdio library has
    been re-implemented to use ASI.  Significant performance advantages are
    demonstrated for Stdio applications that are linked to this new library and
    particularly for applications that are modified to use ASI directly.  For
    example, on typical Unix platforms, some standard I/O intensive utilities
    are shown to run up to twice as fast when re-linked to use this library and
    up to three times as fast when converted to use ASI.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cco,
  author       = "Eduardo Krell and Balachander Krishnamurthy",
  title        = "{COLA}: Customied Overlaying",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "3--7",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "",
  abstract     = "System calls are the basic building blocks for writing
    programs in the UNIX operating system.  From the canonical read, write,
    open, close, seek, ...  to the more obscure ones, programs have been
    written to use system calls in a variety of ways.  Often there is a need to
    intercept a few system calls to perform some special task.  Given that it
    is hard to go below the level of system calls and still write portable
    programs, it is easy to see the need for intercepts at the system call
    level.  A simple example of a useful interception facility is a library
    that watches for file creation and modifications.  In this paper we
    describe COLA, an elegant, customizable and dynamic facility to overlay a
    variety of system call intercepts.  With COLA, users can specify an
    arbitrary number of system call filters, each of which may intercept
    different system calls and perform different actions upon interception.
    The set of overlaying filters can be modified at any time during the
    session.  A program run under COLA will have any of the filtered system
    calls processed at each layer before control is passed on to the next
    layer.  The final layer always is the standard UNIX system call layer.
    System calls not intercepted by any of the overlaying filters will execute
    transparently.  No recompilation of programs or static relinking is
    necessary.  It should be noted that this facility depends on availability
    of shared libraries.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{abp,
  author       = "Susan~L. Graham and Steven Lucco and Robert Wahbe",
  title        = "Adaptable Binary Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "315--325",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "disassembly, control-flow, relocation, register management",
  abstract     = "To accurately and comprehensively monitor a program's
    behavior, many performance measurement tools transform the program's
    executable representation or binary.  By instrumenting binary programs to
    monitor program events, tools can precisely analyze compiler optimization
    effectiveness, memory system performance, pipeline interlocking, and other
    dynamic program characteristics that are fully exposed only at this level.
    Binary transformation has also been used to support software-enforced fault
    isolation, debugging, machine re-targeting, and machine-dependent
    optimization.At present, binary transformation applications face a
    difficult trade-off.  Previous approaches to implementing robust
    transformations result in significant disk space and run-time overhead.  To
    improve efficiency, some current systems sacrifice robustness, relying on
    heuristic assumptions about the program and recognition of
    compiler-dependent code generation idioms.  In this paper we begin by
    investigating the run-time and disk space overhead of transformation
    strategies that do not require assumptions about the program's control flow
    or register usage.  We then detail simple information about the binary
    program that can significantly reduce this overhead.  For each type of
    information, we show how it enables a corresponding type of binary
    transformation.  We call binary programs that contain such enabling
    information adaptable binaries.  Because adaptable binary information is
    simple, any compiler can generate it.  Despite its simplicity, adaptable
    binary information has the necessary and sufficient expressive power to
    support a rich set of binary transformations.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/adaptable-binary-programs"
}

@InProceedings{aafifbhppat95,
  author       = "Eustace, Alan and Srivastava, Amitabh",
  title        = "{ATOM}, {A} Flexible Interface for Building High-Peformance Program Analysis Tools",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "303--314",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cache simulator, instruction profiling, analysis tools,
    compiler auditing",
  abstract     = "ATOM (Analysis Tools with OM) is a single framework for
    building a wide range of customized program analysis tools.  It provides
    the common infrastructure present in all code-instrumenting tools; this is
    the difficult and time-consuming part.  The user simply defines the
    tool-specific details in instrumentation and analysis routines.  Building a
    basic block counting tool like Pixie with ATOM requires only a page of
    code.ATOM, using OM link-time technology, organizes the final executable
    such that the application program and user's analysis routines run in the
    same address space.  Information is directly passed from the application
    program to the analysis routines through simple procedure calls instead of
    inter-process communication or files on disk.  ATOM takes care that
    analysis routines do not interfere with the program's execution, and
    precise information about the program is presented to the analysis routines
    at all times.  ATOM uses no simulation or interpretation.  ATOM has been
    implemented on the Alpha AXP under OSF/1.  It is efficient and has been
    used to build a diverse set of tools for basic block counting, profiling,
    dynamic memory recording, instruction and data cache simulation, pipeline
    simulation, evaluating branch prediction, and instruction scheduling.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/atom-flexible-interface-building-high-performance",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-44.html"
}

@InProceedings{ltcuu,
  author       = "James~S. Plank and Micah Beck and Gerry Kingsley and Kai Li",
  title        = "Libckpt:  Transparent Checkpointing under " # unix,
  booktitle    = usenix95,
  year         = 1995,
  pages        = "213--224",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "checkpointing, optimizations, fault tolerance",
  abstract     = "Checkpointing is a simple technique for rollback recovery:
    the state of an executing program is periodically saved to a disk file from
    which it can be recovered after a failure.  While recent research has
    developed a collection of powerful techniques for minimizing the overhead
    of writing checkpoint files, checkpointing remains unavailable to most
    application developers.  In this paper we describe libckpt, a portable
    checkpointing tool for Unix that implements all applicable performance
    optimizations which are reported in the literature.  While libckpt can be
    used in a mode which is almost totally transparent to the programmer, it
    also supports the incorporation of user directives into the creation of
    checkpoints.  This user-directed checkpointing is an innovation which is
    unique to our work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/libckpt-transparent-checkpointing-under-unix"
}

@InProceedings{otpodlp,
  author       = "W.~Wilson Ho and Wei-Chau Chang and Lilian~H. Leung",
  title        = "Optimizing the Performance of Dynamically-Linked Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "225--233",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indirect addressing, data structures, procedure
    repositioning",
  abstract     = "Dynamically-linked programs in general do not perform as well
    as statically-linked programs.  This paper identifies three main areas that
    account for the performance loss.  First, symbols are referenced indirectly
    and thus extra instructions are required.  Second, the overhead in run-time
    symbol resolution is significant.  Third, poor locality of functions in
    shared libraries and data structures maintained by the run-time linker may
    result in poor memory utilization.  This paper presents new optimization
    techniques we developed that address these three areas and significantly
    improve the performance of dynamically-linked programs.  Also, we provide
    measurements of the performance improvement achieved.  Most importantly, we
    show that all desirable features of shared libraries can be achieved
    without sacrificing performance.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267430"
}

@InProceedings{dalfbprda,
  author       = "David~M. Arnow",
  title        = "{DP}:  {A} Library for Building Portable, Reliable Distributed Applications",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "235--247",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed programming, libraries, process management,
    communication",
  abstract     = "DP is a library of process management and communication tools
    for writing portable, reliable distributed applications.  It provides
    support for a flexible set of message operations as well as process
    creation and management.  It has been successfully used in developing
    distributed Monte Carlo, disjunctive programming and integer goal
    programming codes.It differs from PVM and similar libraries in its support
    for lightweight, unreliable messages, as well as asynchronous delivery of
    interrupt-generating messages.  In addition, DP supports the development of
    long-running distributed applications tolerant to the failure or loss of a
    subset of its processors.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/arnow.html"
}

@InProceedings{fslvcapc,
  author       = "Margo Seltzer and Keith~A. Smith and Hari Balakrishnan and Jacqueline Chang and Sara McMains and Venkata Padmanabhan",
  title        = "File System Logging Versus Clustering:  {A} Performance Comparison",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "249--264",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "fast file system, log structured file system, i-o
    performance",
  abstract     = "The Log-structured File System (LFS), introduced in 1991 [8],
    has received much attention for its potential order-of-magnitude
    improvement in file system performance.  Early research results [9] showed
    that small file performance could scale with processor speed and that
    cleaning costs could be kept low, allowing LFS to write at an effective
    bandwidth of 62 to 83% of the maximum.  Later work showed that the presence
    of synchronous disk operations could degrade performance by as much as 62%
    and that cleaning overhead could become prohibitive in transaction
    processing workloads, reducing performance by as much as 40% [10].  The
    same work showed that the addition of clustered reads and writes in the
    Berkeley Fast File System [6] (FFS) made it competitive with LFS in
    large-file handling and software development environments as approximated
    by the Andrew benchmark [4].These seemingly inconsistent results have
    caused confusion in the file system research community.  This paper
    presents a detailed performance comparison of the 4.4BSD Log-structured
    File System and the 4.4BSD Fast File System.  Ignoring cleaner overhead,
    our results show that the order-of-magnitude improvement in performance
    claimed for LFS applies only to meta-data intensive activities,
    specifically the creation of files one-kilobyte or less and deletion of
    files 64 kilobytes or less.For small files, both systems provide comparable
    read performance, but LFS offers superior performance on writes.  For large
    files (one megabyte and larger), the performance of the two file systems is
    comparable.  When FFS is tuned for writing, its large-file write
    performance is approximately 15% better than LFS, but its read performance
    is 25% worse.  When FFS is optimized for reading, its large-file read and
    write performance is comparable to LFS.Both LFS and FFS can suffer
    performance degradation, due to cleaning and disk fragmentation
    respectively.  We find that active FFS file systems function at
    approximately 85-95% of their maximum performance after two to three years.
    We examine LFS cleaner performance in a transaction processing environment
    and find that cleaner overhead reduces LFS performance by more than 33%
    when the disk is 50% full.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267432"
}

@InProceedings{mlians,
  author       = "Uresh Vahalia and Cary~G. Gray and Dennis Ting",
  title        = "Metadata Logging in an {NFS} Server",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "265--276",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "log-structured file systems, nfs, file-system metadata, fault
    tolerance, crash recovery, ",
  abstract     = "Over the last few years, there have been several efforts to
    use logging to improve performance, reliability, and recovery times of file
    systems.  The two major techniques are metadata logging, where the log
    records metadata changes and is a supplement to the on-disk file system,
    and log-structured file systems, whose log is their only on-disk
    representation.  When the file system is mainly or wholly accessed through
    the Network File System (NFS) protocol, it adds new considerations to the
    suitability of the logging technique.  NFS requires that all operations be
    updated to stable storage before returning.  As a result, file system
    implementations that were effective for local access may perform poorly on
    an NFS server.  This paper analyzes the issues regarding the use of logging
    on an NFS server, and describes an implementation of a BSD Fast File System
    (FFS) with metadata logging that performs effectively for a dedicated NFS
    server.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267433", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/metadata-logging-nfs-server"
}

@InProceedings{hcailsfs,
  author       = "Trevor Blackwell and Jeffrey Harris and Margo Seltzer",
  title        = "Heuristic Cleaning Algorithms in Log-Structured File Systems",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "277--288",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "garbage collection, log-structured file systems,
    benchmarking, performance, trace-driven analysis, scheduling",
  abstract     = "Research results show that while Log-Structured File Systems
    (LFS) offer the potential for dramatically improved file system
    performance, the cleaner can seriously degrade performance, by as much as
    40% in transaction processing workloads [9].  Our goal is to examine trace
    data from live file systems and use those to derive simple heuristics that
    will permit the cleaner to run without interfering with normal file access.
    Our results show that trivial heuristics perform very well, allowing 97% of
    all cleaning on the most heavily loaded system we studied to be done in the
    background.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/blackwell.html", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267434"
}

@InProceedings{muuogoos,
  author       = "J.~Mark Stevenson and Daniel~P. Julin",
  title        = "Mach-{US}: " # unix # " on Generic {OS} Object Servers",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "119--130",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "unix, apis, mach, object-oriented design, ipc",
  abstract     = "This paper examines the Mach-US operating system, its unique 
    architecture, and the lessons demonstrated through its implementation.
    Mach-US is an object-oriented multi-server OS which runs on the Mach3.0
    kernel.  Mach-US has a set of separate servers supplying orthogonal OS
    services and a library which is loaded into each user process.  This
    library uses the services to generate the semantics of the Mach2.5/4.3BSD
    application programmers interface (API).  This architecture makes Mach-US a
    flexible research platform and a powerful tool for developing and examining
    various OS service options.  We will briefly describe Mach-US, the
    motivations for its design choices, and its demonstrated strengths and
    weaknesses.  We will then discuss the insights that we've acquired in the
    areas of multi-server architecture, OS remote method invocation, Object
    Oriented technology for OS implementation, API independent OS services,
    UNIX API re-implementation, and smart user-space API emulation libraries.", 
  location     = "https://www.researchgate.net/publication/2811091_Mach-US_UNIX_on_generic_OS_object_servers"
}

@InProceedings{eiarbds,
  author       = "Jim Waldo and Ann Wollrath and Geoff Wyant and Samuel~C. Kendall",
  title        = "Events in an {RPC} Based Distributed System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "131--142",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "events, publish-subscribe, ipc, corba, interface
    polymorphism, third-party servers",
  abstract     = "We show how to build a distributed system allowing objects to
    register interest in and receive notifications of events in other objects.
    The system is built on top of a pair of interfaces that are interesting
    only in their extreme simplicity.  We then present a simple and efﬁcient
    implementation of these interfaces.  We then show how more complex
    functionality can be introduced to the system by adding third-party
    services.  These services can be added without changing the simple
    interfaces, and without changing the objects in the system that do not need
    the functionality of those services.  Finally, we note a number of open
    issues that remain, and attempt to draw some conclusions based on the
    work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/events-rpc-based-distributed-system"
}

@InProceedings{ttaosiamco,
  author       = "Jacques Talbot",
  title        = "Turning the {AIX} Operating System into an {MP}-capable {OS}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "143--153",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "powerscale, hardware architecture, atomic operations, caches,
  interrupt handling, locks, deadlocks, debugging, affinity scheduling",
  abstract     = "This paper describes those MP features that Bull and IBM
    together introduced into the AIX operating system to support the Symmetric
    Multiprocessor machine marketed by Bull under the Escala name and by IBM
    under the RS/6000 Models G30, J30 and R30 names.  The PowerPC architecture
    and the AIX operating system present some specific challenges.  We present
    the major problems encountered and how they were solved.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/turning-aix-operating-system-mp-capable-os"
}

@InProceedings{afmbfs,
  author       = "Atsuo Kawaguchi and Shingo Nishioka and Hiroshi Motoda",
  title        = "{A} Flash-Memory Based File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "155--164",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "flash memory, file systems, ",
  abstract     = "A flash memory device driver that supports a conventional
    UNIX file system transparently was designed.  To avoid the limitations due
    to flash memory's restricted number of write cycles and its inability to be
    overwritten, this driver writes data to the flash memory system
    sequentially as a Log-structured File System (LFS) does and uses a cleaner
    to collect valid data blocks and reclaim invalid ones by erasing the
    corresponding flash memory regions.  Measurements showed that the overhead
    of the cleaner has little effect on the performance of the prototype when
    utilization is low but that the effect becomes critical as the utilization
    gets higher, reducing the random write throughput from 222 Kbytes/s at 30%
    utilization to 40 Kbytes/s at 90% utilization.  The performance of the
    prototype in the Andrew Benchmark test is roughly equivalent to that of the
    4.4BSD Pageable Memory based File System (MFS).", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flash-memory-based-file-system"
}

@InProceedings{tpsfpftuos,
  author       = "Andrew Berman and Virgil Bourassa and Erik Selberg",
  title        = "{TRON}:  Process-Specific File Protection for the " # unix # " Operating System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "165--175",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "capabilities, protection domains",
  abstract     = "The file protection mechanism provided in UNIX is
    insufficient for current computing environments.  While the UNIX file
    protection system attempts to protect users from attacks by other users, it
    does not directly address the agents of destruction-executing processes.
    As computing environments become more interconnected and interdependent,
    there is increasing pressure and opportunity for users to acquire and test
    non-secure, and possibly malicious, software.We introduce TRON, a
    process-level discretionary access control system for UNIX.  TRON allows
    users to specify capabilities for a process' access to individual files,
    directories, and directory trees.  These capabilities are enforced by
    system call wrappers compiled into the operating system kernel.  No
    privileged system calls, special files, system administrator intervention,
    or changes to the file system are required.  Existing UNIX programs can be
    run without recompilation under TRON-enhanced UNIX.  Thus, TRON improves
    UNIX security while maintaining current standards of flexibility and
    openness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/tron-process-specific-file-protection-unix-operating"
}

@InProceedings{satfwaid,
  author       = "Tak Yan and Hector Garcia-Molina",
  title        = "{SIFT} --- {A} Tool for Wide-Area Information Dissemination",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "177--186",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indexing, user interest, information filtering",
  abstract     = "The dissemination model is becoming increasingly important in 
    wide-area information system.  In this model, the user subscribes to an
    information dissemination service by submitting profiles that describe his
    interests.  He then passively receives new, filtered information.  The
    Stanford Information Filtering Tool (SIFT) is a tool to help provide such
    service.  It supports full-text filtering using well-known information
    retrieval models.  The SIFT filtering engine implements novel indexing
    techniques, capable of processing large volumes of information against a
    large number of profiles.  It runs on several major Unix platforms and is
    freely available to the public.  In this paper we present SIFT's approach
    to user interest modeling and user-server communication.  We demonstrate
    the processing capability of SIFT by describing a running server that
    disseminates USENET News.  We present an empirical study of SIFT's
    performance, examining its main memory requirement and ability to scale
    with information volume and user population.", 
  location     = "http://ilpubs.stanford.edu/73/1/1994-7.pdf"
}

@InProceedings{tfsbitk,
  author       = "Brent Welch",
  title        = "The File System Belongs in the Kernel",
  booktitle    = pot # "Second USENIX Mach Symposium",
  year         = 1991,
  pages        = "233--250",
  address      = "Monterey, " # CA,
  month        = "20--22 " # nov,
  keywords     = "os architecture, file systems, sprite, mach, naming, name
  spaces, microkernels",
  abstract     = "This paper argues that a shared, distributed name space and
    I/O interface should be implemented inside the operating system kernel.
    The grounding for the argument is a comparison between the Sprite network
    operating system and the Mach microkernel.  Sprite optimizes the common
    case of file and device access, both local and remote, by providing a
    kernel-level implementation.  Sprite also allows for user-level
    extensibility by letting a user-level process implement the naming and I/O
    interfaces of the file system.  Mach, in contrast, provide general
    interprocess communication and does not define a file system protocol in
    the kernel.", 
  location     = "http://ftp.dk.netbsd.org/pub/doc/OS/Sprite/welch.filesys.ps.Z"
}

@InProceedings{piomps,
  author       = "Jeffrey~C. Mogul and Joel~F. Bartlett and Robert~N. Mayo and Amitabh Srivastava",
  title        = "Performance Implications of Multiple Pointer Sizes",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "187--200",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "performance, address spaces, tlb",
  abstract     = "Many users need 64-bit architectures: 32-bit systems cannot
    support the largest applications, and 64-bit systems perform better for
    some applications.  However, performance on some other applications can
    suffer from the use of large pointers; large pointers can also constrain
    feasible problem size.  Such applications are best served by a 64-bit
    machine that supports the use of both 32-bit and 64-bit pointer variables.
    This paper analyzes several programs and programming techniques to
    understand the performance implications of different pointer sizes.  Many
    (but not all) programs show small but definite performance consequences,
    primarily due to cache and paging effects.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/performance-implications-multiple-pointer-sizes"
}

@InProceedings{dcswmbs,
  author       = "Steven~W. Schlosser and John Linwood Griffin and David~F. Nagle and Gregory~R. Ganger",
  title        = "Designing Computer Systems with {MEMS}-based Storage",
  booktitle    = asplos00,
  year         = 2000,
  pages        = "1--12",
  address      = boma,
  month        = "9--13" # oct,
  keywords     = "mems, non-volatile storage, semi-conductor technology",
  abstract     = "For decades the RAM-to-disk memory hierarchy gap has plagued
    computer architects.  An exciting new storage technology based on
    microelectromechanical systems (MEMS) is poised to fill a large portion of
    this performance gap, significantly reduce system power consumption, and
    enable many new applications.  This paper explores the system-level
    implications of integrating MEMS-based storage into the memory hierarchy.
    Results show that standalone MEMS-based storage reduces I/O stall times by
    4-74X over disks and improves overall application runtimes by 1.9-4.4X.
    When used as on-board caches for disks, MEMS-based storage improves I/O
    response time by up to 3.5X.  Further, the energy consumption of MEMS-based
    storage is 10-54X less than that of state-of-the-art low-power disk drives.
    The combination of the high-level physical characteristics of MEMS-based
    storage (small footprints, high shock tolerance) and the ability to
    directly integrate MEMS-based storage with processing leads to such new
    applications as portable gigabit storage systems and ubiquitous active
    storage nodes.",  
  location     = "https://doi.org/10.1145/378993.378996"
}

@InProceedings{iins95,
  author       = "Richard Golding and Peter Bosch and Carl Staelin and Tim Sullivan and John Wilkes",
  title        = "Idleness is not Sloth",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "201--212",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cycle harvesting, idle-time processing, process dispatch,
    idle detection",
  abstract     = "Many people have observed that computer systems spend much of
    their time idle, and various schemes have been proposed to use this idle
    time productively.  The commonest approach is to off-load activity from
    busy periods to less-busy ones in order to improve system responsiveness.
    In addition, speculative work can be performed in idle periods in the hopes
    that it will be needed later at times of higher utilization, or
    non-renewable resource like battery power can be conserved by disabling
    unused resources.We found opportunities to exploit idle time in our work on
    storage systems, and after a few attempts to tackle specific instances of
    it in ad hoc ways, began to investigate general mechanisms that could be
    applied to this problem.  Our results include a taxonomy of idle-time
    detection algorithms, metrics for evaluating them, and an evaluation of a
    number of idleness predictors that we generated from our taxonomy.", 
  location     = "https://john.e-wilkes.com/papers/idleness.pdf"
}

@InProceedings{eodafacfs,
  author       = "Murthy Devarakonda and Ajay Mohindra and Jill Simoneaux and William~H. Tetzlaff",
  title        = "Evaluation of Design Alternatives for a Cluster File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "35--46",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cluster computing, token-based consistency, disk sharing,
    file-system sharing, journaling file systems",
  abstract     = "Based on implementation experience and measurements, this
    paper presents an evaluation of design alternatives for a cluster file
    system.  The file system is targeted for IBM cluster systems, Scalable
    POWERparallel and AIX HACMP/6000.  We considered a shared disk approach
    where serialized, multiple instances of a single-system file system
    directly access file data as disk blocks, and a shared system approach
    which is the conventional method of distributing file system function
    between client and server.  We conclude the shard disk approach suffers
    form the difficulties serializing metadata, poor write-sharing performance,
    and low read throughput.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/evaluation-design-alternatives-cluster-file-system"
}

@InProceedings{mraaaims,
  author       = "Jonathan~S. Goldick and Kathy Benninger and Christopher Kirby and Christopher Maher and Bill Zumach",
  title        = "Multi-Resident {AFS}: An Adventure in Mass Storage",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "47--58",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed file systems, mass storage, afs",
  abstract     = "The Pittsburgh Supercomputing Center has been working to
    integrate distributed file system technology with hierarchical mass
    storage.  We produced a system utilizing the Andrew File System that can be
    interfaced to many mass storage systems.  We retained the semantics of AFS
    and compatibility with standard clients and servers.  The architecture has
    a logical separation between the facility that provides the user interface
    and access semantics and the management of the storage systems that contain
    user data.  Support for file level replication is provided for high
    availability to data in a fashion that is transparent to users.  This
    system is called Multi-Resident AFS.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/multi-resident-afs-adventure-mass-storage", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267416"
}

@InProceedings{reatahbmpfs,
  author       = "Ethan~L. Miller and Randy~H. Katz",
  title        = "{RAMA}:  Easy Access to a High-Bandwidth Massively Parallel File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "59--69",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "",
  abstract     = "Massively parallel file systems must provide high bandwidth
    file access to programs running on their machines.  Most accomplish this
    goal by striping files across arrays of disks attached to a few specialized
    I/O nodes in the massively parallel processor (MPP).  This arrangement
    requires programmers to give the file system many hints on how their data
    is to be laid out on disk if they want to achieve good performance.
    Additionally, the custom interface makes massively parallel file systems
    hard for programmers to use and difficult to seamlessly integrate into an
    environment with workstations and tertiary storage.The RAMA file system
    addresses these problems by providing a massively parallel file system that
    does not need user hints to provide good performance.  RAMA takes advantage
    of the recent decrease in physical disk size by assuming that each
    processor in an MPP has one or more disks attached to it.  Hashing is then
    used to pseudo-randomly distribute data to all of these disks, insuring
    high bandwidth regardless of access pattern.  Since MPP programs often have
    many nodes accessing a single file in parallel, the file system must allow
    access to different parts of the file without relying on a particular node.
    In RAMA, a file request involves only two nodes -- the node making the
    request and the node on whose disk the data is stored.  Thus, RAMA scales
    well to hundreds of processors.  Since RAMA needs no layout hints from
    applications, it fits well into systems where users cannot (or will not)
    provide such hints.  Fortunately, this flexibility does not cause a large
    loss of performance.  RAMA's simulated performance is within 10-15% of the
    optimum performance of a similarly-sized striped file system, and is a
    factor of 4 or more better than a striped file system with poorly laid out
    data.",
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/rama-easy-access-high-bandwidth-massively-parallel-file",
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267417"
}

@InProceedings{irtpfpus,
  author       = "Ian Wakeman and Atanu Ghosh and Jon Crowcroft and Van Jacobson and Sally Floyd",
  title        = "Implementing Real-Time Packet Forwarding Policies Using {Streams}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "71--82",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "link sharing, diff serve, queue scheduling, streams, packet
    forwarding",
  abstract     = "This paper describes an implementation of the class based
    queueing (CBQ) mechanisms proposed by Sally Floyd and Van Jacobson to
    provide real time policies for packet forwarding.  CBQ allows the traffic
    flows sharing a data link to be guaranteed a share of the bandwidth when
    the link is congested, yet allows flexible sharing of the unused bandwidth
    when the link is unloaded.  In addition, CBQ provides mechanisms which give
    flows requiring low delay priority over other flows.  In this way, links
    can be shared by multiple flows yet still meet the policy and Quality of
    Service (QoS) requirements of the flows.We present a brief description of
    the implementation and some preliminary preformance measurements.  The
    problems of packet classification are addressed in a flexible and
    extensible, yet efficient manner, and whilst the Streams implementation
    cannot cope with very high speed interfaces, it can cope with the serial
    link speeds that are likely to be loaded.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/implementing-real-time-packet-forwarding-policies-using", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267418"
}

@InProceedings{stwotckaptplsa,
  author       = "Jeffrey I. Schiller and Derek Atkins",
  title        = "Scaling the {Web of Trust}:  Combining {Kerberos} and {PGP} to Provide Large Scale Authentication",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "83--94",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "e-mail, pgp, public key cryptography, kerberos, web of trust,
    authentication",
  abstract     = "Internet Security has become more important recently as the
    Internet grows exponentially and security breaches become more publicized.
    An important area of concern for many Internet users is the privacy and
    integrity of their electronic files and messages.  Phil Zimmermann's Pretty
    Good Privacy (PGP) provides a general purpose utility for file and message
    protection.  However PGP requires that communicating users be 'introduced'
    to each other.  This paper describes a scheme that permits an enterprise
    using Kerberos to create an automated introducer called the PGP Key Signer
    Service.  Using this service people in the enterprise who have no common
    acquaintances to act as introducers can be introduced through the Key
    Signer.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/scaling-web-trust-combining-kerberos-and-pgp-provide"
}

@InProceedings{fasrofc,
  author       = "Puneet Kumar and M.~Satyanarayanan",
  title        = "Flexible and Safe Resolution of File Conflicts",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "95--106",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "coda file system, conflict resolution, consistency",
  abstract     = "In this paper we describe the support provided by the Coda
    File System for transparent resolution of conflicts arising from concurrent
    updates to a file in different network partitions.  Such partitions often
    occur in mobile computing environments.  Coda provides a framework for
    invoking customized pieces of code called application-specific resolvers
    (asrs) that encapsulate the knowledge needed for file resolution.  If
    resolution succeeds, the user notices nothing more than a slight
    performance delay.  Only if resolution fails does the user have to resort
    to manual repair.  Our design combines a rule-based approach to ASR
    selection with transactional encapsulation of ASR execution.  This paper
    shows how such an approach leads to flexible and efficient file resolution
    without loss of security or robustness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flexible-and-safe-resolution-file-conflicts", 
  location     = "https://dl.acm.org/citation.cfm?id=1267419"
}

@InProceedings{oacfftodce,
  author       = "John Dilley",
  title        = "{OODCE}: {A} {C}++ Framework for the {OSF Distributed Computing Environment}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "107--118",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "dce, distributed programming",
  abstract     = "This paper presents a method for developing object-oriented
    distributed applications using the C++ and DCE technologies.  The core of
    this package is a DCE IDL-to-C++ compiler and a set of C++ classes
    providing easy access to DCE functionality.  Using this approach we were
    able to develop more object-oriented distributed applications, and saw a
    significant decrease in application code size.  This contributed to an
    increase in developer productivity and code maintainability.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/oodce-c-framework-osf-distributed-computing-environment"
}

@InProceedings{umi44l,
  author       = "Jan-Simon Pendry and Marshall Kirk McKusick",
  title        = "Union Mounts in 4.{4BSD}-Lite",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "25--33",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "file systems, union file mounts, vnodes",
  abstract     = "This paper describes the design and rationale behind union
    mounts, a new filesystem-namespace management tool available in
    4.4BSD-Lite.  Unlike a traditional mount that hides the contents of the
    directory on which it is placed, a union mount presents a view of a merger
    of the two directories.  Although only the filesystem at the top of the
    union stack can be modified, the union filesystem gives the appearance of
    allowing any- thing to be deleted or modified.  Files in the lower layer
    may be deleted with whiteout in the top layer.  Files to be modified are
    automatically copied to the top layer.  This new functionality makes
    possible several new applications including the ability to apply patches to
    a CD-ROM and eliminate symbolic links generated by an automounter.  Also
    possible is the provision of per- user views of the filesystem, allowing
    private views of a shared work area, or local builds from a centrally
    shared read-only source tree.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/mckusick.html"
}

@InProceedings{pi44,
  author       = "W.~Richard Stevens and Jan-Simon Pendry",
  title        = "Portals in {4.4BSD}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "1--10",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "remote access, portals",
  abstract     = "Portals were added to 4.4BSD as an experimental feature and
    are in the publicly available 4.4BSD-Lite distribution.  Portals provide
    access to alternate file types or devices using names in the normal
    filesystem that a process just opens.  For example, an open of
    /p/tcp/foo.com/smtp returns a TCP socket descriptor to the calling process
    that is connected to the SMTP server on the specified host.  By providing
    access through the normal filesystem, the calling process need not be aware
    of the special functions necessary to create a TCP socket and establish a
    TCP connection.  This makes TCP connections, for example, available to
    programs such as Awk, Tcl, and shell scripts.  This paper describes the
    implementation of portals in 4.4BSD as another type of filesystem and
    provides some examples.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/portals-44bsd"
}

@InProceedings{dvdai,
  author       = "Aju John",
  title        = "Dynamic {Vnodes} --- Design and Implementation",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "11--23",
  organization = "USENIX Association",
  address      = nola,
  month        = "11--23 " # jan,
  keywords     = "osf, dynamic storage management, coordinated timeouts",
  abstract     = "Dynamic vnodes make the UNIX kernel responsive to a varying
    demand for vnodes, without a need to rebuild the kernel.  It also optimizes
    the usage of memory by deallocating excess vnodes.  This paper describes
    the design and implementation of dynamic vnodes in DEC OSF/1 V3.0.  The
    focus is on the vnode deallocation logic in a Symmetric Multi-Processing
    environment.Deallocation of vnodes differs from the familiar concept of
    dynamically allocated data structures in the following ways: the legacy
    name-cache design implicitly assumes that vnodes are never deallocated, and
    the vnode free-list needs to cache unused vnodes effectively.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/dynamic-vnodes-design-and-implementation"
}

@InProceedings{pitccl,
  author       = "Vijay Saraswat and Radha Jagadeesan and Vinheet Gupta",
  title        = "Programming in Timed Concurrent Constraint Languages",
  booktitle    = "Constraint Programming",
  year         = 1994,
  editor       = "B.~Mayoh and E.~Tyugu and J.~Penjam",
  series       = "NATO ASI Series (Series F: Computer and Systems Sciences)",
  volume       = 131,
  publisher    = sv,
  address      = begr,
  keywords     = "synchronous programming, operational semantics, constraint
    systems, negative information, parallel composition",
  abstract     = "The areas of Qualitative Reasoning about physical systems
    (Weld and de Kleer 1989), reasoning about action and state change (Ginsberg
    1987), reactive, realtime computing (Real-time systems 1991) and concurrent
    programming languages (Milner 1980; Hoare 1985) are areas of inquiry that
    are fundamentally about the same subject matter — the representation,
    design and analysis of continuous and discrete dynamical systems.", 
  location     = "https://doi.org/10.1007/978-3-642-85983-0_15"
}

@InProceedings{srmc,
  author       = "Diomidis Spinellis",
  title        = "Software Reliability: Modern Challenges",
  booktitle    = "Proceedings ESRL '99 --- The Tenth European Conference on Safety and Reliability",
  year         = 1999,
  editor       = "G.~I. Schu{\" e}ller and P.~Kafka",
  pages        = "589--592",
  address      = "Munich-Garching, Germany",
  month        = sep,
  keywords     = "hardware, operating systems, software system architecture,
    programming languages, software development",
  abstract     = "The evolution of computer technology is creating for
    safety-critical systems new challenges and different types of failure
    modes.  Modern computer processors are often delivered with errors, while
    intelligent hardware subsystems may exhibit nondeterministic behaviour.
    Operating systems and programming languages are becoming increasingly
    complicated and their implementations less trustworthy.  In addition,
    component-based multi-tier software system architectures exponentially
    increase the number of failure modes, while Internet connectivity exposes
    systems to malicious attacks.  Finally, IT outsourcing and blind reliance
    on standards can provide developers with a false sense of security.
    Planning in advance for the new challenges is as important as embracing the
    new technology.", 
  location     = "https://www2.dmst.aueb.gr/dds/pubs/conf/1999-ESREL-SoftRel/html/chal.html"
}

@InProceedings{daomflfc,
  author       = "Gokul~V. Subramaniam and Eric~J. Byrne",
  title        = "Deriving an Object Model from Legacy {Fortran} Code",
  booktitle    = pot # "1996 International Conference on Software Maintenance (ICSM '96)",
  year         = 1996,
  pages        = "3--12",
  publisher    = "IEEE Press",
  address      = moncal,
  month        = "4--8 " # nov,
  keywords     = "re-engineering, object models, object-oriented design",
  abstract     = "The practice of software development continues to shift
    towards the use of object-oriented approaches.  The motivation for this
    trend is the benefits attributed to object-oriented software, including
    improved maintainability.  As organizations develop new object-oriented
    software, they face the problem of maintaining their older software.  How
    can existing non-objected-oriented software benefit from this new software
    engineering technology? This paper presents a nine step process for
    deriving an object model from existing unstructured FORTRAN source code.
    Both top-down and bottom-up approaches are used to derive objects, classes,
    class attributes and methods, and relationships among classes.  This
    process can be used within a reengineering project to convert legacy
    FORTRAN code into a new object-oriented implementation written in a
    language such as C++.  Experience with using this process is also
    described.", 
  location     = "https://dl.acm.org/doi/10.5555/645544.655855"
}

@InProceedings{aaotdfpfcimc,
  author       = "Geoffrey~H. Kuenning and Gerald~J. Popek and Peter~L. Reiher",
  title        = "An Analysis of Trace Data for Predictive File Caching in Mobile Computing",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "291--303",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file caching, off-line execution, workloads, trace analysis,
    user behavior",
  abstract     = "One way to provide mobile computers with access to the resources of a network, even in the absence of communication, is to predict which information will be used during disconnection and cache the appropriate data while still connected.  To determine the feasibility of this approach, traces of file-access activity for three diverse application domains were collected for periods of over two months.  Analysis of these traces using traditional and new measures reveals that user working sets tend to be small compared to modern disk sizes, that users tend to reference the same files for several days or even weeks at a time, and that different users do not tend to write to the same file except in highly constrained circumstances.  These factors encourage the conclusion that an automated caching system can be built for a wide variety of environments.",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267277",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/analysis-trace-data-predictive-file-caching"
}

@InProceedings{sscrfmi,
  author       = "Trevor Blackwell and Kee Chan and Koling Chang and Thomas Charuhas and James Gwertzman and Brad Karp and H.~T. Kung and W.~David Li and Dong Lin and Robert Morris and Robert Polansky and Diane Tang and Cliff Young and John Zao",
  title        = "Secure Short-Cut Routing for Mobile {IP}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "305--316",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "ip, mobility, routing, security, encapsulation, forwarding,
    short-cut routing",
  abstract     = "This paper describes the architecture and implementation of a mobile IP system.  It allows mobile hosts to roam between cells implemented with 2-Mbps radio base stations, while maintaining Internet connectivity.  The system is being developed as part of a course on wireless networks at Harvard and has been operational since March 1994.The architecture scales well, both geographically and in the number of mobile hosts supported.  It supports secure short-cut routing to mobile hosts using the existing Internet routing system without change.  The implementation demonstrates a robust, low complexity realization of the architecture, and provides trade-off opportunities between efficiency and cost.Measured performance of the mobile system is generally excellent.  The system can handle a high rate of location updates, and routes packets almost as efficiently for mobile hosts as the Internet does for stationary hosts.  We observe reasonable TCP behavior during hand-offs.",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/secure-short-cut-routing-mobile-ip",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267278"
}

@InProceedings{prtpopwtl,
  author       = "Arthur Bernstein and Paul~K. Harter",
  title        = "Proving Real-Time Properties of Programs with Temporal Logic",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "1--11",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "temporal logic, proof systems, real-time systems",
  abstract     = "Wirth [Wi77] categorized programs into three classes.  The
    most difficult type of program to understand and write is a real-time
    program.  Much work has been done in the formal verification of sequential
    programs, but much remains to be done for concurrent and real-time
    programs.  The critical nature of typical real-time applications makes the
    validity problem for real-time programs particularly important.  Owicki and
    Lamport [OL80] present a relatively new method for verifying concurrent
    programs using temporal logic.  This paper presents an extension of their
    work to the area of real-time programs.  A model and proof system are
    presented and their use demonstrated using examples from the literature.", 
  location     = "https://doi.org/10.1145/1067627.806585"
}

@InProceedings{davoss,
  author       = "J.~M. Rushby",
  title        = "Design and Verifiation of Secure Systems",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "12--21",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed systems, isolation, security, mechanism",
  abstract     = "This paper reviews some difficulties arising when verifying
    kernelized secure systems and suggests new techniques for their resolution.
    It proposes that secure systems be conceived as distributed systems in
    which security is achieved partly through the physical separation of its
    individual components and partly through the mediation of trusted functions
    performed within some of those components. The security kernel lets such a
    'distributed' system run within a single processor; policy enforcement is
    not the concern of a security kernel. This approach decouples component
    verification, which perform trusted functions, from security kernel
    verification.  This latter task may be accomplished by a new verification
    technique called 'proof of separability' which explicitly addresses the
    security relevant aspects of interrupt handling and other issues ignored by
    present methods.", 
  location     = "https://doi.org/10.1145/1067627.806586"
}

@InProceedings{ankjfb,
  author       = "Joel~F. Bartlett",
  title        = "{A} {NonStop} Kernel",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "22--29",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "redundancy, fault models, processes, message passing, error
    recovery", 
  abstract     = "The Tandem NonStop System is a fault-tolerant, expandable,
    and distributed computer system designed for online transaction processing.
    This paper describes the operating system kernel's key primitives.  The
    first section describes the basic hardware building blocks and introduces
    their software analogs: processes and messages.  Using these primitives, a
    mechanism allowing fault-tolerant resource access, the process-pair, is
    described.  The paper concludes with some observations on this type of
    system structure, and on system use.",
  location     = "https://doi.org/10.1145/1067627.806587",
  location     = "https://www.hpl.hp.com/techreports/tandem/TR-81.4.pdf"
}

@InProceedings{ootdoaos,
  author       = "Hugh~C. Lauer",
  title        = "Observations on the Development of an Operating System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "30--36",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "software development, the five-to-seven-year model, os
    classification, pilot, mesa, software management",
  abstract     = "The development of Pilot, an operating system for a personal
    computer, is reviewed, including a brief history and some of the problems
    and lessons encountered during this development.  As part of understanding
    how Pilot and other operating systems come about, an hypothesis is
    presented that systems can be classified into five kinds according to the
    style and direction of their development, independent of their structure.
    A further hypothesis is presented that systems such as Pilot, and many
    others in widespread use, take about five to seven years to reach maturity,
    independent of the quality and quantity of the talent applied to their
    development.  The pressures, constraints, and problems of producing Pilot
    are discussed in the context of these hypotheses.",
  location     = "https://doi.org/10.1145/800216.806588"
}

@InProceedings{tffsmf,
  author       = "Marek Fridrich and William~J. Older",
  title        = "The {Felix} File Server",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "37--44",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "consistency, file sets, transactions, resiliency",
  abstract     = "This paper describes Felix - a File Server for an
    experimental distributed multicomputer system.  Felix is designed to
    support a variety of file systems, virtual memory, and database
    applications with access being provided by a local area network.  Its
    interface combines block oriented data access with a high degree of crash
    resistance and a comprehensive set of primitives for controlling data
    sharing and consistency.  An extended set of access modes allows increased
    concurrency over conventional systems.", 
  location     = "https://doi.org/10.1145/800216.806589"
}

@InProceedings{ewastfdntps,
  author       = "James~D. Guyton and Michael~F. Schwartz",
  title        = "Experiences with a Survey Tool for Discovering {Network Time Protocol} Servers",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "257--265",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "clock synchronization, ntp, remote monitoring, surveys",
  abstract     = "The Network Time Protocol (NTP) is widely used to synchronize
    computer clocks throughout the Internet.  Existing NTP clients and servers
    form a very large distributed system, and yet the tools available to
    observe and manage this system are fairly primitive.  This paper describes
    our experiences with a prototype tool that attempts to discover relevant
    information about every NTP site on the Internet.  The data produced by
    this tool can be used for a variety of purposes, including locating nearby
    accurate time servers and computing aggregate and long-term evaluations of
    the size and health of the NTP system.  Importantly, our tool provides a
    means by which new NTP server administrators can make informed choices
    among the possible servers with which to synchronize, balancing the need
    for accurate time with the need to distribute server load.  This is an
    important step towards improving global NTP system scalability, since at
    present our measurements indicate that the high-stratum servers are heavily
    overloaded.", 
  location     = "https://www.usenix.org/biblio-4262"
}

@InProceedings{lgccfic,
  author       = "Mummert, Lily~B. and Satyanarayanan, M.",
  title        = "Large Granularity Cache Coherence for Intermittent Connectivity",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "279--289",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "cache coherence, distributed file systems, coda, callbacks,
    cache management",
  abstract     = "To function in mobile computing environments, distributed
    file systems must cope with networks that are slow, intermittent, or both.
    Intermittence vitiates the effectiveness of callback-based cache coherence
    schemes in reducing client-server communication, because clients must
    validate files when connections are reestablished.  In this paper we show
    how maintaining cache coherence at a large granularity alleviates this
    problem.  We report on the implementation and performance of large
    granularity cache coherence for the Coda File System.  Our measurements
    confirm the value of this technique.  At 9.6 Kbps, this technique takes
    only 4 -- 20% of the time required by two other strategies to validate the
    cache for a sample of Coda users.  Even at this speed, the network is
    effectively eliminated as the bottleneck for cache validation.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/large-granularity-cache-coherence-intermittent",
  location     = "https://www.cs.cmu.edu/~satya/docdir/mummert-usenix-large-granularity-1994.pdf"
}

@InProceedings{patdluvi,
  author       = "Timothy~W. Curry",
  title        = "Profiling and Tracing Dynamic Library Usage Via Interposition",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "267--278",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "profiling, tracing, dynamic libraries, dynamic linking,
    interposition",
  abstract     = "Run-time resolution of library functions provides a rich and
    powerful opportunity to collect workload profiles and function/parameter
    trace information without source, special compilation, or special linking.
    This can be accomplished by having the linker resolve library functions to
    special wrapper functions that collect statistics before and after calling
    the real library function, leaving both the application and real library
    unaltered.  The set of dynamic libraries is quite large including
    interesting libraries like libc (the C library and Operating System
    interface), graphics, database, network interface, and many more.  Coupling
    this with the ability to simultaneously trace multiple processes on
    multiple processors covering both client and server processes yields
    tremendous feedback.  We have found the amount of detailed information that
    can be gathered has been useful in many stages of the project life-cycle
    including the design, development, tuning, and sustaining of hardware,
    libraries, and applications.This paper first contrasts our extended view of
    interposition to other profiling, tracing, and interposing techniques.
    This is followed by a description and sample output of tools developed
    around this view; a discussion of obstacles encountered developing the
    tools; and finally, a discussion of anticipated and unanticipated ways
    those tools have been applied.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267275"
}

@InProceedings{ptidec,
  author       = "Douglas~E. Comer and John~C. Lin",
  title        = "Probing {TCP} Implementations",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "245--255",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "remote monitoring, tcp, measurements, testing",
  abstract     = "In this paper, we demonstrate a technique called active
    probing used to study TCP implementations.  Active probing treats a TCP
    implementation as a black box, and uses a set of procedures to probe the
    black box.  By studying the way TCP responds to the probes, one can deduce
    several characteristics of the implementation.  The technique is
    particularly useful if TCP source code is unavailable.To demonstrate the
    technique, the paper shows example probe procedures that examine three
    aspects of TCP.  The results are informative: they reveal implementation
    flaws, protocol violations, and the details of design decisions in five
    vendor-supported TCP implementations.  The results of our experiment
    suggest that active probing can be used to test TCP implementations.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/probing-tcp-implementations"
}

@InProceedings{ossfdm,
  author       = "Ian~M. Leslie and Derek McAuley and Sape~J. Mullender",
  title        = "Operating-System Support for Distributed Multimedia",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "209--219",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "atm, desk-area network, log-structured file systems",
  abstract     = "Multimedia applications place new demands upon processors,
    networks and operating systems.  While some network designers, through ATM
    for example, have considered revolutionary approaches to supporting
    multimedia, the same cannot be said for operating systems designers.  Most
    work is evolutionary in nature, attempting to identify additional features
    that can be added to existing systems to support multimedia.  Here we
    describe the Pegasus project¿s attempt to build an integrated hardware and
    operating system environment from the ground up specifically targeted
    towards multimedia.", 
  location     = "https://research.utwente.nl/en/publications/pegasus-operating-system-support-for-distributed-multimedia-syste"
}

@InProceedings{suiagml,
  author       = "Lincoln Stein and Andre Marquis and Ert Dredge and Mary Pat Reeve and Mark Daly and Steve Rozen and Nathan Goodman",
  title        = "Splicing " # unix # " into a Genome Mapping Laboratory",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "221--229",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "database access, user interfaces",
  abstract     = "The Whitehead Institute/MIT Center for Genome Research is
    responsible for a number of large genome mapping efforts, the scale of
    which create problems of data and workflow management that dictate reliance
    on computer support.  Two years ago, when we started to design the
    informatics support for the laboratory, we realized that the fluid and
    ever-changing nature of the experimental protocols precluded any effort to
    create a single monolithic piece of software.  Instead we designed a system
    that relied on multiple distributed data analysis and processing tools knit
    together by a centralized database.  The obvious choice of operating
    systems was UNIX.  In order to make this choice palatable to the laboratory
    biologists--who rightly consider it their job to do experiments rather than
    to interact with computers, and who have come to expect all software to be
    as intuitive and responsive as the Apple Macintoshes on their desks--we
    designed a system that runs automatically and essentially invisibly.
    Whenever it is necessary for the informatics system to interact with a
    member of the laboratory we have carefully chosen a user interface paradigm
    that best balances the user's expectations against the system's
    capabilities.  When possible we have chosen to adapt familiar software to
    our user interface needs rather than to write user interfaces from scratch.
    We've managed to hide the power of UNIX behind the innocuous personal
    computer-based front ends our users know and love, using techniques that
    should be applicable in other environments as well.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/splicing-unix-genome-mapping-laboratory"
}

@InProceedings{atrpftuos,
  author       = "Liam R.~E. Quin",
  title        = "{A} Text Retrieval Package for the Unix Operating System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "231--243",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "search, text databases",
  abstract     = "This paper describes lq-text, an inverted index text
    retrieval package written by the author.  Inverted index text retrieval
    provides a fast and effective way of searching large amounts of text.  This
    is implemented by making an index to all of the natural-language words that
    occur in the text.  The actual text remains unaltered in place, or, if
    desired, can be compressed or archived; the index allows rapid searching
    even if the data files have been altogether removed.The design and
    implementation of lq-text are discussed, and performance measurements are
    given for comparison with other text searching programs such as grep and
    agrep.  The functionality provided is compared briefly with other packages
    such as glimpse and zbrowser.The lq-text package is available in source
    form, has been successfully integrated into a number of other systems and
    products, and is in use at over 100 sites.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/text-retrieval-package-unix-operating-system"
}

@InProceedings{aekbiopt,
  author       = "Robert~A. Alfieri",
  title        = "An Efficient Kernel-Based Implementation of {POSIX} Threads",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "59--72",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "kernel-space threading, kernel function calls, address-space
  
		  transitions", 
  abstract     = "This paper describes the kernel-based implementation of
    POSIX Threads (Pthreads) in the DG/UXTM operating system.  The
    implementation achieves time efficiency by using a general-purpose trap
    mechanism, known as a Kernel Function Call (KFC), that carries an order of
    magnitude less overhead than a traditional system call.  On a 50 MHz
    Motorola MC88110, the implementation can create and exit a thread (with the
    associated context switch) in 8.1 microseconds and yield to another thread
    in 4.0 microseconds.  The implementation also achieves space efficiency by
    paging and decoupling bulky data structures.The advantages of a
    kernel-based implementation include design simplicity, less code
    redundancy, optimization of global (interprocess) operations, avoidance of
    inopportune preemption, and global semantic flexibility.  The disadvantage
    is a monolithic design that lacks user-level flexibility.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267262", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/efficient-kernel-based-implementation-posix"
}

@InProceedings{uolstiadaer,
  author       = "Andrea~H. Skarra",
  title        = "Using {OS} Locking Services to Implement a {DBMS}:  An Experience Report",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "73--86",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "lock managers, file locking, two-pahse commit, lock servers",
  abstract     = "The paper describes a black-box analysis of the locking
    facilities in several UNIX-compatible operating systems for their ability
    to support transaction synchronization.  It assesses the facilities for
    their adequacy, flexibiilty, and performance.  Most of the operating
    systems in the study provide adequate support for simple two-phase locking
    transaction systems that don't require customized or priority-based
    scheduling of lock requests.  The performance depends on a variety of
    factors: the average execution time for a lock request varies directly with
    the number of cuncurrent locks in the system and indrectly with the number
    of files locked for a given number of lock requests.  The request time is
    smaller when the locked files are local to the requesting process instead
    of remote, and when a process locks a file's segments in order of adjacency
    rather than randomly.  For the areas in which the OS provides inadequate
    support, the paper proposes several specific remedies.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/using-os-locking-services-implement-dbms"
}

@InProceedings{tsaaokma,
  author       = "Jeff Bonwick",
  title        = "The Slab Allocator:  An Object-Cache Kernel Memory Allocator",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "87--98",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "storage allocation, free lists, constructing, finializing,
    homogeneity",
  abstract     = "This paper presents a comprehensive design overview of the
    SunOS 5.4 kernel memory allocator.  This allocator is based on a set of
    object-caching primitives that reduce the cost of allocating complex
    objects by retaining their state between uses.  These same primitives prove
    equally effective for managing stateless memory (e.g.  data pages and
    temporary buffers) because they are space-efficient and fast.  The
    allocator's object caches respond dynamically to global memory pressure,
    and employ an object-coloring scheme that improves the system's overall
    cache utilization and bus balance.  The allocator also has several
    statistical and debugging features that can detect a wide range of problems
    throughout the system.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267263"
}

@InProceedings{abup,
  author       = "Jeffrey~C. Mogul",
  title        = "{A} Better Update Policy",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "99--111",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file cache management, write back, cache flush, performance",
  abstract     = "Some file systems can delay writing modified data to disk, in
    order to reduce disk traffic and overhead.  Prudence dictates that such
    delays be bounded, in case the system crashes.  We refer to an algorithm
    used to decide when to write delayed data back to disk as an update policy.
    Traditional UNIX® systems use a periodic update policy, writing back all
    delayed-write data once every 30 seconds.  Periodic update is easy to
    implement but performs quite badly in some cases.  This paper describes an
    approximate implementation of an interval periodic update policy, in which
    each individual delayed-write block is written when its age reaches a
    threshold.  Interval periodic update adds little code to the kernel and can
    perform much better than periodic update.  In particular, interval periodic
    update can avoid the huge variances in read response time caused by using
    periodic update with a large buffer cache.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267264", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-94-4.html"
}

@InProceedings{tdfs,
  author       = "Morgan Clark and Stephen Rago",
  title        = "The Desktop File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "113--124",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file systems, block allocation, b+ trees, shadow paging",
  abstract     = "This paper describes the structure and performance
    characteristics of a commercial file system designed for use on desktop,
    laptop, and notebook computers running the UNIX operating system.  Such
    systems are characterized by their small disk drives dictated by system
    size and power requirements.  In addition, these systems are often used by
    people who have little or no experience administering Unix systems.  The
    Desktop File System attempts to improve overall system usability by
    transparently compressing files, increasing file system reliability, and
    simplifying administrative interfaces.  The Desktop File System has been in
    production use for over a year, and will be included in future versions of
    the SCO Open Desktop Unix system.  Although originally intended for a
    desktop environment, the file system is also being used on many larger,
    server-style machines.",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/desktop-file-system"
}

@InProceedings{sahblfs,
  author       = "Ken Shirriff and John Ousterhout",
  title        = "Sawmill:  {A} High-Bandwidth Logging File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "125--136",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "raid, log-structured file systems, read batching, metadata,
    cleaning", 
  abstract     = "This paper describes the implementation of Sawmill, a network
    file system using the RAID-II storage system.  Sawmill takes advantage of
    the direct data path in RAID-II between the disks and the network, which
    bypasses the file server CPU.  The key ideas in the implementation of
    Sawmill are combining logging (LFS) with RAID to obtain fast small writes,
    using new log layout techniques to improve bandwidth, and pipelining
    through the controller memory to reduce latency.  The file system can
    currently read data at 21 MB/s and write data at 15 MB/s, close to the raw
    disk array bandwidth, while running on a relatively slow Sun-4.
    Performance measurements show that LFS improved performance of a stream of
    small writes by over a order of magnitude compared to writing directly to
    the RAID, and this improvement would be even larger with a faster CPU.
    Sawmill demonstrates that by using a storage system with a direct data
    path, a file system can provide data at bandwidths much higher than the
    file server itself could handle.  However, processor speed is still an
    important factor, especially when handling many small requests in
    parallel.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267265", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/sawmill-high-bandwidth-logging-file-system"
}

@InProceedings{nv3dai,
  author       = "Brian Pawlowski and Chet Juszczak and Peter Staubach and Carl Smith and Diane Lebel and Dave Hitz",
  title        = "{NFS} Version 3 Design and Implementation",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "137--152",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "nfs, large files, asynchronous writes, crash recovery, weak
    cache consistency, protocol development",
  abstract     = "This paper describes a new version of the Network File System
    (NFS) that supports access to files larger than 4GB and increases
    sequential write throughput sevenfold when compared to unaccelerated NFS
    Version 2.  NFS Version 3 maintains the stateless server design andsimple
    crash recovery of NFS version 2, and the philosophy of building a
    distrubted file service from cooperating protocols.  We describe the
    protocol and its implementation, and provide initial performance
    measurements.  We then describe the implementation effort.  Finally, we
    contrast this work with other distributed file systems and discuss future
    revisions of NFS.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/nfs-version-3-design-and-implementation"
}

@InProceedings{ctaddbnm,
  author       = "Cheng-Zen Yang and Chih-Chung Chen and Yen-Jen Oyang",
  title        = "Clue Tables:  {A} Distributed, Dynamic-Binding Naming Mechanism",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "153--160",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed naming, remote file systems",
  abstract     = "This paper presents a distributed, dynamic naming mechanism
    called clue tables for building highly scalable, highly available
    distributed file systems.  The clue tables naming mechanism is distinctive
    in three aspects.  First, it is designed to cope well with the hierarchical
    structure of the modern large-scale computer networks.  Second, it
    implicitlycarries out load balancing among servers to improve
    systemscalability.  Third, it supports file replication and dynamically
    designates a primary copy to resolve possible data inconsistency.  This
    paper also reports a performance evaluation of the clue tables mechanism
    when compared with NFS, a popular distributed file system.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/1267257.1267266"
}

@InProceedings{olownpiaso,
  author       = "Dan Duchamp",
  title        = "Optimistic Lookup of Whole {NFS} Paths in a Single Operation",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "161--169",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  abstract     = "VFS lookup code examines and translates path names one
    component at a time, checking for special cases such as mount points and
    symlinks.  VFS calls the NFS lookup operation as necessary.  NFS employs
    caching to reduce the number of lookup operations that go to the server.
    However, when part or all of a path is not cached, NFS lookup operations go
    back to the server.  Although NFS's caching is effective,
    component-by-component translation of an uncached path is inefficient,
    enough so that lookup is typically the operation most commonly processed by
    servers.  We study the effect of augmenting the VFS lookup algorithm and
    the NFS protocol so that a client can ask a server to translate an entire
    path in a single operation.  The preconditions for a successful request are
    usually but not always satisfied, so the algorithm is optimistic.  This
    small change can deliver substantial improvements in client latency and
    server load.", 
  keywords     = "path lookup, mount points, path cache", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267267", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/optimistic-lookup-whole-nfs-paths-single"
}

@InProceedings{acfcp,
  author       = "Pei Cao and Edward~W. Felten and Kai Li",
  title        = "Application-Controlled File Caching Policies",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "171--182",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "kernel allocation, swapping, file sharing, prefetching",
  abstract     = "We consider how to improve the performance of file caching by
    allowing user-level control over file cache replacement decisions.  We use
    two-level cache management: the kernel allocates physical pages to
    individual applications (allocation), and each application is responsible
    for deciding how to use its physical pages (replacement).  Previous work on
    two-level memory management has focused on replacement, largely ignoring
    allocation.The main contribution of this paper is our solution to the
    allocation problem.  Our solution allows processes to manage their own
    cache blocks, while at the same time maintains the dynamic allocation of
    cache blocks among processes.  Our solution makes sure that good user-level
    policies can improve the file cache hit ratios of the entire system over
    the existing replacement approach.  We evaluate our scheme by trace-based
    simulation, demonstrating that it leads to significant improvements in hit
    ratios for a variety of applications.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267268", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-445-94"
}

@InProceedings{rfcitffs,
  author       = "Peter Reiher and John Heidemann and David Ratner and Greg Skinner and Gerald Popek",
  title        = "Resolving File Conflicts in the {Ficus} File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed file system, conflict resulution",
  abstract     = "Ficus is a flexible replication facility with optimistic
    concurrency control designed to span a wide range of scales and network
    environments.  Optimistic concurrency control provides rapid local access
    and high availability of files for update in the face of disconnection, at
    the cost of occasional conflicts that are only discovered when the system
    is reconnected.  Ficus reliably detects all possible conflicts.  Many
    conflicts can be automatically resolved by recognizing the file type and
    understanding the file's semantics.  This paper describes experiences with
    conflicts and automatic conflict resolution in Ficus.  It presents data on
    the frequency and character of conflicts in our environment.  This paper
    also describes how semantically knowledgeable resolvers are designed and
    implemented, and discusses our experiences with their strengths and
    limitations.  We conclude from our experience that optimistic concurrency
    works well in at least one realistic environment, conflicts are rare, and a
    large proportion of those conflicts that do occur can be automatically
    solved without human intervention.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267269", 
  location     = "https://www.isi.edu/~johnh/PAPERS/Reiher94a.pdf"
}

@InProceedings{rfsluapa,
  author       = "James Griffioen and Randy Appleton",
  title        = "Reducing File System Latency using a Predictive Approach",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file system caching, prefetching, probabilistic prefetching",
  abstract     = "Despite impressive advances in file system through put
    resulting from technologies such as high-bandwidth networks and disk
    arrays, file system latency has not improved and in many cases has become
    worse.  Consequently, file system I/O remains one of the major bottlenecks
    to operating system performance [10].This paper investigates an automated
    predictive approach towards reducing file latency.  Automatic Prefetching
    uses past file accesses to predict future file systemrequests.  The
    objective is to provide data in advance of the request for the data,
    effectively masking access latencies.  We have designed and implement a
    system to measure the performance benefits of automatic prefetching.  Our
    current results, obtained from a trace-driven simulation, show that
    prefetching results in as much as a 280% improvement over LRU especially
    for smaller caches.  Alternatively, prefetching can reduce cache size by up
    to 50%.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267270"
}

@InProceedings{saifsnp,
  author       = "Thomas~Y.~C. Woo and Raghuram Bindignavle and Shaowen Su and Simon~S. Lam",
  title        = "{SNP}:  An Interface for Secure Network Programming",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "45--58",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "application-layer security, secure data transfer, sockets",
  abstract     = "SNP provides a high-level abstraction for secure end-to-end
    network communications.  It supports both stream and datagram semantics
    with security guarantees (e.g., data origin authenticity, data integrity
    and data confidentiality).  It is designed to resemble the Berkeley sockets
    interface so that security can be easily retrofitted into existing socket
    programs with only minor modifications.  SNP is built on top of GSS-API,
    thus making it relatively portable across different authentication
    mechanisms conforming to GSSAPI.  SNP hides the details of GSS-API (e.g.,
    credentials and contexts management), the communication sublayer as well as
    the cryptographic sublayer from the application programmers.  It also
    encapsulates security sensitive information, thus preventing accidental or
    intentional disclosure by an application program.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/snp-interface-secure-network-programming", 
  location     = "https://www.cs.utexas.edu/users/lam/Vita/Cpapers/WBSL94.pdf"
}

@InProceedings{anoopls,
  author       = "Jeffrey~S. Haemer",
  title        = "{A} New Object-Oriented Programming Language: {\it sh}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "1--13",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "shell, object-oriented programming, file systems, process trees",
  abstract     = "Many have frittered away their time on C++, while overlooking
    the new POSIX.2-required, object-oriented language: sh.  As will be clear 
    from the enclosed code, the name may allude to the fact that the author
    would be embarrassed to have anyone ﬁnd out about it.  This paper
    introduces a tinybject-oriented programming system written entirely in
    POSIX-conforming shell scripts.", 
  location     = "https://www.usenix.org/biblio-4241"
}

@InProceedings{tomatc,
  author       = "Evan Adams",
  title        = "The Old Man and the {C}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "15--26",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "c++, generics",
  abstract     = "'You can't teach an old dog new tricks' goes the old proverb.
    This is a story about a pack of old dogs (C programmers) and their odyssey
    of trying to lean new tricks (C++ programming).  C++ is a large, complex
    language which can easily be abused, but also includes may features to help
    programmers more quickly write higher quality code.  The TeamWare group
    consciously decided which C++ features to use and, just as importantly,
    which features not to use.  We also incrementally adopted those features we
    chose to use.  This resulted in a successful C++ experience." 
}

@InProceedings{kmiaefs,
  author       = "Matt Blaze",
  title        = "Key Management in an Encrypting File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "27--35",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "key escrow, smart cards, cryptographic file systems, ",
  abstract     = "As distributed computing systems grow in size, complexity and
    variety of application, the problem of protecting sensitive data from
    unauthorized disclosure and tampering becomes increasingly important.
    Cryptographic techniques can play an important role in protecting
    communication links and file data, since access to data can be limited to
    those who hold the proper key.  In the case of file data, however, the
    routine use of encryption facilities often places the organizational
    requirements of information security in opposition to those of information
    management.  Since strong encryption implies that only the holders of the
    cryptographic key have access to the cleartext data, an organization may be
    denied the use of its own critical business records if the key used to
    encrypt these records becomes unavailable (e.g., through the accidental
    death of the key holder).  This paper describes a system, based on
    cryptographic 'smartcards,' for the temporary 'escrow' of file encryption
    keys for critical files in a cryptographic file system.  Unlike
    conventional escrow schemes, this system is bilaterally auditable, in that
    the holder of an escrowed key can verify that, in fact, he or she holds the
    key to a particular directory and the owner of the key can verify, when the
    escrow period is ended, that the escrow agent has neither used the key nor
    can use it in the future.  We describe a new algorithm, based on the DES
    cipher, for the online encryption of file data in a secure and efficient
    manner that is suitable for use in a smartcard.", 
  location     = "https://www.mattblaze.org/papers/cfskey.pdf"
}

@InProceedings{atamfif,
  author       = "Marcus~J. Ranum and Frederick~M. Avolio",
  title        = "{A} Toolkit and Methods for {Internet} Firewalls",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "37--44",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "firewalls, application gateways, internet security",
  abstract     = "As the number of businesses and government agencies
    connecting to the Internet continues to increase, the demand for Internet
    firewalls — points of security guarding a private network from intrusion —
    has created a demand for reliable tools from which to build them.  We
    present the TIS Internet Firewall Toolkit, which consists of software
    modules and configuration guidelines developed in the course of a broader
    ARPA sponsored project.  Components of the toolkit, while designed to work
    together, can be used in isolation or can be combined with other firewall
    components.  The Firewall Toolkit software runs on UNIX systems using
    TCP/IP with the Berkeley socket interface.  We describe the Firewall
    Toolkit and the reasoning behind some of its design decisions, discuss some
    of the ways in which it may be configured, and conclude with some
    observations as to how it has served in practice."
}

@InProceedings{reoui,
  author       = "E. Merlo and J.~F. Girard and K. Kontogiannis and P. Panangaden and R. {De Mori}",
  title        = "Reverse Engineering of User Interfaces",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "171--179",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "user interfaces, reverse engineering, behavioral
    representations, interface structure, object oriented approach, process
    algebra, CCS, specification language, target language, representational
    method, COBOL/CICS environment",
  abstract     = "A method for reverse engineering user interfaces based on
    their structural and behavioral representations is presented.  The
    interface structure is represented using an object oriented approach while
    interface behavior is described using Milner's process algebra (CCS).  A
    specification language for user interfaces is designed for the multiple
    purposes of serving as a target language for the reverse engineering
    process, as a working specification language for interface redesign, and as
    a specification language for generating a new user interface for a specific
    platform.  The motivations and advantages of such a representational method
    are discussed together with examples of user interface reverse engineering
    in a common business-oriented language (COBOL)/CICS environment.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287767"
}

@InProceedings{affredlis,
  author       = "Peter Aiken and Alice Muntz and Russ Richards",
  title        = "A Framework for Reverse Engineering {DoD} Legacy Information Systems",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "180--191",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "design, reverse engineering, software and system requirements
    and specifications, data architecture, business rules, data modeling",
  abstract     = "A framework to legacy information systems, which have been
    selected by reverse engineers, in the US Department of Defense
    heterogeneous environment, is reported.  This approach was developed to
    recover business rules, domain information, functional requirements, and
    data architectures, largely in the form of normalized, logical data models.
    In a pilot study, the data from diverse systems (ranging from home grown
    languages and database management systems developed during the late 1960s
    to those using high order languages and commercial network database
    management systems) are reverse engineered.  The pilot study is being used
    to validate and refine the framework with real-life systems to develop a
    baseline approach for reverse engineering existing systems; to scope and
    estimate future system re-engineering costs; and to determine the economic
    viability of re-engineering, reverse, and forward engineering efforts.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287766"
}

@InProceedings{repvda,
  author       = "Herbert Ritsch and Harry~M. Sneed",
  title        = "Reverse Engineering Programs via Dynamic Analysis",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "192--201",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, dynamic analysis, program behavior,
    static analysis, esprit docket project, knowledge acquisition,
    post-documentation, software systems", 
  abstract     = "A tool-supported approach to extracting information about
    programs via a dynamic analysis of the program behavior is described.  The
    information obtained in this fashion is intended to supplement the
    information gained through the static analysis of the program source.  By
    joining the two different views of a program, it is hoped that a more
    complete specification of the program function may be developed.  This
    research has taken place within the scope of the ESPRIT DOCKET project to
    study means of knowledge acquisition from and post-documentation of
    existing software systems.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287765"
}

@InProceedings{drfstooop,
  author       = "C.~H. Kung and J.~Gao and P.~Hsia and J.~Lin and Y.~Yoyoshima",
  title        = "Design Recovery for Software Testing of Object-Oriented Programs",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "202--211",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "software testing, reverse engineering, object-oriented
    programming, functional testing, test strategy, test tool, object relation
    diagrams, block branch diagrams, object state diagrams",
  abstract     = "A reverse engineering approach for software testing of
    object-oriented programs is described.  The approach is based on a graphic
    model which consists of three types of diagrams: object relation diagrams;
    block branch diagrams; and object state diagrams.  These diagrams may be
    used to provide guidance on the order to test the classes and member
    functions; prepare member function test cases; prepare test cases for
    object state dependent behaviors and interaction between such behaviors;
    and provide graphic display of coverage information to a tester.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287764"
}

@InProceedings{apptsfre,
  author       = "M.~P. Ward and K.~H. Bennett",
  title        = "{A} Practical Program Transformation System for Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "212--221",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "semantic refinement, proof-theoretical refinement,
  wide-spectrum language",
  abstract     = "Program transformation systems provide one means of formally
    deriving a program from its specification.  A tool called ReForm is
    described.  It is designed to address the inverse problem to support the
    extraction of a specification from existing program code, using
    transformations.  This is an important activity during software
    maintenance.  One of the problems of transformation systems is the scarcity
    of practical tools which can address industrial scale problems, rather than
    contrived laboratory problems.  An analysis of the important software
    engineering factors that contribute to a successful transformation based
    tool is provided.  Results from using the tool are also presented.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287763"
}

@InProceedings{atmolcpaoaetfr,
  author       = "Philip Newcomb and Lawrence Markosian",
  title        = "Automating the Modularization of Large {COBOL} Programs:  Application of an Enabling Technology for Reengineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "222--230",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "modularization, cobol programs, reverse engineering, user interfaces",
  abstract     = "The development of a tool for modularizing large common
    business-oriented language (COBOL) programs is described.  The motivation
    for modularizing these programs is discussed, together with a manual
    modularization process.  The business motivation for building a tool to
    automate the manual process is indicated.  An enabling technology and its
    use in the development of the tool are discussed.  Experience to date in
    alpha-testing the tool is reported.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287762"
}

@InProceedings{spaairet,
  author       = "Howard~B. Reubenstein, Richard~L. Piazza, Susan~N. Roberts",
  title        = "Separating Parsing and Analysis in Reverse Engineering Tools",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "117--125",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "tool maintenance, language independence, design recovery",
  abstract     = "This paper describes the lessons learned in extending the
    capabilities of a reverse engineering tool to analyze both an additional
    dialect of the language it was initially built to parse and a new embedded
    assembly language.  The effort involved in this extension provides data to
    support the assertion that reverse engineering tools should create a clean
    separation between parsing the source code and analyzing it.  We discuss a
    language-independent modeling approach that lets us achieve this
    separation.  This paper also describes additional advantages to maintaining
    this separation such as support for multiple languages and design
    recovery.",
  location     = "http://doi.ieeecomputersociety.org/10.1109/WCRE.1993.287773"
}

@InProceedings{recsd,
  author       = "James~M. {Cross II}",
  title        = "Reverse Engineering Control Structue Diagrams",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "107--116",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "ada, flow charts, control structure diagrams, guis",
  abstract     = "The GRASP/Ada project (Graphical Representations of
    Algorithms, Structures, and Processes for Ada) has successfully created and
    prototyped a new algorithmic level graphical representation for Ada
    software, the Control Structure Diagram (CSD).  The primary impetus for
    creation of the CSD is to improve the comprehension efficiency of Ada
    software and, as a result, improve reliability and reduce costs.  The
    emphasis is on the automatic generation of the CSD from Ada PDL or source
    code to support reverse engineering and maintenance.  The CSD has the
    potential to replace traditional pretty-printed Ada source code.  The
    current prototype provides the capability for the user to generate CSDs
    from Ada PDL or source code with a level of flexibility suitable for
    practical application.",
  location     = "https://doi.org/10.1109/WCRE.1993.287774"
}

@InProceedings{ahatrpp,
  author       = "Alex Quilici",
  title        = "{A} Hybrid Approach to Recognizing Programming Plans",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "126--133",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "student programmers, program understanding, plan libraries,
    plan indexing, plan recognition",
  abstract     = "Most current models of program understanding are unlikely to
    scale up successfully.  Top-down approaches require advance knowledge of
    what the program is supposed to do, which is rarely available with aging
    software systems.  Bottom-up approaches require complete matching of the
    program against a library of programming plans, which is impractical with
    the large plan libraries needed to understand programs that contain many
    domain-specific plans.  This paper presents a hybrid approach to program
    understanding that uses an indexed, hierarchical organization of the plan
    library to limit the number of candidate plans considered during program
    understanding.  This approach is based on observations made from studying
    student programmers attempt to perform bottom-up understanding on
    geometrically-oriented C functions.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287772"
}

@InProceedings{fcfpr,
  author       = "Linda~M. Wills",
  title        = "Flexible Control for Program Recognition",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "134--143",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "graspr, subgraph parsing, chart parsing, advice, indexing,
    partitioning",
  abstract     = "Recognizing commonly used data structures and algorithms is a
    key activity in reverse engineering.  Systems developed to automate this
    recognition process have been isolated, stand-alone systems, usually
    targeting a specific task.  We are interested in applying recognition to
    multiple tasks requiring reverse engineering, such as inspecting,
    maintaining, and reusing software.  This requires a flexible, adaptable
    recognition architecture, since the tasks vary in the amount and accuracy
    of knowledge available about the program, the requirements on recognition
    power, and the resources available.  We have developed a recognition system
    based on graph parsing.  It has a flexible, adaptable control structure
    that can accept advice from external agents.  Its flexibility arises from
    using a chart parsing algorithm.  We are studying this graph parsing
    approach to determine what types of advice an enhance its capabilities,
    performance, and scalability.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287771",
  location     = "ftp://ftp.cc.gatech.edu/pub/groups/reverse/repository/flexible.ps"
}

@InProceedings{cttfore,
  author       = "Peter~G. Selfridge and Richard~C. Waters and Elliot~J. Chikofsky",
  title        = "Challenges to the Field of Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "144--150",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, realistic data, economic impact,
    communication, explicit goals, standard data sets",
  abstract     = "Driven by the economic importance of maintaining and
    improving the enormous base of existing software systems, the reverse
    engineering of software has been of rapidly growing interest over the past
    decade.  More and more commercial software tools support aspects of reverse
    engineering, and more and more researchers in academic and industrial
    organizations are addressing themselves to the fundamental problems of
    reverse engineering.  In the best of all worlds, we researchers on reverse
    engineering would be working together toward clear goals of great economic
    importance.  Unfortunately, it appears that we are mostly just groping
    around in a swamp, each looking for a bit of dry ground (whether or not it
    actually leads out of the swamp), and running into each other only
    occasionally.  If we are to make rapid and effective joint progress, a
    number of improvements need to be made in the way we are pursuing research.
    This position paper presents ten challenges for improvement in three areas:
    avoiding artificial data, focusing on concrete impact, and facilitation
    researcher communication.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287770"
}

@InProceedings{aafreord,
  author       = "William~J. Premerlani and Michael~R. Blaha",
  title        = "An Approach for Reverse Engineering of Relational Databases",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "151--160",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "omt notation, forward engineering, reverse engineering, data
    dictionaries, rdbms",
  abstract     = "The process of software re-engineering consists of a reverse 
    engineering step followed by a forward engineering step.  During reverse
    engineering, one takes a past design or an implementation that embodies a
    design and extracts the essential problem domain content while discarding
    design optimizations and implementation decisions.  During forward
    engineering, this model of the essence of an application becomes the basis
    for reimplementation in a new medium.  Object-oriented models facilitate
    the re-engineering proceeds because the same modeling paradigm is adept at
    representing abstract conceptual models and models with implementation
    decisions.  Based on our experience with several examples, we propose a
    process for reverse engineering of relational databases.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287769"
}

@InProceedings{ctatodre,
  author       = "Jean-Luc Hainaut and M.~Chandelon and C.~Tonneau and M.~Joris",
  title        = "Contribution to a Theory of Database Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "161--170",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "database design, database reverse engineering, data structure
  modeling, data structure extraction, data structure conceptualization",
  abstract     = "This paper proposes both a general framework and specific
    techniques for file and database reverse engineering, i.e.  recovering its
    conceptual schema.  The framework relies on a process/product model that
    matches formal as well as empirical design procedures.  Based on the
    analysis of database design processes, two major phases are defined, data
    structure extraction and data structure conceptualization.  For each phase,
    a set of activities is proposed.  Most of these activities can be described
    as transformation and integration of specification.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287768"
}

@InProceedings{tcapipu,
  author       = "Ted~J. Biggerstaff and Bharat~G. Mitbander and Dallas Webster",
  title        = "The Concept Assignment Problem in Program Understanding",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "27--43",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, slicing, knowledge base, domain,
    connectionist, concept recognition, plausible reasoning",
  abstract     = "A person understands a program because he is able to relate
    the structures of the program and its environment to his conceptual
    knowledge about the world.  The problem of discovering individual human
    oriented concepts and assigning them to their implementation oriented
    counterparts for a given program is the concept assignment problem.  We
    argue that the solution to this problem requires methods that have a strong
    plausible reasoning component based on a priori knowledge.  We illustrate
    these ideas through example scenarios using an existing design recovery
    system called DESIRE.  Finally, we will evaluate DESIRE based on its usage
    on real-world problems over the years.", 
  location     = "https://dl.acm.org/doi/10.5555/257572.257679"
}

@InProceedings{rrefctss,
  author       = "Helen~M. Edwards and Malcolm Munro",
  title        = "{RECAST}:  Reverse Engineering from {COBOL} to {SSADM} Specification",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "44--53",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "repositories, ssadm, procedural models, software tools",
  abstract     = "The Reverse Engineering into CASE Technology method (RECAST)
    takes the source code for an existing COBOL system and derives a no-loss
    representation of the system documented in an Structured Systems Analysis
    and Design Method (SSADM) format.  This representation of the system is
    derived through the use of a series of transformations.  This paper
    describes the environment within which recast has been developed, outlines
    the stages and steps of the recast method and discusses the use of software
    support tools.  An overview is given of a case study that has been carried
    out for a live system.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/257572.257681"
}

@InProceedings{paisfre,
  author       = "Jon Beck and David Eichmann",
  title        = "Program and Interface Slicing for Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "54--63",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program slicing, redoumentation, design recovery, interface
    slicing, tree shaking, dependence graphs, ",
  abstract     = "Reverse engineering involves comprehending a software
    system's implementation and the ways the implementation evolved from the
    original design.  Automated support tools are an integral part of such
    effort.  This paper describes how program slicing techniques can be
    employed to assist in the comprehension of large software systems, through
    traditional slicing techniques at the statement level, and through a new
    technique, interface slicing, at the module level.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/257572.257682"
}

@InProceedings{rrcflsbps,
  author       = "Jim~Q. Ning and Andre Engberts and Wojtek Kozaczynski",
  title        = "Recovering Reusable Components from Legacy Systems by Program Segmentation",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "64--72",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "cobol, data model recovery, program segmentation",
  abstract     = "There are many reasons to retire a legacy system.  But the
    system may contain critical business rules and other reusable assets that
    are not explicitly documented anywhere else.  A software reengineering
    technique called program segmentation is described.  It supports the
    recovery of these reusable assets from old code.  This technique consists
    of a focusing step, which helps the analyst localize, understand, and
    combine functional pieces in large programs, and a factoring step, which
    extracts the focused functional pieces and packages them into independent
    reusable modules.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287778"
}

@InProceedings{remfiradt,
  author       = "G.~Canfora and A.~Cimitile and M.~Munro",
  title        = "Reverse Engineering Method for Identifying Reusable Abstract Data Types",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "73--82",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "abstract data types",
  abstract     = "This paper presents results from an experiment in reuse
    within the RE2 project.  It shows how a particular candidature criterion
    for identifying abstract data types in existing software systems can be
    applied both at the theoretical and practical level.  The RE2 project is
    concerned with the exploration of reverse engineering and reengineering
    techniques to facilitate reuse reengineering by the identification and
    classification of approximate candidature criteria.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287777"
}

@InProceedings{iaeodicilp,
  author       = "Filippo Cutillo and Piernicola Fiore and Giuseppe Visaggio",
  title        = "Idenfification and Extraction of ``Domain Independent'' Components in Large Programs",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "83--92",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program slicing",
  abstract     = "This study addresses the problem of the identification and
    extraction of domain independent components from a large working program
    lacking complete documentation.  A modified version of Weiser's slice,
    which we shall call 'Direct Slice,' is defined which makes it possible to
    individuate and extract particular kinds of code segments, distributed
    among many of the modules composing the structure of the working program.
    This provides the advantage of being able to individuate and reaggregate
    program components coherently according to the principles of Information
    Hiding.  The techniques described are also characterized by a certain
    simplicity and can thus be automized.  In fact, a commercially available
    tool was used for experimenting the process.  The present study is based on
    the results of an experimental project performed on applicative programs
    used in the banking sector, characterized by a high grade of difficulty as
    regards maintenance.",
  location     = "https://doi.org/10.1109/WCRE.1993.287776"
}

@InProceedings{fciahomastau,
  author       = "Jean-Fran{\c c}ois Girard and Rainer Koschke",
  title        = "Finding Components in a Hierarchy of Modules:  a Step Towards Architectural Understanding",
  booktitle    = pot # "International Conference on Software Maintenance, ICSM '97",
  year         = 1997,
  pages        = "58--65",
  address      = "Bari, Italy",
  month        = "1--3 " # oct,
  keywords     = "atomic components, dominance analysis",
  abstract     = "This paper presents a method to view a system as a hierarchy
    of modules according to information hiding concepts and to identify
    architectural component candidates in this hierarchy.  The result of the
    method eases the understanding of a system's underlying software
    architecture.  A prototype tool implementing this method was applied to
    three systems written in C (each over 30 Kloc).  For one of these systems,
    an author of the system created an architectural description.  The
    components generated by our method correspond to those of this
    architectural description in almost all cases.  For the other two systems,
    most of the components resulting from the method correspond to meaningful
    system abstractions.",
  location     = "https://dl.acm.org/doi/10.5555/645545.656026"
}

@InProceedings{psfsadpu,
  author       = "David~P. Olshefski and Alan Cole",
  title        = "Prototype System for Static and Dynamic Program Understanding",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "93--106",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program understanding, pundit, code browsing, semantic
    analysis, program analysis",
  abstract     = "A tool called PUNDIT (Program Understanding Investigation
    Tool) is described.  It is a prototype intended to serve as a vehicle for
    exploring and testing ideas in the area of program understanding; it
    combines static analysis information with information collected at runtime.
    The architecture of PUNDIT is described, together with its two main
    components (the C source analyzer and a graphical user interface).  Several
    of the views provided by the tool are explained, including a high-level
    structure chart, a dynamic call graph, a control flow graph animated during
    program execution, a type definition window, and others.  By integrating
    static and dynamic information, the tool provides a more comprehensive
    understanding of a program as the first step to reengineering or
    maintaining the application that can be obtained by static analysis
    alone.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287775"
}

@Manual{ai43ict,
  title        = "An Introductory 4.{3BSD} Interprocess Communiation Tutorial",
  author       = "Stuart Sechrest",
  organization = csd # ucb,
  address      = beca,
  keywords     = "pipes, socket pairs",
  location     = "https://docs.freebsd.org/44doc/psd/20.ipctut/paper.pdf"
}

@Manual{prapirpca,
  title        = "{PI}-{RPC}:  {A} Platform-Independent Remote Procedure Call Architecture",
  author       = "Randy~J. Ray",
  year         = 2000,
  keywords     = "transport model, message architecture, rpc",
  abstract     = "This introduction of remote procedure call (RPC) services
    using XML and the encoding scheme simplified what was once a daunting
    aspect of distributed computing.  However, the current work stands in a
    frozen state for the sake of compliance with existing software
    implementations This proposal outlines a new model that builds on the
    existing basis of XML-RPC in a modular fashion, maintaining compatibility
    with the existing specification.", 
  location     = "http://www.blackperl.com/xml/PI-RPC.html"
}

@Manual{dcwlmtldhohs,
  title        = "Deplate --- Convert Wiki-like Markup to Latex, {Docbook, HTML}, or ``{HTML} Slides''",
  author       = "Thomas Link",
  year         = 2004,
  month        = aug,
  keywords     = "latex, html, docbook",
  location     = "http://deplate.sourceforge.net/"
}

@Misc{tfmcdp,
  author       = "Gopalan Suresh Raj",
  OPTtitle     = "The Factory Method (Creational) Design Pattern",
  howpublished = "Web page",
  year         = "2000",
  keywords     = "design patterns, factory, instance creation",
  location     = "http://gsraj.tripod.com/design/creational/factory/factory.html"
}

@Misc{cfsrcai,
  author       = "Avi Rappoport",
  title        = "Checklist for Search Robot Crawling and Indexing",
  howpublished = "http://www.searchtools.com/robots/robot-checklist.html",
  year         = "2003",
  month        = "18 " # jul,
  keywords     = "web crawling, spiders, indexing",
  abstract     = "This document provides both technical information and some
    background and insight into what search engine indexing robots should
    expect to encounter .  Technically, the problems arise from
    misunderstandings and exploitation of anomalies by HTML creators (direct
    tagging, WYSIWYG and automated systems), and the tendency of browser
    applications to be very forgiving in their interpretation of pages and
    links.  Therefore, it's impossible to simply read the HTML and HTTP
    specifications and follow the rules there -- the real world is much messier
    than that.", 
  location     = "http://www.searchtools.com/robots/robot-checklist.html"
}

@Misc{ejag,
  author       = "Alan Griffiths",
  title        = "Exceptional Java",
  howpublished = "web page",
  year         = "2002",
  keywords     = "java, exceptions, programming style",
  location     = "https://accu.org/index.php/journals/406"
}

@Misc{gfmv,
  author       = "Mateusz Viste",
  title        = "Gopher {FAQ}",
  howpublished = "ftp",
  year         = "2015",
  month        = sep,
  keywords     = "gopher, faq",
  abstract     = "Common Questions and Answers about the Internet Gopher, a
    client/server protocol for making a world wide information service, with
    many implementations.",
  location     = "URL:gopher://gopher.viste.fr/1/gopher-faq"
}

@Misc{tropf,
  author       = "Daved Niewert",
  title        = "The Rise of Pseudo Fascism",
  howpublished = "web page",
  year         = 2005,
  month        = "25 " # feb,
  keywords     = "conservatism, fascism, pseudo-fascism, one-party states",
  location     = "https://dneiwert.blogspot.com/The%20Rise%20Of%20Pseudo%20Fascism.pdf"
}

@InBook{aaiissd,
  author       = "Douglas~E. Comer and David~L. Stevens",
  title        = "Internetworking with {TCP/IP}",
  chapter      = "Chapter 8: Algorithms and Issues in Server Software Design",
  publisher    = ph,
  year         = 2001,
  pages        = "101--123",
  volume       = "3",
  address      = usrnj,
  keywords     = "concurrency, iterative, connection-oriented, connectionless,
    transport protocol, statelessness, reliability, sockets, helper processes",
  location     = "TK 5105.585.C66"
}

@InBook{hpw,
  author       = "Network Associates, Inc",
  title        = "Introduction to Cryptography",
  chapter      = "How {PGP} Works",
  year         = 1999,
  keywords     = "encryption, decryption, public key cryptography, digital
    signatures, digital certificates, validity, trust",
  location     = "https://www.itu.int/en/ITU-D/Cybersecurity/Documents/01-Introduction%20to%20Cryptography.pdf"
}

@PhDThesis{cqtg,
  author       = "Grust, Torsten",
  title        = "Comprehending Queries",
  school       = "University of Konstanz",
  year         = 1999,
  address      = "Konstanz, Germany",
  month        = sep,
  keywords     = "category theory, categorical datatypes, query compilation,
    normalization, combinators, comprehensions, query deforestation, functors,
    catamorphisms",
  abstract     = "There are no compelling reasons why database-internal query
    representations have to be designated by operators.  This text describes a
    world in which datatypes determine the comprehension of queries.  In this
    world, a datatype is characterized by its algebra of value constructors.
    These algebras are principal.  Query operators are secondary in the sense
    that they simply box (recursive) programs that describe how to form a query
    result by application of datatype constructors.  Often, operators will be
    unboxed to inspect and possibly rewrite these programs.  Query optimization
    then means to deal with the transformation of programs.  The predominant
    role of the constructor algebras suggests that this model understands
    queries as mappings between such algebras.  The key observation that makes
    the whole approach viable is that (a) homomorphic mappings are expressive
    enough to cover declarative user query languages like OQL or recent SQL
    dialects, and, at the same time, (b) a single program form suffices to
    express homomorphisms between constructor algebras.  Reliance on a single
    combining form, catamorphisms, renders the query programs susceptible to
    Constructive Algorithmics, an effective and extensive algebraic theory of
    program transformations.  The text then takes a step from catamorphisms
    towards a higher-level query representation based on the categorical notion
    of monads.  In a nutshell, monads are algebras exhibiting exactly the
    structure that is needed to support the interpretation of a query calculus,
    the monad comprehension calculus.  Built on top of the abstract monad
    notion, the calculus maps a variety of query constructs (e.g., bulk
    operations, aggregates, and quantifiers) to few syntactic forms.  The
    uniformity of the calculus facilitates the analysis and transformation,
    especially the normalization, of its expressions.  Few but generic calculus
    rewriting rules suffice to implement query transformations that would
    otherwise require extensive rule sets.  The text rediscovers well-known
    query optimization knowledge on sometimes unusual paths that are more
    practicable to follow for an optimizer, though.  Solutions previously
    proposed by others can be simpliﬁed and generalized mainly due to the clear
    account of the structure of queries that the monad comprehension
    calculus—thanks to its density—provides.  The calculus effectively supports
    query optimization in the presence of grouping, various forms of nesting,
    aggregates, and quantifiers.  Although built on top of abstract concepts
    like homomorphisms and monads, this query model is specific enough to grasp
    implementation issues, such as the generation of stream-based (pipelined)
    query execution plans, whose treatment has traditionally been delayed until
    query runtime.  It is the main objective of this thesis to show that
    catamorphisms and monad comprehensions enable a comprehension of queries
    that is eﬀective and easily exploitable inside a query optimizer.", 
  location     = "http://kops.uni-konstanz.de/urn/urn:nbn:de:bsz:352-opus-3120"
}

% Local Variables:
% eval: (set-register ?b "  booktitle    = pot # \"Working Conference on Reverse Engineering\",\n  year         = 1993,\n  editor       = \"Richard~C. Waters and Elliot~J. Chikofsky\",\n  pages        = \"--\",\n  organization = \"IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering\",\n  publisher    = \"IEEE Computer Society Press\",\n  address      = bama,\n  month        = \"21--23 \" # may,\n")
% End:
		  
@Proceedings{fecotfsd,
  title        = pot # "First European Conference on \TeX\ for Scientific Documentation",
  year         = 1985,
  editor       = "Dario Lucarella",
  publisher    = aw,
  address      = "Como, Italy",
  month        = "16--17 " # may,
  keywords     = "tex, document preparation, metafont"
}

@Proceedings{tugnam88,
  title        = pot # "\TeX\ Users Group Ninth Annual Meeting",
  year         = 1988,
  editor       = "Christina Thiele",
  organization = "\TeX\ Users Group",
  address      = "Montreal, Ontario, Canada",
  month        = "22--24 " # aug,
  keywords     = "tex, document production, typesetting, textbook production,
    multilingual documents, sgml, dvi"
}

@Proceedings{tugnam87,
  title        = pot # "\TeX\ Users Group Eighth Annual Meeting",
  year         = 1987,
  editor       = "Christina Thiele",
  organization = "\TeX\ Users Group",
  address      = sewa,
  month        = "24--26 " # aug,
  keywords     = "tex, document production, typesetting, textbook production,
    multilingual documents, sgml, dvi"
}

