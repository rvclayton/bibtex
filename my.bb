.so bibtex.header

@Article{pvsame,
  author       = "Boehm, Barry~W. and Gray, Terence~E. and Seewaldt, Thomas",
  title        = "Prototyping Versus Specifying:  {A} Multiproject Experiment",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 3,
  pages        = "290-302",
  month        = may,
  keywords     = "prototyping, specifying, requirements analysis, software
    engineering, software engineering education, software management, software
    metrics, cocomo, user interfaces",
  abstract     = "In this experiment, seven software teams developed versions
    of the same small-size (2000-4000 source instruction) application software
    product.  Four teams used the Specifying approach.  Three teams used the
    Prototyping approach.  The main results of the experiment were the
    following.  1) Prototyping yielded products with roughly equivalent
    performance, but with about 40 percent less code and 45 percent less
    effort.  2) The prototyped products rated somewhat lower on functionality
    and robustness, but higher on ease of use and ease of learning.  3)
    Specifying produced more coherent designs and software that was easier to
    integrate.  The paper presents the experimental data supporting these and a
    number of additional conclusions.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010238"
}
@Article{acsosapbhfi,
  author       = "Arnold, Matthew and Fink, Stephen and Sarkar, Vivek and Sweeney, Peter~F.",
  title        = "{A} Comparative Study of Static and Profile-Based Heuristics for Inlining",
  journal      = sigplan # " (" # pot # "Workshop on Dynamic and Adaptive
		  Compilation and Optimization - Dynamo '00)",
  year         = 2000,
  volume       = 35,
  number       = 7,
  pages        = "52--64",
  month        = jul,
  keywords     = "call graphs, inlining, program optimization, static analysis,
    dynamic analysis",
  abstract     = "In this paper, we present a comparative study of static and 
    profile-based heuristics for inlining.  Our motivation for this study is to
    use the results to design the best inlining algorithm that we can for the
    Jalapeño dynamic optimizing compiler for Java [6].  We use a well-known
    approximation algorithm for the KNAPSACK problem as a common
    “meta-algorithm” for the inlining heuristics studied in this paper.  We
    present performance results for an implementation of these inlining
    heuristics in the Jalapeño dynamic optimizing compiler.  Our performance
    results show that the inlining heuristics studied in this paper can lead to
    significant speedups in execution time (up to 1.68x) even with modest
    limits on code size expansion (at most 10%).",  
  location     = "http://dx.doi.org/10.1145/351403.351416"
}

@Article{cfsrmc,
  author       = "Iannino, Anthony and Musa, John~D. and Okumoto, Kazuhira and Littlewood, Bev",
  title        = "Criteria for Software Reliability Model Comparisons",
  journal      = tse,
  year         = 1983,
  volume       = 8,
  number       = 3,
  pages        = "12--16",
  month        = jul,
  keywords     = "model comparisons, predictive validity, software failures,
    software reliability, capability, assumption quality, applicability,
    simplicity",
  abstract     = "A set of criteria is proposed for the comparison of software
    reliability models.  The intention is to provide a logically organized
    basis for determining the superior models and for the presentation of model
    characteristics.  It is hoped that in the future, a software manager will
    be able to more easily select the model most suitable for his/her
    requirements from among the preferred ones.", 
  location     = "http://dx.doi.org/10.1145/1010891.1010893"
}

@Article{dotpedm,
  author       = "Barach, David~R. and Taenzer, David~H. and Wells, Robert~E.",
  title        = "Design of the {PEN} Editor Display Module",
  journal      = sotm,
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "130--136",
  month        = jun,
  keywords     = "software design, terminal handling, optimization",
  abstract     = "PEN, a new portable video editor, uses a number of simple but
    effective techniques.  Most are not new, but are unavailable in the
    literature.  We will describe our goals for PEN's display module, discuss
    implementation alternatives and describe in detail the techniques used in
    the editor.", 
  location     = "http://dx.doi.org/10.1145/800209.806464"
}
@Book{dsdi,
  author       = "Daniel~C. Dennett",
  title        = "Darwin's Dangerous Idea",
  publisher    = "Simon \& Schuster",
  year         = 1995,
  address      = nyny,
  keywords     = "evolution, darwin, scientific debate",
  location     = "QH 375 D45"
}

@Article{rtpt,
  author       = "Thomas~H. {Cheatham, Jr.}",
  title        = "Reusability Through Program Transformation",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "574--588",
  month        = sep,
  keywords     = "programming environments, porgram transformations, rapid
    prototyping, reusability, specification languages",
  abstract     = "We describe a methodology and supporting programming
    environment that provide for reuse of abstract programs.  Abstract programs
    are written using notations and constructs natural to the problem domain in
    a language realized by syntactic extension of a base language.  Program
    transformations are employed to refine an abstract program into its
    concrete counterpart.  We discuss the use of the methodology in the setting
    of rapid prototyping and custom tailoring.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010282"
}

@Article{arajg,
  author       = "James Gosling",
  title        = "{A} Redisplay Algorithm",
  journal      = sotm,
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "123--129",
  month        = jun,
  keywords     = "dynamic programming",
  abstract     = "This paper presents an algorithm for updating the image
    displayed on a conventional video terminal.  It assumes that the terminal
    is capable of doing the usual insert/delete line and insert/delete
    character operations.  It takes as input a description of the image
    currently on the screen and a description of the new image desired and
    produces a series of operations to do the desired transformation in a
    near-optimal manner.  The algorithm is interesting because it applies
    results from the theoretical string-to-string correction problem (a
    generalization of the problem of finding a longest common subsequence), to
    a problem that is usually approached with crude ad-hoc techniques.", 
  location     = "http://dx.doi.org/10.1145/872730.806463"
}

@Article{otlbpitf,
  author       = "Achugbue, James~O.",
  title        = "On the Line Breaking Problem in Text Formatting",
  journal      = sotm,
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "117--122",
  month        = jun,
  keywords     = "line breaking, text formatting, dynamic programming",
  abstract     = "A basic problem in text formatting is that of determining the
    break points for separating a string of words into lines to obtain a
    formatted paragraph.  When formatted text is required to be aligned with
    both the left and right margins, the choice of break points greatly affects
    the quality of the formatted document.  This paper presents and discusses
    solutions to the line breaking problem.  These include the usual
    line-by-line method, a dynamic programming approach, and a new algorithm
    which is optimal and runs almost as fast as the line-by-line method.", 
  location     = "http://dx.doi.org/10.1145/800209.806462"
}

@Article{esopk,
  author       = "Soloway, Elliot and Ehrlich, Kate",
  title        = "Emperical Studies of Programming Knowledge",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "595--609",
  month        = sep,
  keywords     = "cognitive models of programming, novice/expert differences,
    program comprehension, software psychology",
  abstract     = "We suggest that expert programmers have and use two types of 
    programming knowledge: 1) programming plans, which are generic program
    fragments that represent stereotypic action sequences in programming, and
    2) rules of programming discourse, which capture the conventions in
    programming and govern the composition of the plans into programs.  We
    report here on two empirical studies that attempt to evaluate the above
    hypothesis.  Results from these studies do in fact support our claim.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010283"
}

@Article{prtpt,
  author       = "Boyle, James~M. and Muralidharan, Monagur~N.",
  title        = "Program Reusability through Program Transformation",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 5,
  pages        = "574--588",
  month        = sep,
  keywords     = "abstract programming, canonical forms, optimization, program
    transformation, pure applicative lisp, rewrite rules, stepwise refinement,
    tampr",
  abstract     = "How can a program written in pure applicative LISP be reused
    in a Fortran environment? One answer is by automatically transforming it
    from LISP into Fortran.  In this paper we discuss a practical application
    of this technique-one that yields an efficient Fortran program.  We view
    this process as an example of abstract programming, in which the LISP
    program constitutes an abstract specification for the Fortran version.  The
    idea of strategy-a strategy for getting from LISP to Fortran-is basic to
    designing and applying the transformations.  One strategic insight is that
    the task is easier if the LISP program is converted to ``recursive''
    Fortran, and then the recursive Fortran program is converted to
    nonrecursive standard Fortran.  Another strategic insight is that much of
    the task can be accomplished by converting the program from one canonical
    form to another.  Developing a strategy also involves making various
    implementation decisions.  One advantage of program transformation
    methodology is that it exposes such decisions for examination and review.
    Another is that it enables optimizations to be detected and implemented
    easily.  Once a strategy has been discovered, it can be implemented by
    means of rewrite-rule transformations using the TAMPR program
    transformation system.  The transformational approach to program reuse
    based on this strategy has a measure of elegance.  It is also practical-the
    resulting Fortran program is 25 percent faster than its compiled LISP
    counterpart, even without extensive optimization.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010281"
}

@Article{tioeaiaidps,
  author       = "Hammer, Michael and Ilson, Richard and Anderson, Tim and Gilbert, Edward and Good, Michael and Niamir, Bahram and Rosentein, Larry and Schoichet, Sandor",
  title        = "The Implementation of {E}tude, An integrated and Interactive Document Production System",
  journal      = sotm,
  year         = 1981,
  volume       = 16,
  number       = 6,
  pages        = "117--122",
  month        = jun,
  keywords     = "software design, user-interface design, document
    representation",
  abstract     = "Etude is an experimental text processing system that is being
    developed in order to formulate and evaluate new approaches to the design
    of user interfaces for office automation tools.  The primary design goal
    for Etude is to provide the user with substantial functionality in the
    editing and formatting of documents in the context of a system that is easy
    to learn and use.", 
  location     = "http://dx.doi.org/10.1145/800209.806465"
}

@Article{aisercdcall,
  author       = "Ben-David, Amram and Ben-Porath, Moshe~I. and Loeb, Jonah~Z. and Rich, Michael",
  title        = "An Industrial Software Engineering Retraining Course:  Development Considerations and Lessons Learned",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "748--755",
  month        = nov,
  keywords     = "",
  abstract     = "Israel Aircraft Industries has recently been conducting a novel
    six-month intensive course to retrain practicing engineers to become
    software engineers working on embedded computer systems.  The first course
    was concluded in January 1982 and the second course began in November 1982.
    This paper describes the objectives, educational philosophy, course
    content, and practical experience of the first course.  It also describes
    how the second course was modified as a result of the lessons learned from
    the successes and failures of the first course.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010303"
}

@Article{acgpoi,
  author       = "Corrigan, Neil~B. and Starkey, J.~Denbigh",
  title        = "{A} Concurrent General Purpose Operator Interface",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "738--748",
  month        = nov,
  keywords     = "computer graphics, concurrent pascal, concurrent programming,
    interactive system, man-machine interface, operator interface, process
    control",
  abstract     = "Compact interactive control consoles are rephcing traditional
    control rooms as operator interfaces for physical processes.  In the irust
    major application of concurrent programming outside the area of operating
    systems, this paper presents a design for a general purpose operator
    interface which uses a color graphics terminal with a touch-sensitive
    screen as the control console.  Operators interact with a process through a
    collection of application-dependent displays generated interactively by
    users familiar with the physical process.  The use of concurrent
    programming results in a straightforward and reliable design which may
    easily be extended to support multiple devices of varying types in the
    control console.  An implementation of the Operator Interface in Concurrent
    Pascal currently in progress is also discussed.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010302"
}

@Article{eoerbufcp,
  author       = "Kang~G. Shin and Yann-Hang Lee",
  title        = "Evaluation of Error Recovery Blocks Used for Cooperating Processes",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "692--700",
  month        = nov,
  keywords     = "backward error recovery, conversation scheme, domino effect,
    pseudorecovery points and line(s), recovery block(s), recovery line(s),
    rollback propagations",
  abstract     = "Three alternatives for implementing recovery blocks (RB's) 
    are conceivable for backward error recovery in concurrent processing.
    These are the asynchronous, synchronous, and the pseudorecovery point
    implementations.  Asynchronous RB's are based on the concept of maximum
    autonomy in each of concurrent processes.  Consequently, establishment of
    RB's in a process is made independently of others and unbounded rollback
    propagations become a serious problem.  In order to completely avoid
    unbounded rollback propagations, it is necessary to synchronize the
    establishment of recovery blocks in all cooperating processes.  Process
    autonomy is sacrificed and processes are forced to wait for commitments
    from others to establish a recovery line, leading to inefficiency in time
    utilization.  As a compromise between asynchronous and synchronous RB's we
    propose to insert pseudorecovery points (PRP's) so that unbounded rollback
    propagations may be avoided while maintaining process autonomy.  We
    developed probabilistic models for analyzing these three methods under
    standard assumptions in computer performance analysis, i.e., exponential
    distributions for related random variables.  With these models we have
    estimated 1) the interval between two successive recovery lines for
    asynchronous RB's, 2) mean loss in computation power for the synchronized
    method, and 3) additional overhead and rollback distance in case PRP's are
    used.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010298"
}

@Article{rtem,
  author       = "Bernhard Plattner",
  title        = "Real-Time Execution Monitoring",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "756--764",
  month        = nov,
  keywords     = "debugging, monitor, performance evaluation, process
    interaction, process monitor, real-time monitoring, timing",
  abstract     = "Today's programming methodology emphasizes the study of static
    aspects of programs.  In practice, however, monitoring a program in
    execution, i.e., monitoring a process, is routinely done by any programmer
    whose task it is to produce a reliable piece of software.  There are two
    reasons why one might want to examine the dynamic aspects of a program:
    first, to evaluate the performance of a program, and hence to assess its
    overall behavior; and second, to demonstrate the presence of programming
    errors, isolate erroneous program code, and correct it.  This latter task
    is commonly called ``debugging a program'' and requires a detailed insight
    into the innards of a program being executed.  Today, many computer systems
    are being used to measure and control real-world processes.  The pace of
    execution of these systems and their control programs is therefore bound to
    timing constraints imposed by the real-world process.  As a step towards
    solving the problems associated with execution monitoring of real-time
    programs, we develop a set of appropriate concepts and define the basic
    requirements for a real-time monitoring facility.  As a test case for the
    theoretical treatment of the topic, we design hardware and software for an
    experimental real-time monitoring system and describe its implementation.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010304"
}

@Article{dcobnfgpc,
  author       = "Alan Mainwaring and David~E. Culler",
  title        = "Design Challenges of Virtual Networks:  Fast, General-Purpose Communication",
  journal      = ppopp99,
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "119--130",
  month        = aug,
  keywords     = "virtual networks, high-performance clusters, direct network
    access, application programming interfaces, system resource management,
    protocol architecture and implementation",
  abstract     = "Virtual networks provide applications with the illusion of 
    having their own dedicated, high-performance networks, although network
    interfaces posses limited, shared resources.  We present the design of a
    large-scale virtual network system and examine the integration of
    communication programming interface, system resource management, and
    network interface operation.  Our implementation on a cluster of 100
    workstations quantifies the impact of virtualization on small message
    latencies and throughputs, shows full hardware performance is delivered to
    dedicated applications and time-shared workloads, and shows robust
    performance under demanding workloads that overcommit interface
    resources.", 
  location     = "http://dx.doi.org/10.1145/329366.301115"
}

@Article{aeiojsrmi,
  author       = "Maassen, Jason and {van Nieuwpoort}, Rob and Veldema, Ronald and Bal, Henri~E. and Plaat, Aske",
  title        = "An Efficient Implementation of {J}ava's {R}emote {M}ethod {I}nvocation",
  journal      = ppopp99,
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "173--182",
  month        = aug,
  keywords     = "java, rmi, performance, native-code compilation",
  abstract     = "Java offers interesting opportunities for parallel computing.
    In particular, Java Remote Method Invocation provides an unusually flexible
    kind of Remote Procedure Call.  Unlike RPC, RMI supports polymorphism,
    which requires the system to be able to download remote classes into a
    running application.  Sun's RMI implementation achieves this kind of
    flexibility by passing around object type information and processing it at
    run time, which causes a major run time overhead.  Using Sun's JDK 1.1.4 on
    a Pentium Pro/Myri.net cluster, for example, the latency for a null RMI
    (without parameters or a return value) is 1228 &mu;sec, which is about a
    factor of 40 higher than that of a user-level RPC.  In this paper, we study
    an alternative approach for implementing RMI, based on native compilation.
    This approach allows for better optimization, eliminates the need for
    processing of type information at run time, and makes a light weight
    communication protocol possible.  We have built a Java system based on a
    native compiler, which supports both compile time and run time generation
    of marshallers.  We find that almost all of the run time overhead of RMI
    can be pushed to compile time.  With this approach, the latency of a null
    RMI is reduced to 34 &mu;sec, while still supporting polymorphic RMIs (and
    allowing interoperability with other JVMs).", 
  location     = "http://dx.doi.org/10.1145/301104.301120"
}

@Article{cmobst,
  author       = "Udi Manber",
  title        = "Concurrent Maintenance of Binary Search Trees",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "777--784",
  month        = nov,
  keywords     = "concurrent algorithms, data structures, distributed
    algorithms, locking, transactions, trees",
  abstract     = "The problem of providing efficient concurrent access for 
    independent processes to a dynamic search structure is the topic of this
    paper.  We develop concurrent algorithms for search, update, insert, and
    delete in a simple variation of binary search trees, called external trees.
    The algorithm for deletion, which is usually the most difficult operation,
    is relatively easy in this data structure.  The advantages of the data
    structure and the algorithms are that they are simple, flexible, and
    efficient, so that they can be used as a part in the design of more
    complicated concurrent algorithms where maintaining a dynamic search
    structure is necessary.  In order to increase the efficiency of the
    algorithms we introduce maintenance processes that independently reorganize
    the data structure and relieve the user processes of nonurgent operations.
    We also discuss questions of transactions in a dynamic environment and
    replicated copies of the data structure.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010306"
}

@Article{mfdabiat,
  author       = "Steven~M. German",
  title        = "Monitoring for Deadlock and Blocking in {A}da Tasking",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "764--777",
  month        = nov,
  keywords     = "concurrent algorithms, concurrent programming languages,
    correctness proofs of concurrent programs, deadlock detection, exceptions,
    program transformations, semantics of ada tasking, state graph models, task
    identifiers", 
  abstract     = "We present a deadlock monitoring algorithm for Ada tasking
    programs which is based on transforming the source program.  The
    transformations introduce a new task called the monitor, which receives
    information from all other tasks about their tasking activities.  The
    monitor detects deadlocks consisting of circular entry calls as well as
    some noncircular blocking situations.  The correctness of the program
    transformations is formulated and proved using an operational state graph
    model of tasking.  The main issue in the correctness proof is to show that
    the deadlock monitor algorithm works correctly without having simultaneous
    information about the state of the program.  In the course of this work, we
    have developed some useful techniques for programming tasking applications,
    such as a method for uniformly introducing task identifiers.  We argue that
    the ease of finding and justifying program transformations is a good test
    of the generality and uniformity of a programming language.  The complexity
    of the full Ada language makes it difficult to safely apply
    transformational methods to arbitrary programs.  We discuss several
    problems with the current semantics of Ada's tasks.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010305"
}

@Article{apodaca,
  author       = "Rugina, Radu and Rinard, Martin",
  title        = "Automatic Parallelization of Divide and Conquer Algorithms",
  journal      = ppopp99,
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "72--83",
  month        = aug,
  keywords     = "parallelization, region analysis, pointer analysis, bounds
    analysis, initial value analysis, correlation analysis",
  abstract     = "Divide and conquer algorithms are a good match for modern
    parallel machines: they tend to have large amounts of inherent parallelism
    and they work well with caches and deep memory hierarchies.  But these
    algorithms pose challenging problems for parallelizing compilers.  They are
    usually coded as recursive procedures and often use pointers into
    dynamically allocated memory blocks and pointer arithmetic.  All of these
    features are incompatible with the analysis algorithms in traditional
    parallelizing compilers.This paper presents the design and implementation
    of a compiler that is designed to parallelize divide and conquer algorithms
    whose subproblems access disjoint regions of dynamically allocated arrays.
    The foundation of the compiler is a flow-sensitive, context-sensitive, and
    interprocedural pointer analysis algorithm.  A range of symbolic analysis
    algorithms build on the pointer analysis information to extract symbolic
    bounds for the memory regions accessed by (potentially recursive)
    procedures that use pointers and pointer arithmetic.  The symbolic bounds
    information allows the compiler to find procedure calls that can execute in
    parallel without violating the data dependences.  The compiler generates
    code that executes these calls in parallel.  We have used the compiler to
    parallelize several programs that use divide and conquer algorithms.  Our
    results show that the programs perform well and exhibit good speedup.", 
  location     = "http://dx.doi.org/10.1145/329366.301111"
}

@Article{amfcvsed,
  author       = "Victor~R. Basili and David~M. Weiss",
  title        = "A Methodology for Collecting Valid Software Engineering Data",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "728--738",
  month        = nov,
  keywords     = "data collection, data collection methodology, error analysis,
    error classification, software engineering experiments",
  abstract     = "An effective data collection method for evaluating software
    development methodologies and for studying the software development process
    is described.  The method uses goal-directed data collection to evaluate
    methodologies with respect to the claims made for them.  Such claims are
    used as a basis for defining the goals of the data collection, establishing
    a list of questions of interest to be answered by data analysis, defining a
    set of data categorization schemes, and designing a data collection form.
    The data to be collected are based on the changes made to the software
    during development, and are obtained when the changes are made.  To ensure
    accuracy of the data, validation is performed concurrently with software
    development and data collection.  Validation is based on interviews with
    those people supplying the data.  Results from using the methodology show
    that data validation is a necessary part of change data collection.
    Without it, as much as 50 percent of the data may be erroneous.
    Feasibility of the data collection methodology was demonstrated by applying
    it to five different projects in two different environments.  The
    application showed that the methodology was both feasible and useful.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010301",
  location     = "http://www.cs.umd.edu/projects/SoftEng/ESEG/papers/82.21.pdf"
}

@Article{bcafpp,
  author       = "Lee, Jaejin and Padua, David~A. and Midkiff, Samuel~P.",
  title        = "Basic Compiler Algorithms for Parallel Programs",
  journal      = ppopp99,
  year         = 1999,
  volume       = 34,
  number       = 8,
  pages        = "1--12",
  month        = aug,
  keywords     = "delay set analysis, concurrent control flow graphs,
    concurrent static single-assignment form, copy propagation, dead-code
    elimination, global value numbering, cse, ",
  abstract     = "Traditional compiler techniques developed for sequential
    programs do not guarantee the correctness (sequential consistency) of
    compiler transformations when applied to parallel programs.  This is
    because traditional compilers for sequential programs do not account for
    the updates to a shared variable by different threads.  We present a
    concurrent static single assignment (CSSA) form for parallel programs
    containing cobegin/coend and parallel do constructs and post/wait
    synchronization primitives.  Based on the CSSA form, we present copy
    propagation and dead code elimination techniques.  Also, a global value
    numbering technique that detects equivalent variables in parallel programs
    is presented.  By using global value numbering and the CSSA form, we extend
    classical common subexpression elimination, redundant load/store
    elimination, and loop invariant detection to parallel programs without
    violating sequential consistency.  These optimization techniques are the
    most commonly used techniques for sequential programs.  By extending these
    techniques to parallel programs, we can guarantee the correctness of the
    optimized program and maintain single processor performance in a
    multiprocessor environment.", 
  location     = "http://dx.doi.org/10.1145/301104.301105"
}

@Article{omfold,
  author       = "Stefanovi{\' c}, Darko and McKinley, Kathryn~S. and Moss, J.~Eliot~B.",
  title        = "On Models for Object Lifetime Distributions",
  journal      = sigplan # " (" # pot # " Second International Symposium on Memory Management, ISMM '00)",
  year         = 2001,
  volume       = 36,
  number       = 1,
  pages        = "137--142",
  month        = jan,
  keywords     = "object lifetimes, lifetime distributions, garbage collection
    modeling, storage management", 
  abstract     = "Analytical models of memory object lifetimes are appealing
    because having them would enable mathematical analysis or fast simulation
    of the memory management behavior of programs.  In this paper, we
    investigate models for object lifetimes drawn from programs in
    object-oriented languages such as Java and Smalltalk.  We present certain
    postulated analytical models and compare them with observed lifetimes for
    58 programs.  We find that observed lifetime distributions do not match
    previously proposed object lifetime models, but do agree in salient shape
    characteristics with the gamma distribution family used in statistical
    survival analysis for general populations.", 
  location     = "http://dx.doi.org/10.1145/362426.362477"
}

@Article{cdpmfs,
  author       = "Haj-Yihia, Jawad and Asher, Yosi Ben and Rotem, Efraim and Yasin, Ahmad and Ginosar, Ran",
  title        = "Compiler-Directed Power Management for Superscalars",
  journal      = "ACM Transactions on Architecture and Code Optimization",
  year         = 2015,
  volume       = 11,
  number       = 4,
  pages        = "48:1--48:21",
  month        = jan,
  keywords     = "static analysis, power management, power demand modeling, cpu
    architectures, power distribution networks",
  abstract     = "Modern superscalar CPUs contain large complex structures and
    diverse execution units, consuming wide dynamic power range.  Building a
    power delivery network for the worst-case power consumption is not energy
    efficient and often is impossible to fit in small systems.  Instantaneous
    power excursions can cause voltage droops.  Power management algorithms are
    too slow to respond to instantaneous events.  In this article, we propose a
    novel compiler-directed framework to address this problem.  The framework
    is validated on a 4th Generation Intel® Core™ processor and with simulator
    on output trace.  Up to 16% performance speedup is measured over baseline
    for the SPEC CPU2006 benchmarks.",
  location     = "http://dx.doi.org/10.1145/2685393"
}

@Article{oret,
  author       = "Ntafos, Simon~C.",
  title        = "On Required Element Testing",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "795--803",
  month        = nov,
  keywords     = "data-flow analysis, required element testing, testing
    strategies", 
  abstract     = "In this paper we introduce two classes of program testing
    strategies that consist of specifying a set of required elements for the
    program and then covering those elements with appropriate test inputs.  In
    general, a required element has a structural and a functional component and
    is covered by a test case if the test case causes the features specified in
    the structural component to be executed under the conditions specified in
    the functional component.  Data flow analysis is used to specify the
    structural component and data flow interactions are used as a basis for
    developing the functional component.  The strategies are illustrated with
    examples and some experimental evaluations of their effectiveness are
    presented.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010308"
}

@Article{iaotfgcfj,
  author       = "Domani, Tamar and Kolodner, Elliot~K. and Lewis, Ethan and
    Salant, Eliot~E. and Barabash, Katherine and Lahan, Itai and Levanoni,
    Yossi and Petrank, Erez and Yanorer, Igor", 
  title        = "Implementing an On-the-fly Garbage Collector for {J}ava",
  journal      = sigplan # " (" # pot # " Second International Symposium on Memory Management, ISMM '00)",
  year         = 2001,
  volume       = 36,
  number       = 1,
  pages        = "155--166",
  month        = jan,
  keywords     = "java, programming languages, memory management, garbage
    collection, concurrent garbage collection, on-the-fly garbage collection",
  abstract     = "Java uses garbage collection (GC) for the automatic
    reclamation of computer memory no longer required by a running application.
    GC implementations for Java Virtual Machines (JVM) are typically designed
    for single processor machines, and do not necessarily perform well for a
    server program with many threads running on a multiprocessor.  We designed
    and implemented an on-the-fly GC, based on the algorithm of Doligez, Leroy
    and Gonthier [13, 12] (DLG), for Java in this environment.  An on-the-fly
    collector, a collector that does not stop the program threads, allows all
    processors to be utilized during collection and provides uniform response
    times.  We extended and adapted DLG for Java (e.g., adding support for weak
    references) and for modern multiprocessors without sequential consistency,
    and added performance improvements (e.g., to keep track of the objects
    remaining to be traced).  We compared the performance of our implementation
    with stop-the-world mark-sweep GC.  Our measurements show that the
    performance advantage for our collector increases as the number of threads
    increase and that it provides uniformly low response times.", 
  location     = "http://dx.doi.org/10.1145/362426.362484"
}

@Article{upgist,
  author       = "Sirer, Emin G{\" u}n and Bershad, Brian~N.",
  title        = "Using Production Grammars in Software Testing",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "1--13",
  month        = jan,
  keywords     = "testing, virtual machines, production grammars, test
    generation, comparative testing, self-describing test cases",
  abstract     = "Extensible typesafe systems, such as Java, rely critically on
    a large and complex software base for their overall protection and
    integrity, and are therefore difficult to test and verify.  Traditional
    testing techniques, such as manual test generation and formal verification,
    are too time consuming, expensive, and imprecise, or work only on abstract
    models of the implementation and are too simplistic.  Consequently,
    commercial virtual machines deployed so far have exhibited numerous bugs
    and security holes.In this paper, we discuss our experience with using
    production grammars in testing large, complex and safety-critical software
    systems.  Specifically, we describe lava, a domain specific language we
    have developed for specifying production grammars, and relate our
    experience with using lava to generate effective test suites for the Java
    virtual machine.  We demonstrate the effectiveness of production grammars
    in generating complex test cases that can, when combined with comparative
    and variant testing techniques, achieve high code and value coverage.  We
    also describe an extension to production grammars that enables concurrent
    generation of certificates for test cases.  A certificate is a behavioral
    description that specifies the intended outcome of the generated test case,
    and therefore acts as an oracle by which the correctness of the tested
    system can be evaluated in isolation.  We report the results of applying
    these testing techniques to commercial Java implementations.  We conclude
    that the use of production grammars in combination with other automated
    testing techniques is a powerful and effective method for testing software
    systems, and is enabled by a special purpose language for specifying
    extended production grammars.", 
  location     = "http://dx.doi.org/10.1145/331963.331965"
}

@Article{ujrtaelp,
  author       = "Dale Parson",
  title        = "Using Java Reflection to Automate Extension Langugae Parsing",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "67--80",
  month        = jan,
  keywords     = "reflection, embedded languages, scripting languages, tcl,
    java, c++",
  abstract     = "An extension language is an interpreted programming language
    designed to be embedded in a domain-specific framework.  The addition of
    domain-specific primitive operations to an embedded extension language
    transforms that vanilla extension language into a domain-specific language.
    The LUxWORKS processor simulator and debugger from Lucent uses Tcl as its
    extension language.  After an overview of extension language embedding and
    LUxWORKS experience, this paper looks at using Java reflection and related
    mechanisms to solve three limitations in extension language - domain
    framework interaction.  The three limitations are gradual accumulation of
    ad hoc interface code connecting an extension language to a domain
    framework, over-coupling of a domain framework to a specific extension
    language, and inefficient command interpretation.  Java reflection consists
    of a set of programming interfaces through which a software module in a
    Java system can discover the structure of classes, methods and their
    associations in the system.  Java reflection and a naming convention for
    primitive domain operations eliminate ad hoc interface code by supporting
    recursive inspection of a domain command interface and translation of
    extension language objects into domain objects.  Java reflection,
    name-based dynamic class loading, and a language-neutral extension language
    abstraction eliminate language over-coupling by transforming the specific
    extension language into a run-time parameter.  Java reflection and command
    objects eliminate inefficiency by bypassing the extension language
    interpreter for stereotyped commands.  Overall, Java reflection helps to
    eliminate these limitations by supporting reorganization and elimination of
    handwritten code, and by streamlining interpretation.", 
  location     = "http://www.cs.tufts.edu/~nr/cs257/archive/dale-parson/dsl99lux.ps.gz"
}

@Article{aasohpaiv,
  author       = "Higashino, Teruo and Mori, Masaaki and Sugiyama, Yuji and Taniguchi, Kenichi and Kasami, Tadao",
  title        = "An Algebraic Specification of {HDLC} Procedures and Its Verification",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "825--836",
  month        = nov,
  keywords     = "algebraic specifications, church-rosser property, hdlc
    procedures, term rewriting system, verification",
  abstract     = "It is well known that algebraic specification methods are 
    promising for specifying programs and for verifying their various
    properties formally.  In this paper, an algebraic specification of
    information transfer procedures of high-level data link control (HDLC)
    procedures is presented and some of the main properties of the
    specification are shown.  First, we introduce abstract states, state
    transition functions, and output functions corresponding to elementary
    notions extracted from the description of HDLC procedures in ISO 3309-1979
    (E) and ISO 4335-1979 (E).  Second, we show axioms which represent the
    relations between the values of functions before and after the state
    transitions.  Then, it is proved that the specification is ``consistent,''
    ``sufficiently complete,'' and ``nonredundant.'' Also it is shown that an
    implementation which realizes the specification is naturally derived.  In
    the last section, verification of various properties of HDLC procedures is
    formulated in the same framework as the algebraic specification, and some
    verification examples are presented.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010311"
}

@Article{fra,
  author       = "Elliott, Conal and Hudak, Paul",
  title        = "Functional Reactive Animation",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN International Conference on Functional Programming, ICFP '97)",
  year         = 1997,
  volume       = 32,
  number       = 8,
  pages        = "263--273",
  month        = aug,
  keywords     = "functional programming, reactive programming, animation",
  abstract     = "Fran (Functional Reactive Animation) is a collection of data
    types and functions for composing richly interactive, multimedia
    animations.  The key ideas in Fran are its notions of behaviors and events.
    Behaviors are time-varying, reactive values, while events are sets of
    arbitrarily complex conditions, carrying possibly rich information.  Most
    traditional values can be treated as behaviors, and when images are thus
    treated, they become animations.  Although these notions are captured as
    data types rather than a programming language, we provide them with a
    denotational semantics, including a proper treatment of real time, to guide
    reasoning and implementation.  A method to effectively and efficiently
    perform event detection using interval analysis is also described, which
    relies on the partial information structure on the domain of event times.
    Fran has been implemented in Hugs, yielding surprisingly good performance
    for an interpreter-based system.  Several examples are given, including the
    ability to describe physical phenomena involving gravity, springs,
    velocity, acceleration, etc.  using ordinary differential equations.", 
  location     = "http://dx.doi.org/10.1145/258949.258973"
}

@Article{dsec,
  author       = "Leijen, Daan and Meijer, Erik",
  title        = "Domain Specific Embedded Compilers",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "109--122",
  month        = jan,
  keywords     = "haskell, embedded languages, little languages, phantom
    types",
  abstract     = "Domain-specific embedded languages (DSELs) expressed in 
    higher-order, typed (HOT) languages provide a composable framework for
    domain-specific abstractions.  Such a framework is of greater utility than
    a collection of stand-alone domain-specific languages.  Usually, embedded
    domain specific languages are build on top of a set of domain specific
    primitive functions that are ultimately implemented using some form of
    foreign function call.  We sketch a general design pattern/or embedding
    client-server style services into Haskell using a domain specific embedded
    compiler for the server's source language.  In particular we apply this
    idea to implement Haskell/DB, a domain specific embdedded compiler that
    dynamically generates of SQL queries from monad comprehensions, which are
    then executed on an arbitrary ODBC database server.", 
  location     = "http://dx.doi.org/10.1145/331963.331977", 
  location     = "https://www.usenix.org/events/dsl99/full_papers/leijen/leijen.pdf"
}

@Article{shlrs,
  author       = "Dennis~W. Leinbaugh",
  title        = "Selectors: High-Level Resource Schedulers",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "810--825",
  month        = nov,
  keywords     = "nonprocedural langauge, process synchronization, protected
  resource, resource scheduling, resource sharing, starvation",
  abstract     = "Resource sharing problems can be described in three basically
    independent modular components.  The constraints the resource places upon
    sharing because of physcal limitations and consistency requirements.  The
    desired ordering of resource requests to achieve efficiency-either
    efficiency of resource utilization or efficiency of processes making the
    requests.  Modifications to the ordering to prevent starvation of processes
    waiting for requests which might otherwise never receive service.  A
    high-level nonprocedural language to specify these components of resource
    sharing problems is described.  General deadlock and starvation properties
    of selectors are proven.  Solutions to several classic resource sharing
    problems are shown to illustrate the expressiveness of this language.
    Proof techniques for this high-level language are introduced to show how to
    prove particular selectors are or are not deadlock and starvation free.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010310"
}

@Article{vveis,
  author       = "James Jennings and Eric Beuscher",
  title        = "Verischemelog: {V}erilog Embedded in {S}cheme",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "109--122",
  month        = jan,
  keywords     = "scheme, verilog, macros, extension languages, embedded
    languages, hardware description languages",
  abstract     = "Verischemelog (pronounced with 5 syllables, 
    ver-uh-scheme-uh-log) is a language and programming environment embedded in
    Scheme for designing digital electronic hardware systems and for
    controlling the simulatin of these circuits.  Simulation is performed by a
    separate program, often a commercial product.  Verischemelog compiles to
    Verilog, an industry standard language accepted by several commercial and
    public domain simulators.  Because many design elements are easily
    parameterized, design engineers currently write scripts which generate
    hardware description code in Verilog.  These scripts work by textual
    substitution, and are typically ad-hoc and quite limited.  Preprocessors
    for Verilog, on the other hand, are hampered by their macro-expansion
    languages, which support few data types and lack procedures.  Verischemelog
    obviates the need for scripts and prepocessors by providing a hardware
    description language with list-based syntax, and Scheme to manipulate it.
    An interactive development environment gives early and specific feedback
    about errors, and structured access to the compiler and run-time
    environment provide a high degree of reconfigurability and extensibility of
    Verischemelog.", 
  location     = "http://usenix.org/publications/library/proceedings/dsl99/full_papers/jennings/jennings.pdf"
}

@Article{cctvpfcfsm,
  author       = "Mohammed~C. Gouda",
  title        = "Closed Covers:  To Verify Progress for Communicating Finite State Machines",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "846--855",
  month        = nov,
  keywords     = "communicating finite state machines, communication progress,
    communication protocols, verification techniques, automata, data
    structures, protocols, resource management, system recovery",
  abstract     = {Consider communicating finite state machines which exchange
    messages over unbounded FIFO channels.  We discuss a technique to verify
    that the communication between a given pair of such machines will progress
    indefinitely; this implies that the communication is free from deadlocks
    and unspecified receptions.  The technique is based on finding a set of
    global states for the communicating pair such that the following two
    conditions (along with other conditions) are satisfied: 1) the initial
    global state is in that set; and 2) starting from any global state in that
    set, an "acyclic version" of the communicating pair must reach a global
    state in that set.  We call such a set a closed cover, and show that the
    existence of a closed cover for a communicating pair is sufficient to
    guarantee indefinite communication progress.  We also show that in many
    practical instances, if the communication is guaranteed to progress
    indefinitely, then the existence of a closed cover is necessary.},
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010313",
  location     = "http://www.cs.utexas.edu/~gouda/papers/journal/10-whole.pdf"
}

@Article{aooassl,
  author       = "Giuseppe Serazzi and Maria Caizarossa",
  title        = "Adaptive Optimization of a System's Load",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  month        = nov,
  pages        = "837--845",
  keywords     = "adaptive control, adaptive scheduling algorithm, asymptotic
    bound analyasis, load balancing, real-time performance optimization",
  abstract     = "Applications of modeling techniques based on queueing theory
    to computer system performance analysis normally assume the existence of
    steady-state conditions.  However, these conditions are often violated
    since the unpredictable composition of workload causes peaks having highly
    variable intensities and durations.  Furthermore, computer system
    performance is highly dependent on how the system reacts to workload
    fluctuations.  Automatic control mechanisms are required to take care of
    the high variance of resource demands.  Real-time optimization of the
    overall performance of a computer system requires the introduction of
    adaptive control on the controlled functions, An adaptive scheduling
    algorithm which controls the input of the system in order to maximize a
    given performance criterion, such as the system throughput, is presented.
    The system load is adjusted depending on the characteristics of both the
    mix of jobs in execution and the mix of jobs submitted to the system and
    waiting in the input queue.  The asymptotic analysis of the performance
    bounds provides useful information about the limits on the performance
    indexes that can be achieved with a multiclass workload.  The evaluation of
    the adaptive control system is performed through simulation experiments
    using data collected from two real workloads.  This technique could be used
    to optimize the throughput of a centralized system as well as for the
    automatic load balancing in a distributed environment.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010312"
}

@Article{acm,
  author       = "Khayat, Mohammad G.",
  title        = "{A} Concurrency Measure",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "804--810",
  month        = nov,
  keywords     = "compatibility, concurrency control, degree of concurrency,
    dining philosophers problem, interprocess communication, maximal
    compatibility, parallel processing, readers/writers problem,
    synchronization policies, update synchronization",
  abstract     = "With the new advents of technology and the availability of 
    microprocessors and minicomputers, parallel and distributed processing is
    gaining widespread acceptability.  In such systems resources are shared
    among a number of processes.  Accesses to the resources must be
    synchronized in order to guarantee proper operation of a system.  In this
    research work, a measure, called maximal compatibility, is developed to
    measure the degree of concurrency (parallelism) a synchronization policy
    achieves.  A set of accesses is considered compatible if it only contains
    accesses that are permitted to occur simultaneously.  A policy is maximally
    compatible if it allows every compatible set of accesses to occur
    simultaneously and if the maximum number of requests is always satisfied
    without allowing incompatible accesses to occur simultaneously.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010309"
}

@Article{dore,
  author       = "Janusz~A. Brzozowski",
  title        = "Derivatives of Regular Expressions",
  journal      = jacm,
  year         = 1964,
  volume       = 11,
  number       = 4,
  pages        = "481--494",
  month        = oct,
  keywords     = "symbolic manipulation, algebraic manipulation",
  abstract     = "",
  location     = "http://dx.doi.org/10.1145/321239.321249"
}

@Article{cpnala,
  author       = "David~B. Benson",
  title        = "Counting Paths:  Nondeterminism as Linear Algebra",
  journal      = tse,
  year         = 1984,
  volume       = "SE-10",
  number       = 6,
  pages        = "785--794",
  month        = nov,
  keywords     = "convergent iterative programs, deterministic divergence,
    nondeterministic divergence, nondeterministic programs, nonnegative
    matrices, semirings, computer errors, concurrent computing, control
    systems, convergence, design engineering, distributed computing, linear
    algebra, operating systems, programming profession, redundancy",
  abstract     = "Nondeterminism is considered to be ignorance about the actual
    state transition sequence performed during a computation.  The number of
    distinct potential paths from state i to j forms a matrix [nij].  The
    behavior of a nondeterministic program is defined to be this multiplicity
    matrix of the state transitions.  The standard programming constructs have
    behaviors defined in terms of the behaviors of their constituents using
    matrix addition and multiplication only.  The spectral radius of the matrix
    assigned to an iterating component characterizes its convergence.  The
    spectral radius is shown to be either 0 or else >= 1.  The program
    converges iff the spectral radius is zero, diverges deterministically iff
    the spectral radius is one, and has a proper nondeterministic divergence
    iff the spectral radius exceeds one.  If the machine has an infinite number
    of states the characterization of convergence is given graph theoretically.
    The spectral radii of synchronous and interleaved parallel noncommunicating
    systems are easily computed in terms of the spectral radii of the
    components.", 
  location     = "http://dx.doi.org/10.1109/TSE.1984.5010307"
}

@Article{dsodiws,
  author       = "Fern{\' a}ndez, Mary and Suciu, Dan and Tatarinov, Igor",
  title        = "Declarative Specification of Data-Intensive Web Sites",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "135--148",
  month        = jan,
  keywords     = "domain-specific languages, functional languages,
    semistructured data, query languages, template languages",
  abstract     = "Integrated information systems are often realized as 
    data-intensive Web sites, which integrate data from multiple data sources.
    We present a system, called STRUDEL, for specifying and generating
    data-intensive Web sites.  STRUDEL separates the tasks of accessing and
    integrating a site's data sources, building its structure, and generating
    its HTML representation.  STRUDEL's declarative query language, called
    StruQL, supports the first two tasks.  Unlike ad-hoc database queries, a
    StruQL query is a software artifact that must be extensible and reusable To
    support more modular and reusable site definition queries, we extend StruQL
    with functions and describe how the new language, FunStruQL, better
    supports common site-engineering tasks, such as choosing a strategy for
    generating the site's pages dynamically and/or statically To substantiate
    STRUDEL's benefits, we describe the re-engineering of a production Web site
    using FunStruQL and show that the new site is smaller, more reusable, and
    unlike the original site, can be analyzed and optimized.", 
  location     = "http://dx.doi.org/10.1145/331960.331979"
}

@Article{narwf,
  author       = "Robert~W. Floyd",
  title        = "Nondeterministic Algorithms",
  journal      = jacm,
  year         = 1967,
  volume       = 14,
  number       = 4,
  pages        = "636--644",
  month        = oct,
  keywords     = "nondeterminism, algorithms, program transforms",
  abstract     = "Programs to solve combinatorial search problems may often be 
    simply written by using multiple-valued functions.  Such programs, although
    impossible to execute directly on conventional computers, may be converted
    in a mechanical way into conventional backtracking programs.  The process
    is illustrated with algorithms to find all solutions to the eight queens
    problem on the chessboard, and to find all simple cycles in a network.", 
  location     = "http://dx.doi.org/10.1145/321420.321422"
}

@Article{biagcs,
  author       = "Lindstrom, Gary",
  title        = "Backtracking in a Generalized Control Setting",
  journal      = toplas,
  year         = 1979,
  volume       = 1,
  number       = 1,
  pages        = "8--26",
  month        = jul,
  keywords     = "backtracking, coroutines, nondeterministic programming,
    simulation, control structures, pascal",
  abstract     = "Backtracking is a powerful conceptual and practical
    programming language control structure.  However, its application in
    general has been limited to global control over recursive programs.  In
    this paper we explore the coherence and utility of applying backtracking in
    a more general control setting, namely, block-structured coroutines.  The
    following criteria are proposed for such a control combination to be judged
    successful: (i) retention of each control form's individual semantics; (ii)
    coherent semantics for each legal application of the combination; (iii)
    nonpreeminence of either control form, and (iv) facilitation of genuinely
    novel programming effects.  The attainability of these criteria is
    assessed, with the aid of an informal language design and three
    illustrative applications: (i) a dual tree walk program using
    coroutine-managed backtracking subsystems; (ii) a context-free language
    intersection tester using bilevel hierarchical backtracking, and (iii) an
    optimizing computer job scheduler using backtracking in a simulation
    language context.  Full programs are given for each example, phrased in a
    Pascal extension offering both coroutines and backtracking (expressed
    through nondeterministic control).", 
  location     = "http://dx.doi.org/10.1145/357062.357063"
}

@Article{eeoartsfams,
  author       = "Blake, Ben~A. and Schwan, Karsten",
  title        = "Experimental Evaluation of a Real-Time Scheduler for a Multiprocessor System",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 1,
  pages        = "34--44",
  month        = jan,
  keywords     = "bin packing, deadline, distributed scheduling",
  abstract     = "A description is given of the design, implementation, and
    experimental evaluation of a multiprocessor scheduler used with robotics
    applications and other real-time programs.  The scheduler makes decisions
    concerning both the assignment of processes and the scheduling of these
    processes on each processor such that a near-optimal numer of processor
    deadlines is satisfied.  It assumes that process execution times,
    deadlines, and earliest possible start times are known.", 
  location     = "http://dx.doi.org/10.1109/32.67577"
}

@Article{aalfosl,
  author       = "Guyer, Samuel~Z. and Lin, Calvin",
  title        = "An Annotation Language for Optimizing Software Libraries",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "39--52",
  month        = jan,
  keywords     = "performance annotations, library optimization",
  abstract     = "This paper introduces an annotation language and a compiler
    that together can customize a library implementation for specific
    application needs.  Our approach is distinguished by its ability to exploit
    high level, domain-specific information in the customization process.  In
    particular, the annotations provide semantic information that enables our
    compiler to analyze and optimize library operations as if they were
    primitives of a domain-specific language.  Thus, our approach yields many
    of the performance benefits of domain-specific languages, without the
    effort of developing a new compiler for each domain.This paper presents the
    annotation language, describes its role in optimization, and illustrates
    the benefits of the overall approach.  Using a partially implemented
    compiler, we show how our system can significantly improve the performance
    of two applications written using the PLAPACK parallel linear algebra
    library.", 
  location     = "http://dx.doi.org/10.1145/331960.331970",
  location     = "http://www.cs.tufts.edu/~sguyer/publications/dsl99.pdf"
}

@Article{sams,
  author       = "Robert~F. Rosen",
  title        = "Supervisory and Monitor Systems",
  journal      = surveys,
  year         = 1969,
  volume       = 1,
  number       = 1,
  pages        = "37--54",
  month        = mar,
  keywords     = "executive systems, supervisory systems, operating systems,
    monitors, history, tutorial, command language, interrupts, software",
  abstract     = "A survey of the development of executive systems, covering
    primitive as well as more refined concepts, is presented.  It is arranged
    chronologically, from the early 1950s through the present time, and is
    based upon representative systems of each period which where in active use
    at several installations and which implicitly defined the them existing
    state-of-the-art.  The material is presented in a progression from the
    simple to the most complex developments, in the same way they  
    occurred during the period considered.", 
  location     = "http://dx.doi.org/10.1145/356540.356544"
}

@Article{asopanatsp,
  author       = "David Harel",
  title        = "And/Or Programs:  A New Approach to Structured Programming",
  journal      = toplas,
  year         = 1980,
  volume       = 2,
  number       = 1,
  pages        = "1--17",
  month        = jan,
  keywords     = "alternation, and/or programs, program verification,
    structured programming, textual complexity",
  abstract     = "A simple tree-like programming/specification language is 
    presented.  The central idea is the dividing of conventional programming
    constructs into the two classes of and and or subgoaling, the subgoal tree
    itself constituting the program.  Programs written in the language can, in
    general, be both nondeterministic and parallel.  The syntax and semantics
    of the language are defined, a method for verifying programs written in it
    is described, and the practical significance of programming in the language
    assessed.  Finally, some directions for further research are indicated.", 
  location     = "http://dx.doi.org/10.1145/357084.357085"
}

@Article{dcapbde,
  author       = "Tai, Kuo-Chung and Carver, Richard~H. and Obaid, Evelyn~E.",
  title        = "Debugging Concurrent {A}da Programs by Deterministic Execution",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  OPTnumber    = 1,
  pages        = "45--63",
  month        = jan,
  keywords     = "ada, debugging, concurrency, execution traces, deterministic
    execution", 
  abstract     = "A language-based approach to deterministic execution
    debugging of concurrent Ada programs is presented.  The approach is to
    define synchronization (SYN) -sequences of a concurrent Ada program in
    terms of Ada language constructs and to replay such SYN-sequences without
    the need for system-dependent debugging tools.  It is shown how to define a
    SYN-sequence of a concurrent Ada program in order to provide sufficient
    information for deterministic execution.  It is also shown how to transform
    a concurrent Ada program P so that the SYN-sequences of previous executions
    of P can be replayed.  This transformation adds an Ada task to P that
    controls program execution by synchronizing with the original tasks in P.
    A brief description is given of the implementation of tools supporting
    deterministic execution debugging of concurrent Ada programs.", 
  location     = "http://dx.doi.org/10.1109/32.67578"
}

@Article{acsl,
  author       = "Li, Du and Muntz, Richard~R.",
  title        = "{A} Collaboration Specification Language",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "149--162",
  month        = jan,
  keywords     = "computer-supported cooperative work, declarative
    specifications, concurrency control, session control, roles",
  abstract     = "COCA (Collaborative Objects Coordination Architecture) was
    proposed as a novel means to model and support collaborations over the
    Internet.  Our approach separates coordination policies from user
    interfaces and the policies are specified in a logic-based language.  Over
    the past year, both the collaboration model and the specification language
    have been substantially refined and evaluated through our experience in
    building real-life collaboration systems.  This paper presents the design
    of the specification language and illustrates the main ideas with a few
    simple design examples.  Semantics, implementation, runtime support, and
    applications are also covered but not as the focus of this paper.", 
  location     = "http://dx.doi.org/10.1145/331960.331980"
}

@Article{halfpvlsd,
  author       = "Bonachea, Dan and Fisher, Kathleen and Rogers, Anne and Smith, Frederick",
  title        = "Hancock: A Language For Processing Very Large-Scale Data",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "xxx--yyy",
  month        = jan,
  keywords     = "domain specific languages, stream processing, high-level
    languages, data mining",
  abstract     = "A signature is an evolving customer profile computed from
    call records.  AT&T uses signatures to detect fraud and to target
    marketing.  Code to compute signatures can be difficult to write and
    maintain because of the volume of data.  We have designed and implemented
    Hancock, a C-based domain-specific programming language for describing
    signatures.  Hancock provides data abstraction mechanisms to manage the
    volume of data and control abstractions to facilitate looping over records.
    This paper describes the design and implementation of Hancock, discusses
    early experiences with the language, and describes our design process.",  
  location     = ""
}

@Article{asoatsm,
  author       = "McKinney, J. M.",
  title        = "A Survey of Analytical Time-Sharing Models",
  journal      = surveys,
  year         = 1969,
  volume       = 1,
  number       = 2,
  pages        = "105--116",
  month        = jun,
  keywords     = "time-sharing, queueing analysis, mathematical time-sharing
    studies, scheduling algorithm, operating systems, multiprogramming
    systems",
  abstract     = "Queueing models of time-sharing algorithms have been
  appearing in the literature with increasing frequency.  This survey 
    delineates the parameters distinguishing different analytic models and 
    presents the research results of most of the papers to date.  The analysis
    techniques for various round-robin and multiple-level queue models both
    with and without priorities are emphasized.  The results indicated provide
    the system designer with a number of degrees of freedom with which to
    synthesize a time-shared processing system and point out needed research 
    directions.  An annotated bibliography is provided.",
  location     = "http://dx.doi.org/10.1145/356546.356549"
}

@Article{aattuopisp,
  author       = "Griswold, Ralph~E. and Hanson, David~R.",
  title        = "An Alternative to the Use of Patterns in String Processing",
  journal      = toplas,
  year         = 1980,
  volume       = 2,
  number       = 2,
  pages        = "153--172",
  month        = apr,
  keywords     = "string processing, pattern matching, programming languages,
    snobol4, icon, generators",
  abstract     = "SNOBOL4 is best known for its string processing facilities,
    which are based on patterns as data objects.  Despite the demonstrated
    success of patterns, there are many shortcomings associated with their use.
    The concept of patterns in SNOBOL4 is examined and problem areas are
    discussed.  An alternative method for high-level string processing is
    described.  This method, implemented in the programming language Icon,
    employs generators, which are capable of producing alternative values.
    Generators, coupled with a goal-driven method of expression evaluation,
    provide the string processing facilities of SNOBOL4 without the
    disadvantages associated with patterns.  Comparisons between SNOBOL4 and
    Icon are included and the broader implications of the new approach are
    discussed.", 
  location     = "http://dx.doi.org/10.1145/357094.357096"
}

@Article{tgoiimals,
  author       = "Maurice~V. Wilkes",
  title        = "The Growth of Interest in Microprogramming:  A Literature Survey",
  journal      = surveys,
  year         = 1969,
  volume       = 1,
  number       = 3,
  pages        = "139--145",
  month        = sep,
  keywords     = "microprogramming, machine architecture, optimization",
  abstract     = "The literature is surveyed beginning with the first paper
    published in 1951.  At that time microprogramming was proposed primarily as
    a means for designing the control unit of an otherwise conventional digital
    computer, although the possible use of a read/write control memory was
    noted.  The survey reveals the way in which interest has successively
    developed in the following aspects of the subject: stored logic, the
    application of microprogramming to the design of a range of computers,
    emulation, microprogramming in support of software, the read/white control
    memories.  The bibliography includes 55 papers.", 
  location     = "http://dx.doi.org/10.1145/356551.356553"
}

@Article{mrsurc,
  author       = "Daniel~G. Bobrow",
  title        = "Managing Reentrant Structures Using Reference Counts",
  journal      = toplas,
  year         = 1980,
  volume       = 2,
  number       = 3,
  pages        = "369--273",
  month        = jul,
  keywords     = "storage management, garbage collection, reference counting",
  abstract     = "Automatic storage management requires that one identify
    storage unreachable by a user's program and return it to free status.  One
    technique maintains a count of the references from user's programs to each
    cell, since a count of zero implies the storage is unreachable.  Reentrant
    structures are self-referencing; hence no cell in them will have a count of
    zero, even though the entire structure is unreachable.  A modification of
    standard reference counting can be used to manaage the deallocation of a
    large class of frequently used reentrant structures, including two-way and
    circularly linked lists.  All the cells of a potentially reentrant
    structure are considered as part of a single group for deallocation
    purposes.  Information associated with each cell specifies its group
    membership.  Internal references (pointers from one cell of the group to
    another) are not reference counted.  External references to any cell of
    this group are counted as references to the group as a whole.  When the
    external reference count goes to zero, all the cells of the group can be
    deallocated.  This paper describes several ways of specifying group
    membership, properties of each implementation, and properties of mutable
    and immutable group membership.", 
  location     = "http://dx.doi.org/10.1145/357103.357104"
}

@Article{saarinfs,
  author       = "Jason Gait",
  title        = "Stability, Availability, and Response in Network File Service",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 2,
  OPTpages     = "133--140",
  month        = feb,
  keywords     = "competition to respond, duplicate message exceptions,
    throughput, redundancy",
  abstract     = "A network file system called Multifile is described.  It 
    meets response, availability, and stability requirements as primitive
    functions.  Multifile has a high degree of responsiveness because its
    component parts compete among themselves to service file requests; it has
    high availability because it maintains multiple copies of files; and it
    exhibits stable behavior over wise range of system parameters.  The
    responsiveness of Multifile to read requests improves as the number of
    pages per request rises, implying that read ahead pages can profitably be
    cached at client sites.  The throughput of Multifile improves as the
    request size increases and as the number of clients increases.  As server
    load increases, the responsiveness of Multifile to read requests is stable
    in most configurations.  The throughput of writes is unstable as the number
    of pages in the wire request rises, implying that write back pages should
    not be cached at client sites.  The scale of events in file service is
    dominated by disk activity, so lost message exceptions do not occur
    frequently enough to affect file service; however, duplicate message
    exceptions are a factor in performance.", 
  location     = "http://dx.doi.org/10.1109/32.67594"
}

@Article{mrjpgh,
  author       = "Peterson, John and Hager, Greg",
  title        = "Monadic Robotics",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "95--108",
  month        = jan,
  keywords     = "robotics, functional reactive programming, monads, functional
    programming, haskell, software abstraction, frob",
  abstract     = "We have developed a domain specific language for the 
    construction of robot controllers, Frob (Functional ROBotics).  The
    semantic basis for Frob is Functional Reactive Programming, or simply FRP,
    a purely functional model of continuous time, interactive systems.  FRP is
    built around two basic abstractions: behaviors, values defined continuously
    in time, and events, discrete occurances in time.  On this foundation, we
    have constructed abstractions specific to the domain of robotics.  Frob
    adds yet another abstraction: the task, a basic unit of work defined by a
    continuous behavior and a terminating event.This paper examines two
    interrelated aspects of Frob.  First, we study the design of systems based
    on FRP and how abstractions defined using FRP can capture essential
    domain-specific concepts for systems involving interaction over time.
    Second, we demonstrate an application of monads, used here to implement
    Frob tasks.  By placing our task abstraction in a monadic framework, we are
    able to organize task semantics in a modular way, allowing new capabilities
    to be added without pervasive changes to the system.We present several
    robot control algorithms specified using Frob.  These programs are clear,
    succinct, and modular, demonstrating the power of our approach.", 
  location     = "http://dx.doi.org/10.1145/331963.331976"
}

@Article{aepss,
  author       = "Selby, Richard~W. and Basili, Victor~R.",
  title        = "Analyzing Error-Prone System Structure",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 2,
  pages        = "141--152",
  month        = feb,
  keywords     = "emperical measurement and evaluation, error analysis,
    software inspections, software metrics, system decomposition, system
    strength and coupling",
  abstract     = "Using measures of data interaction called data bindings, the
    authors quantify ratios of coupling and strength in software systems and
    use the ratios to identify error-prone system structures.  A 148000 source
    line system from a prediction environment was selected for empirical
    analysis.  Software error data were collected from high-level system design
    through system testing and from field operation of the system.  The authors
    use a set of five tools to calculate the data bindings automatically and
    use a clustering technique to determine a hierarchical description of each
    of the system's 77 subsystems.  A nonparametric analysis of variance model
    is used to characterize subsystems and individual routines that had either
    many or few errors or high or low error correction effort.  The empirical
    results support the effectiveness of the data bindings clustering approach
    for localizing error-prone system structure.", 
  location     = "http://dx.doi.org/10.1109/32.67595"
}

@Article{lsfogfsa,
  author       = "Cohen, Doron~J. and Gotlieb, C. C.",
  title        = "A List Structure Form of Grammars for Syntatic Analysis",
  journal      = surveys,
  year         = 1970,
  volume       = 2,
  number       = 1,
  pages        = "65--82",
  month        = mar,
  keywords     = "grammars, parsing, data structures, lists, syntax-directed
    compilation, graphs, top-down parsing, bottom-up parsing, backtracking,
    context free grammars, bnf form",
  abstract     = "The syntax graph is a representation for grammars in list
    structure form, in which printers link the various components and
    alternatives of a production.  Rules for constructing the syntax graph are
    explicitly stated.  When this structure is used the algorithms for
    syntactic analysis become particularly simple.  Two algorithms for
    top-to-bottom parsing without backtracking are given, and one algorithm
    with backtracking, all of which can e used on any context-free grammar.
    From the syntax graph a reversed graph can be defined, and this naturally
    leads to a simple algorithm for bottom-to-top parsing.", 
  location     = "http://dx.doi.org/10.1145/356561.356565"
}

@Article{aeipe,
  author       = "Jaydev Misra",
  title        = "An Exercise in Program Explanation",
  journal      = toplas,
  year         = 1981,
  volume       = 3,
  number       = 1,
  pages        = "104--109",
  month        = jan,
  keywords     = "prime sieves, stepwise refinement, formal development",
  abstract     = "A combination of program-proving and stepwise refinement is
    used to develop and explain an algorithm which uses a version of the sieve
    method for computing primes.",
  location     = "http://dx.doi.org/10.1145/357121.357128"
}

@Article{wiafpnafp,
  author       = "John Longley",
  title        = "When is a Functional Program Not a Functional Program?",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN " # icfp # ", ICFP '99)",
  year         = 1999,
  volume       = 32,
  number       = 9,
  pages        = "1--7",
  month        = sep,
  keywords     = "sequentially realizable functionals, functional algorithms",
  abstract     = "In an impure functional language, there are programs whose
    behaviour is completely functional (in that they behave extensionally on
    inputs), but the functions they compute cannot be written in the purely
    functional fragment of the language.  That is, the class of programs with
    functional behaviour is more expressive than the usual class of pure
    functional programs.  In this paper we introduce this extended class of
    functional programs by means of examples in Standard ML, and explore what
    they might have to offer to programmers and language implementors.After
    reviewing some theoretical background, we present some examples of
    functions of the above kind, and discuss how they may be implemented.  We
    then consider two possible programming applications for these functions:
    the implementation of a search algorithm, and an algorithm for exact
    real-number integration.  We discuss the advantages and limitations of this
    style of programming relative to other approaches.  We also consider the
    increased scope for compiler optimizations that these functions would
    offer.", 
  location     = "http://dx.doi.org/10.1145/317636.317775"
}

@Article{jfde,
  author       = "Nakatani, Lloyd~H. and Ardis, Mark~A. and Olsen, Robert~G. and Pontrelli, Paul~M.",
  title        = "Jargons for Domain Engineering",
  journal      = dsl99,
  year         = 2000,
  volume       = 35,
  number       = 1,
  pages        = "15--24",
  month        = jan,
  keywords     = "domain engineering, domain-specific languages, software
    engineering",
  abstract     = "In the Family-oriented Abstraction, Specification and 
    Translation (FAST) domain engineering process for software production, a
    member of a software product family is automatically generated from a model
    expressed in a DSL.  In practice, the time and skill needed to make the
    DSLs proved to be bottlenecks.  FAST now relies on jargons, a kind of
    easy-to-make DSL that domain engineers who are not language experts can
    quickly make themselves.  We report our experiences with jargons in the
    FAST process, and describe the benefits they provide above and beyond
    conventional DSLs for software production and other purposes.", 
  location     = "http://dx.doi.org/10.1145/331960.331966"
}

@Article{aefdfts,
  author       = "James~M. Purtilo and Pankaj Jalote",
  title        = "An Environment for Developing Fault-Tolerant Software",
  journal      = tse,
  year         = 1991,
  volume       = "SE-17",
  number       = 2,
  pages        = "153--159",
  month        = feb,
  keywords     = "module interconnection language, n-version programming,
    recovery block, software bus, software fault tolerance",
  abstract     = "An environment that supports execution of programs using both
    N-version programming and recovery blocks in a uniform manner is described.
    For N-version programming, the system offers an easy and flexible way of
    specifying the target machines for the separate versions.  The basic unit
    of fault tolerance supported by this system is at the procedure or function
    level.  Each such program unit can be packaged as its own task, and
    different fault tolerance techniques can subsequently be employed, even
    within the same application.  The environment also allows versions to be
    written in different programming languages and executed on different
    machines.  This enhances the independence between the different versions,
    making the fault tolerance techniques more effective.  This environment has
    been developed for use on Unix-based hosts and currently runs on a network
    of Sun and DEC workstations", 
  location     = "http://dx.doi.org/10.1109/32.67596"
}

@Article{aapnwtiov,
  author       = "Martin Rem",
  title        = "Associons:  {A} Program Notation with Tuples Instead of Variables",
  journal      = toplas,
  year         = 1981,
  volume       = 3,
  number       = 3,
  pages        = "251--262",
  month        = jul,
  keywords     = "programming languages, data structures, content-retrievable
    tuples, associons, closure statement, concurrency, associative addressing",
  abstract     = "A program notation without variables is proposed.  The state
    of a computation is recorded as a set of associons. Each associon is a
    tuple of names representing a relation between the entities with those
    names.  The state of the computation can be changed by the creation of new
    associons representing new relations deducible from those already recorded.
    The building block prescribing such deductions is called the closure
    statement. The notation proposed is the result of our search for a basic
    statement the execution of which may employ more concurrency than is the
    case for the traditional assignment statement.  Some possible extensions to
    the notation are discussed.",
  location     = "http://dx.doi.org/10.1145/357139.357142"
}

@Article{vmpjd,
  author       = "Denning, Peter~J.",
  title        = "Virtual Memory",
  journal      = surveys,
  year         = 1970,
  volume       = 28,
  number       = 1,
  pages        = "153--189",
  month        = sep,
  keywords     = "virtual memory, one-level store, memory allocation, storage
    allocation, dynamic storage allocation, segmentation, paging, replacement
    algoritms, storage fragmentation, thrashing, working set",
  abstract     = "The need for automatic storage allocation arises from the 
    desire for program modularity, machine independence, and resource sharing.
    Virtual memory is an elegant way of achieving these objectives.  In a
    virtual memory, the addresses a program may use to identify information are
    distinguished from the addresses the memory system uses to identify
    physical storage sites, and program-generated addresses are translated
    automatically to the corresponding machine addresses.  Two principal
    methods for implementing virtual memory, segmentation and paging, are
    compared and contrasted.  Many contemporary implementations have
    experienced one or more of these problems: poor utilization of storage,
    thrashing, and high costs associated with loading information into memory.
    These and subsidiary problems are studied from a theoretic view, and are
    shown to be controllable by a proper combination of hardware and memory
    management policies.", 
  location     = "http://dx.doi.org/10.1145/356571.356573"
}

@Article{tyohslasepi,
  author       = "Krzysztof~R. Apt",
  title        = "Ten Years of {H}oare's Logic: {A} Survey---Part {I}",
  journal      = toplas,
  year         = 1981,
  volume       = 3,
  number       = 4,
  pages        = "431--483",
  month        = oct,
  keywords     = "Hoare's logic, partial correctness, total correctness, 
    soundness, Cook completeness, expressiveness, arithmetical interpretation,
    while programs, recursive procedures, variable declarations, subscripted
    variables, call-by-name, call-by-value, call-by-variable, static scope,
    dynamic scope, procedures as parameters", 
  abstract     = "A survey of various results concerning Hoare's approach to 
    proving partial and total correctness of programs is presented.  Emphasis
    is placed on the soundness and completeness issues.  Various proof systems
    for while programs, recursive procedures, local variable declarations, and
    procedures with parameters, together with the corresponding soundness,
    completeness, and incompleteness results, are discussed.", 
  location     = "http://dx.doi.org/10.1145/357146.357150"
}

@Article{autss,
  author       = "Foster, Caxton~C.",
  title        = "An Unclever Time-Sharing System",
  journal      = surveys,
  year         = 1971,
  volume       = 3,
  number       = 1,
  pages        = "23--48",
  month        = mar,
  keywords     = "time sharing, operating system, tutorial, command
    language, scheduling",
  abstract     = "This paper describes the internal structure of a time-sharing
    system in some detail.  This system is dedicated to providing remote
    access, and has a simple file structure.  It is intended for use in a
    university type environment where there are many short jobs that will
    profit from one- or two-second turnaround.  Despite its simplicity, this
    system can serve as a useful introduction to the problems encountered by
    the designers of any time-sharing system.  Included are a discussion of the
    command language, the hardware organization toward which the design is
    oriented, the general internal organization, the command sequences, the CPU
    scheduler, handling of interrupts, the assignment of core space, execution
    and control of the user's program, backup storage management, and the
    handling of errors.", 
  location     = "http://dx.doi.org/10.1145/356583.356585"
}

@Article{sapcfsulm,
  author       = "Nishida, Fujio and Takamatsu, Shinobu and Fujita, Yoneharu and Tani, Tadaski",
  title        = "Semi-Automatic Program Construction From Specifications Using Library Modules",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "853--871",
  month        = sep,
  keywords     = "automatic programming, formal specifications, software tools,
    subroutines, maps, library modules, program generation, refinement system",
  abstract     = "A method of semiautomatic specification refinement and 
    program generation using library modules, is described.  Users write their
    specifications and modify and rearrange them so that they can be refined
    with the aid of the library modules.  When a specification is given, a
    refinement system, called MAPS (module-aided program construction system)
    searches for library modules applicable to the given specification,
    replaces the specification with a more detailed description written in the
    operation part of the modules, and converts the refined specification into
    a program written in a programming language designated by the user.
    Case-like expressions or pseudo-natural language expressions are used for
    describing user's specifications and specifications for library modules", 
  location     = "http://dx.doi.org/10.1109/32.92909"
}

@Article{sidp,
  author       = "Schneider, Fred~B.",
  title        = "Synchronization in Distributed Programs",
  journal      = toplas,
  year         = 1982,
  volume       = 4,
  number       = 2,
  pages        = "125--148",
  month        = apr,
  keywords     = "logical clocks, synchronization, distributed semaphores,
    guarded commands, monotonic predicates, phase transitions",
  abstract     = "A technique for solving synchronization problems in 
    distributed programs is described.  Use of this technique in environments
    in which processes may fail is discussed.  The technique can be used to
    solve synchronization problems directly, to implement new synchronization
    mechanisms (which are presumably well suited for use in distributed
    programs), and to construct distributed versions of existing
    synchronization mechanisms.  Use of the technique is illustrated with
    implementations of distributed semaphores and a conditional message-passing
    facility.", 
  location     = "http://dx.doi.org/10.1145/357162.357163"
}

@Article{oeamdlwh,
  author       = "Launchbury, John and Lewis, Jeffrey~R. and Cook, Byron",
  title        = "On Embedding a Microarchitectural Design Langauge Within {H}askell",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN " # icfp # ", ICFP '99)",
  year         = 1999,
  volume       = 32,
  number       = 9,
  pages        = "60--69",
  month        = sep,
  keywords     = "embedded languages, hardware description language, lazy
    evaluation, state monads, reactive systems, symbolic simulation",
  abstract     = "Based on our experience with modelling and verifying
    microarchitectural designs within Haskell, this paper examines our use of
    Haskell as host for an embedded language.  In particular, we highlight our
    use of Haskell's lazy lists, type classes, lazy state monad, and unsafe
    Perform I0, and point to several areas where Haskell could be improved in
    the future.  We end with an example of a benefit gained by bringing the
    functional perspective to microarchitectural modelling.", 
  location     = "http://dx.doi.org/10.1145/317636.317784g"
}

@Article{iwwtbswj,
  author       = "Benton, Nick and Kennedy, Andrew",
  title        = "Interlanguage Working Without Tears: Blending {SML} with {J}ava",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN " # icfp # ", ICFP '99)",
  year         = 1999,
  volume       = 32,
  number       = 9,
  pages        = "126--137",
  month        = sep,
  keywords     = "foreign function interfaces, java, sml, null values,  ",
  abstract     = "A good foreign-language interface is crucial for the success
    of any modern programming language implementation.  Although all serious
    compilers for functional languages have some facility for interlanguage
    working, these are often limited and awkward to use.This article describes
    the features for bidirectional interlanguage working with Java that are
    built into the latest version of the MLj compiler.  Because the MLj foreign
    interface is to another high-level typed language which shares a garbage
    collector with compiled ML code, and because we are willing to extend the
    ML language, we are able to provide unusually powerful, safe and easy to
    use interlanguage working features.  Indeed, rather then being a
    traditional foreign interface, our language extensions are more a partial
    integration of Java features into SML.We describe this integration of
    Standard ML and Java, first informally with example program fragments, and
    then formally in the notation used by The Definition of Standard ML.",  
  location     = "http://dx.doi.org/10.1145/317636.317791"
}

@Article{sdegc,
  author       = "Coffman, E. G. and Elphick, M. and Shoshani, A.",
  title        = "System Deadlocks",
  journal      = surveys,
  year         = 1971,
  volume       = 3,
  number       = 2,
  pages        = "67--78",
  month        = jun,
  keywords     = "deadlock, deadly embrace, system deadlocks, multiprogramming,
  interlock problems, prevention, avoidance",
  abstract     = "A problem of increasing importance in the design of large
    multiprogramming systems is the, so-called, deadlock or deadly-embrace
    problem.  In this article we survey the work that has been done on the
    treatment of deadlocks from both the theoretical and practical points of
    view.", 
  location     = "http://dx.doi.org/10.1145/356586.356588"
}

@Article{tmlasotiipl,
  author       = "Ledgard, Henry~F.",
  title        = "Ten Mini-Languages: {A} Study of Topical Issues in Programming Languages",
  journal      = surveys,
  year         = 1971,
  volume       = 3,
  number       = 3,
  pages        = "115--146",
  month        = sep,
  keywords     = "computer science curriculum, language design, compiler
    design, formal definition, syntax, semantics, assignment, transfer of
    control, functions, parameter passing, type checking, string manipulation,
    data structure, input/output",
  abstract     = "The proliferation of programming languages has raised many 
    issues of language design, definition, and implementation.  This paper
    presents a series of ten mini-languages, each of which exposes salient
    features found in existing programming languages.  The value of the
    mini-languages lies in their brevity of description and the isolation of
    important linguistic features: in particular, the notions of assignment,
    transfer of control, functions, parameter passing, type checking, data
    structures, string manipulation, and input/output.  The mini-languages may
    serve a variety of uses: notably, as a pedagogical tool for teaching
    programming languages, as a subject of study for the design of programming
    languages, and as a set of test cases for methods of language
    implementation or formal definition.", 
  location     = "http://dx.doi.org/10.1145/356589.356592"
}

@Article{swam,
  author       = "Martin, William~A.",
  title        = "Sorting",
  journal      = surveys,
  year         = 1971,
  volume       = 3,
  number       = 4,
  pages        = "147--174",
  month        = dec,
  keywords     = "sorting, algorithm design, algorithm analysis",
  abstract     = "The bibliography appearing at the end of this article lists
    37 sorting algorithms and 100 books and papers on sorting published in the
    last 20 years.  The basic ideas presented here have been abstracted from
    this body of work, and the best algorithms known are given as examples.  As
    the algorithms are explained, references to related algorithms and
    mathematical or experimental analyses are given.  Suggestions are then made
    for choosing the algorithm best suited to a given situation.", 
  location     = "http://dx.doi.org/10.1145/356593.356594"
}

@Article{haxgcotbt,
  author       = "Malcolm Wallace and Colin Runciman",
  title        = "Haskell and {XML}:  Generic Combinators or Type-Based Translation?",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN " # icfp # ", ICFP '99)",
  year         = 1999,
  volume       = 32,
  number       = 9,
  pages        = "435--468",
  month        = sep,
  keywords     = "document markup langauges, xml, haskell, combinators,
    algebraic properties, dtds, functional programming",
  abstract     = "We present two complementary approaches to writing XML
    document-processing applications in a functional language.  In the first
    approach, the generic tree structure of XML documents is used as the basis
    for the design of a library of combinators for generic processing:
    selection, generation, and transformation of XML trees.  The second
    approach is to use a type-translation framework for treating XML document
    type definitions (DTDs) as declarations of algebraic data types, and a
    derivation of the corresponding functions for reading and writing documents
    as typed values in Haskell.", 
  location     = "http://dx.doi.org/10.1145/317765.317794"
}

@Article{apmgg,
  author       = "Gouda, Mohamed~G. and Herman, Ted",
  title        = "Adaptive Programming",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "911--921",
  month        = sep,
  keywords     = "distributed computing, adaptive systems, programming
    methodology, program composition, self-stabilization",
  abstract     = "An adaptive program is one that changes its behavior base on
    the current state of its environment.  This notion of adaptivity is
    formalized, and a logic for reasoning about adaptive programs is presented.
    The logic includes several composition operators that can be used to define
    an adaptive program in terms of given constituent programs; programs
    resulting from these compositions retain the adaptive properties of their
    constituent programs.  The authors begin by discussing adaptive sequential
    programs, then extend the discussion to adaptive distributed programs.  The
    relationship between adaptivity and self-stabilization is discussed.  A
    case study for constructing an adaptive distributed program where a token
    is circulated in a ring of processes is presented.", 
  location     = "http://dx.doi.org/10.1109/32.92911"
}

@Article{aaalfst,
  author       = "Cohen, Kenneth. and Wegstein, J.~H.",
  title        = "{AXLE}: An Axiomatic Language for String Transformations",
  journal      = cacm,
  year         = 1965,
  volume       = 8,
  number       = 11,
  pages        = "657--661",
  month        = nov,
  keywords     = "string processing, rewriting systems"
}

@Article{amtip,
  author       = "Hoffman, Daniel~M. and Strooper, Paul",
  title        = "Automated Module Testing in {P}rolog",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "934--943",
  month        = sep,
  keywords     = "testing, automation, prolog, modules, definitions, oracles",
  abstract     = "Tools and techniques for writing scripts in Prolog that
    automatically test modules implemented in C are presented.  Both the input
    generation and the test oracle problems are addressed, focusing on a
    balance between the adequacy of the test inputs and the cost of developing
    the output oracle.  The authors investigate automated input generation
    according to functional testing, random testing, and a novel approach based
    on trace invariants.  For each input generation scheme, a mechanism for
    generating the expected outputs has been developed.  The methods are
    described and illustrated in detail.  Script development and maintenance
    costs appear to be reasonable, and run-time performance appears to be
    acceptable.", 
  location     = "http://dx.doi.org/10.1109/32.92913"
}

@Article{scvp,
  author       = "Aggarwal, Aneesh",
  title        = "Software Caching Vs. Prefetching",
  journal      = sigplan # " (" # pot # " Third International Symposium on Memory Management, ISMM '02)",
  year         = 2002,
  volume       = 38,
  number       = 2,
  pages        = "157--162",
  month        = jan,
  abstract     = "The performance gap between memory subsystem and 
    high-performance processors is ever-increasing.  Prefetching is one method
    to bridge this performance gap.  Prefetching has been proposed for
    array-based and pointer applications, typically using software-based
    techniques with the help of the compiler.  Prefetching suffers from certain
    disadvantages such as an increase in memory traffic, an increase in the
    number of executed instructions, and an increase in memory requirement for
    some cases.In this paper, we investigate the technique of software caching
    for applications that perform searches or sorted insertions.  For data
    structures larger than the processor data cache, such a search or sorted
    insert may result in multiple cache misses before the correct value is
    found.  In a software caching approach, a small software buffer is
    maintained that records the most recently added values (these values are
    the ones that are later searched) along with their addresses and is
    consulted during a search or an insert.  In this paper, we present results
    of our initial experiments.  We found that for applications involving a
    search, software caching performs as high as 30% better than the original
    application.  Also, this technique executes upto 14% less instructions and
    the number of cache accesses decrease by around upto 18%.  We also compared
    this technique against software prefetching and found that in some cases,
    the performance improvement was as high as 40%.  This technique also has an
    added advantage of using less memory space than prefetching (all the
    improvements were obtained for very small software caches).  These initial
    results encourage a more in-depth study of this technique.", 
  keywords     = "optimization, memory management, caching, prefetching, data
    structures",
  location     = "http://dx.doi.org/10.1145/512429.512450"
}

@Article{ewtsc,
  author       = "Freudenberger, Stefan~M. and Schwartz, Jacob~T. and Sharir, Micha",
  title        = "Experience with the {SETL} Optimizer",
  journal      = toplas,
  year         = 1983,
  volume       = 5,
  number       = 1,
  pages        = "26--45",
  month        = jan,
  keywords     = "very high-level languages, setl, language processors, 
    optimization, automatic programming, algorithms analysis; program
    modification, program transformation, languages, automatic data-structure
    selection, copy optimization", 
  abstract     = "The structure of an existing optimizer for the very
    high-level, set theoretically oriented programming language SETL is
    described, and its capabilities are illustrated.  The use of novel
    techniques (supported by state-of-the-art interprocedural program analysis
    methods) enables the optimizer to accomplish various sophisticated
    optimizations, the most significant of which are the automatic selection of
    data representations and the systematic elimination of superfluous copying
    operations.  These techniques allow quite sophisticated data-structure
    choices to be made automatically.", 
  location     = "http://dx.doi.org/10.1145/357195.357197"
}

@Article{mcnbmd,
  author       = "Wolfson, Ouri and Sengupta, Soumitra and Yemini, Yechiam",
  title        = "Managing Communication Networks by Monitoring Databases",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "944--953",
  month        = sep,
  keywords     = "network management, database systems, language features,
    events, real-time databases",
  abstract     = "The problem of managing large communication networks using
    statistical tests, alerts, and correlation among alerts is considered.  The
    authors propose a model of these network management functions as
    data-manipulation operations.  They argue that this approach can improve
    the flexibility of network management systems by providing a language that
    is declarative and set-oriented.  These are properties of existing
    data-manipulation languages and it is shown that any data-manipulation
    language, augmented with several new capabilities, can serve as a language
    for specifying the aforementioned network management functions.  The new
    capabilities required are specification of events, correlation among
    events, and change-tracking.", 
  location     = "http://dl.acm.org/citation.cfm?id=126277"
}

@Article{gocspmfs,
  author       = "Steven~P. Reiss",
  title        = "Generation of Compiler Symbol Processing Mechanisms from Specifications",
  journal      = toplas,
  year         = 1983,
  volume       = 5,
  number       = 2,
  pages        = "127--163",
  month        = apr,
  keywords     = "symbol processing, compilation, non-procedureal
    specifications, ada, symbol tables, compiler generators",
  abstract     = "Compiler symbol processing has become complex as programming
    languages have evolved.  In this paper we describe a comprehensive model of
    symbol processing, and a system that uses this model to generate the symbol
    processing portions of a compiler from simple specifications.  The model
    supports a variety of entities with different types of names.  It includes
    a detailed view of scoping.  It provides a simple picture of the complex
    mapping from a source token to a specific entity.  The model is illustrated
    with examples from Ada.  The system is divided into two parts.  The first
    takes a nonprocedural description of the semantics of symbols in the
    language and produces a symbol table module for a compiler for that
    language.  The second supports a simple symbol processing language that
    allows the easy connection of syntax and semantic processing to the symbol
    module.", 
  location     = "http://dx.doi.org/10.1145/69624.69625"
}

@Article{amtfeosd,
  author       = "Sergio C{\' a}rdenas-Garc{\' \i}a and Marvin~V. Zelkowitz",
  title        = "A Management Tool For Evaluation of Software Designs",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "961--971",
  month        = sep,
  keywords     = "correctness, decision support systems, design evaluation,
    prototyping, risk analysis, software reuse.",
  location     = "http://www.cs.umd.edu/~mvz/pub/cardenas-zelkowitz-tse.pdf"
}

@Article{icarh,
  author       = "Hoare, C.~A.~R. and Allison, D.~C.~S.",
  title        = "Incomputability",
  journal      = surveys,
  year         = 1972,
  volume       = 4,
  number       = 3,
  pages        = "169--178",
  month        = sep,
  keywords     = "incomputability, logical paradox, termination, recursion,
    programming languages, theory of types, interpreter",
  abstract     = "Russell's logical paradox, formulated in terms of English
    adjectives, is cons]dered as a convenient starting point for this
    discussion of incomputablhty.  It is shown to be impossible, under a wide
    variety of circumstances, to program a function which will determine
    whether another function written in the same programming language wdl
    terminate.  The theory of types is introduced in an attempt to evade the
    paradox Finally, it is shown that any language containing conditionals and
    recursive function definitions, which is powerful enough to program its own
    interpreter, cannot be used to program its own terminates function.", 
  location     = "http://dx.doi.org/10.1145/356603.356606"
}

@Article{gaalsfrdp,
  author       = "Liskov, Barbara and Scheifler, Robert",
  title        = "Guardians and Actions: Linguistic Support for Robust, Distributed Programs",
  journal      = toplas,
  year         = 1983,
  volume       = 5,
  number       = 3,
  pages        = "381--404",
  month        = jul,
  keywords     = "languages,  reliability, atomicity, nested atomic actions,
    remote procedure call, clu, argus, distributed objects",
  abstract     = "An overview is presented of an integrated programming
    language and system designed to support the construction and maintenance of
    distributed programs: programs in which modules reside and execute at
    communicating, but geographically distinct, nodes.  The language is
    intended to support a class of applications concerned with the manipulation
    and preservation of long-lived, on-line, distributed data.  The language
    addresses the writing of robust programs that survive hardware failures
    without loss of distributed information and that provide highly concurrent
    access to that information while preserving its consistency.  Several new
    linguistic constructs are provided; among them are atomic actions, and
    modules called guardians that survive node failures.", 
  location     = "http://dx.doi.org/10.1145/2166.357215", 
  location     = "http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-824-distributed-computer-systems-engineering-spring-2006/readings/argus.pdf"
}

@Article{tpompurvmr,
  author       = "Tzou, Shin-Yuan and Anderson, David~P.",
  title        = "The Performance of Message-Passign Using Restricted Virtual Memory Remapping",
  journal      = spe,
  year         = 1991,
  volume       = 21,
  number       = 3,
  pages        = "251--267",
  month        = mar,
  keywords     = "performance, operating systems, virtual memory, remapping,
    message-passing Interprocess communication", 
  abstract     = "DASH is a distributed operating system kernel.
    Message-passing (MP) is used for local communication,and the MP system uses
    virtual memory (VM) remapping instead of software memory copying for moving
    large amounts of data between virtual address spaces.  Remapping eliminates
    a potential communication bottleneck and may increase the feasibility of
    moving services such as file services to the user level.  Previous systems
    that have used VM remapping for message transfer, however, have suffered
    from higher-operation delay, limiting the use of the technique.  The DASH
    design reduces this delay by restricting the generality of remapping: a
    fixed part of every space is reserved for remapping, and a page's virtual
    address does not change when it is moved between spaces.  We measured the
    performance of the DASH kernel for Sun 3/50 workstations, on which memory
    can be copied at 3.9 MB/s.  Using remapping, DASH can move large messages
    between user spaces at a rate of 39 MB/s if they are not referenced and
    24.8 MB/s if each page is referenced.  Furthermore, the per-operation delay
    is low, so VM remapping is beneficial even for messages containing only one
    page.  To further understand the performance of the DASH MP system, we
    broke an MP operation into short code segments and timed them with
    microsecond precision.  The results show the relative costs of data
    movement and the other components of MP operations, and allow us to
    evaluate several specific design decisions.", 
  location     = "http://dx.doi.org/10.1002/spe.4380210303"
}

@Article{chfhahfh,
  author       = "Finne, Sigbjorn and Leijen, Daan and Meijer, Erik and Peyton Jones, Simon",
  title        = "Calling Hell from Heaven and Heaven from Hell",
  journal      = sigplan # " (" # pot # "Second ACM SIGPLAN " # icfp # ", ICFP '99)",
  year         = 1999,
  volume       = 32,
  number       = 9,
  pages        = "114--125",
  month        = sep,
  keywords     = "com, foreign-function interfaces, argument marshaling",
  abstract     = "The increasing popularity of component-based programming
    tools offer a big opportunity to designers of advanced programming
    languages, such as Haskell.  If we can package our programs as software
    components, then it is easy to integrate them into applications written in
    other languages.In earlier work we described a preliminary integration of
    Haskell with Microsoft's Component Object Model (COM), focusing on how
    Haskell can create and invoke COM objects.  This paper develops that work,
    concentrating on the mechanisms that support externally-callable Haskell
    functions, and the encapsulation of Haskell programs as COM objects.", 
  location     = "http://dx.doi.org/10.1145/317765.317790"
}

@Article{psvoa,
  author       = "Barnes, Bruce~H.",
  title        = "Programmer's View of Automata",
  journal      = surveys,
  year         = 1972,
  volume       = 4,
  number       = 4,
  pages        = "221--239",
  month        = dec,
  keywords     = "automata, computer programming, finite state machine,
    deterministic automaton, nondeterministic automaton",
  abstract     = "This discussion introduces computer scientists to the 
    concepts, definitions, notation and ideas of finite automata theory and to
    suggest how these ideas might relate to some of the other fields of
    Computer Science.  Many of the ideas presented will be familiar to computer
    programmers, although they might not be immediately recognizable, for it is
    the precise nature of mathematics that renders these concepts tractable to
    investigation.  Since understanding of the basic concepts is the purpose of
    this discussion, more emphasis will be placed on definitions and examples
    than on theorems and proofs.  The principal benefit of the study of finite
    automata to the computer programmer is not in theories and theorems, but is
    rather in the insight and understanding it can contribute to many
    programming problems; specifically, in determining the underlying structure
    of the problem and in establishing how this structure is modified as the
    computation progresses.",  
  location     = "http://dx.doi.org/10.1145/356608.356610"
}

@Article{sostaom,
  author       = "J.~L. Baer",
  title        = "Survey of Some Theoretical Aspects of Multiprocessing",
  journal      = surveys,
  year         = 1973,
  volume       = 5,
  number       = 1,
  pages        = "31--80",
  month        = mar,
  keywords     = "multiprocessing, performance, parallelism, multlprocessing,
    mutual exclusion, semaphores, automatic paralleism detection, graph models,
    Petri  nets, flow graph schemata, scheduling, array processors, pipe-line
    computers.",
  abstract     = "This paper surveys several theoretical aspects of 
    multiprocessing.  First, we look at the language features that help exploit
    parallelism.  The additional instructions needed for a multiprocessor
    architecture; problems, such as mutual exclusion, raised by the concurrent
    processing of parts of a program; and the extensions to existing high-level
    languages are examined.  The methods for automatic detection of parallelism
    in current high-level languages are reviewed both at the inter- and
    intra-statement levels.  The following part of the paper deals with more
    theoretical aspects of multiprocessing.  Different models for parallel
    computation such as graph models, Petri nets, parallel flowcharts, and flow
    graph schemata are introduced.  Finally, multiprocessors performance
    prediction either through analysis of models or by simulation is examined.
    In an appendix, an attempt is made toward the classification of existing
    multlprocessors", 
  location     = "http://dx.doi.org/10.1145/356612.356615"
}

@Article{raaesipd,
  author       = "Colussi, L.",
  title        = "Recursion As an Effective Step in Program Development",
  journal      = toplas,
  year         = 1984,
  volume       = 6,
  number       = 1,
  pages        = "55--67",
  month        = jan,
  keywords     = "programming languages, pascal, language constructs, data
     structures, program specifying, program verifying, program reasoning,
     assertions, pre- and postconditions, program and recursion schemes
     algorithms, languages, theory, verification, correctness, iteration,
     program development, program transformations, program optimization, proof
     translation", 
  abstract     = "A general translation rule from recursive procedures to 
    iterative ones is augmented by also translating the correctness proof.  The
    augmented translation rule is defined within the framework of Pascal
    programs, with the correctness proof expressed in the Hoare style.  The
    translation is consistent with the stepwise development of Pascal programs.
    Moreover the translation can be done automatically provided that the
    assertions are expressed in a formal specification language.  Given are a
    few simplification rules, whose applicability, after the translation, is
    easily detectable and strongly suggested by the translated assertions.  It
    is shown that the particular cases of linear, tail, and simple recursion
    (which are known to be easily handled and simplified when expressed in a
    schematic functional notation) can be simplified automatically after the
    translation step without leaving the Pascal language.  Two examples are
    provided to show that the given simplification rules can also effectively
    apply to more general recursive procedures.", 
  location     = "http://dx.doi.org/10.1145/357233.357236"
}

@Article{tfascaifj,
  author       = "Flanagan, Cormac and Freund, Stephen~N. and Lifshin, Marina and Qadeer, Shaz",
  title        = "Types for Atomicity: Static Checking and Inference for Java",
  journal      = toplas,
  year         = 2008,
  volume       = 30,
  number       = 4,
  month        = jul,
  keywords     = "type systems, atomicity, verification, synchronization,
    specification",
  abstract     = "Atomicity is a fundamental correctness property in 
    multithreaded programs.  A method is atomic if, for every execution, there
    is an equivalent serial execution in which the actions of the method are
    not interleaved with actions of other threads.  Atomic methods are amenable
    to sequential reasoning, which significantly facilitates subsequent
    analysis and verification.  This article presents a type system for
    specifying and verifying the atomicity of methods in multithreaded Java
    programs using a synthesis of Lipton's theory of reduction and type systems
    for race detection.  The type system supports guarded, write-guarded, and
    unguarded fields, as well as thread-local data, parameterized classes and
    methods, and protected locks.  We also present an algorithm for verifying
    atomicity via type inference.  We have applied our type checker and type
    inference tools to a number of commonly used Java library classes and
    programs.  These tools were able to verify the vast majority of methods in
    these benchmarks as atomic, indicating that atomicity is a widespread
    methodology for multithreaded programming.  In addition, reported atomicity
    violations revealed some subtle errors in the synchronization disciplines
    of these programs.", 
  location     = "http://dx.doi.org/10.1145/1377492.1377495"
}

@Article{tlbdafa,
  author       = "Karam, Gerald~M. and Buhr, Raymond~J. A.",
  title        = "Temporal Logic-Based Deadlock Analysis for {A}da",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 10,
  pages        = "1109--1125",
  month        = oct,
  keywords     = "ada, environment, design, concurrent systems, deadlock,
  specification language, prolog, temporal logic, executable specifications",
  abstract     = "A temporal logic-based specification language and deadlock
    analyzer for Ada is described.  The deadlock analyzer is intended for use
    within Timebench, a concurrent system-design environment with support for
    Ada.  The specification language, COL, uses linear-time temporal logic to
    provide a formal basis for axiomatic reasoning.  The deadlock analysis tool
    uses the reasoning power of COL to demonstrate that Ada designs specified
    in COL are systemwide deadlock-free: in essence, it uses a specialized
    theorem prover to deduce the absence of deadlock.  The deadlock algorithm
    is shown to be decidable for finite systems and acceptable otherwise.  It
    is also shown to have a worst-case computational complexity that is
    exponential with the number of tasks.  The analyzer has been implemented in
    Prolog.  Numerous examples are evaluated using the analyzer, including
    readers and writers, gas station, five dining philosophers, and a layered
    communications system.  The results indicate that analysis time is
    reasonable for moderate designs in spite of the worst-case complexity of
    the algorithm .", 
  location     = "http://dx.doi.org/10.1109/32.99197"
}

@Article{cspfcadosd,
  author       = "Hull, M.~Elizabeth~C. and McKeag, R.~M.",
  title        = "Communicating Sequential Processes for Centralized and Distributed Operating System Design",
  journal      = toplas,
  year         = 1984,
  volume       = 6,
  number       = 2,
  pages        = "175--191",
  month        = apr,
  keywords     = "Design, Languages, operating systems, parallel programming,
    program design, communicating sequential processes, distributed systems,
    pascal-plus",
  abstract     = "This paper demonstrates how the notation of Communicating
    Sequential Processes may be used in the design of an operating system.  It
    goes further to show how such an approach assists in the design and
    development of a system distributed over a network of computers.  The
    technique uses a well-defined design methodology.", 
  location     = "http://dx.doi.org/10.1145/2993.2381"
}

@Article{pipp17,
  author       = "Rosenfeld, Azriel",
  title        = "Progress in Picture Processing: 1969--71",
  journal      = surveys,
  year         = 1973,
  volume       = 5,
  number       = 2,
  pages        = "81--108",
  month        = jun,
  keywords     = "picture processing, image processing, pattern recognition,
    bandwidth compression, encoding, approximation, spatial filtering, image
    enhancement, picture properties, segmentation, geometrical properties,
    scene analysis, picture languages.", 
  abstract     = "Developments in the field of picture processing by computer
    during 1969-71 are surveyed.  The topics covered include picture
    compression, image enhancement, pictorial pattern recognition, scene
    analysis, and picture grammars.",
  location     = "http://dx.doi.org/10.1145/356616.356617"
}

@Article{tsmic,
  author       = "Grossman, Dan",
  title        = "Type-Safe Multithreading in {C}yclone",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN International Workshop on Types in Langauge, Design and Implementation)",
  year         = 2003,
  volume       = 38,
  number       = 3,
  pages        = "13--25",
  month        = mar,
  keywords     = "concurrent programming structures, operational semantics,
    data races, cyclone, static analysis, abstract machines",
  abstract     = "We extend Cyclone, a type-safe polymorphic language at the C
    level of abstraction, with threads and locks.  Data races can violate type
    safety in Cyclone.  An extended type system statically guarantees their
    absence by enforcing that thread-shared data is protected via locking and
    that thread-local data does not escape the thread that creates it.  The
    extensions interact smoothly with parametric polymorphism and region-based
    memory management.  We present a formal abstract machine that models the
    need to prevent races, a polymorphic type system for the machine that
    supports thread-local data, and a corresponding type-safety result.", 
  location     = "http://dx.doi.org/10.1145/604174.604177",
  location     = "http://homes.cs.washington.edu/~djg/papers/cycthreads.pdf"
}

@Article{eobsat,
  author       = "Couger, J.~Daniel",
  title        = "Evolution of Business System Analysis Techniques",
  journal      = surveys,
  year         = 1973,
  volume       = 5,
  number       = 3,
  pages        = "167--198",
  month        = sep,
  keywords     = "business system analysis, decision table processor, forms
    flow chart, information process chart, logical system design, problem
    statement analyzer, problem statement language, physical system design,
    system development cycle, time automated grid", 
  abstract     = "Until recently, the evolution of business system analysis
    lagged hardware evolution by one full generation.  During the first
    generation of business-oriented computers, in the 1950s, system analysts
    used unit record-oriented analysis and design techniques.  Between 1960 and
    1970, computer oriented techniques for system analysis were developed.  The
    gap has narrowed to half a generation.  Third generation techniques emerged
    six years after the first installation of third generation computers.  The
    evolution of techniques for business system analysis is described, with
    emphasis on the requirements of new techniques to handle today's
    complicated business systems.", 
  location     = "http://dx.doi.org/10.1145/356619.356621"
}

@Article{toagip,
  author       = "Katayama, Takuya",
  title        = "Translation of Attribute Grammars into Procedures",
  journal      = toplas,
  year         = 1984,
  volume       = 6,
  number       = 3,
  pages        = "345--369",
  month        = jul,
  keywords     = "algorithms, languages, attribute grammar, attribute
    evaluator, compiler generator",
  abstract     = "An efficient method for evaluating attribute grammars by 
    translating them into sets of procedures is presented.  The basic idea
    behind the method is to consider nonterminal symbols of the grammar as
    functions that map their inherited attributes to their synthesized
    attributes.  Associated with the nonterminal symbols are procedures that
    realize the functions.  The attribute grammar is translated into a program
    consisting of these procedures.  The essential point about this method is
    that attribute grammars are completely compiled into procedures, in
    contrast with evaluation algorithms that work interpretively in a
    table-driven manner.  No information is stored in the nodes of derivation
    trees.  Although this evaluation method is applicable principally to
    absolutely noncircular attribute grammars in which the dependency relation
    among attribute occurrences of every production rule does not contain
    cycles, it is shown how the method is extended to the general noncircular
    attribute grammars.  Problems of evaluating a set of attributes
    simultaneously and of recursive descent evaluation are also discussed.", 
  location     = "http://dx.doi.org/10.1145/579.586"
}

@Article{cftavatacsc,
  author       = "Richard~A. Kemmerer and Phillip~A. Porras",
  title        = "Covert Flow Trees:  {A} Visual Approach to Analyzing Covert Storage Channels",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1166--1185",
  month        = nov,
  keywords     = "attributes, covert channel operation sequences, covert flow
    tree, covert storage channels, listening process, resource attribute, tree
    structure, security analysis, security tools, data security",
  abstract     = "The authors introduce a technique for detecting covert 
    storage channels using a tree structure called a covert flow tree (CFT).
    CFTs are used to perform systematic searches for operation sequences that
    allow information to be relayed through attributes and eventually detected
    by a listening process.  When traversed, the paths of a CFT yield a
    comprehensive list of operation sequences which support communication via a
    particular resource attribute.  These operation sequences are then analyzed
    and either discharged as benign or determined to be covert communication
    channels.  Algorithms for automating the construction of CFTs and potential
    covert channel operation sequences are presented.  To illustrate this
    technique, two example systems are analyzed and their results compared to
    two currently accepted analysis techniques performed on identical systems.
    This comparison shows that the CFT approach not only identified all covert
    storage channels found by the other analysis techniques, but discovered a
    channel not detected by the other techniques", 
  location     = "http://dx.doi.org/10.1109/32.106972"
}

@Article{tpaasitp,
  author       = "Richard~S. Bird",
  title        = "The Promotion and Accumulation Strategies in Transformational Programming",
  journal      = toplas,
  year         = 1984,
  volume       = 6,
  number       = 4,
  pages        = "487--504",
  month        = oct,
  keywords     = "programming, code transformations, algorithms, verification,
    transformational programming, operators, algorithm refinement",
  abstract     = "The promotion strategy in transformational programming is a 
    general method for achieving efficiency by exploiting the recursive
    structure in the dominant term of an algorithmic expression.  For it to be
    carried out successfully, the original problem often has to be generalized
    by the inclusion of an extra parameter, a technique known as accumulation.
    These strategies are illustrated by deriving algorithms for two nontrivial
    problems: (1) determining whether a given digraph is acyclic, and (2)
    constructing the longest subsequence of a given sequence of vertices that
    forms a connected path in a given digraph.  The derivations also serve to
    emphasize the usefulness of certain notational devices in applicative
    expressions, especially infix operators.  Not only do such operators
    enhance the succinctness and readability of expressions, they also allow
    many transformations to be formulated as algebraic laws about their
    distributive and other properties.", 
  location     = "http://dx.doi.org/10.1145/1780.1781"
}

@Article{gcil,
  author       = "David Gelernter",
  title        = "Generative Communication in {L}inda",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 1,
  pages        = "80--112",
  month        = jan,
  keywords     = "distributed programming languages, generative communication,
    linda, tuple-space communication", 
  abstract     = "Generative communication is the basis of a new distributed
    programming langauge that is intended for systems programming in
    distributed settings generally and on integrated network computers in
    particular.  It differs from previous interprocess communication models in
    specifying that messages be added in tuple-structured form to the
    computation environment, where they exist as named, independent entities
    until some process chooses to receive them.  Generative communication
    results in a number of distinguishing properties in the new language,
    Linda, that is built around it.  Linda is fully distributed in space and
    distributed in time; it allows distributed sharing, continuation passing,
    and structured naming.  We discuss these properties and their implications,
    then give a series of examples.  Linda presents novel implementation
    problems that we discuss in Part II.  We are particularly concerned with
    implementation of the dynamic global name space that the generative
    communication model requires.", 
  location     = "http://dx.doi.org/10.1145/2363.2433"
}

@Article{lpava,
  author       = "Aho, Albert~V. and Johnson, Steven~C.",
  title        = "{LR} Parsing",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 2,
  pages        = "99--124",
  month        = jun,
  keywords     = "grammars, parsers, compilers, ambiguous grammars,
     context-free languages, LR grammars, parser generators, error detection
     and recovery",
  abstract     = "The LR syntax analysis method is a useful and versatile
     technique for parsing deterministic context-free languages in compiling
     applications.  This paper provides an informal exposition of LR parsing
     techniques emphasizing the mechanical generation of efficient LR parsers
     for context-free grammars.  Particular attention is given to extending the
     parser generation techniques to apply to ambiguous grammars.", 
  location     = "http://dx.doi.org/10.1145/356628.356629"
}

@Article{nlpafs,
  author       = "Collobert, Ronan and Weston, Jason and Bottou, L{\' e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel",
  title        = "Natural Language Processing (almost) from Scratch",
  journal      = "The Journal of Machine Learning Research",
  year         = 2011,
  volume       = 12,
  pages        = "2493--2537",
  month        = aug,
  keywords     = "natural language processing, neural networks, deep learning,
    part-of-speech tagging, chunking, semantic role labeling, feature vectors,
    tagging scheme, training, feature extraction, unlabeled data",
  abstract     = "We propose a unified neural network architecture and learning
    algorithm that can be applied to various natural language processing tasks
    including part-of-speech tagging, chunking, named entity recognition, and
    semantic role labeling.  This versatility is achieved by trying to avoid
    task-specific engineering and therefore disregarding a lot of prior
    knowledge.  Instead of exploiting man-made input features carefully
    optimized for each task, our system learns internal representations on the
    basis of vast amounts of mostly unlabeled training data.  This work is then
    used as a basis for building a freely available tagging system with good
    performance and minimal computational requirements." 
}

@Article{moichwml,
  author       = "Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O'Reilly, Una-May",
  title        = "Meta Optimization:  Improving compiler Heuristics with Machine Learning",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation, PLDI '03)",
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "77--90",
  month        = may,
  keywords     = "machine learning, priority functions, generic programming,
    compiler heruistics",
  abstract     = "Compiler writers have crafted many heuristics over the years
    to approximately solve NP-hard problems efficiently.  Finding a heuristic
    that performs well on a broad range of applications is a tedious and
    difficult process.  This paper introduces Meta Optimization, a methodology
    for automatically fine-tuning compiler heuristics.  Meta Optimization uses
    machine-learning techniques to automatically search the space of compiler
    heuristics.  Our techniques reduce compiler design complexity by relieving
    compiler writers of the tedium of heuristic tuning.  Our machine-learning
    system uses an evolutionary algorithm to automatically find effective
    compiler heuristics.  We present promising experimental results.  In one
    mode of operation Meta Optimization creates application-specific heuristics
    which often result in impressive speedups.  For hyperblock formation, one
    optimization we present in this paper, we obtain an average speedup of 23%
    (up to 73%) for the applications in our suite.  Furthermore, by evolving a
    compiler's heuristic over several benchmarks, we can create effective,
    general-purpose heuristics.  The best general-purpose heuristic our system
    found for hyperblock formation improved performance by an average of 25% on
    our training set, and 9% on a completely unrelated test set.  We
    demonstrate the efficacy of our techniques on three different optimizations
    in this paper: hyperblock formation, register allocation, and data
    prefetching.", 
  location     = "http://groups.csail.mit.edu/commit/papers/03/metaopt-pldi.pdf"
}

@Article{paai,
  author       = "Dershowitz, Nachum",
  title        = "Program Abstraction and Instantiation",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 3,
  pages        = "446--477",
  month        = jul,
  keywords     = "verification, abstraction, analogy, instantiation, program
    schemas, correctness proofs, axiomatic semantics",
  abstract     = "Our goal is to develop formal methods for abstracting a given
    set of programs into a program schema and for instantiating a given schema
    to satisfy concrete specifications.  Abstraction and instantiation are two
    important phases in software development which allow programmers to apply
    knowledge learned in the solutions of past problems when faced with new
    situations.  For example, from two programs using a linear (or binary)
    search technique, an abstract schema can be derived that embodies the
    shared idea and that can be instantiated to solve similar new problems.
    Along similar lines, the development and application of program
    transformations are considered.  We suggest the formulation of analogies as
    a basic tool in program abstraction.  An analogy is first sought between
    the specifications of the given programs; this yields an abstract
    specification that may be instantiated to any of the given concrete
    specifications.  The analogy is then used as a basis for transforming the
    existing programs into an abstract schema that represents the embedded
    technique, with the invariant assertions and correctness proofs of the
    given programs helping to verify and complete the analogy.  A given
    concrete specification of a new problem may then be compared with the
    abstract specification of the schema to suggest an instantiation of the
    schema that yields a correct program.", 
  location     = "http://dx.doi.org/10.1145/3916.3986"
}

@Article{bstafo,
  author       = "Nievergelt, J.",
  title        = "Binary Search Trees and File Organization",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 3,
  pages        = "195--207",
  month        = sep,
  keywords     = "Search, search tree, search time, binary search tree, optimal
    tree, balanced tree, height-balance, weight-balance, file, static file,
    dynamic file, file organization, file processing, restructuring of file,
    random access, sequential access, access frequencies", 
  abstract     = "Binary search trees are an important technique for organizing
    large files because they are efficient for both random and sequential
    access of records, and for modification of a file.  Because of this, they
    have received a great deal of attention in recent years, and their
    properties are now better understood than those of most other file
    organization methods.  This paper surveys the main results which have been
    obtained.", 
  location     = "http://dx.doi.org/10.1145/356631.356634"
}

@Article{ccdasmp,
  author       = "Gill, Geoffrey~K. and Kemerer, Chris~F.",
  title        = "Cyclomatic Complexity Density and Software Maintenance Productivity",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 12,
  pages        = "1284--1288",
  month        = dec,
  keywords     = "management, measurement, performance, software productivity,
    software maintenance, software complexity, mccabe metrics, cyclomatic
    complexity",
  abstract     = "A study of the relationship between the cyclomatic complexity
    metric (T.  McCabe, 1976) and software maintenance productivity, given that
    a metric that measures complexity should prove to be a useful predictor of
    maintenance costs, is reported.  The cyclomatic complexity metric is a
    measure of the maximum number of linearly independent circuits in a program
    control graph.  The current research validates previously raised concerns
    about the metric on a new data set.  However, a simple transformation of
    the metric is investigated whereby the cyclomatic complexity is divided by
    the size of the system in source statements.  thereby determining a
    complexity density ratio.  This complexity density ratio is demonstrated to
    be a useful predictor of software maintenance productivity on a small pilot
    sample of maintenance projects.", 
  location     = "http://dx.doi.org/10.1109/32.106988"
}

@Article{citrw,
  author       = "Condit, Jeremy and Harren, Matthew and McPeak, Scott and Necula, George~C. and Weimer, Westley",
  title        = "{CCured} in the Real World",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation, PLDI '03)",
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "232--244",
  month        = may,
  keywords     = "reliability, experimentation, security, languages, type
  safety, memory safety, c, run-time type information, library compatibility",
  abstract     = "CCured is a program transformation system that adds memory
    safety guarantees to C programs by verifying statically that memory errors
    cannot occur and by inserting run-time checks where static verification is
    insufficient.This paper addresses major usability issues in a previous
    version of CCured, in which many type casts required the use of pointers
    whose representation was expensive and incompatible with precompiled
    libraries.  We have extended the CCured type inference algorithm to
    recognize and verify statically a large number of type casts; this goal is
    achieved by using physical subtyping and pointers with run-time type
    information to allow parametric and subtype polymorphism.  In addition, we
    present a new instrumentation scheme that splits CCured's metadata into a
    separate data structure whose shape mirrors that of the original user data.
    This scheme allows instrumented programs to invoke external functions
    directly on the program's data without the use of a wrapper function.With
    these extensions we were able to use CCured on real-world security-critical
    network daemons and to produce instrumented versions without memory-safety
    vulnerabilities.", 
  location     = "http://www.cs.berkeley.edu/~necula/Papers/ccured_pldi03.pdf",
  location     = "http://dx.doi.org/10.1145/780822.781157"
}

@Article{aaocswtcet,
  author       = "Avrunin, George~S. and Buy, Ugo~A. and Corbett, James~C. and Dillon, Laura~K. and Wileden, Jack~C.",
  title        = "Automated Analysis of Concurrent Systems With the Constrained Expression Toolset",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1204--1222",
  month        = nov,
  keywords     = "concurrent systems, automated analysis, analysis tools,
    experimental evaluation, toolset performance, constrained expressions,
    formal methods, event-based model",
  abstract     = "The constrained expression approach to analysis of concurrent
    software systems can be used with a variety of design and programming
    languages and does not require a complete enumeration of the set of
    reachable states of the concurrent system.  The construction of a toolset
    automating the main constrained expression analysis techniques and the
    results of experiments with that toolset are reported.  The toolset is
    capable of carrying out completely automated analyses of a variety of
    concurrent systems, starting from source code in an Ada-like design
    language and producing system traces displaying the properties represented
    bv the analysts queries.  The strengths and weaknesses of the toolset and
    the approach are assessed on both theoretical and empirical grounds.", 
  location     = "http://dx.doi.org/10.1109/32.106975"
}

@Article{fpadai,
  author       = "Charles~R. Symons",
  title        = "Function Point Analysis:  Difficulties and Improvements",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "2--11",
  month        = jan,
  keywords     = "complexity, estimating, function points maintenance,
    productivity, system development",
  abstract     = "The method of function point analysis was developed by A.  J.
    Albrecht (1979) to help measure the size of a computerized business
    information system.  Such sizes are needed as a component of the
    measurement of productivity in system development and maintenance
    activities, and as a component of estimating the effort needed for such
    activities.  Close examination of the method shows certain weaknesses, and
    the author proposes a partial alternative.  A description is given of the
    principles of this Mark II approach.  The results are presented of some
    measurements of actual systems to calibrate the Mark II approach, and
    conclusions are drawn on the validity and applicability of function point
    analysis generally.", 
  location     = "http://dx.doi.org/10.1109/32.4618"
}

@Article{htm,
  author       = "Maurer, W.~D. and Lewis, T.~G.",
  title        = "Hash Table Methods",
  journal      = surveys,
  year         = 1975,
  volume       = 7,
  number       = 1,
  pages        = "5--19",
  month        = mar,
  keywords     = "hash code, hashing function, randomizing technique,
    key-to-address transform, hash table, scatter storage, searching, symbol
    table, file systems, file search, bucket, collision, synonyms",
  abstract     = "This is a tutorial survey of hash table methods, chiefly
    intended for programmers and students of programming who are encountering
    the subject for the first time.  The better-known methods of calculating
    hash addresses and of handling collisions and bucket overflow are presented
    and compared.  It is shown that under certain conditions we can guarantee
    that no two items belonging to a certain class will have the same hash
    code, thus providing an improvement over the usual notion of a hash code as
    a randomizing technique.  Several alternatives to hashing are discussed,
    and suggestions are made for further research and further development.", 
  location     = "http://dx.doi.org/10.1145/356643.356645"
}

@Article{spdrgd,
  author       = "R.~Geoff Dromey",
  title        = "Systematic Program Development",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "12--29",
  month        = jan,
  keywords     = "constructive program proof, formal specifications,
    goal-oriented programming, invariants, post-conditions, program derivation,
    top-down refinement, weakest preconditions",
  abstract     = "A constructive method of program development is presented.
    It is based on a simple strategy for problem decomposition that is claimed
    to be more supportive of goal-oriented programming than the Wirth-Dijkstra
    top-down refinement method.  With the proposed method, a program is
    developed by making a sequence of refinements, each of which can establish
    the postcondition for a corresponding sequence of progressively weaker
    preconditions until a mechanism has been composed that will establish the
    postcondition for the original given precondition for the problem.  The
    strategy can minimize case analysis, simplify constructive program proofs,
    and ensure a correspondence between program structure and data structure.",
  location     = "http://dx.doi.org/10.1109/32.4619"
}

@Article{rarpcmsodak,
  author       = "Panzieri, Fabio and Shrivastava, Santosh~K.",
  title        = "Rajdoot:  {A} Remote Procedure Call Mechanism Supporting Orphan Detection and Killing",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "30--37",
  month        = jan,
  keywords     = "distributed systems, fault tolerance, interprocess
    communication, network protocols, remote procedure calls",
  abstract     = "Rajdoot is a remote procedure call (RPC) mechanism with a 
    number of fault tolerance capabilities.  A discussion is presented of the
    reliability-related issues and how these issues have been dealt with in the
    RPC design.  Rajdoot supports exactly-once semantics with call nesting
    capability, and incorporates effective measures for orphan detection and
    killing.  Performance figures show that the reliability measures of Rajdoot
    impose little overhead.", 
  location     = "http://dx.doi.org/10.1109/32.4620"
}

@Book{natrbp,
  author       = "Rober~B. Parker",
  title        = "Now and Then",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2007,
  address      = nyny,
  keywords     = "the trickster, murrdaar",
  location     = "PS 3566.A6868 N69"
}

@Book{sdrbp,
  author       = "Rober~B. Parker",
  title        = "School Days",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2005,
  address      = nyny,
  keywords     = "slaughter de jour, murrdaar",
  location     = "PS 3566.A6868 N69"
}

@Book{osid,
  author       = "Thomas~W. Doeppner",
  title        = "Operating Systems in Depth",
  publisher    = "Wiley",
  year         = 2011,
  keywords     = "operating systems, multithreaded programming, processor
    management, file systems, memory management, security, networking,
    distributed file systems",
  location     = "QA 76.76.O63 D64"
}

@Book{aiama,
  author       = "Stuart Russell and Peter Norvig",
  title        = "Artificial Intelligence: {A} Modern Approach",
  publisher    = "Prentice Hall",
  year         = 2010,
  address      = srnj,
  edition      = "third",
  keywords     = "artificial intelligence, agents, search, constraint
    satisfaction, first-order logic, inference, planning, knowledge
    representations, uncertainty, probabilistic reasoning, decision making,
    leaning, natural language processing, perception, robotics",
  location     = "Q 335 R86"
}

@Book{isppap,
  author       = "Robert~J. Schalkoff",
  title        = "Intelligent Systems:  Principles, Pradigms, and Pragmatics",
  publisher    = "Jones and Bartlett",
  year         = 2011,
  address      = "Sudbury, " # MA,
  keywords     = "search, constraint satisfaction, natural language
    understanding, production systems, soar, uncertainty, fuzzy systems,
    planning, neural networks, learning, evolutionary computing",
  location     = "QA 76.76 I58 S323"
}

@Book{tiloeb,
  author       = "David Bromwich",
  title        = "The Intellectual Life of Edmund Burke",
  publisher    = "Harvard University Press",
  year         = 2014,
  address      = cma,
  keywords     = "edmund burke, aesthetics, the revolutionary war, democracy,
    political thought, politics",
  location     = "DA 506 B9 B69"
}

@Book{tips,
  author       = "Paul Vaderlind and Richard~K. Guy and Loren~C. Larsen",
  title        = "The Inquisitive Problem Solver",
  publisher    = "The Mathematical Association of America",
  year         = 2002,
  address      = "Washington, D.C.",
  keywords     = "mathematics, problems, problem solving",
  location     = "QA 43.V28 2002"
}

@Book{bbrbp,
  author       = "Robert~B. Parker",
  title        = "Bad Business",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2004,
  address      = nyny,
  keywords     = "financial chicanery, murrdaar",
  location     = "PS 3566.A686 B34"
}

@Book{hifh,
  author       = "Helen Macdonald",
  title        = "{H} Is for Hawk",
  publisher    = "Grove Press",
  year         = 2014,
  address      = nyny,
  keywords     = "hawks, death, history",
  location     = "QL 696.F3 M324"
}

@Book{apc,
  author       = "Peter Carey",
  title        = "Amnesia",
  publisher    = "Knopf",
  year         = 2015,
  address      = nyny,
  keywords     = "hackers",
  location     = "PR 9619.3 C36 A83"
}

@Book{hmrbp,
  author       = "Robert~B. Parker",
  title        = "Hush Money",
  publisher    = "G.~P. Putnam's Sons",
  year         = 1999,
  address      = nyny,
  keywords     = "politics, outery, hypocrisy, murrdaar",
  location     = "PS 3566.A686 H87"
}

@Book{prbp,
  author       = "Robert~B. Parker",
  title        = "Potshot",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2001,
  address      = nyny,
  keywords     = "real estate, murrdaar",
  location     = "PS 3566.A686 P68"
}

@Book{tbaoon,
  author       = "Stephen Pinker",
  title        = "The Better Angels of our Nature",
  publisher    = "Penguin",
  year         = 2011,
  address      = nyny,
  keywords     = "violence, pacification, civilization, humanitarianism,
    warfare, rights, psychology, moral systems, politics",
  location     = "HM 111.6 P57"
}

@Book{rrju,
  author       = "John Updike",
  title        = "Rabbit Redux",
  publisher    = "Knopf",
  year         = 1971,
  address      = nyny,
  keywords     = "sucking in the 60s",
  location     = "PS 3571.P4"
}

@Book{srbp,
  author       = "Robert~B. Parker",
  title        = "Sixkill",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2011,
  address      = nyny,
  keywords     = "wha happened?, murrdaar",
  location     = "PS 3566.A686 S57"
}

@Book{tpwg,
  author       = "William Gibson",
  title        = "The Peripheral",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2014,
  address      = nyny,
  keywords     = "the future is now",
  location     = "PS 3557 I2264 P47"
}

@Book{igdeb,
  author       = "Eric Broug",
  title        = "Islamic Geometric Design",
  publisher    = "Thames \& Hudson",
  year         = 2013,
  address      = nyny,
  keywords     = "design, art, geometry",
  location     = "NK 1270 B7613"
}

@Book{jmhgu,
  author       = "Harlow Giles Unger",
  title        = "John Marshall",
  publisher    = "Da Capo Press",
  year         = 2014,
  address      = boma,
  keywords     = "john marshall, revolutionary america, federalism, the supreme
    court",
  location     = "KF 8745.M3 U54"
}

@Book{fmtgp,
  author       = "Alexander~A. Stepanov and Daniel~E. Rose",
  title        = "From Mathematics to Generic Programming",
  publisher    = aw,
  year         = 2015,
  address      = usrnj,
  keywords     = "generic programming, abstract algebra, gcd, cryptography",
  location     = "QA 76.6245 S74"
}

@Book{cttahyop,
  author       = "Haruki Murakami",
  title        = "Colorless Tsukuru Tazaki and His Years of Pilgrimage",
  publisher    = "Knopf",
  year         = 2014,
  address      = nyny,
  keywords     = "mystery",
  location     = "PL 856 U673 S5513"
}

@Book{srbp1990,
  author       = "Robert~B. Parker",
  title        = "Stardust",
  publisher    = "G.~P. Putnam's Sons",
  year         = 1990,
  address      = nyny,
  keywords     = "herassment, murrdaar",
  location     = "PS 3566.A686 S7"
}

@Book{tgc1,
  author       = "John Kenneth Galbraith",
  title        = "The Great Crash 1929",
  publisher    = "Mariner Books",
  year         = 1997,
  address      = boma,
  keywords     = "the great crash, economics, politics, finance, wall street,
    human nature",
  location     = "HB 3717 1929.G32 1997"
}

@Book{tholk,
  author       = "George~H. Marcus and William Whitaker",
  title        = "The Houses of Louis Kahn",
  publisher    = yup,
  year         = 2013,
  address      = nhco,
  keywords     = "lewis kahn, architecture, houses",
  location     = "NA 737.K32 M35"
}

@Book{tlats,
  author       = "Lan Cao",
  title        = "The Lotus and the Storm",
  publisher    = "Viking",
  year         = 2014,
  address      = nyny,
  keywords     = "vietnam, history, duplicity, diaspora",
  location     = "PS 3553.A5823 L68"
}

@Book{dagbs,
  author       = "Bruce Schneier",
  title        = "Data and Goliath",
  publisher    = "W. W. Norton",
  year         = 2015,
  address      = nyny,
  keywords     = "data surveillance, nsa, edward snowdon, privacy, security, 
    internet",
  location     = "HM 846.S362"
}

@Book{smrbp,
  author       = "Robert~B. Parker",
  title        = "Sudden Mischief",
  publisher    = "G.~P. Putnam's Sons",
  year         = 1998,
  address      = nyny,
  keywords     = "bad decisions, murrdaar",
  location     = "PS 3566.A686 S83"
}

@Book{naja,
  author       = "Jane Austen",
  title        = "Northanger Abbey",
  publisher    = "Anchor Books",
  year         = 2013,
  address      = nyny,
  keywords     = "innocence, impressionable youth, metafiction",
  location     = "PR 4034 N7"
}

@Book{tarbp,
  author       = "Robert~B. Parker",
  title        = "Thin Air",
  publisher    = "G.~P. Putnam's Sons",
  year         = 1995,
  address      = nyny,
  keywords     = "delusion",
  location     = "PS 3566.A686 T47"
}

@Book{pdra,
  author       = "Renata Adler",
  title        = "Pitch Dark",
  publisher    = "NYRB",
  year         = 2013,
  address      = nyny,
  keywords     = "women in travel",
  location     = "PS 3551 D63 P5"
}

@Book{fgnh,
  author       = "Nick Hornby",
  title        = "Funny Girl",
  publisher    = "Riverhead Books",
  year         = 2014,
  address      = nyny,
  keywords     = "comedy, careerism, television entertainments",
  location     = "PR 6058 O689 F86"
}

@Book{nadbe,
  author       = "Barbara Ehrenreich",
  title        = "Nickel and Dimed",
  publisher    = "Metropolitian Books",
  year         = 2001,
  address      = nyny,
  keywords     = "minimum wage, unskilled labor, poverty, class warefare,
    waitressing, maid service, retail clerks",
  location     = "HD 4918 E375"
}

@Book{wswrbp,
  author       = "Robert~B. Parker",
  title        = "Widow's Walk",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2003,
  address      = nyny,
  keywords     = "murrdaar, hidden shame",
  location     = "PS 3566 A686 W53"
}

@Book{mwg,
  author       = "Walter Sinnott-Armstrong",
  title        = "Morality Without God",
  publisher    = oup,
  year         = 2009,
  address      = oxuk,
  keywords     = "theology, morality, religion, ethics, atheism",
  location     = "BJ 47.S49"
}

@Book{tprbp,
  author       = "Robert~B. Parker",
  title        = "The Professional",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2009,
  address      = nyny,
  keywords     = "sexual pathology, murrdaar",
  location     = "PS 3566 A686 P75"
}

@Book{dpcb11,
  title        = "Dorothy Parker: Complete Broadway, 1918--1923",
  publisher    = "iUniverse",
  year         = 2014,
  editor       = "Kevin~C. Fitzpatrick",
  address      = "Bloomington, Indiana",
  keywords     = "broadway, reviews, theater",
  location     = "978 1 4917 2267 1"
}

@Book{acifd,
  title        = "Android Cookbook",
  publisher    = "O'Reilly",
  year         = 2012,
  editor       = "Ian~F. Darwin",
  address      = seca,
  keywords     = "android, mobile programming, java",
  location     = "QA 76.76.A65"
}

@Book{aidwsmm,
  author       = "Walter Mosley",
  title        = "All I Did Was Shoot My Man",
  publisher    = "Riverhead Books",
  year         = 2012,
  address      = nyny,
  keywords     = "treachery, murrdaar",
  location     = "PS 3563.O88456 A77 2012"
}

@Book{sws,
  author       = "Will Self",
  title        = "Shark",
  publisher    = "Grove Press",
  year         = 2014,
  address      = nyny,
  keywords     = "stream of consciousness",
  location     = "PR 6069.E3654 S53 2014"
}

@Book{amb,
  author       = "Mark Blyth",
  title        = "Austerity",
  publisher    = oup,
  year         = 2013,
  address      = nyny,
  keywords     = "austerity, economic policy, economic history, fiscal policy",
  location     = "HJ 8015 B59"
}

@Book{bmag,
  author       = "Atul Gawande",
  title        = "Being Mortal",
  publisher    = "Metropolitan Books",
  year         = 2014,
  address      = nyny,
  keywords     = "death, end of life, hospices, mortatlity, geriatrics",
  location     = "R 726.8"
}

@Book{bfwm,
  author       = "Walter Mosley",
  title        = "Blonde Faith",
  publisher    = "Little, Brown",
  year         = 2007,
  address      = nyny,
  keywords     = "murrdaar, bygons being bygons",
  location     = "PS 3563.O88456 B59"
}

@Book{wtcwwo,
  author       = "Chandra Manning",
  title        = "What This Cruel War Was Over",
  publisher    = "Vintage Books",
  year         = 2007,
  address      = nyny,
  keywords     = "civil war, slavery",
  location     = "E 607.M32"
}

@Book{dtldttdte,
  author       = "Valerie Curtis",
  title        = "Don't Look, Don't Touch, Don't Eat",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "aversion, hygiene, evolution, manners, social organization,
    disease",
  location     = "BF 575.A886 C87"
}

@Book{msmesa,
  author       = "Edward {St. Aubyn}",
  title        = "Mother's Milk",
  publisher    = "Open City Books",
  year         = 2005,
  address      = nyny,
  keywords     = "parenthood, parents, dying",
  location     = "PR 6069.T134"
}

@Book{tpsd,
  author       = "Dario Fo",
  title        = "The Pope's Daughter",
  publisher    = "Europa Editions",
  year         = 2015,
  address      = nyny,
  keywords     = "italy, politics, religion",
  location     = ""
}

@Book{ckwm,
  author       = "Walter Mosley",
  title        = "Cinnamon Kiss",
  publisher    = "Little, Brown",
  year         = 2005,
  address      = nyny,
  keywords     = "the long arm of history, murrdaar",
  location     = "PS 3563.O88456 C56"
}

@Book{tsos,
  author       = "Stephen Pinker",
  title        = "The Sense of Style",
  publisher    = "Viking",
  year         = 2014,
  address      = nyny,
  keywords     = "grammar, writing, style",
  location     = "PE 1421.P56"
}

@Book{tgdtf,
  author       = "Thomas Fleming",
  title        = "The Great Divide",
  publisher    = "Da Capo Press",
  year         = 2015,
  address      = boma,
  keywords     = "colonial america, washington, jefferson",
  location     = ""
}

@Book{tmimb,
  author       = "Walter Mosley",
  title        = "The Man in My Basement",
  publisher    = "Little, Brown",
  year         = 2004,
  address      = boma,
  keywords     = "crime, punishment",
  location     = "PS 3563.O88456 M36"
}

@Book{alesa,
  author       = "Edward {St. Aubyn}",
  title        = "At Last",
  publisher    = "Picador",
  year         = 2011,
  address      = nyny,
  keywords     = "death, rememberance",
  location     = "PR 6069.T134 A93"
}

@Book{ddtdia,
  author       = "Walter Mosley",
  title        = "Debbie Doesn't Do It Anymore",
  publisher    = "Doubleday",
  year         = 2014,
  address      = nyny,
  keywords     = "life changes",
  location     = "PS 3563.O88456 D43"
}

@Book{dbrg,
  author       = "R{\' o}mulo Gallegos",
  title        = "Do{\~ n}a Barbara",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "the plains, life on the frontier",
  location     = "PQ 8549.G24 D613"
}

@Book{ojajs,
  author       = "Jonathan Schaeffer",
  title        = "One Jump Ahead",
  publisher    = "Springer",
  year         = 2009,
  address      = nyny,
  keywords     = "checkers, artificial intelligence, game-playing programs",
  location     = "GV 1464.S33 2009"
}

@Book{pjf,
  author       = "Jonathan Franzen",
  title        = "Purity",
  publisher    = "Farrar, Straus and Giroux",
  year         = 2015,
  address      = nyny,
  keywords     = "your mom and dad, the internet, secrets",
  location     = "PS 3556.R352 P87"
}

@Book{fotdwm,
  author       = "Walter Mosley",
  title        = "Fear of the Dark",
  publisher    = "Little, Brown",
  year         = 2006,
  address      = nyny,
  keywords     = "turmoil, murrdaar",
  location     = "PS 3563.O88456 F384"
}

@Book{1920eb,
  author       = "Eric Burns",
  title        = "1920",
  publisher    = "Pegasus Books",
  year         = 2015,
  address      = nyny,
  keywords     = "1920, prohibition, harding, anarcists, wilson, ponzi",
  location     = "E 784.B872 2015b"
}

@Book{fgck,
  author       = "Christopher Kemp",
  title        = "Floating Gold",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "ambergris, whaling, natural resources",
  location     = "QD 331.K457"
}

@Book{ersc,
  author       = "Stephanie Clifford",
  title        = "Everybody Rise",
  publisher    = "St. Martin's Press",
  year         = 2015,
  address      = nyny,
  keywords     = "eat the rich, the elephants dance",
  location     = "PS 3603.L499"
}

@Book{fswm,
  author       = "Walter Mosley",
  title        = "Fortunate Son",
  publisher    = "Little, Brown",
  year         = 2006,
  address      = nyny,
  keywords     = "fate",
  location     = "PS 3563.O88456 F67"
}

@Book{bpal,
  author       = "Alan Gilbert",
  title        = "Black Patriots and Loyalists",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "american revolution, slavery, abolition",
  location     = "E 269.N3G55"
}

@Book{rjw,
  author       = "Jonathan Waldman",
  title        = "Rust",
  publisher    = "Simon \& Schuster",
  year         = 2015,
  address      = nyny,
  keywords     = "corrosion, galvanization, pigs, statue of liberty, stainless
    steel, cans",
  location     = "TA 418.74.W35"
}

@Book{maadad,
  author       = "Thomas Aiello",
  title        = "Model Airplanes are Decadent and Depraved",
  publisher    = "NIU Press",
  year         = 2015,
  address      = "DeKalb, Illinois",
  keywords     = "drug addiction, moral panics, glue sniffing, moral panics",
  location     = "HV 5822.G4 A44"
}

@Book{gfwm,
  author       = "Walter Mosley",
  title        = "Gone Fishin'",
  publisher    = "Black Classic Press",
  year         = 1997,
  address      = "Baltimore, Maryland",
  keywords     = "impetuous youth",
  location     = "PS 3563.O88r56 G66"
}

@Book{cmaos,
  author       = "Elmore Leonard",
  title        = "Charlie Martz and Other Stories",
  publisher    = "William Morrow",
  year         = 2015,
  address      = nyny,
  keywords     = "a man alone",
  location     = "PS 3562.E55 A6 2015"
}

@Book{lgwm,
  author       = "Walter Mosley",
  title        = "Little Green",
  publisher    = "Doubleday",
  year         = 2013,
  address      = nyny,
  keywords     = "brains on drugs, murrdaar",
  location     = "PS 3563.O88456 L44"
}

@Book{desa,
  author       = "Harvey~G. Cohen",
  title        = "Duke Ellington's America",
  publisher    = ucp,
  year         = 2010,
  address      = chil,
  keywords     = "jazz, duke ellington",
  location     = "ML 410.E44 C56"
}

@Book{lmsss,
  author       = "Walter Mosley",
  title        = "Love Machine\slash{}Stepping Stone",
  publisher    = "Tor",
  year         = 2013,
  address      = nyny,
  keywords     = "the hive mind, humans are a virus from outer space",
  location     = "PS 3563.O88456 S74"
}

@Book{msdwm,
  author       = "Walter Mosley",
  title        = "Merge\slash{}Disciple",
  publisher    = "Tor",
  year         = 2012,
  address      = nyny,
  keywords     = "sticks from outer space, friends in strange places",
  location     = "PS 3563.O88456 M47"
}

@InBook{lphoor,
  author       = "Hamdy~A. Taha",
  title        = "Handbook of Operations Research",
  chapter      = "II-1: Linear Programming",
  publisher    = "Van Nostrand Reinhold",
  year         = 1978,
  editor       = "Joseph~J. Moder and Salah~E. Elmaghraby",
  pages        = "85--119",
  address      = nyny,
  keywords     = "linear programming, the simplex method, duality theory,
    sensitivity analyses",
  location     = "T 57.6.H35"
}

@TechReport{rsurr,
  author       = "Fr{\' e}d{\' e}ric Boussinot",
  title        = "{RC} Semantics using Rewriting Rules",
  institution  = "Centre de Math{\' e}matiques Appliqu{\' e}es, Ecole des Mines
  de Paris",
  year         = 1992,
  number       = "18--92",
  address      = "Sophia-Antipolis, France",
  month        = "23 " # sep,
  keywords     = "rewriting rules, semantics, rc, reactive semantics",
  abstract     = "This paper describes a formal semantics for a new
     programming language called reactive C.  This language is an extension of
     C to program reactive systems i.e.  systems that react to sequences of
     activations from the external world.  Reactive statements are introduced
     to code these systems.  The semantics of reactive statements is described
     in an operational framework using conditional rewriting rules.", 
  location     = "http://www-sop.inria.fr/meije/rp/RapportsRecherche/rapport18-92.pdf"
}

@TechReport{pbacus4,
  author       = "Mario Wolczko and Randall~B. Smith",
  title        = "Prototype-Based Application Construction Using {S}elf 4.0",
  institution  = "Sun Microsystems Laboratories",
  year         = 1995,
  number       = "SMLI 95-0257",
  address      = mvca,
  keywords     = "self, prototypical programming",
  location     = "https://www.cs.ucsb.edu/~urs/oocsb/self/release/Self-4.0/manuals/tutorial.ps.gz"
}

@TechReport{amgp,
  author       = "Marwan Shaban",
  title        = "{A} Minimal {GB} Parser",
  institution  = csd # "Boston University",
  year         = 1993,
  number       = "93-013",
  address      = boma,
  month        = oct,
  keywords     = "parser",
  abstract     = "We describe a GB parser implemented along the lines of those
    written by Fong [4] and Dorr [2].  The phrase structure recovery component
    is an implementation of Tomita's generalized LR parsing algorithm
    (described in [10]), with recursive control flow (similar to Fong's
    implementation).  The major principles implemented are government, binding,
    bounding, trace theory, case theory, $\theta$-theory, and barriers.  The
    particular version of GB theory we use is that described by Haegeman [5].
    The parser is minimal in the sense that it implements the major principles
    needed in a GB parser, and has fairly good coverage of linguistically
    interesting portions of the English language.", 
  location     = "http://open.bu.edu/bitstream/handle/2144/1467/1993-013-gb-parser.pdf"
}

@TechReport{hcttopic,
  author       = "Nicolas Wirth",
  title        = "Hardware Compilation:  The Translation of Programs into Circuits",
  institution  = "Departement Informatik, Institut f{\" u}r Computersysteme,
    Eidgen{\" o}ssische Technische Hochschule", 
  year         = 1997,
  number       = 286,
  address      = "Z{\" u}rich, Switzerland",
  month        = jan,
  keywords     = "hardware compilation, sequential circuits, circuit design",
  abstract     = "Abstract We explain how programs specified in a sequential
    programming language can be translated automatically into a digital
    circuit.  The possibility to specify which parts of a program are to be
    compiled into instruction sequences for a conventional processor, and which
    ones are to be translated into customized circuits has gained relevance
    with the advent of large programmable gate arrays (FPGA).  They open the
    door to introduce massive, fine-grained parallelism.  In order to
    demonstrate the feasibility of this approach, we present a tiny programming
    language featuring the basic programming and circuit design facilities.", 
  location     = "https://e-collection.library.ethz.ch/view/eth:4328"
}

@TechReport{itlpkra,
  author       = "Krzystof~R. Apt",
  title        = "Introduction to Logic Programming",
  institution  = dcs # uta,
  year         = 1897,
  number       = "TR-87-35",
  address      = atx,
  month        = sep,
  keywords     = "logic programming, proof theory, semantics, computability,
    negative information, general goals"
}

@TechReport{hcp,
  author       = "Jeremiah Blocki and Manuel Blum and Anupam Datta and Santosh Vempala",
  title        = "Human Computable Passwords",
  year         = 2014,
  institution  = "arXiv",
  number       = "1404.0024v3",
  month        = oct,
  keywords     = "passwords, human authentication, planned satisfiability,
    statistical algorithms, statistical dimension",
  abstract     = "An interesting challenge for the cryptography community is to
    design authentication protocols that are so simple that a human can execute
    them without relying on a fully trusted computer.  We propose several
    candidate authentication protocols for a setting in which the human user
    can only receive assistance from a semi-trusted computer --- a computer
    that stores information and performs computations correctly but does not
    provide confidentiality.  Our schemes use a semi-trusted computer to store
    and display public challenges $C_i\in[n]^k$.  The human user memorizes a
    random secret mapping $\sigma:[n]\rightarrow\mathbb{Z}_d$ and authenticates
    by computing responses $f(\sigma(C_i))$ to a sequence of public challenges
    where $f:\mathbb{Z}_d^k\rightarrow\mathbb{Z}_d$ is a function that is easy
    for the human to evaluate.  We prove that any statistical adversary needs
    to sample $m=\tilde{\Omega}(n^{s(f)})$ challenge-response pairs to recover
    $\sigma$, for a security parameter $s(f)$ that depends on two key
    properties of $f$.  To obtain our results, we apply the general
    hypercontractivity theorem to lower bound the statistical dimension of the
    distribution over challenge-response pairs induced by $f$ and $\sigma$.
    Our lower bounds apply to arbitrary functions $f $ (not just to functions
    that are easy for a human to evaluate), and generalize recent results of
    Feldman et al.  As an application, we propose a family of human computable
    password functions $f_{k_1,k_2}$ in which the user needs to perform
    $2k_1+2k_2+1$ primitive operations (e.g., adding two digits or remembering
    $\sigma(i)$), and we show that $s(f) = \min\{k_1+1, (k_2+1)/2\}$.  For
    these schemes, we prove that forging passwords is equivalent to recovering
    the secret mapping.  Thus, our human computable password schemes can
    maintain strong security guarantees even after an adversary has observed
    the user login to many different accounts.", 
  location     = "http://arxiv.org/abs/1404.0024"
}

@InProceedings{eyawtkasbwata,
  author       = "David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios",
  title        = "Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask",
  booktitle    = port # "Twenty-Fourth ACM Symposium on Operating System Principles",
  year         = 2013,
  pages        = "33--48",
  address      = "Framington, Pennsylvania",
  month        = "3--6 " # nov,
  keywords     = "synchronization, machine architecture, operon, xeon",
  abstract     = "This paper presents the most exhaustive study of 
    synchronization to date.  We span multiple layers, from hardware
    cache-coherence protocols up to high-level concurrent software.  We do so
    on different types of architectures, from single-socket -- uniform and
    non-uniform -- to multi-socket -- directory and broadcast-based --
    many-cores.  We draw a set of observations that, roughly speaking, imply
    that scalability of synchronization is mainly a property of the hardware.",
  location     = "http://dx.doi.org/10.1145/2517349.2522714"
}

@InProceedings{oteogij,
  author       = "Ran Shaham, Elliot~K. Kolodner and Mooly Sagiv",
  title        = "On the Effectiveness of {GC} in {J}ava",
  booktitle    = pot # "International Symposium on Memory Management",
  year         = 2000,
  pages        = "12--17",
  address      = "Minneapolis, MN",
  month        = "15--16 " # oct,
  keywords     = "java, garbage collection, dynamic analysis",
  abstract     = "We study the effectiveness of garbage collection (GC)
    algorithms by measuring the time difference between the actual collection
    time of an object and the potential earliest collection time for that
    object.  Our ultimate goal is to use this study in order to develop static
    analysis techniques that can be used together with GC to allow earlier
    reclamation of objects.  The results may also be used to pinpoint
    application source code that could be rewritten in a way that would allow
    more timely GC.  Specifically, we compare the objects reachable from the
    root set to the ones that are actually used again.  The idea is that GC
    could reclaim unused objects even if they are reachable from the root set.
    Thus, our experiments indicate a kind of upper bound on storage savings
    that could be achieved.  We also try to characterize these objects in order
    to understand the potential benefits of various static analysis algorithms.
    The Java Virtual Machine (JVM) was instrumented to measure objects that are
    reachable, but not used again, and to characterize these objects.
    Experimental results are shown for the SPECjvm98 benchmark suite.  The
    potential memory savings for these benchmarks range from 23% to 74%.", 
  location     = "http://dx.doi.org/10.1145/362422.362430"
}

@InProceedings{dmdcsm,
  author       = "Watabe, Kazuo and Sakata, Shiro and Maeno, Kazutoshi and Fukuoka, Hideyuki and Ohmori, Toyoko",
  title        = "Distributed Multiparty Desktop Conferencing System: {MERMAID}",
  booktitle    = pot # "1990 ACM Conference on Computer-Supported Cooperative Work (CSCW '90)",
  year         = 1990,
  pages        = "27--38",
  address      = laca,
  month        = "7--10 " # oct,
  keywords     = "client-server, shared workspaces, video conferencing",
  abstract     = "This describes a distributed multiparty desktop conferencing
    system (MERMAID) and presents its preliminary brief evaluation, obtained as
    a result of daily use.  MERMAID, which is designed based on group
    collaboration system architecture, provides an environment for widely
    distributed participants, seated at their desks, to hold real-time
    conferences by interchanging information through video, voice, and
    multimedia documents.  This system is implemented by using narrow-band
    ISDN, high-speed data network, and UNIX-based EWSs with electronic writing
    pads, image scanners, video cameras, microphone-installed loudspeakers,
    etc.  The system provides participants with the means for sharing
    information in such multimedia forms as video images, voice, text,
    graphics, still images, and hand drawn figures.", 
  location     = "http://dx.doi.org/10.1145/99332.99338"
}

@InProceedings{jadfml,
  author       = "Grande, Johan and Boudol, G{\' e}rard and Serrano, Manuel",
  title        = "Jthread, a deadlock-free mutex library",
  booktitle    = pot # "17th International Symposium on and Practice of Declarative Programming",
  year         = 2015,
  pages        = "149--160",
  address      = "Siena, Italy",
  month        = "14--16 " # jul,
  keywords     = "mutual exclusion, deadlock prevention, deadlock avoidance,
    concurrency control, concurrent programming",
  abstract     = "We design a mutex library for Hop -- a dialect of Scheme
    which supports preemptive multithreading and shared memory -- that mixes
    deadlock prevention and deadlock avoidance to provide an easy to use,
    expressive, and safe locking function.  This requires an operation to
    acquire several mutexes simultaneously, for which we provide a
    starvation-free algorithm.  Choosing a formal definition of
    starvation-freedom leads us to identify the concept of asymptotic deadlock.
    Preliminary observations seem to show that our library has negligible
    impact on the performance of real-life applications.  Our work could be
    applied to other languages such as Java.", 
  location     = "http://dx.doi.org/10.1145/2790449.2790523"
}

@InProceedings{oaurocaltat,
  author       = "Tarau, Paul",
  title        = "On a Uniform Representation of Combinators, Arithmetic, Lambda Terms and Types",
  booktitle    = pot # "17th International Symposium on Principles and Practice of Declarative Programming (PPDP '15)",
  year         = 2015,
  pages        = "244--255",
  address      = "Siena, Italy",
  month        = "14--16 " # jul,
  keywords     = "algorithms, languages, theory, rosser's x-combinator,
    tree-based numbering systems, lambda calculus, normalization of de bruin
    terms, type inference, bijective godel numberings, logic programming as
    metalanguage",
  abstract     = "A uniform representation, as binary trees with empty leaves,
    is given to expressions built with Rosser's X-combinator, natural numbers,
    lambda terms and simple types.  Type inference, normalization of combinator
    expressions and lambda terms in de Bruijn notation, ranking/unranking
    algorithms and tree-based natural numbers are described as a literate
    Prolog program.  With sound unification and compact expression of
    combinatorial generation algorithms, logic programming is shown to
    conveniently host a declarative playground where interesting properties and
    behaviors emerge from the interaction of heterogenous but deeply connected
    computational objects.", 
  location     = "http://dx.doi.org/10.1145/2790449.2790526"
}

@InProceedings{fammoc,
  author       = "Prodromos Gerakios and George Fourtounis and Yannis Smaragdakis",
  title        = "{FOO}: A Minimal Modern {OO} Calculus",
  booktitle    = pot # "17th Workshop on Formal Techniques for Java-Like Programs",
  year         = 2015,
  address      = "Prague, Czech Republic",
  month        = "7 " # jul,
  keywords     = "object-oriented programming, formal semantics, type systems,
    structural types, nominal types",
  abstract     = "We present the Flyweight Object-Oriented (FOO) calculus for 
    the modeling of object-oriented languages.  FOO is a simple, minimal
    class-based calculus, modeling only essential computational aspects and
    emphasizing larger-scale features (e.g., inheritance and generics).  FOO is
    motivated by the observation that recent language design work focuses on
    elements not well-captured either by traditional object calculi or by
    language-specific modeling efforts,such as Featherweight Java.  FOO
    integrates seamlessly both nominal and structural subtyping ideas,
    leveraging the latter to eliminate the need for modeling object fields and
    constructors.  Comparing to recent formalization efforts in the literature,
    FOO is more compact,yet versatile enough to be usable in multiple settings
    modeling Java, C#, or Scala extensions.", 
  location     = "https://yanniss.github.io/foo-ftfjp15.pdf"
}

@InProceedings{ecviscw,
  author       = "Watson, Robert N.~M.",
  title        = "Exploiting Concurrency Vulnerabilities in System Call Wrappers",
  booktitle    = pot # "first USENIX workshop on Offensive Technologies (WOOT'07)",
  year         = 2007,
  address      = boma,
  month        = "6--10 " # aug,
  keywords     = "security, concurrency, operating systems, system calls",
  abstract     = "System call interposition allows the kernel security model to
    be extended.  However, when combined with current operating systems, it is
    open to concurrency vulnerabilities leading to privilege escalation and
    audit bypass.  We discuss the theory and practice of system call wrapper
    concurrency vulnerabilities, and demonstrate exploit techniques against
    GSWTK, Systrace, and CerbNG.", 
  location     = "http://www.watson.org/~robert/2007woot/2007usenixwoot-exploitingconcurrency.pdf"
}

@Misc{tpbadtkla,
  author       = "Peter {Van Roy} and Seif Haridi",
  title        = "Teaching Programming Broadly and Deeply:  The Kernel Language Approach",
  howpublished = "lecture slides",
  year         = 2002,
		  
  month        = "17 " # jul,
  keywords     = "programming, teaching"
}

@Misc{1rfwmcq,
  author       = "Timothy~W. Bothell",
  title        = "14 Rules for Writing Multiple-Choice Questions",
  howpublished = "web page",
  year         = 2001,
  keywords     = "testing, multiple-choice questions",
  location     = "https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf"
}

