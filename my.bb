.so bibtex.header

@string{asplos91 = sigplan # " (" # pot # "Fourth International Conference on " # asplos # ", ASPLOS IV)"}
@string{asplos92 = sigplan # " (" # pot # "Fifth International Conference on " # asplos # ", ASPLOS V)"}
@string{asplos94 = sigplan # " (" # pot # "Sixth International Conference on " # asplos # ", ASPLOS VI)"}
@string{asplos96 = sigplan # " (" # pot # "Seventh International Conference on " # asplos # ", ASPLOS VII)"}
@string{asplos98 = sigplan # " (" # pot # "Eighth International Conference on " # asplos # ", ASPLOS VIII)"}
@string{asplos00 = sigplan # " (" # pot # "Ninth International Conference on " # asplos # ", ASPLOS IX)"}
@string{pldi03 = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming
Language Design and Implementation, PLDI '03)"}
@string{sosp87    = osr # " (" # pot # "Eleventh" # sosp # ", SOSP '87)"}
@string{usenixw92 = pot # "Winter 1992 USENIX Conference"}
@string{usenixs94 = pot # "USENIX Summer 1994 Technical Conference"}
@string{usenix95  = pot # "1995 USENIX Technical Conference"}
		  
		  
@Book{tcwekm,
  author       = "Ella~K. Maillart",
  title        = "The Cruel Way",
  subtitle     = "Switzerland to Afghanistan in a Ford, 1939",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "travel, middle east",
  location     = "DS 352.M18"
}

@Book{taeojl,
  author       = "Yves Beauchemin",
  title        = "The Accidental Education of Jerome Lupien",
  publisher    = "House of Anansi Press",
  year         = 2018,
  address      = "Canada",
  keywords     = "political machinations, lobbyists, con games",
  location     = "PS 8553.E172 E4813"
}

@Book{himt,
  author       = "Matt Taibbi",
  title        = "Hate Inc.",
  publisher    = "OR Books",
  year         = 2019,
  address      = nyny,
  keywords     = "media, propaganda, chomsky, maddow, journalism, the profit motive",
  location     = "9781949017250"
}

@Book{eolp,
  author       = "Christopher John Hogger",
  title        = "Essentials of Logic Programming",
  publisher    = oup,
  year         = 1990,
  series       = "Graduate Texts in Computer Science",
  address      = nyny,
  keywords     = "logic programming, first-order logic, causal-form logic,
    herbrand domain, sld resolution, definite program semantics, finite
    failure, program verification",
  location     = "QA 76.63 H64"
}

@Book{tmjc,
  author       = "Jorge Comensal",
  title        = "The Mutations",
  publisher    = fsg,
  year         = 2019,
  address      = nyny,
  keywords     = "cancer, survivorship, psychotherapy",
  location     = "PQ 7298.413.O438 M8813 "
}

@Book{ojhhdb,
  author       = "John~H. Halpern and David Blistein",
  title        = "Opium",
  subtitle     = "How an Ancient Flower Shaped and Poisoned Our World",
  publisher    = "Hachette",
  year         = 2019,
  address      = nyny,
  keywords     = "opium, addiction, poppies, drugs, medicine",
  location     = "HV 5816.H35"
}

@Book{tlpmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Laughing Policeman",
  publisher    = "Vintage",
  year         = 1977,
  price        = "$1.65",
  address      = nyny,
  keywords     = "murrdaar, doggedness, lone wolves",
  location     = "PT 9876.29.J63"
}

@Book{topsad,
  author       = "Edward Yourdon",
  title        = "Techniques of Program Structure and Design",
  publisher    = ph,
  year         = 1975,
  address      = ecnj,
  keywords     = "computer programs, top-down design, modular programming,
    structured programming, antidebugging, program testing, debugging,
    programming style",
  location     = "QA 76.6.6.Y68"
}

@Book{natb,
  author       = "Lars Iyer",
  title        = "Nietzsche and the Burbs",
  publisher    = "Melville House",
  year         = 2019,
  address      = "Brooklyn, N.Y.",
  keywords     = "suburban life, wasted youth",
  location     = ""
}

@Book{hwsts,
  author       = "Thomas Hockey",
  title        = "How We See the Sky",
  subtitle     = "A Naked-Eye Tour of Day and Night",
  publisher    = ucp,
  year         = 2011,
  address      = chil,
  keywords     = "the night sky, stars, planets, the moon",
  location     = "QB 44.3 H635"
}

@Book{htrm,
  author       = "Randall Munroe",
  title        = "How To",
  subtitle     = "Absurd Scientific Advice for Common Real-World Problems",
  publisher    = "Riverhead Books",
  year         = 2019,
  address      = nyny,
  keywords     = "jumping, pool parties, digging holes, piano playing,
    emergency landings, crossing rivers, moving, stability, lava moats,
    throwing, football, weather prediction, physics, algebra, modeling.",
  location     = ""
}

@Book{tnbcw,
  author       = "Colson Whitehead",
  title        = "The Nickel Boys",
  publisher    = "Doubleday",
  year         = 2019,
  address      = nyny,
  keywords     = "racism, florida, juvenile detention, the past",
  location     = "PS 3573.H4768 N53"
}

@Book{ystb,
  author       = "Wendy Lesser",
  title        = "You Say to Brick",
  subtitle     = "The Life of Louis Kahn",
  publisher    = fsg,
  year         = 2017,
  address      = nyny,
  keywords     = "louis kahn, american architecture",
  location     = "NA 737.K32 L48"
}

@Book{cnast,
  author       = "Andrew~S. Tanenbaum and David~J. Wetherall",
  title        = "Computer Networks",
  publisher    = ph,
  year         = 2011,
  address      = boma,
  keywords     = "iso osi stack, network security",
  location     = "TK 5105.5.T36"
}

@Book{tjs,
  author       = "John Suchet",
  title        = "Tchaikovsky",
  title        = "The Man Revealed",
  publisher    = "Pegasus Books",
  year         = 2018,
  address      = nyny,
  keywords     = "p.i. tchaikovsky, russian romanticism, music composition",
  location     = "ML 410.C4 S8"
}

@Book{bleh,
  author       = "Elizabeth Hand",
  title        = "Blacklight",
  publisher    = "HarperColins",
  year         = 1999,
  address      = nyny,
  keywords     = "occult, small-town life, into the weird",
  location     = "PS 3558.A4619 B53"
}

@Book{qsr,
  author       = "Salman Rushdie",
  title        = "Quichotte",
  publisher    = "Random House",
  year         = 2019,
  address      = nyny,
  keywords     = "quests, the near future, metafiction",
  location     = "PR 6068,U757 Q53"
}

@Book{gase,
  author       = "Shimon Even",
  title        = "Graph Algorithms",
  publisher    = "Computer Science Press",
  year         = 1979,
  address      = "Potomac, Maryland",
  keywords     = "graph theory, paths, trees, depth-first search, ordered
    trees, maximum flow, planar graphs, graph planarity tests, np completeness",
  location     = "QA 166.E93"
}

@Book{ckmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "Cop Killer",
  publisher    = "Vintage",
  year         = 1975,
  address      = nyny,
  price        = "$1.75",
  keywords     = "murrdaar, coincidences",
  location     = "PT 9876.29.J63"
}

@Book{teor,
  author       = "Stephen~P. Kershaw",
  title        = "The Enemies of Rome",
  subtitle     = "The Barbarian Rebellion Against the Roman Empire",
  publisher    = "Pegasus Books",
  year         = 2019,
  address      = nyny,
  keywords     = "ancient rome, warfare, barbarians",
  location     = ""
}

@Book{ldods,
  author       = "Arthur~D. Friedman",
  title        = "Logical Design of Digital Systems",
  publisher    = "Computer Science Press",
  year         = 1975,
  address      = "Woodland Hills, California",
  keywords     = "number systems, nondecimal arithmetic, codes, combinational
    circuits, sequential circuits, circuit design",
  location     = "TK 7868.S9 F74"
}

@Book{tcdtrhpcp,
  author       = "Peter Buse",
  title        = "The Camera Does the Rest",
  subtitle     = "How Polaroid Changed Photography",
  publisher    = ucp,
  year         = 2016,
  address      = chil,
  keywords     = "photography, polaroid, technology, history, marketing",
  location     = "TR 269.B87"
}

@Book{tlabt,
  author       = "Barbara Taylor",
  title        = "The Last Asylum",
  subtitle     = "A Memoir of Madness in Our Times",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "mental health, psychotherapy",
  location     = "RC 451.4.W6 T38"
}

@Book{awg,
  author       = "William Gibson",
  title        = "Agency",
  publisher    = "Berkley",
  year         = 2020,
  address      = nyny,
  keywords     = "travel broadens the mind, temporal meddling",
  location     = "PS 3557.I2264 A34"
}

@Book{dswj,
  author       = "John~R. Hubbard",
  title        = "Data Structures with {Java}",
  publisher    = "Schaum's Outline Series",
  year         = 2007,
  address      = nyny,
  keywords     = "java, data structures, oo programming, arrays, linked data
    structures, java collection framework, stacks, queues, lists, hash tables,
    recursion, trees, binary trees, search trees, heaps and priority queues,
    sorting, graphs, mathematics",
  location     = "QA 76.73.J38 H82"
}

@Book{gjg,
  author       = "James Gleick",
  title        = "Genius",
  subtitle     = "The Life and Science of Richard Feynman",
  publisher    = "Vintage Books",
  year         = 1993,
  address      = nyny,
  keywords     = "richard feynman, 2nd-half 20th century atomic physics, biography",
  location     = "QC 16.F49 G55"
}

@Book{hotd,
  author       = "Joe Meno",
  title        = "Hairstyles of the Damned",
  publisher    = "Punk Planet Books",
  year         = 2004,
  address      = chil,
  keywords     = "teenage wasteland, lurv",
  location     = "PS 3563.E53 H35"
}

@Book{afane,
  author       = "David~C. Korten",
  title        = "Agenda for a New Economy",
  subtitle     = "From Phantom Wealth to Real Wealth",
  publisher    = "Berrett-Koehler Publishers",
  year         = 2010,
  address      = sfca,
  keywords     = "economics, development, society, finance, ",
  location     = "HC 106.83.K67"
}

@Book{esaaf,
  author       = "\AE leen Frisch",
  title        = "Essential System Administration",
  publisher    = "O'Reilly \& Associates, Inc.",
  year         = 1991,
  address      = seca,
  keywords     = "unix, system administration, startup, shutdown, user
    accounts, security, automation, managing system resources, filesystems,
    disks, backup, restore, terminals, modems, printers, spooling, tcp/ip
    network management, accounting, bourne shell programming",
  location     = "QA 76.76.O63 F782"
}

@Book{ttcjd,
  author       = "Jared Diamond",
  title        = "The Third Chimpanzee",
  subtitle     = "The Evolution and Future of the Human Animal",
  publisher    = "Harper Perennial",
  year         = 1992,
  address      = nyny,
  keywords     = "human development, language, warfare, sex, evolution",
  location     = "GN 281.D53"
}

@Book{aese,
  author       = "William~C. Deitz",
  title        = "At Empire's Edge",
  publisher    = "Ace Books",
  year         = 2009,
  address      = nyny,
  keywords     = "empath cops, empath killer shape shifters, intergalactic
    corruption", 
  location     = "PS 3554.I388 A84"
}

@Book{ltzbee,
  author       = "Bret Easton Ellis",
  title        = "Less Than Zero",
  publisher    = "Vintage",
  year         = 1985,
  address      = nyny,
  keywords     = "sex, drugs, rock and roll, ennui",
  location     = "PS 3555.L5937 L4"
}

@Book{epekb,
  author       = "Kent Beck",
  title        = "Extreme Programming Explained",
  publisher    = aw,
  year         = 2000,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, economics, design",
  location     = "QA 76.76.D47 B434"
}

@Book{ttmspw,
  author       = "Maj Sj{\" o}wall and Per Wahl{\" o}{\" o}",
  title        = "The Terrorists",
  publisher    = "Vintage",
  year         = 1976,
  address      = nyny,
  keywords     = "terrorism",
  location     = "PT 9876.29.J63"
}

@Book{ttatb,
  author       = "David~P. Billington",
  title        = "The Tower and the Bridge",
  subtitle     = "The New Art of Structural Engineering",
  publisher    = pup,
  year         = 1983,
  address      = prnj,
  keywords     = "structural engineering, structural art",
  location     = "TA 636.B54"
}

@Book{lgew,
  author       = "Elizabeth Wilson",
  title        = "Love Game",
  subtitle     = "A History of Tennis from Victorian Pastime to Global Phenomenon",
  publisher    = ucp,
  year         = 2014,
  address      = chil,
  keywords     = "tennis, commerce, amateurism, professionalism",
  location     = "GV 992.W558"
}

@Book{waacf,
  author       = "Daryl Gregory",
  title        = "We are All Completely Fine",
  publisher    = "Tachyon",
  year         = 2014,
  address      = sfca,
  keywords     = "monsters, the talking cure",
  location     = "978-1-61696-173-5"
}

@Book{epirj,
  author       = "Ron Jeffries and Ann Anderson and Chet Hendrickson",
  title        = "Extreme Programming Installed",
  publisher    = aw,
  year         = 2001,
  series       = "The XP Series",
  address      = boma,
  keywords     = "software development, extreme programming, experience, practice",
  location     = "QA 76.76 D47 J44"
}

@Book{epewcw,
  author       = "William~C. Wake",
  title        = "Extreme Programming Explored",
  publisher    = aw,
  year         = 2002,
  series       = "The XP Series",
  address      = boma,
  keywords     = "extreme programming, software development, programming,
    teamwork, process",
  location     = "QA 76.76 D47 W34"
}

@Book{lmbfwy,
  author       = "Richard Ford",
  title        = "Let Me Be Frank With You",
  publisher    = "Ecco",
  year         = 2014,
  address      = nyny,
  keywords     = "new jersey, super storm sandy, real estate, disaster,
    divorce, murderous passions, death-bed confessions",
  location     = "PS 3556.O713 L48"
}

@Book{butl,
  author       = "Tanith Lee",
  title        = "Black Unicorn",
  publisher    = "Atheneum",
  year         = 1991,
  address      = nyny,
  keywords     = "quests, unicorns",
  location     = "PZ 7.L5149 Bl"
}

@Book{okotew,
  author       = "Bertrand Russell",
  title        = "Our Knowledge of the External World",
  publisher    = "The New American Library",
  year         = 1960,
  address      = nyny,
  month        = sep,
  price        = "$0.60",
  keywords     = "philosophy, science, logic, infinity, free will,
    epistemology, continuity, causality",
  location     = "B 1649.R93 O8"
}

@Book{cgfjp,
  author       = "Leen Ammeraal and Kang Zhang",
  title        = "Computer Graphics for Java Programmers",
  publisher    = "Springer Science+Business Media",
  year         = 2017,
  address      = nyny,
  edition      = "third",
  keywords     = "geometry, transformations, 2d algorithms, perspective, 3d
    algorithms, hidden line removal, hidden face removal, color, texture,
    shading, fractals",
  location     = "T 385.A488"
}

@Book{tpwmdg,
  author       = "Michael~D. Gordin",
  title        = "The Pseudoscience Wars",
  subtitle     = "Immanuel Velikovsky and the Birth of the Modern Fringe",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "psuedoscience, astronomy, history, catastrophism, bible
    interpretation",
  location     = "Q 172.5.P77 G674"
}

@Book{mdsacg,
  author       = "Kurt Mehlhorn",
  title        = "Multi-Dimensional Searching and Computational Geometry",
  publisher    = sv,
  year         = 1984,
  volume       = 3,
  series       = "EACTS Monographs on Theoretical Computer Science",
  address      = bege,
  keywords     = "multidimensional data structures, computational geometry,
    algorithms",
  location     = "0-387-13642-8"
}

@Book{msbtm,
  author       = "Thomas McMahon",
  title        = "McKay's Bees",
  publisher    = ucp,
  year         = 1979,
  address      = chil,
  keywords     = "frontier and pioneer life, bee culture, beekeepers, kansas",
  location     = "PS 3563.A3188 M39"
}

@Book{pldab,
  author       = "Arnold Businger",
  title        = "{PORTAL} Language Description",
  publisher    = sv,
  year         = 1988,
  volume       = "198",
  series       = lncs,
  address      = bege,
  edition      = "second",
  keywords     = "real-time systems, language definition",
  location     = "QA 76.73.P66 B87 "
}

@Book{ltdpl,
  author       = "William~W. Wadage and Edward~A. Ashcroft",
  title        = "Lucid, the Dataflow Programming Language",
  publisher    = "Academic Press",
  year         = 1985,
  volume       = "22",
  series       = "APIC Studies in Data Processing",
  address      = loen,
  keywords     = "iswim, luswim, iterations, program transformations",
  location     = "QA 76.7"
}

@Book{laofcm,
  author       = "Francisis~G. McCabe",
  title        = "Logic and Objects",
  publisher    = phi,
  year         = 1992,
  address      = "Hertfordshire, U.K.",
  keywords     = "logic programming, scheduler, semantics, implementation",
  location     = "QA 76.63.M42"
}

@Book{ncehm,
  author       = "Edward~H. Miller",
  title        = "Nut Country",
  subtitle     = "Right-Wing Dallas and the Birth of the Southern Strategy",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "american political history, dallas texas, republicans,
    ultraconservatives, the southern strategy, racism",
  location     = "JK 2359.D35 M55"
}

@Book{gmmem,
  author       = "Michael~E. Mortenson",
  title        = "Geometric Modeling",
  publisher    = "John Wiley \& Sons",
  year         = 1985,
  address      = nyny,
  keywords     = "curves, surfaces, solids, analytic properties, relational
    properties, intersections, transformations, solid modeling fundamentals,
    solid model construction, global properties, computer graphics, cad/cam",
  location     = "QA 447.M62"
}

@Book{itor,
  author       = "Frederick~S. Hillier and Gerald~J. Lieberman",
  title        = "Introduction to Operations Research",
  publisher    = "McGraw-Hill",
  year         = 2001,
  address      = boma,
  keywords     = "modeling, linear programming, simplex method, duality theory,
    sensitivity analysis, transportation problems, assignment problems, network
    optimization, project management, pert/cpm, dynamic programming, integer
    programming, nonlinear programming, game theory, decision analysis, markov
    chains, queueing theory, inventory theory, forecasting, markov decision
    processes, simulation",
  location     = "T 57.6 M53"
}

@Book{skr,
  author       = "Karen Russell",
  title        = "Swamplandia!",
  publisher    = "Vintage",
  year         = 2011,
  address      = nyny,
  keywords     = "florida, gator rasslin', ",
  location     = "PS 3618.U755 S39"
}

@Book{canpjdc,
  author       = "Jeffery~D. Clements",
  title        = "Corporations are not People",
  publisher    = "Berrett-Koehler",
  year         = 2014,
  address      = sfca,
  edition      = "second",
  keywords     = "corporate governance, constitutional law, the judicial
    system, the powell doctrine",
  location     = "JK 467.C55"
}

@Book{bhobc,
  author       = "David Crystal",
  title        = "By Hook or by Crook",
  subtitle     = "A Journey in Search of English",
  publisher    = "The Overlook Press",
  year         = 2007,
  address      = "Woodstock, " # ny,
  keywords     = "english as she is spoke",
  location     = "PE 1711.C79"
}

@Book{afgaip,
  author       = "Theo Pavlidis",
  title        = "Algorithms for Graphics and Image Processing",
  publisher    = "Computer Science Press",
  year         = 1982,
  keywords     = "digitization, gray-scale images, segmentation, data
    structures, bilevel pictures, projections, contour filling, thinning
    algorithms, curve fitting, curve displaying, splines, curve approximation,
    surface fitting, surface displaying, mathematics, polygon clipping, 2d
    graphics, 3d graphics"
}

@Book{pfai,
  author       = "Randall Jarrell",
  title        = "Pictures from an Institution",
  publisher    = ucp,
  year         = 1986,
  address      = chil,
  keywords     = "college life, writers, character studies",
  location     = "PS 3519.A86 P5"
}

@Book{mttp,
  author       = "Thierry Poibeau",
  title        = "Machine Translation",
  publisher    = mitp,
  year         = 2017,
  address      = cma,
  keywords     = "automatic translation, linguistics, statistics, deep
    learning, evaluation",
  location     = "P 308.P65"
}

@Book{pjb95,
  author       = "Julia Barrett",
  title        = "Presumption",
  subtitle     = "An Entertainment",
  publisher    = ucp,
  year         = 1995,
  address      = chil,
  keywords     = "the bennets, pride and prejudice",
  location     = "PS 3552.A73463 P74"
}

@Book{sptml,
  author       = "Mike Loukides",
  title        = "System Performance Tuning",
  publisher    = "O'Reilly \& Associates",
  year         = 1991,
  address      = seca,
  keywords     = "performance, monitoring, workload, storage, disk, network,
    terminal, kernel, real-time processes, tuning",
  location     = "QA 76.76.O63 L66"
}

@Book{hose,
  title        = "Handbook of Software Engineering",
  publisher    = "Van Nostrand Reinhold",
  year         = 1984,
  editor       = "C.~R. Vick and C.~V. Ramamoorthy",
  address      = nyny,
  keywords     = "software engineering, concurrency control, testing, formal
    verification, reliability, performance, fault tolerance, costing, life
    cycle factors, requirements, process design, applicative programming, array
    machines",
  location     = "QA 76.6 H3335"
}

@Book{mfmmms,
  author       = "Mark McKenney and Markus Schneider",
  title        = "Map Framework",
  subtitle     = "A Formal Model of Maps as a Fundamental Data Type in Information Systems",
  publisher    = sv,
  year         = 2916,
  address      = nyny,
  keywords     = "maps, formal models, points and lines, map operations,
  discrete map models, 2d maps",
  location     = "978-3-319-46764-1"
}

@Book{ttetc,
  author       = "David Rapp",
  title        = "Tinkers to Evers to Chance",
  subtitle     = "The Chicago Cubs and the Dawn of Modern America",
  publisher    = ucp,
  year         = 2018,
  address      = chil,
  keywords     = "baseball, america, chicago sports, chicago cubs",
  location     = "GV 875.C6 R36"
}

@Book{tboem,
  author       = "Caroline Chute",
  title        = "The Beans of Egypt, Maine",
  publisher    = "Warner Books",
  year         = 1985,
  address      = nyny,
  keywords     = "maine, rural poverty",
  location     = "PS 3553.H87 B4"
}

@Book{tpfows,
  author       = "Aaron Brown",
  title        = "The Poker Face of Wall Street",
  publisher    = "John Wiley \& Sons",
  year         = 2006,
  address      = honj,
  keywords     = "poker, finance, risk management",
  location     = "HG 4661.B766"
}

@Book{ttllr,
  author       = "Laurence Ralph",
  title        = "The Torture Letters",
  subtitle     = "Reckoning with Police Violence",
  publisher    = ucp,
  year         = 2020,
  address      = chil,
  keywords     = "police, chicago, law enforcement, torture",
  location     = "HV 8148.C52 R35"
}

@Book{viusax,
  title        = "Visualizing Information Using {SVG} and {X3D}",
  publisher    = sv,
  year         = 2005,
  editor       = "Vladimir Geroimenko and Chaomei Chen",
  address      = loen,
  keywords     = "sgv, x3d, semantic web, user interfaces, visualization",
  location     = "T 385.V597"
}

@Book{trosh,
  author       = "Arthur Conan Doyle",
  title        = "The Return of Sherlock Holmes",
  subtitle     = "Including the Hounds of the Baskervilles",
  publisher    = "Bramhall House",
  year         = 1976,
  address      = nyny,
  keywords     = "sherlock homes",
  location     = "PR 4622.R48"
}

@Book{taumg,
  author       = "Martin Gardner",
  title        = "The Ambidextrous Universe",
  subtitle     = "Left, Right, and the Fall of Parity",
  publisher    = "New American Library",
  year         = 1969,
  address      = nyny,
  price        = "$1.25",
  keywords     = "symmetry, asymmetry, physics, parity, mirrors",
  location     = "QC 793.3.S9 G37"
}

@Book{caccf,
  author       = "Caxton~C. Foster",
  title        = "Computer Architecture",
  publisher    = "Van Nostrand Reinhold",
  year         = 1976,
  address      = nyny,
  edition      = "second",
  keywords     = "information representation, gates, elementary logic, storage,
    addressing, input, output",
  location     = "QA 76.9 A73 F67"
}

@Book{xdbld,
  author       = "Don Brutzman and Loenard Daly",
  title        = "{X3D}",
  subtitle     = "Extensible 3d Graphics for Web Authors",
  publisher    = mk,
  year         = 2007,
  address      = sfca,
  keywords     = "geometry, nodes, navigation, textures, polygons, event
    animation, interpolation, interactivity, scripting, lighting, environment,
    sensors, sound, quadrilaterals",
  location     = "TR 897.7.B796"
}

@Book{itpcmg,
  author       = "Charles~M. Grinstead and J.~Laurie Snell",
  title        = "Introduction to Probability",
  publisher    = "American Mathematical Society",
  year         = 1997,
  address      = "Providence, Rhode Island",
  keywords     = "discrete probability, continuous probability, distributions,
    random walks",
  location     = "QA 273.S668"
}

@Book{dafew,
  author       = "Evelyn Waugh",
  title        = "Decline and Fall",
  publisher    = "Knopf",
  year         = 1993,
  address      = nyny,
  keywords     = "wrong place, wrong time, the knock-about life, satire",
  location     = "PR 6045.A97 D4"
}

@Book{vbew,
  author       = "Evelyn Waugh",
  title        = "Vile Bodies",
  publisher    = "Chapman \& Hall",
  year         = 1930,
  address      = loen,
  keywords     = "bright young things, gossip, dissolution",
  location     = "PZ 3.W356"
}

@Book{ahod,
  author       = "Evelyn Waugh",
  title        = "{A} Handful of Dust",
  publisher    = "Chapman \& Hall",
  year         = 1934,
  address      = loen,
  keywords     = "marriage, divorce, the explorin' life",
  location     = "PR 6045.A97"
}

@Book{avqfi,
  author       = "Simon Goldhill",
  title        = "{A} Very Queer Family Indeed",
  subtitle     = "Sex, Religion, and the Bensons in Victorian Britain",
  publisher    = ucp,
  year         = 2016,
  address      = chil,
  keywords     = "the benson family, victorian england, edwardian england,
    homosexuality, religion, anglicans, catholics, family life",
  location     = "DA 562.G65"
}

@Book{wwp,
  author       = "Jefferson~D. Bates",
  title        = "Writing With Precision",
  subtitle     = "How To Write So That You Cannot Possibly Be Misunderstood",
  publisher    = "Acropolis Books",
  year         = 1978,
  address      = wdc,
  keywords     = "writing, grammar, rhetoric, english, exposition",
  location     = "PE 1429.B35"		  
}

@Book{cowdls,
  author       = "Dorothy~L. Sayers",
  title        = "Clouds of Witness",
  publisher    = "Avon",
  year         = 1966,
  address      = nyny,
  keywords     = "class warfare, eat the rich, the honey trap",
  location     = "PR 6037.A95 C5"
}

@Book{hndc,
  author       = "Douglas Coupland",
  title        = "Hey Nostradamus!",
  publisher    = "Bloomsbury",
  year         = 2003,
  address      = nyny,
  keywords     = "school shootings, trauma, religion, religious fundamentalists, sex",
  location     = "PS 3553.O855 H49"
}

@Book{sew77,
  author       = "Evelyn Waugh",
  title        = "Scoop",
  publisher    = "Little, Brown and Co.",
  year         = 1977,
  address      = boma,
  keywords     = "war correspondents, newspaper media, revolutionaries",
  location     = "PZ 3.W356 Sc"
}

@Book{hacpc,
  author       = "Patrick Carey",
  title        = "{HTML} and {CSS}",
  publisher    = "Cengage Course Technology",
  year         = 2012,
  address      = boma,
  keywords     = "html 5, page layouts, css, tables, web forms, multimedia,
    xhtml, javascript, colors",
  location     = "978-1-111-52644-3"
}

@Book{ccjekpw,
  author       = "Eli K.~P. William",
  title        = "Cash Crash Jubilee",
  subtitle     = "Book One of the Jubilee Cycle",
  publisher    = "Talos Press",
  year         = 2015,
  address      = nyny,
  keywords     = "distopias, neoliberalism, tokyo, augmented reality",
  location     = "PR 9199.4.W5433 C37"
}

@Book{tfsd,
  title        = "\TeX\ for Scientific Documentation",
  subtitle     = pot # "Second European Conference",
  publisher    = sv,
  year         = 1986,
  editor       = "Jacques D{\' e}sarm{\' e}nien",
  volume       = 236,
  series       = lncs,
  address      = bege,
  keywords     = "document processing, multi-lingual documents, document
    preparation systems, font design",
  location     = "Z 253.4.T47 T49"
}

@Book{lopym,
  author       = "Yann Mantel",
  title        = "Life of Pi",
  publisher    = "Harcourt",
  year         = 2001,
  price        = "$14.00",
  address      = "Orlando, Flordia",
  keywords     = "survival, religion, ship wreck",
  location     = "PR 9199.2.M3855 L54"
}

@Book{lll86,
  author       = "Leslie Lamport",
  title        = "\LaTeX",
  subtitle     = "A Document Preparation System",
  publisher    = aw,
  year         = 1986,
  address      = rma,
  keywords     = "document preparation, tex, document formatting",
  location     = "Z 253.4.L38 L35"
}

@Book{pampk,
  author       = "Philip Kraft",
  title        = "Programmers and Managers",
  subtitle     = "The Routinization of Computer Programming in the United States",
  publisher    = sv,
  year         = 1977,
  address      = nyny,
  keywords     = "computer programmers, personnel management, industrial
    relations, deskilling, fragmentation, professionalism",
  location     = "HD 8039.D37 K7"
}

@Book{tmcts,
  author       = "Terry Southern",
  title        = "The Magic Christian",
  publisher    = "Bantam",
  year         = 1960,
  address      = nyny,
  price        = "$0.75",
  keywords     = "satire",
  location     = "PS 3569.O8 M3"
}

@Book{wfss,
  author       = "Stuart Shea",
  title        = "Wrigley Field",
  subtitle     = "The Long Life & Contentious Times of the Friendly Confines",
  publisher    = ucp,
  year         = 2014,
  address      = chil,
  keywords     = "baseball, chicago cubs, wrigly field, history",
  location     = "GV 417.W75 S54"
}

@Book{cnllpbsd,
  author       = llp # " and Bruce~S. Davie",
  title        = "Computer Networks",
  subtitle     = "A Systems Approach",
  publisher    = "Morgan-Kaufman",
  year         = 2012,
  address      = "Burlington, " # MA,
  edition      = "fifth",
  keywords     = "internetworking, end-to-end protocols, congestion control,
    resource allocation, end-to-end data, network security, applications",
  location     = "TK 5105.5.P479"
}

@Book{tsoann,
  author       = "Elena Ferrante",
  title        = "The Story of a New Name",
  publisher    = "Europa Editions",
  year         = 2013,
  address      = nyny,
  price        = "$16.20",
  keywords     = "marriage, education, friendship",
  location     = "PQ 4866.E6345 S7713"
}

@Book{jaxbm,
  author       = "Brett McLaughlin",
  title        = "Java and {XML}",
  publisher    = "O'Reilly",
  year         = 2000,
  address      = seca,
  keywords     = "java, xml, jdom, web publishing, xml-rpc, xml schema",
  location     = "QA 76.73.J38 M39"
}

@Book{hatt,
  author       = "Laurie Colwin",
  title        = "Happy All the Time",
  publisher    = "Pocket Books",
  year         = 1978,
  address      = nyny,
  price        = "$2.25",
  keywords     = "romance, marriage",
  location     = "PS 3553.O4783 H3"
}

@Book{taor,
  author       = "Richard Hofstadter",
  title        = "The Age of Reform",
  subtitle     = "From Bryan to F.D.R.",
  publisher    = "Knopf",
  year         = 1955,
  address      = nyny,
  keywords     = "populism, progressivism, the new deal, politics, economics,
    movements", 
  location     = "E 743.H63"
}

@Book{notdkg,
  author       = "Doris Kearns Goodwin",
  title        = "No Ordinary Time",
  subtitle     = "Franklin and Eleanor Roosevelt: The Home Front in World War II",
  publisher    = "Simon \& Schuster",
  year         = 1994,
  address      = nyny,
  keywords     = "elenor & franklin roosevelt, ww2, american politics,
    emotional intelligence",
  location     = "E 807.G66"
}

@Book{maaew,
  author       = "Evelyn Waugh",
  title        = "Men at Arms",
  publisher    = "Little, Brown",
  year         = 1952,
  address      = boma,
  keywords     = "ww2, military bureaucracy, sad sacks",
  location     = "PZ 3.W356 PR6045.A97"
}

@Book{wwrh,
  author       = "Rob Hengeveld",
  title        = "Wasted World",
  subtitle     = "How Our Consumption Challenges the Planet",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "population growth, pollution, sustainability, ecological
    systems, population control",
  location     = "GF 75.H45"
}

@Book{tpmgws,
  author       = "Gary~W. Sabot",
  title        = "The Paralation Model",
  subtitle     = "Architecture-Independent Parallel Programming",
  publisher    = mitp,
  year         = 1988,
  address      = cama,
  keywords     = "parallel programming, lisp",
  location     = "QA 76.6.S216"
}

@Book{fatsk,
  author       = "S{\o}ren Kierkegaard",
  title        = "Fear and Trembling",
  publisher    = "Princeton University Press",
  year         = 1941,
  address      = prnj,
  keywords     = "faith, abraham, isaaic",
  location     = "BR 100.K52 1941"
}

@Book{gbeg,
  author       = "Yiyun Li",
  title        = "Gold Boy, Emerald Girl",
  publisher    = "Random House",
  year         = 2010,
  address      = nyny,
  keywords     = "the wonders of marriage, china",
  location     = "PL 2946 Y59 G65"
}

@Book{latr,
  author       = "Evelyn Waugh",
  title        = "Love Among the Ruins",
  subtitle     = "A Romance of the Near Future",
  publisher    = "Chapman \& Hall",
  year         = 1953,
  address      = loen,
  keywords     = "the near future, bitter satire",
  location     = "PZ 3.W356 PR6045.A97"
}

@Book{aljs,
  author       = "John Szwed",
  title        = "Alan Lomax",
  subtitle     = "The Man Who Recorded the World",
  publisher    = "Viking",
  year         = 2010,
  address      = nyny,
  keywords     = "folk music, obsession, biography",
  location     = "ML 423 L63478 S98"
}

@Book{twlatws,
  author       = "Elena Ferrante",
  title        = "Those Who Leave and Those Who Stay",
  publisher    = "Europa",
  year         = 2013,
  address      = nyny,
  keywords     = "management-labor strife, marriage, women's careers",
  location     = "PQ 4866.E6345 S77813"
}

@Book{nrzj,
  author       = "Zachary Jernigan",
  title        = "No Return",
  publisher    = "Night Shade Books",
  year         = 2013,
  address      = sfca,
  keywords     = "religion, aggression",
  location     = "PS 3610.E73 N6"
}

@Book{tmog,
  author       = "Yasunari Kawabata",
  title        = "The Master of Go",
  publisher    = "Berkeley Books",
  year         = 1972,
  address      = nyny,
  price        = "$1.75",
  keywords     = "go, mortality, sportsmanship",
  location     = "PL 832.A9 M413"
}

@Book{oagew,
  author       = "Evelyn Waugh",
  title        = "Officers and Gentlemen",
  publisher    = "Little, Brown",
  year         = 1995,
  address      = boma,
  keywords     = "war, planning",
  location     = "PR 6045.A97"
}

@Book{chac,
  author       = "Stephen~E. Ambrose",
  title        = "Crazy Horse and Custer",
  subtitle     = "The Parallel Lives of Two American Warriors",
  publisher    = "Anchor Books",
  year         = 1996,
  address      = nyny,
  keywords     = "19th century u.s. history, great plains wars, u.s. army,
    north american indians, sioux",
  location     = "E 99.03 A46"
}

@Book{tscc,
  author       = "Christopher Clark",
  title        = "The Sleepwalkers",
  subtitle     = "How Europe Went to War in 1914",
  publisher    = "HarperCollins",
  year         = 2012,
  address      = nyny,
  keywords     = "world war i, diplomacy, early 20th century european history, ",
  location     = "D 511.C54"
}

@Book{agirr,
  author       = "Julia~L. Mickenberg",
  title        = "American Girls in Red Russia",
  subtitle     = "Chasing the Soviet Dream",
  publisher    = ucp,
  year         = 2017,
  address      = chil,
  keywords     = "1930s russian history, socialism, development",
  location     = "DK 34.A45 M54"
}

@Book{toogp,
  author       = "Evelyn Waugh",
  title        = "The Ordeal of Gilbert Pinfold",
  subtitle     = "A Conversation Piece",
  publisher    = "Little, Brown",
  year         = 1957,
  address      = boma,
  keywords     = "travel, imagination",
  location     = "PR 6045.A97 O73 "
}

@Book{ocbgh,
  author       = "Barbara Grizzuti Harrison",
  title        = "Off Center",
  subtitle     = "Essays",
  publisher    = "Dial Press",
  year         = 1980,
  address      = nyny,
  keywords     = "joan didion, jane fonda, est, relationships, dick cavett",
  location     = "AC 5.O33"
}

@Book{wawawsimit2c ,
  author       = "Gary Younge",
  title        = "Who Are We---And Why Should It Matter in the 21th Century?",
  publisher    = "Nation Books",
  year         = 2011,
  address      = nyny,
  keywords     = "identity, xenophobia, nationalism",
  location     = "HM 753.Y68"
}

@Book{brggob,
  author       = "Gillian O'Brien",
  title        = "Blood Runs Green",
  subtitle     = "The Murder That Transfixed Gilded Age Chicago",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "nationalism, factionalism, journalism, late 19th century chicago",
  location     = "HV 6534.C4 O27"
}

@Book{ialg,
  author       = "Lisa Goldstein",
  title        = "Ivory Apples",
  publisher    = "Tachyon Publications",
  year         = 2019,
  address      = sfca,
  keywords     = "muses, obsession"
}

@Book{gcm2003,
  author       = "Caroline Moorehead",
  title        = "Gellhorn",
  subtitle     = "A Twentieth-Century Life",
  publisher    = "Henry Holt",
  year         = 2008,
  address      = nyny,
  keywords     = "war reporting, biography",
  location     = "PN 4874.G348 M66"
}

@Book{jdmeh,
  author       = "Elsa Hart",
  title        = "Jade Dragon Mountain",
  publisher    = "Minotaur Books",
  year         = 2015,
  address      = nyny,
  month        = sep,
  keywords     = "18th century china, astronomy, white devils",
  location     = "PS 3608.A78455 J33"
}

@Book{avftbdf,
  author       = "Doug Feldmann",
  title        = "A View From Two Benches",
  subtitle     = "Bob Thomas in Football and the Law",
  publisher    = "Northern Illinois University Press",
  year         = 2020,
  address      = "Ithica, N.Y.",
  keywords     = "football, law",
  location     = "GV939.T454.F44"
}

@Book{wps80,
  author       = "Pamela Sargent",
  title        = "Watchstar",
  subtitle     = "Book One of the Watchstar Trilogy",
  publisher    = "Pocket Books",
  year         = 1980,
  address      = nyny,
  keywords     = "coming of age, clash of cultures, religion in the world",
  location     = "PS 3569.A6887 W38"
}

@Book{daoitne,
  author       = "Ilana Gershon",
  title        = "Down and Out In the New Economy",
  subtitle     = "How People Find (Or Don't Find) Work Today",
  publisher    = ucp,
  year         = 2017,
  address      = chil,
  keywords     = "linked in, job hunting, brand you, quitting, the gig
    economy",
  location     = "HF 5382.75.U6 G465"
}

@Book{tsvmjc,
  author       = "M.~J. Carter",
  title        = "The Strangler Vine",
  publisher    = "G.~P. Putnam's Sons",
  year         = 2014,
  address      = nyny,
  keywords     = "colonialism, the india company, murrdaar, thugs",
  location     = "PR 6103.A7735 S77"
}

@Book{eotc,
  author       = "Pamela Sargent",
  title        = "Eye of the Comet",
  subtitle     = "Book Two of the Watchstar Trilogy",
  publisher    = "Harper \& Row",
  year         = 1984,
  address      = nyny,
  keywords     = "home, conflict, the machines",
  location     = "PZ7.S2472 Ey"
}

@Book{tsoalg,
  author       = "Elena Ferrante",
  title        = "The Story of the Lost Girl",
  publisher    = "Europa",
  year         = 2015,
  address      = nyny,
  keywords     = "the end game, mothers and daughters",
  location     = "PQ 4866.E6345 S77513"
}

@Book{thotfs,
  author       = "Edward~R. Pease",
  title        = "The History of the Fabian Society",
  publisher    = "E.~P. Dutton",
  year         = 1916,
  address      = nyny,
  keywords     = "english socialism, g.b. shaw, s. webb, h.g. wells, poverty",
  location     = "HX11.F5 P4"
}

@Article{famw,
  author       = "Charles~P. Thacker and Lawrence~C. Stewart",
  title        = "Firefly:  {A} Multiprocessor Workstation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "164--172",
  month        = oct,
  keywords     = "multiprocessor architecture, vax, caching, trade-offs,
    simulation",
  abstract     = "Firefly is a shared-memory multiprocessor workstation that
    contains from one to seven MicroVAX 78032 processors, each with a floating
    point unit and a sixteen kilobyte cache.  The caches are coherent, so that
    all processors see a consistent view of main memory.  A system may contain
    from four to sixteen megabytes of storage.  Input-output is done via a
    standard DEC QBus.  Input-output devices are an Ethernet controller, fixed
    disks, and a monochrome 1024 x 768 display with keyboard and mouse.
    Optional hardware includes a high resolution color display and a controller
    for high capacity disks.  Figure 1 is a system block diagram.The Firefly
    runs a software system that emulates the Ultrix system call interface.  It
    also supports medium- and coarse-grained multiprocessing through multiple
    threads of control in a single address space.  Communications are
    implemented uniformly through the use of remote procedure calls.This paper
    describes the goals, architecture, implementation and performance analysis
    of the Firefly.  It then presents some measurements of hardware
    performance, and discusses the degree to which SRC has been successful in
    producing software to take advantage of multiprocessing.", 
  location     = "https://doi.org/10.1145/36177.36199", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-23.html"
}

@Article{paritv8p,
  author       = "Douglas~W. Clark",
  title        = "Pipelining and Performance in the {VAX} 8800 Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "173--177",
  month        = oct,
  keywords     = "pipelining, microcode, micropipelining, instruction set
    architecture, performance", 
  abstract     = "The VAX 8800 family (models 8800, 8700, 8550), currently the
    fastest computers in the VAX product line, achieve their speed through a
    combination of fast cycle time and deep pipelining.  Rather than pipeline
    highly variable VAX instructions as such, the 8800 design pipelines uniform
    microinstructions whose addresses are generated by instruction unit
    hardware.  This design approach helps achieve a fast cycle time, which is
    the prime determinan of performance.  Some preliminary measurements of
    cycles per average instruction are reported.", 
  location     = "https://doi.org/10.1145/36177.36200"
}

@Article{avafatsc,
  author       = "Robert~P. Colwell and Robert~P. Nix and John~J. O'Donnell and David~B. Papworth and Paul~K. Rodman",
  title        = "{A} {VLIW} Architecture for a Trace Scheduling Compiler",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "180--192",
  month        = oct,
  keywords     = "vliw architecture, pipelining, compilation, trace scheduling,
    scientific computation, instruction set architecture, branching",
  abstract     = "Very Long Instruction Word (VLIW) architectures were promised
    to deliver far more than the factor of two or three that current
    architectures achieve from overlapped execution.  Using a new type of
    compiler which compacts ordinary sequential code into long instruction
    words, a VLIW machine was expected to provide from ten to thirty times the
    performance of a more conventional machine built of the same implementation
    technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE
    along with its companion Trace Scheduling compacting compiler.  This new
    machine has fulfilled the performance promises that were made. Using many
    fast functional units in parallel, this machine extends some of the basic
    Reduced-Instruction-Set precepts: the architecture is load/store, the
    microarchitecture is exposed to the compiler, there is no microcode, and
    there is almost no hardware devoted to synchronization, arbitration, or
    interlocking of any kind (the compiler has sole responsibility for runtime 
    resource usage).  This paper discusses the design of this machine and
    presents some initial performance results.", 
  location     = "https://doi.org/10.1145/36206.36201"
}

@Article{dttstcplitcm,
  author       = "David~R. Ditzel and Hubert~R. McLellan and Alan~D. Berenbaum",
  title        = "Design Tradeoffs to Support the {C} Programming Language in the {CRISP} Microprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "158--162",
  month        = oct,
  keywords     = "system architecture, language support, context switching,
    code density, stack cache, compiler support, code analysis, path length",
  abstract     = "The CRISP Microprocessor contains a number of new
    architectural features to achieve high performance and support the C
    programming language.  The instruction set was designed to be independent
    of architectural tradeoffs used in any single implementation.  This paper
    describes the particular tradeoffs used in the implementation of a 172,163
    transistor 32-bit single chip microprocessor.  Many tradeoffs were used in
    the design of CRISP, this paper tries to focus on those particular to C. ", 
  location     = "https://doi.org/10.1145/36177.36198"
}

@Article{arafsc,
  author       = "Richard~B. Kieburtz",
  title        = "{A} {RISC} Architecture for Symbolic Computation",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "146--155",
  month        = oct,
  keywords     = "graph reduction, combinators, g-machines, tagged data types,
    pipelining",
  abstract     = "The G-machine is a language-directed processor architecture
    designed to support graph reduction as a model of computation.  It can
    carry out lazy evaluation of functional language programs and can evaluate
    programs in which logical variables are used.  To support these language
    features, the abstract machine requires tagged memory and executes some
    rather complex instructions, such as to evaluate a function
    application.This paper explores an implementation of the G-machine as a
    high performance RISC architecture.  Complex instructions can be
    represented by RISC code without experiencing a large expansion of code
    volume.  The instruction pipeline is discussed in some detail.  The
    processor is intended to be integrated into a standard, 32-bit memory
    architecture.  Tagged memory is supported by aggregating data with tags in
    a cache.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{rvcfpacs,
  author       = "Gaetano Borriello and Andrew~R. Cherenson and Peter Bernard Danzig and Michael Newell Nelson",
  title        = "{RISCs} vs. {CISCs} for {Prolog}: {A} Case Study",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "136--145",
  month        = oct,
  keywords     = "prolog, logic programming, abstract machines, compilation,
    spur, tagged data types, unification, backtracking, coprocessors",
  abstract     = "This paper compares the performance of executing compiled
    Prolog code on two different architectures under development at
    U. C. Berkeley.  The first is the PLM, a special-purpose CISC architecture
    intended as a coprocessor for a host machine.  The second is SPUR, a
    general-purpose RISC architecture that supports tagged data.  Fourteen
    standard benchmark programs were run on both the PLM and SPUR simulators.
    The compiled code for SPUR was obtained by simple macro-expansion of PLM
    code generated by the PLM Prolog compiler.  The two implementations are
    compared with regard to static and dynamic program size, execution speed,
    and memory system performance.  On average, the macrocoded SPUR
    implementation has a static code size 14 times larger than the PLM,
    executes 16 times more instructions, yet requires only 2.3 times the number
    of machine cycles (or has the performance of 0.43 PLMs).  When memory
    system performance is taken into account, SPUR is equivalent to 0.29 PLMs.
    Optimizations of the macro-expanded code and minor architectural changes to
    SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks.
    Thus a tagged RISC architecture can execute Prolog at least half as fast as
    a special-purpose CISC architecture for Prolog.", 
  location     = "https://doi.org/10.1145/36177.36196"
}

@Article{tmeuailatmd,
  author       = "David~W. Wall and Michael~L. Powell",
  title        = "The {Mahler} Experience:  Using an Intermediate Language as the Machine Description",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "100--104",
  month        = oct,
  keywords     = "intermediate language, machine independence, compilers,
    global optimizations, link-time optimizations, instruction scheduling,
    pipelining, abstract machines",
  abstract     = "Division of a compiler into a front end and a back end that 
    communicate via an intermediate language is a well-known technique.  We go
    farther and use the intermediate language as the official description of a
    family of machines with simple instruction sets and addressing
    capabilities, hiding some of the inconvenient details of the real machine
    from the users and the front end compilers.To do this credibly, we have had
    to hide not only the existence of the details but also the performance
    consequences of hiding them.  The back end that compiles and links the
    intermediate language tries to produce code that does not suffer a
    performance penalty because of the details that were hidden from the front
    end compiler.  To accomplish this, we have used a number of link-time
    optimizations, including instruction scheduling and interprocedural
    register allocation, to hide the existence of such idiosyncracies as
    delayed branches and non-infinite register sets.  For the most part we have
    been successful.", 
  location     = "https://doi.org/10.1145/36177.36190",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-1.pdf"
}

@Article{asosctfps,
  author       = "Shlomo Weiss and James~E. Smith",
  title        = "{A} Study of Scalar Compilation Techniques for Pipelined Supercomputers",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "cray computers, loop unrolling, software pipelining,
    performance, scientific computing, optimization, machine architectures,
    register files, vectorization",
  abstract     = "This paper studies two compilation techniques for enhancing
    scalar performance in high-speed scientific processors: software pipelining
    and loop unrolling.  We study the impact of the architecture (size of the
    register file) and of the hardware (size of instruction buffer) on the
    efficiency of loop unrolling.  We also develop a methodology for
    classifying software pipelining techniques.  For loop unrolling, a
    straightforward scheduling algorithm is shown to produce near-optimal
    results when not inhibited by recurrences or memory hazards.  Our study
    indicates that the performance produced with a modified CRAY-1S scalar
    architecture and a code scheduler utilizing loop unrolling is comparable to
    the performance achieved by the CRAY-1S with a vector unit and the CFT
    vectorizing compiler.", 
  location     = "https://doi.org/10.1145/79505.79508"
}

@Article{cs8tar,
  author       = "William~R. Bush and A.~Dain Samples and David Ungar and Paul~N. Hilfinger",
  title        = "Compiling {Smalltalk-80} to a {RISC}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "105--109",
  month        = oct,
  keywords     = "risc architecture, bytecode, compilation, register windows,
    soar, dynamic languages, caching, performance",
  abstract     = "The Smalltalk On A RISC project at U.  C.  Berkeley proves
    that a high-level object-oriented language can attain high performance on a
    modified reduced instruction set architecture.  The single most important
    optimization is the removal of a layer of interpretation, compiling the
    bytecoded virtual machine instructions into low-level, register-based,
    hardware instructions.  This paper describes the compiler and how it was
    affected by SOAR architectural features.  The compiler generates code of
    reasonable density and speed.  Because of Smalltalk-80's semantics,
    relatively few optimizations are possible, but hardware and software
    mechanisms at runtime offset these limitations.  Register allocation for an
    architecture with register windows comprises the major task of the
    compiler.  Performance analysis suggests that SOAR is not simple enough;
    several hardware features could be efficiently replaced by instruction
    sequences constructed by the compiler.", 
  location     = "https://doi.org/10.1145/36177.36192"
}

@Article{hmamae,
  author       = "F.~Chow and S.~Correll and M.~Himelstein and E.~Killian and L.~Weber",
  title        = "How Many Addressing Modes are Enough?",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "117--121",
  month        = oct,
  keywords     = "risc, addressing modes, addressing architecture, offset
    indexing, optimizations, performance, simplicity",
  abstract     = "Programs naturally require a variety of memory-addressing
    modes.  It isn't necessary to provide them in hardware, however, if a
    compiler can synthesize them from a few primitive modes.  This not only
    simplifies the hardware, but also permits the compiler to use its
    understanding of the program to economize on the modes which it uses.  We
    present some compilation techniques that allow the compiler to deal
    effectively with a single addressing mode in a target RISC processor.  We
    also give measurements to show the benefits of such techniques, and to
    support our assertion that a single addressing mode is adequate for a
    general purpose processor, provided that mode incorporates both a pointer
    and an offset.", 
  location     = "https://doi.org/10.1145/36177.36193"
}

@Article{salatsp,
  author       = "Henry Massalin",
  title        = "Superoptimizer --- {A} Look at the Smallest Program",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "122--126",
  month        = oct,
  keywords     = "optimization, exhaustive search, probabilistic testing,
    assembly code",
  abstract     = "Given an instruction set, the superoptimizer finds the
    shortest program to compute a function.  Startling programs have been
    generated, many of them engaging in convoluted bit-fiddling bearing little
    resemblance to the source programs which defined the functions.  The key
    idea in the superoptimizer is a probabilistic test that makes exhaustive
    searches practical for programs of useful size.  The search space is
    defined by the processor's instruction set, which may include the whole
    set, but it is typically restricted to a subset.  By constraining the
    instructions and observing the effect on the output program, one can gain
    insight into the design of instruction sets.  In addition, superoptimized
    programs may be used by peephole optimizers to improve the quality of
    generated code, or by assembly language programmers to improve manually
    written code.", 
  location     = "https://doi.org/10.1145/36177.36194"
}

@Article{paaeotpm,
  author       = "Kazuo Taki and Katsuto Nakajima and Hiroshi Nakashima and Morihiro Ikeda",
  title        = "Performance and Architectural Evaluation of the {PSI Machine}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "128--135",
  month        = oct,
  keywords     = "kl0, prolog, logic programming, caching, branching, psi machine",
  abstract     = "We evaluated a Prolog machine PSI (Personal Sequential
    Inference machine) for the purpose of improving and redesigning it.  In
    this evaluation, we measured the execution speed and the dynamic
    characteristics of cache memory, register file, and branching hardware
    introduced for high-speed execution of Prolog programs.Execution speed of
    the PSI firmware interpreter was found to be comparable to that of the
    DEC-10 Prolog compiled code on the DEC-2060.  It was also found that PSI
    was faster than DEC for executing programs containing much unification and
    backtracking that require runtime processing.With the cache memory, the hit
    ratio for application programs was found higher than 96%; this demonstrates
    that the Prolog execution has much memory access locality.  The memory
    access frequency and the appearance ratio between Read and Write command
    were also investigated.Concerning the register file, use rate of each
    dedicated access mode was measured and effect of each mode was discussed.
    In the branching function we confirmed a high appearance rate of
    conditional branches and multi-way branches based on tag values.", 
  location     = "https://doi.org/10.1145/36177.36195"
}

@Article{cbatccaisohci,
  author       = "Sam Wineburg and Susan Mosborg and Dan Porat and Ariel Duncan",
  title        = "Common Belief and the Cultural Curriculum:  An Intergenerational Study of Historical Consciousness",
  journal      = "American Education Research Journal",
  year         = 2007,
  volume       = 44,
  number       = 1,
  pages        = "40--76",
  month        = mar,
  keywords     = "history instruction, collective memory, vietnam war, united
    states history, curricula, veterans, soldiers, student protests, political
    protests, textbooks, forest gump",
  abstract     = "How is historical knowledge transmitted across generations?
    What is the role of schooling in that transmission? The authors address
    these questions by reporting on a thirty-month longitudinal study into how
    home, school, and larger society served as contexts for the development of
    historical consciousness among adolescents.  Fifteen families drawn from
    three different school communities participated.  By adopting an
    intergenerational approach, the authors sought to understand how the
    defining moments of one generation-its lived history'-becomes the available
    history to the next.  In this article, the authors focus on what parents
    and children shared about one of the most formative historical events in
    parents' lives: the Vietnam War.  Drawing on notions of collective memory,
    as articulated by the French sociologist Maurice Halbwachs, the authors
    sought to understand which stories, archived in historical memory and
    available to the disciplinary community, are remembered and used by those
    beyond its borders.  In contrast, which stories are no longer widely
    shared, eclipsed by time's passage and unable to cross the bridge
    separating generation from generation? The authors conclude by discussing
    the forces that act to historicize today's youth and suggest how educators
    might marshal these forces-rather than spurning or simply ignoring them-to
    advance young people's historical understanding.",
  location     = "https://www.jstor.org/stable/30069471?sa=X&ved=2ahUKEwiOxfHPx__mAhVph-AKHfibCVcQFjAAegQICBAB"
}

@Article{imadothpa,
  author       = "Daniel~J. Magenheimer and Liz Peters and Karl Pettis and Dan Zuras",
  title        = "Integer Multiplication and Division on the {HP Precision Architecture}",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "90--99",
  month        = oct,
  keywords     = "multiplication, division, approximations",
  abstract     = "In recent years, many architectural design efforts have
    focused on maximizing performance for frequently executed, simple
    instructions.  Although these efforts have resulted in machines with better
    average price/performance ratios, certain complex instructions and, thus,
    certain classes of programs which heavily depend on these instructions may
    suffer by comparison.  Integer multiplication and division are one such set
    of complex instructions.  This paper describes how a small set of primitive
    instructions combined with careful frequency analysis and clever
    programming allows the Hewlett-Packard Precision Architecture integer
    multiplication and division implementation to provide adequate performance
    at little or no hardware cost.", 
  location     = "https://doi.org/10.1145/36206.36189"
}

@Article{aecfipooai4,
  author       = "Christos John Georgiou and Stewart~L. Palmer and P.~L. Rosenfeld",
  title        = "An Experimental Coprocessor for Implementing Persistent Objects on an {IBM} 4381",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "84--87",
  month        = oct,
  keywords     = "coprocessors, persistent objects, system architecture",
  abstract     = "In this paper we describe an experimental coprocessor for an
    IBM 4381 that is designed to facilitate the exploration of persistent objects.",
  location     = "https://doi.org/10.1145/36177.36188"
}

@Article{chsfsdap,
  author       = "Thomas~A. Cargill and Burt~N. Locanthi",
  title        = "Cheap Hardware Support for Software Debugging and Profiling",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "82--83",
  month        = oct,
  keywords     = "debugging, watch points, reverse execution",
  abstract     = "We wish to determine the effectiveness of some simple
    hardware for debugging and profiling compiled programs on a conventional
    processor. The hardware cost is small -- a counter decremented on each
    instruction that raises an exception when its value becomes zero. With the
    counter a debugger can provide data watchpoints and reverse execution: a
    profiler can measure the total instruction cost of a code segment and
    sample the program counter accurately. Such a counter has been included on
    a single-board MC68020 workstation, for which system software is currently
    being written. We will report our progress at the symposium.", 
  location     = "https://doi.org/10.1145/36177.36187"
}

@Article{cotplictp,
  author       = "Pei~Jyun Leu and Bharat Bhargava",
  title        = "Clarification of Two-Phase Locking in Concurrent Transaction Processing",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "122--125",
  month        = jan,
  keywords     = "two-phase locking, atomic operations, transactions, read
    locks, relaxation, serializability, write locks, logs",
  abstract     = "The authors propose a formal definition of the two-phase
    locking class derived from the semantic description of the two-phase
    locking protocol, and prove that this definition is equivalent to that
    given by C.H.  Papadimitriou (1979).  They present: (1) a precise
    definition of the two phase locking; (2) a clarification of the occurrence
    and the order ofall events such as lock points, unlock points, read
    operations, and write operations of conflicting transactions; and (3) by
    relaxing some conditions in the given definition, the derivation of a new
    class called restricted-non-two-phase locking (RN2PL), which is a superset
    of the class two-phase locking (2PL) but a subset of the class
    D-serializable (DSR) given by Papadimitriou.",
  location     = "https://doi.org/10.1109/32.4629"
}

@Article{cfmvac,
  author       = "James~R. Goodman",
  title        = "Coherency for Multiprocessor Virtual Address Caches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "72--80",
  month        = oct,
  keywords     = "caches, coherency, virtual storage, virtual addressing, tlbs,
    snooping",
  abstract     = "A multiprocessor cache memory system is described that
    supplies data to the processor based on virtual addresses, but maintains
    consistency in the main memory, both across caches and across virtual
    address spaces.  Pages in the same or different address spaces may be
    mapped to share a single physical page.  The same hardware is used for
    maintaining consistency both among caches and among virtual addresses.
    Three different notions of a cache block are defined: (1) the unit for
    transferring data to/from main storage, (2) the unit over which tag
    information is maintained, and (3) the unit over which consistency is
    maintained.  The relation among these block sizes is explored, and it is
    shown that they can be optimized independently.  It is shown that the use
    of large address blocks results in low overhead for the virtual address
    cache.", 
  location     = "https://doi.org/10.1145/36177.36186"
}

@Article{tdprra,
  author       = "Russell~R. Atkinson and Edward~M. McCreight",
  title        = "The {Dragon} Processor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "65--69",
  month        = oct,
  keywords     = "processor design, instruction sets, procedure calls, dorado,
    instruction density, chip packaging",
  abstract     = "The Xerox PARC Dragon is a VLSI research computer that uses
    several techniques to achieve dense code and fast procedure calls in a
    system that can support multiple processors on a central high bandwidth
    memory bus.",
  location     = "https://doi.org/10.1145/36205.36185"
}

@Article{teoiscopsamp,
  author       = "Jack~W. Davidson and Richard~A. Vaughan",
  title        = "The Effect of Instruction Set Complexity on Program Size and Memory Performance",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "60--64",
  month        = oct,
  keywords     = "instruction set design, risc, cisc, memory pressure, code
    size, portable compilation",
  abstract     = "One potential disadvantage a machines with a simple
    instruction set is object-program size may be substantially larger than
    those for a machine with a complex instruction set.  Groups of simple
    instructions are required to implement the same functions performed by a
    single instruction from a complex instruction set.  In addition, the
    tendency of simple instructions to be fixed length with a few instruction
    formats also increases object-program size.  Larger object-program size
    could adversely affect memory performance and bus traffic.  This paper
    reports the results of experiments to isolate and determine the effect of
    instruction set complexity on cache memory performance and bus traffic.
    Three high-level language compilers were constructed for machines with
    instruction sets of varying complexity.  Using a set of benchmark programs,
    we evaluated the effect of instruction set complexity had on program size.
    Five of the programs were used to perform a set of trace-driven simulations
    to study each machine's cache and bus performance.  While we found that the
    miss ratio is affected by object program size, it appears that this can be
    corrected by increasing the cache size.  Our measurements of bus traffic,
    however, show that even with large caches, machines with simple instruction
    sets can expect substantially more main memory reads than machines with
    complex instruction sets.",  
  location     = "https://doi.org/10.1145/36205.36184"
}

@Article{aafdfdpd,
  author       = "Mike Adler",
  title        = "An Algebra for Data Flow Diagram Process Decomposition",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "169--183",
  month        = feb,
  keywords     = "algebra, automatic process decomposition, data flow diagram,
    dfd, directed acyclic graph, graph-based grammar, software engineering,
    structured analysis",
  abstract     = "Data flow diagram process decomposition, as applied in the
    analysis phase of software engineering, is a top-down method that takes a
    process, and its input and output data flows, and logically implements the
    process as a network of smaller processes.  The decomposition is generally
    performed in an ad hoc manner by an analyst applying heuristics, expertise,
    and knowledge to the problem.  An algebra that formalizes process
    decomposition is presented using the De Marco representation scheme.  In
    this algebra, the analyst relates the disjoint input and output sets of a
    single process by specifying the elements of an input/output connectivity
    matrix.  A directed acyclic graph is constructed from the matrix and is the
    decomposition of the process.  The graph basis, grammar matrix, and graph
    interpretations, and the operators of the algebra are discussed.  A
    decomposition procedure for applying the algebra, prototype, and production
    tools and outlook are also discussed.", 
  location     = "https://doi.org/10.1109/32.4636"
}

@Article{tatcilhasa,
  author       = "Peter Steenkiste and John Hennessy",
  title        = "Tags and Type Checking in {LISP}:  Hardware and Software Approaches",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "50--59",
  month        = oct,
  keywords     = "lisp, type checking, generic operations, tagged
    architectures, performance",
  abstract     = "One  major factor distinguishing LISP from other languages
    (e.g., Pascal, C, Fortran) is the need for run-time type checking.  Run-time
    type checking is implemented by adding to each data     object a tag that
    encodes type information.  Tags must be compared for type compatibility,
    removed when using the data, and inserted when new data items are created.
    This tag manipulation, together with other work related to dynamic type
    checking and generic operations, constitutes a significant component of the
    execution time of LISP programs.  This has led both to the development of
    LISP machines that support tag checking in hardware and to the avoidance of
    type checking by users running on stock hardware.  To understand the role
    and necessity of special-purpose hardware for tag handling, we first
    measure the cost of type checking operations for a group of LISP programs.
    We then examine hardware and software implementations of tag operations and
    estimate the cost of tag handling with the different tag implementation
    schemes.  The data shows that minimal levels of support provide most of the
    benefits, and that tag operations can be relatively inexpensive, even when
    no special hardware support is present.",  
  location     = "https://doi.org/10.1145/36206.36183"
}

@Article{aaftdeotfpl,
  author       = "John~R. Hayes and Martin~E. Fraeman and Robert~L. Williams and Thomas Zaremba",
  title        = "An Architecture for the Direct Execution of the {Forth} Programming Language",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "42--49",
  month        = oct,
  keywords     = "zero-address languages, stack management, embedded systems,
    instruction set architecture, data paths",
  abstract     = "We have developed a simple direct execution architecture for
    a 32-bit Forth microprocessor.  The processor can directly access a linear
    address space of over 4 gigawords.  Two instruction types are defined; a
    subroutine call, and a user defined microcode instruction.  On-chip stack
    caches allow most Forth primitives to execute in a single cycle.", 
  location     = "https://doi.org/10.1145/36206.36182"
}

@Article{asfmppohs,
  author       = "Roberto Bisiani and Alessandro Forin",
  title        = "Architectural Support for Multilanguage Parallel Programming on Heterogeneous Systems",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "21--30",
  month        = oct,
  keywords     = "common runtimes, shared storage, remote operations, mach",
  abstract     = "We have designed and implemented a software facility, called
    Agora, that supports the development of parallel applications written in
    multiple languages.  At the core of Agora there is a mechanism that allows
    concurrent computations to share data structures independently of the
    computer architecture they are executed on.  Concurrent computations
    exchange control information by using a pattern-directed technique.  This
    paper describes the Agora shared memory and its software implementation on
    both tightly and loosely-coupled architectures.", 
  location     = "https://doi.org/10.1145/36177.36180"
}

@Article{mivmmfpuama,
  author       = "Richard Rashid and Avadis Tevanian and Michael Young and David Golub and Robert Baron and David Black and William Bolosky and Jonathan Chew",
  title        = "Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "31--39",
  month        = oct,
  keywords     = "mach, virtual storage, machine independence, portability,
    shared storage, address mapping, paging",
  abstract     = "This paper describes the design and implementation of virtual
    memory management within the CMU Mach Operating System and the experiences
    gained by the Mach kernel group in porting that system to a variety of
    architectures.  As of this writing, Mach runs on more than half a dozen
    uniprocessors and multiprocessors including the VAX family of uniprocessors
    and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the
    Sequent Balance 21000 and several experimental computers.  Although these
    systems vary considerably in the kind of hardware support for memory
    management they provide, the machine-dependent portion of Mach virtual
    memory consists of a single code module and its related header file.  This
    separation of software memory management from hardware support has been
    accomplished without sacrificing system performance.  In addition to
    improving portability, it makes possible a relatively unbiased examination
    of the pros and cons of various hardware memory management schemes,
    especially as they apply to the support of multiprocessors.", 
  location     = "https://doi.org/10.1145/36177.36181"
}

@Article{vafam,
  author       = "Bob Beck and Bob Kasten and Shreekant Thakkar",
  title        = "{VLSI} Assist for a Multiprocessor",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "10--20",
  month        = oct,
  keywords     = "caching, co-processors, ipc, synchronization, buses,
    interrupt handling, system configuration",
  abstract     = "Multiprocessors have long been of interest to computer
    community.  They provide the potential for accelerating applications through
    parallelism and increased throughput for large multi-user system.  Three
    factors have limited the commercial success of multiprocessor systems;
    entry cost, range of performance, and ease of application.  Advances in
    large scale integration (VLSI) and in computer aided design (CAD) have
    removed these limitations, making possible a new class of multiprocessor
    systems based on VLSI components.  A set of requirements for building an
    efficient shared multiprocessor system are discussed, including: low-level
    mutual exclusion, interrupt distribution, inter-processor signaling,
    process dispatching, caching, and system configuration.  A system that
    meets these requirements is described and evaluated.", 
  location     = "https://doi.org/10.1145/36177.36179"
}

@Article{clir,
  author       = "Matthew Flatt",
  title        = "Creating Languages in {Racket}",
  journal      = "ACM Queue",
  year         = 2011,
  volume       = 9,
  number       = 11,
  month        = nov,
  keywords     = "racket, language design, macros, dsl, modules",
  abstract     = "Choosing the right tool for a simple job is easy: a
    screwdriver is usually the best option when you need to change the battery
    in a toy, and grep is the obvious choice to check for a word in a text
    document.  For more complex tasks, the choice of tool is rarely so
    straightforward—all the more so for a programming task, where programmers
    have an unparalleled ability to construct their own tools.  Programmers
    frequently solve programming problems by creating new tool programs, such
    as scripts that generate source code from tables of data.", 
  location     = "https://doi.org/10.1145/2063166.2068896", 
  location     = "https://queue.acm.org/detail.cfm?id=2068896"
}

@Article{hafplaplfha,
  author       = "Niklaus Wirth",
  title        = "Hardware Architecture for Programming Languages and Programming Languages for Hardware Architectures",
  journal      = asplos87,
  year         = 1987,
  volume       = 22,
  number       = 10,
  pages        = "2--8",
  month        = oct,
  keywords     = "abstractions, hardware design, complexity, mathematical
    formalisms, system state, correctness reasoning, programming languages",
  abstract     = "Programming Languages and Operating Systems introduce
    abstractions which allow the programmer to ignore details of an
    implementation.  Support of an abstraction must not only concentrate on
    promoting the efficiency of an implementation, but also on providing the
    necessary guards against violations of the abstractions.  In the frantic
    drive for efficiency the second goal has been neglected.  There are
    indications that recent designs which are claimed to be both simple and
    powerful, achieve efficiency by shifting the complex issues of code
    generation and of appropriate guards onto compilers.Complexity has become
    the common hallmark of software as well as hardware designs.  It cannot be
    mastered by the common practices of testing and simulation.  Hardware
    design may profit from developments in programming methodology by adopting
    proof techniques similar to those used in programming.", 
  location     = "https://doi.org/10.1145/36177.36178"
}

@Article{atosigpdcs,
  author       = "Thomas~L. Casavant and Jon~G. Kuhl",
  title        = "{A} Taxonomy of Scheduling in General-Purpose Distributed Computing Systems",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 2,
  pages        = "141--154",
  month        = feb,
  keywords     = "distributed operating systems, distributed resource
    management, general-purpose distributed computing systems, scheduling, task
    allocation, taxonomy",
  abstract     = "One measure of the usefulness of a general-purpose
    distributed computing system is the system's ability to provide a level of
    performance commensurate to the degree of multiplicity of resources present
    in the system.  A taxonomy of approaches to the resource management problem
    is presented in an attempt to provide a common terminology and
    classification mechanism necessary in addressing this problem.  The
    taxonomy, while presented and discussed in terms of distributed scheduling,
    is also applicable to most types of resource management.", 
  location     = "https://doi.org/10.1109/32.4634"
}

@Article{idfaa,
  author       = "Barbara~G. Ryder and Marvin~C. Paull",
  title        = "Incremental Data-Flow Algorithm Algorithms",
  journal      = toplas,
  year         = 1988,
  volume       = 10,
  number       = 1,
  pages        = "1--50",
  month        = jan,
  keywords     = "incremental algorithms, data-flow equations, interval
    analysis",
  abstract     = "An incremental update algorithm modifies the solution of a
    problem that has been changed, rather than re-solving the entire problem.
    ACINCF and ACINCB are incremental update algorithms for forward and
    backward data-flow analysis, respectively, based on our equations model of
    Allen-Cocke interval analysis.  In addition, we have studied their
    performance on a “nontoy” structured programming language L.  Given a set
    of localized program changes in a program written in L, we identify a
    priori the nodes in its flow graph whose corresponding data-flow equations
    may be affected by the changes.  We characterize these possibly affected
    nodes by their corresponding program structures and their relation to the
    original change sites, and do so without actually performing the
    incremental updates.  Our results can be refined to characterize the
    reduced equations possibly affected if structured loop exit mechanisms are
    used, either singly or together, thereby relating richness of programming
    language usage to the ease of incremental updating.", 
  location     = "https://doi.org/10.1145/42192.42193"
}

@Article{mmiaes,
  author       = "Christopher Rosebrugh and Eng-Kee Kwang",
  title        = "Multiple Microcontrollers in an Embedded System",
  journal      = ddj,
  year         = 1992,
  volume       = 17,
  number       = 1,
  pages        = "48--57",
  month        = jan,
  keywords     = "hardware design, ",
  abstract     = "A case study in system architecture and embedded hardware design"
}

@Article{j14pjsacp1,
  author       = "Wm. Paul Rogers",
  title        = "{J2SE} 1.4 Premieres {Java}'s Assertion Capabilities, Part 1",
  journal      = "Java World",
  year         = 2001,
  month        = November,
  keywords     = "java, assertions",
  location     = "https://www.javaworld.com/article/2075803/j2se-1-4-premieres-java-s-assertion-capabilities--part-1.html"
}

@Article{mmocoaiafoed,
  author       = "Ketabchi, Mohammad~A. and Berzins, Valdis",
  title        = "Mathematical Model of Composite Objects and Its Application for Organizing Engineering Databases",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "71--84",
  month        = jan,
  keywords     = "database partitioning, composite objects, engineering
    databases, clustering concept, component aggregation, assemblies,
    equivalent objects, equivalence classes, Boolean algebra, minterms, stored
    views, relational database, design data, frequent access patterns", 
  abstract     = "The authors introduce a clustering concept called component
    aggregation which considers assemblies having the same types of parts as
    equivalent objects.  The notion of equivalent objects is used to develop a
    mathematical model of composite objects.  It is shown that the set of
    equivalence classes of objects form a Boolean algebra whose minterms
    represent the objects that are not considered composite at the current
    viewing level.  The algebraic structure of composite objects serves as a
    basis for developing a technique for organizing composite objects and
    supporting materialization of explosion views.  The technique provides a
    clustering mechanism which partitions the database into meaningful and
    application-oriented clusters, and allows any desired explosion view to be
    materialized using a minimal set of stored views.  A simplified relational
    database for design data and a set of frequent access patterns in design
    applications are outlined and used to demonstrate the benefits of database
    organizations based on the mathematical model of composite objects.", 
  location     = "https://doi.org/10.1109/32.4624"
}

@Article{elsfpsi,
  author       = "Deepinder~P. Sidhu and Carole~S. Crall",
  title        = "Executable Logic Specifications for Protocol Service Interfaces",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "98--121",
  month        = jan,
  keywords     = "automated development tools, formal description technique,
    formal modeling, protocol specification, protocol verification, state
    transitions, prolog, service specification, iso osi model, tcp, tp2, tp4",
  abstract     = "A general, formal modeling technique for protocol service
    interfaces is discussed.  An executable description of the model using a
    logic-programming-based language, Prolog, is presented.  The specification
    of protocol layers consists of two parts, the specification of the protocol
    interfaces and the specification of entities within the protocol layer.
    The specification of protocol interfaces forms the standard against which
    protocols are verified.  When a protocol has been implemented, the
    correctness of its implementation can be tested using the sequences of
    events generated at the service interface.  If the behavior of the protocol
    implementation is consistent with the behavior at the service interface,
    the implementation conforms to its standard.  To illustrate how it works,
    the model is applied to the service interfaces of protocol standards
    developed for the transport layer of the ISO/OSI architecture.  The results
    indicate that Prolog is a useful formal language for specifying
    protocol interfaces.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4626"
}

@Article{fabfpe,
  author       = "Nazim~H. Madhavji",
  title        = "Fragtypes:  {A} Basis for Programming Environments",
  journal      = tse,
  year         = 1988,
  volume       = 14,
  number       = 1,
  pages        = "85--97",
  month        = jan,
  keywords     = "modula-2, mupe-2, program fragments, programming
    environments, programming in the all, program composition, structured manipulation",
  abstract     = "The author introduces a novel basis for programming
    environments that encourages development of software in fragments of
    various types, called fragtypes.  Fragtypes range from a simple expression
    type to a complete subsystem type.  As a result, they are suited to the
    development of software in an enlarged scope that includes both programming
    in the small and programming in the large.  The author shows how proposed
    operations on fragtypes can achieve unusual effects on the software
    development process.  Fragtypes and their associated construction rules
    form the basis of the programming environment MUPE-2, which is currently
    under development at McGill University.  The target and the implementation
    language of this environment is the programming language Modula-2.", 
  location     = "https://dl.acm.org/doi/10.1109/32.4625"
}

@Article{aaomasisuotsb,
  author       = "Robert~F. Cmelik and Shing~I. Kong and David~R. Ditzel and Edmund~J. Kelly",
  title        = "An Analysis of {MIPS} and {SPARC} Instruction Set Utiliation on the {SPEC} Benchmarks",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "290--302",
  month        = apr,
  keywords     = "instruction sets, sparc, mips, risc, autoincrement,
    compilers, delay slots",
  abstract     = "The dynamic instruction counts of MIPS and SPARC are compared
    using the SPEC benchmarks.  MIPS typically executes more user-level
    instructions than SPARC.  This difference can be accounted for by
    architectural differences, compiler differences, and library differences.
    The most significant differences are that SPARC�S double-precision floating
    point load/store is an architectural advantage in the SPEC floating point
    benchmarks while MIPS�s compare-and-branch instruction is an architectural
    advantage in the SPEC integer benchmarks.  After the differences in the two
    architectures are isolated, it appears that although MIPS and SPARC each
    have strengths and weaknesses in their compilers and library routines, the
    combined effect of compilers and library routines does not give either MIPS
    or SPARC a clear advantage in these areas.", 
  location     = "https://doi.org/10.1145/106974.107001"
}

@Article{pcoafotirs6,
  author       = "C.~Brian Hall and Kevin O'Brien",
  title        = "Performance Characteristics of Architectural Features of the {IBM RISC System/6000}",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "303--308",
  month        = apr,
  keywords     = "instruction sets, count registers, branching, performance,
    condition codes",
  abstract     = "The IBM RISC System/6000 has a number of architectural
    features that, are not usually found on RISC machines.  Among these are
    pre-increment and decrement forms of memory referencing instructions, a
    special purpose count register that can be used as a loop counter, and
    eight independent sets of condition code bits.  This paper examines the
    performance gained on a number of industry sta.udard benchmarks through the
    use of each of these features.", 
  location     = "https://doi.org/10.1145/106974.107002"
}

@Article{pfacaraacws,
  author       = "Dileep Bhandarkar and Douglas~W. Clark",
  title        = "Performance from Architecture:  Comparing a {RISC} and a {CISC} with Similar Hardware Organization",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "310--319",
  month        = apr,
  keywords     = "performance, mips, vax, pipelining, benchmarking, cache
    behavior, system architecture",
  abstract     = "Performance comparisons across different computer
    architectures cannot usually separate the architectural contribution from
    various implementation and technology contributions to performance.  This
    paper compares an example implementation from the RISC and CISC
    architectural schools (a MIPS M/2000 and a Digital VAX 8700) on nine of the
    ten SPEC benchmarks.  The organizational similarity of these machines
    provides an opportunity to examine the purely architectural advantages of
    RISC.  The RISC approach offers, compared with VAX, many fewer cycles per
    instruction but somewhat more instructions per program.  Using results from
    a software monitor on the MIPS machine and a hardware monitor on the VAX,
    this paper shows that the resulting advantage in cycles per program ranges
    from slightly under a factor of 2 to almost a factor of 4, with a geometric
    mean of 2.7.  It also demonstrates the correlation between cycles per
    instruction and relative instruction count.  Various reasons for this
    correlation, and for the consistent net advantage of RISC, are discussed.", 
  location     = "https://doi.org/10.1145/106972.107003"
}

@Article{pcwfai,
  author       = "Eric Freudenthal and Allan Gottlieb",
  title        = "Process Coordination with Fetch-and-Increment",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "260--268",
  month        = apr,
  keywords     = "barrier synchronization, bottleneck-free algorithms,
    fetch-and-add, fetch-and-increment, parallel access queues, process
    coordination, readers-writers problem",
  abstract     = "The fetch-and-add (F&A) operation has been used effectively
    in a number of process coordination algorithms.  In this paper we assess
    the power of fetch-and-increment (F&I) and fetch-and-decrement (F&D), which
    we view as restricted forms of F&A in which the only addends permitted are
    ±1.  F&A-based algorithms that use only unit addends are thus trivially
    expressed with just F&I and F&D.  Our primary contributions are new F&I/F&D
    algorithms for readers/writers coordination and barrier synchronization for
    dynamically-sized groups.  We also restructure an existing F&A-based
    algorithm for queues-with-multiplicity to obtain an algorithm using just
    F&I and F&D.  When executed on certain hardware architectures, most of
    these algorithms are free of serial bottlenecks.  We also discuss a general
    technique for implementing F&A using F&I/F&D at a cost logarithmic in the
    number of processors.", 
  location     = "https://doi.org/10.1145/106974.106998", 
  location     = "https://nyuscholars.nyu.edu/en/publications/process-coordination-with-fetch-and-increment"
}

@Article{tcfarb,
  author       = "Douglas Johnson",
  title        = "The Case for a Read Barrier",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "279--287",
  month        = apr,
  keywords     = "lisp, garbage collection, generational gc, temporal gc,
    virtual memory, paging",
  abstract     = "This paper looks at the performance of two different garbage
    collection algorithms on a large and long running Lips application.
    Both algorithms use write barriers for generational collection.  Only one
    algorithm uses a rad barrier for incremental collection.  The results show
    little difference in the two algorithm's ability to collect garbage and
    some difference in memory size.  Any differences in CPU usage were too
    small to be visible with the measuring techniques used.  However, there
    were major differences in paging behavior with a read barrier permitted the
    garbage collector to work with the virtual memory manager instead of
    independently.", 
  location     = "https://doi.org/10.1145/106974.107000"
}

@Article{swcjmmc,
  author       = "John~M. Mellor-Crummey and Michael~L. Scott",
  title        = "Synchroniation Without Contention",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "269--278",
  month        = apr,
  keywords     = "bbn butterfly multiprocessor and busy-wait synchronization
    and contention-free mutual exclusion and dance hall machines and exploit
    local access and fetch_and_X instructions and large shared-memory
    multiprocessor and local access and memory consistency  and performance and
    reader-writer control and sequent symmetry and special-purpose hardware
    support", 
  abstract     = "Conventional wisdom holds that contention due to busy-wait
    synchronization is a major obstacle to scalability and acceptable
    performance in large shared-memory multiprocessors.  We argue the contrary,
    and present fast, simple algorithms for contention-free mutual exclusion,
    reader-writer control, and barrier synchronization.  These algorithms,
    based on widely available fetch_and_phi instructions, exploit local access
    to shared memory to avoid contention.  We compare our algorithms to
    previous approaches in both qualitative and quantitative terms, presenting
    their performance on the Sequent Symmetry and BBN Butterfly
    multiprocessors.  Our results highlight the importance of local access to
    shared memory, provide a case against the construction of so-called 'dance
    hall' machines, and suggest that special-purpose hardware support for
    synchronization is unlikely to be cost effective on machines with
    sequentially consistent memory.", 
  location     = "https://doi.org/10.1145/106975.106999"
}

@Article{aecbaads,
  author       = "Sang~L. Min and Jong-Deok Choi",
  title        = "An Efficient Cache-based Access Anomaly Detection Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "235--244",
  month        = apr,
  keywords     = "cache coherence, access anomalies, processor scheduling,
    cache-coherence protocols",
  abstract     = "One of the important issues in parallel program debugging is
    an efficient detection of access anomalies caused by uncoordinated access
    to shared variables.  On-the-fly detection of access anomalies has the
    major advantage that it reports only actual anomalies during execution
    while static analysis methods report all the potential anomalies, many of
    which cannot actually materialize during execution.  It also has the
    advantage that shorter traces are produced for post-mortem analysis
    purposes if an anomaly is detected.  The reason for this is that after an
    anomaly occurs, further trace information is of dubious value because the
    first anomaly may have affected subsequent program behavior.  So, once the
    first anomaly occurs, no further trace information need be generated.
    Existing methods for on-the-fly access anomaly detection suffer from
    performance penalties since the execution of the program being debugged has
    to be interrupted on every access to shared variables.  In this paper, we
    propose an efficient cache-based access anomaly detection scheme that
    piggybacks on the overhead already paid by the underlying cache coherence
    protocol.", 
  location     = "https://doi.org/10.1145/106973.106996"
}

@Article{peomcmfsmm,
  author       = "Kourosh Gharachorloo and Anoop Gupta and John Hennessy",
  title        = "Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "245--257",
  month        = apr,
  keywords     = "The memory consistency model supported by a multiprocessor
    architecture determines the amount of buffering and pipelining that may be
    used to hide or reduce the latency of memory accesses.  Several different
    consistency models have been proposed.  These range from sequential
    consistency on one end, allowing limited buffering, to release
    consistency on the other end, allowing extensive buffering and pipelining.
    The processor consistency and weak consistency models fall in between.  The
    advantage of the less strict models is increased performance potential.
    The disadvantage is increased hardware complexity and a more complex
    programming model.  To make an informed decision on the above trade-off
    requires performance data for the various models.  This paper addresses the
    issue of performance benefits from the above four consistency models.  Our
    results are based on simulation studies done for three applications.  The
    results show that in an environment where processor reads are blocking and
    writes are buffered, a significant performance increase is achieved from
    allowing reads to bypass previous writes.  Pipelining of writes, which
    determines the rate at which writes are retired from the write buffer, is
    of secondary importance.  As a result, we show that the sequential
    consistency model performs poorly relative to all other models, while the
    processor consistency model provides most of the benefits of the weak and
    release consistency models.", 
  location     = "https://doi.org/10.1145/106973.106997"
}

@Article{pacup,
  author       = "Jacques Cohen and Timothy~J. Hickey",
  title        = "Parsing and Compiling Using Prolog",
  journal      = toplas,
  year         = 1987,
  volume       = 9,
  number       = 2,
  pages        = "125--163",
  month        = apr,
  keywords     = "code generation, grammar properties, optimization, parsing,
    prolog, backtracking, definite-clause grammars, difference lists",
  abstract     = "This paper presents the material needed for exposing the
    reader to the advantages of using Prolog as a language for describing
    succinctly most of the algorithms needed in prototyping and implementing
    compilers or producing tools that facilitate this task.  The available
    published material on the subject describes one particular approach in
    implementing compilers using Prolog.  It consists of coupling actions to
    recursive descent parsers to produce syntax-trees which are subsequently
    utilized in guiding the generation of assembly language code.  Although
    this remains a worthwhile approach, there is a host of possibilities for
    Prolog usage in compiler construction.  The primary aim of this paper is to
    demonstrate the use of Prolog in parsing and compiling.  A second, but
    equally important, goal of this paper is to show that Prolog is a
    labor-saving tool in prototyping and implementing many non-numerical
    algorithms which arise in compiling, and whose description using Prolog is
    not available in the literature.  The paper discusses the use of
    unification and nondeterminism in compiler writing as well as means to
    bypass these (costly) features when they are deemed unnecessary.  Topics
    covered include bottom-up and top-down parsers, syntax-directed
    translation, grammar properties, parser generation, code generation, and
    optimizations.  Newly proposed features that are useful in compiler
    construction are also discussed.  A knowledge of Prolog is assumed.", 
  location     = "https://doi.org/10.1145/22719.22946"
}

@Article{pcoppida,
  author       = "Edward~K. Lee and Randy~H. Katz",
  title        = "Performance Consequences of Parity Placement in Disk Arrays",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "190--199",
  month        = apr,
  keywords     = "raid, error detection, parity, data mapping, performance",
  abstract     = "Due to recent advances in CPU and memory system performance,
    I/O systems are increasingly limiting the performance of modern computer
    systems.  Redundant Arrays of Inexpensive Disks (RAID) have been proposed
    by Patterson et. al. to meet the impending I/O crisis.  RAIDs substitute
    many small inexpensive disks for a few large expensive disks to provide
    higher performance (both transfer rate and I/O rate), smaller footprints
    and lower power consumption at a lower cost than the large expensive disks
    they replace, Unfortunately, with so many small disks, media availability
    becomes a serious problem.  RAIDs provide high availability by using parity
    encoding of data to survive disk failures.  As will be shown by this paper,
    the way parity is distributed in a RAID has significant consequences for
    performance.  In particular, we show that for relatively large request
    sizes of hundreds of kilobytes, the choice of parity placement
    significantly affects performance (up to 20-30 percent for the typical disk
    array configurations that are common today) and propose properties that are
    generally desirable of parity placements.",  
  location     = "https://doi.org/10.1145/106974.106992"
}

@Article{ctcocacfatlf,
  author       = "Vincent Cate and Thomas Gross",
  title        = "Combining the Concepts of Compression and Caching for a Two-Level Filesystem",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "200--211",
  month        = apr,
  keywords     = "compression algorithms, file migration, caching, temporal
    locality, lru, file migration, storage hierarchies",
  abstract     = "Caching Storage systems have always attempted to use
    properties of the files that are stored in the system to optimize access
    time, capacity, and/or cost.  Compression exploits patterns within files,
    and file migration and file caching exploit file access patterns, but the
    combination of these concepts has not been reported on before.  We discuss
    here the effectiveness of a filesystem that integrates caching and
    compression to provide two levels of file storage on disks.  This
    investigation is based on measurements that were collected on nine
    computers at three different sites.  The data indicate that automatic
    compression of least recently used files doubles the amount of data that
    can be stored on a given disk system, while incurring only a slight
    performance cost.", 
  location     = "https://doi.org/10.1145/106973.106993"
}

@Article{npatrtma,
  author       = "William~J. Bolosky and Michael~L. Scott and Robert~P. Fitzgerald and Robert~J. Fowler and Alan~L. Cox",
  title        = "{NUMA} Policies and Their Relation to Memory Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "212--221",
  month        = apr,
  keywords     = "trace-based analysis, global storage, programming style, ",
  abstract     = "Multiprocessor memory reference traces provide a wealth of
    information on the behavior of parallel programs.  We have used this
    information to explore the relationship between kernel-based NUMA
    management policies and multiprocessor memory architecture.  Our trace
    analysis techniques employ an off-line, optimal cost policy as a baseline
    against which to compare on-line policies, and as a policy-insensitive tool
    for evaluating architectural design alternatives.  We compare the
    performance of our optimal policy with that of three implementable policies
    (two of which appear in a previous work), on a variety of applications,
    with varying relative speeds for page moves and local, global, and remote
    memory references.  Our results indicate that a good NUMA policy must be
    chosen to match its machine, and confirm that such policies can be both
    simple and effective.  They also indicate that programs for NUMA machine
    must be written with care to obtain the best performance.", 
  location     = "https://doi.org/10.1145/106973.106994"
}

@Article{ldasccs,
  author       = "David Chaiken and John Kubiatowicz and Anant Agarwal",
  title        = "{LimitLESS} Directories:  A Scalable Cache Coherence Scheme",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "224--234",
  month        = apr,
  keywords     = "alewife machine, cache coherence, cache coherence protocol,
    interprocessor interrupt, performance",
  abstract     = "Caches enhance the performance of multiprocessors by reducing
    network traffic and average memory access latency.  However, cache-based
    systems must address the problem of cache coherence.  We propose the
    LimitLESS directory protocol to solve this problem.  The LimitLESS scheme
    uses a combination of hardware and software techniques to realize the
    performance of a full-map directory with the memory overhead of a limited
    directory.  This protocol is supported by Alewife, a large-scale
    multiprocessor.  We describe the architectural interfaces needed to
    implement the LimitLESS directory, and evaluate its performance through
    simulations of the Alewife machine.", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a237629.pdf",
  location     = "https://doi.org/10.1145/106973.106995"
}

@Article{iraaisfr,
  author       = "David~G. Bradlee and Susan~J. Eggers and Robert~R. Henry",
  title        = "Integrating Register Allocation and Instruction Scheduling for {RISC}s",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "122--131",
  month        = apr,
  keywords     = "instruction scheduling, register allocation, risc, pass
    phasing, code generation",
  abstract     = "To achieve high performance in uniprocessor RISC systems,
    compilers must perform both register allocation to reduce memory references
    and instruction scheduling to avoid pipeline hazards.  Compilers that
    separate the two functions should perform poorly on uniprocessor RISCS that
    support multi-cycle operations, particularly on computation-intensive
    workloads.  This is because the lack of coordination between register
    allocation and instruction scheduling results in poor use of the register
    set.  In this paper we compare three code generation strategies on three
    RISC processors that support multi-cycle operations.  The first strategy
    completely separates register allocation and instruction scheduling; the
    second logically separates the two phases, but uses a heuristic to force
    the instruction scheduler, which runs first, to adhere to the same
    restrictions as the register allocator; the third performs pre-scheduling
    to calculate schedule cost estimates that enable the register allocator to
    find a balance between using registers to avoid pipeline delays and using
    them to reduce memory references.  Our results show that separating
    register allocation and code scheduling produces inefficient code.
    However, a technique as complex as the third alternative brings little
    added benefit over the second.", 
  location     = "https://doi.org/10.1145/106972.106986"
}

@Article{cgfsaaem,
  author       = "Manuel~E. Benitez and Jack~W. Davidson",
  title        = "Code Generation for Streaming:  an Access\slash Execute Mechanism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "132--141",
  month        = apr,
  keywords     = "wm architecture, code generation, recurrence detection, ",
  abstract     = "Access/execute architectures have several advantages over
    more traditional architectures.  Because address generation and memory
    access are decoupled from operand use, memory latencies are tolerated
    better, there is more potential for concurrent operation, and it permits
    the use of specialized hardware to facilitate fast address generation.
    This paper describes the code generation and optimization algorithms that
    are used in an optimizing compiler for an architecture that contains
    explicit hardware support for the access/execute model of computation.  Of
    particular interest is the novel approach that the compiler uses to detect
    recurrence relations in programs and to generate code for them.  Because
    these relations are often used in problem domains that require significant
    computational resources, detecting and handling them can result in
    significant reductions in execution time.  While the tectilques discussed
    were originally targeted for one specific architecture, many of the
    techniques are applicable to commonly available microprocessors.  The paper
    describes the algorithms as well as our experience with using them on a
    number of machines.", 
  location     = "https://doi.org/10.1145/106975.106987"
}

@Article{eiohlpp,
  author       = "Rajive Bagrodia and Sharad Mathur",
  title        = "Efficient Implementation of High-Level Parallel Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "142--151",
  month        = apr,
  keywords     = "uc, data mapping, connection machine, copying, reducing,
    permuting, folding, matrix computations",
  abstract     = "The efficiency of a parallel program is related to the
    implementation of its data structures on the distributed (or shared) memory
    of a specific architecture.  This paper describes a declarative approach
    that may be used to modify the mapping of the program data on a specific
    architecture.  The ideas are developed in the context of a new language
    called UC and its implementation on the Connection Machine.  The paper also
    contains measurements on sample programs to illustrate the effectiveness of
    data mappings in improving the execution efficiency of example programs.", 
  location     = "https://doi.org/10.1145/106972.376053"
}

@Article{vrdfpvs,
  author       = "William Mangione-Smith and Santosh~G. Abraham and Edward~S. Davidson",
  title        = "Vector Register Design for Polycyclic Vector Scheduling",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "154--163",
  month        = apr,
  keywords     = "instruction scheduling, register naming, memory latency",
  abstract     = "Most vector compilers use a scheduling technique known as
    simple vector scheduling (SVS).  With SVS, all instructions of one loop
    iteration are issued before any succeeding iteration begins.  Long vector
    registers is the primary mechanism for increasing pipeline utilization.
    Some vector compilers use a newer technique, polycyclic vector scheduling
    (PVS), that executes multiple loop iterations concurrently.  Furthermore,
    chaining is not required for optimal performance using PVS.  While PVS code
    schedules typically perform as well as or better than SVS schedules, they
    also tend to require more vector registers.  This limits the applicability
    of PVS for current vector machines.  This paper studies how the register
    requirements of PVS code are related to machine architecture.  An
    architecture similar to the Cray-2 has been used for a series of scheduling
    experiments that cover a wide range of vector register lengths and memory
    latencies.  The results of these experiments indicate that the critical
    machine parameter for a PVS compiler is the number of available vector
    registers.  Little advantage has been found for using a vector register
    length of more than sixteen elements.", 
  location     = "https://doi.org/10.1145/106972.328664"
}

@Article{fgpwmhsacctam,
  author       = "David~E. Culler and Anurag Sah and Klaus~E. Schauser and Thorsten von Eicken and John Wawrzynek",
  title        = "Fine-grain Parallelism with Minimal Hardware Support:  {A} Compiler-Controlled Threaded Abstract Machine",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "164--175",
  month        = apr,
  keywords     = "activations, threads, quanta, id",
  abstract     = "In this paper, we present a relatively primitive execution
    model for fine-grain parallelism, in which all synchronization, scheduling,
    and storage management is explicit and under compiler control.  This is
    defined by a threaded abstract machine (TAM) with a multilevel scheduling
    hierarchy.  Considerable temporal locality of logically related threads is
    demonstrated, providing an avenue for effective register use under
    quasidynamic scheduling.  A prototype TAM instruction set, TLO, has been
    developed, along with a translator to a variety of existing sequential and
    parallel machines.  Compilation of Id, an extended functional language
    requiring fine-grain synchronization, under this model yields performance
    approaching that of conventional languages on current uniprocessors.
    Measurements suggest that the net cost of synchronization on conventional
    multiprocessors can be reduced to within a small factor of that on machines
    with elaborate hardware support, such aa proposed dataflow architectures.
    This brings into question whether tolerance to latency and inexpensive
    synchronization require specific hardware support or merely an appropriate
    compilation strategy and program representation.", 
  location     = "https://doi.org/10.1145/106972.106990"
}

@Article{loilp,
  author       = "David~W. Wall",
  title        = "Limits of Instruction-Level Parallelism",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "176--188",
  month        = apr,
  keywords     = "basic blocks, branch and jump prediction, loop unrolling,
    alias analysis, register renaming, ",
  abstract     = "Growing interest in ambitious multiple-issue machines and
    heavily pipelined machines requires a careful examination of how much
    instruction level parallelism exists in typical programs.  Such an
    examination is complicated by the wide variety of hardware and software
    techniques for increasing the parallelism that can be exploited, including
    branch prediction, register renaming, and alias analysis.  By performing
    simulations based on instruction traces, we can model techniques at the
    limits of feasibility and even beyond.  Our study shows a striking
    difference between assuming that the techniques we use are perfect and
    merely assuming that they are impossibly good.  Even with impossibly good
    techniques, average parallelism rarely exceeds 7, with 5 more common.", 
  location     = "https://doi.org/10.1145/106974.106991",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-15.pdf"
}

@Article{tcpaooba,
  author       = "Monica~D. Lam and Edward~E. Rothberg and Michael~E. Wolf",
  title        = "The Cache Performance and Optimizations of Blocked Algorithms",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "63–-74",
  month        = apr,
  keywords     = "blocking, cache performance, matrix computations, data
    locality, analytic models, data access patterns",
  abstract     = "Blocking is a well-known optimization technique for improving
    the effectiveness of memory hierarchies.  Instead of operating on entire
    rows or columns of an array, blocked algorithms operate on submatrices or
    blocks, so that data loaded into the faster levels of the memory hierarchy
    are reused.  This paper presents cache performance data for blocked
    programs and evaluates several optimization to improve this performance.
    The data is obtained by a theoretical model of data conflicts in the cache,
    which has been validated by large amounts of simulation.  We show that the
    degree of cache interference is highly sensitive to the stride of data
    accesses and the size of the blocks, and can cause wide variations in
    machine performance for different matrix sizes.  The conventional wisdom of
    frying to use the entire cache, or even a fixed fraction of the cache, is
    incorrect.  If a fixed block size is used for a given cache size, the block
    size that minimizes the expected number of cache misses is very small.
    Tailoring the block size according to the matrix size and cache parameters
    can improve the average performance and reduce the variance in performance
    for different matrix sizes.  Finally, whenever possible, it is beneficial
    to copy non-contiguous reused data into consecutive locations.", 
  location     = "https://doi.org/10.1145/106972.106981"
}

@Article{teocsocp,
  author       = "Jeffrey~C. Mogul and Anita Borg",
  title        = "The Effect of Context Switches on Cache Performance",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "75--84",
  month        = apr,
  keywords     = "scheduling, context switching, trace analysis, cache
    simulations, performance",
  abstract     = "The sustained performance of fast processors is critically
    dependent on cache performance.  Cache performance in turn depends on
    locality of reference.  When an operating system switches contexts, the
    assumption of locality may be violated because the instructions and data of
    the newly-scheduled process may no longer be in the cache(s).
    Context-switching thus has a cost above that associated with that of the
    operations performed by the kernel.  We fed address traces of the processes
    running on a multi-tasking operating system through a cache simulator, to
    compute accurate cache-hit rates over short intervals.  By marking the
    output of such a simulation whenever a context switch occurs, and then
    aggregating the post-context-switch results of a large number of context
    switches, it is possible to estimate the cache performance reduction caused
    by a switch.  Depending on cache parameters the net cost of a context
    switch appears to be in the thousands of cycles, or tens to hundreds of
    microseconds.", 
  location     = "https://dl.acm.org/doi/10.1145/106974.106982"
}

@Article{apifotfism,
  author       = "David Keppel",
  title        = "{A} Portable Interface for On-The-Fly Instruction Space Modification",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "86--95",
  month        = apr,
  keywords     = "multiprocessors, protection, coherence, virtual machines,
    portability, address-space management, ",
  abstract     = "Applications such as incremental linking must modify
    instruction space during program execution.  Whenever instruction space is
    modified, machine-dependent systems issues such as instruction caching must
    be dealt with properly.  However, there are no standard idioms for
    signaling a change to the instruction space.  This paper discusses issues
    for instruction space allocation and coherence, describes a portable
    interface for modifying instruction space, and examines the details of
    several implementations of the interface.", 
  location     = "https://doi.org/10.1145/106973.106983"
}

@Article{vmpfup,
  author       = "Andrew~W. Appel and Kai Li",
  title        = "Virtual Memory Primatives for User Programs",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "96--107",
  month        = apr,
  keywords     = "virtual memory, concurrent garbage collection, shared virtual
    storage, concurrent checkpointing, general garbage collection, persistent
    stores, extending addressability, data-compression paging, heap overflow
    detection, performance, tlb consistency, page size, ",
  abstract     = "Memory Management Units (MMUs) are traditionally used by
    operating systems to implement disk-paged virtual memory.  Some operating
    systems allow user programs to specify the protection level (inaccessible,
    read-only, read-write) of pages, and allow user programs to handle
    protection violations, but these mechanisms are not always robust,
    efficient, or well-matched to the needs of applications.  We survey several
    user-level algorithms that make use of page-protection techniques, and
    analyze their common characteristics, in an attempt to answer the question,
    'What virtual-memory primitives should the operating system provide to user
    processes, and how do today's operating systems provide them?'.", 
  location     = "https://doi.org/10.1145/106973.106984", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-276-90"
}

@Article{tioaaosd,
  author       = "Thomas~E. Anderson and Henry~M. Levy and Brian~N. Bershad and Edward~D. Lazowska",
  title        = "The Interaction of Architecture and Operating System Design",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "108--120",
  month        = apr,
  keywords     = "ipc, cross-machine communication, local communication, system
    calls, interrupt handling, virtual memory, translation buffers",
  abstract     = "Today's high-performance RISC microprocessors have been
    highly tuned for integer and floating point application performance.  These
    architectures have paid less attention to operating system requirements.
    At the same time, new operating system designs often have overlooked modern
    architectural trends which may unavoidably change the relative cost of
    certain primitive operations.  The result is that operating system
    performance is well below application code performance on contemporary
    RISCs.  This paper examines recent directions in computer architecture and
    operating systems, and the implications of changes in each domain for the
    other.  The requirements of three components of operating system design are
    discussed in detail: interprocess communication, virtual memory, and thread
    management.  For each component, we relate operating system functional and
    performance needs to the mechanisms available on commercial RISC
    architectures such as the MIPS R2000 and R3000, SUN SPARC, IBM RS6000,
    Motorola 88000, and Intel i860.  Our analysis reveals a number of specific
    reasons why the performance of oeprating system primitives on RISCs has not
    scaled with integer performance.  In addition, we identify areas in which
    architectures could better (and cost-effectively) accommodate operating
    system needs, and areas in which operating system design could accommodate
    certain necessary characteristics of cost-effective high-performance
    microprocessors.", 
  location     = "https://doi.org/10.1145/106973.106985", 
  location     = "https://homes.cs.washington.edu/~tom/pubs/interaction.html"
}

@Article{acffca,
  author       = "S.~S. Reddi and E.~A. Feustel",
  title        = "{A} Conceptual Framework for Computer Architecture",
  journal      = surveys,
  year         = 1976,
  volume       = 8,
  number       = 2,
  pages        = "277--300",
  month        = jun,
  keywords     = "computer architecture, framework, architecture composition,
    information flow, physical organization, diverse architecture conceptual
    unification",
  abstract     = "The purpose of this paper is to describe the concepts,
    definitions, and ideas of computer architecture and to suggest that
    architecture can be viewed as composed of three components: physical
    organization; control and flow of information; and representation,
    interpretation and transformation of information.  This framework can
    accommodate diverse architectural concepts such as array processing,
    microprogramming, stack processing and tagged architecture.  Architectures
    of some existing machines are considered and methods of associating
    architectural concepts with the components are established.  Architecture
    design problems and trade-offs are discussed in terms of the proposed
    framework.", 
  location     = "https://doi.org/10.1145/356669.356673"
}

@Article{hbdmsfsp,
  author       = "Gurindar~S. Sohi and Manoj Franklin",
  title        = "High-Bandwidth Data Memory Systems for Superscalar Processors",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "53--62",
  month        = apr,
  keywords     = "caches, superscalar instruction issue, bus bandwidth, storage
    hierarchy, cpu performance",
  abstract     = "This paper considers the design of a data memory hierarchy,
    with a level 1 (L1) data cache at the top, to support the data bandwidth
    demands of a future-generation superscalar processor capable of issuing
    about ten instructions per clock cycle.  It introduces the notion of cache
    bandwidth — the bandwidth with which a cache can accept requests from the
    processor — and shows how the bandwidth of a standard, blocking cache, can
    degrade greatly because of its inability to overlap the service of misses.
    To improve the data bandwidth to greater than 1 request per cycle,
    multi-port, interleaved caches are introduced.  Simulation results from a
    cycle-by-cycle simulator, using the MIPS R2000 instruction set, suggest
    that memory hierarchies with blocking L1 caches will be unable to support
    the bandwidth demands of future-generation superscalar processors.
    Multi-port, non-blocking (MPNB) L1 caches introduced in this paper for the
    top of the data memory hierarchy appear to be capable of supporting such
    data bandwidth demands.", 
  location     = "https://doi.org/10.1145/106973.106980",
  location     = "https://minds.wisconsin.edu/handle/1793/59366"
}

@Article{spdc,
  author       = "David Callahan and Ken Kennedy and Allan Porterfield",
  title        = "Software Prefetching",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "40--52",
  month        = apr,
  keywords     = "cache management, performance, latency management",
  abstract     = "We present software prefetching, an approach to reducing
    cache miss latencies.  By providing a nonblocking prefetch instruction that
    brings into cache data at a specified memory address, the compiler can
    overlap the memory latency with other computation.  Our simulations show
    that, even when generated by a simple compiler algorithm, prefetch
    instructions can eliminate nearly all cache misses, while modestly
    increasing data traffic between memory and cache.", 
  location     = "https://doi.org/10.1145/106972.106979"
}

@Article{tfppoassp,
  author       = "Roland~L. Lee and Alex~Y. Kwok and Fay{\' e}~A. Briggs",
  title        = "The Floating-Point Performance of a Superscalar {SPARC} Processor",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "28--37",
  month        = apr,
  keywords     = "superscalar architecture, loop unrolling, software
    pipelining",
  abstract     = "superscalar SPARC processors' floating-point performance is
    evaluated based on empirical data from 12 benchmarks.  This evaluation is
    done in the context of two software instruction scheduling optimization,
    loop unrolling and software pipelining, and for three machine models we
    term, 1-scalar, 2-scalar and 4-scalar.  We also consider the effect of the
    memory system on the performance improvements.  Superscalar hardware alone
    exhibit little performance improvement without software optimization.  Of
    the two scheduling methods we study, software pipelining more effectively
    takes advantage of increased hardware parallelism, and achieves near
    optimal speedup on the 4-scalar machine model.  Loop-unrolling performance
    is restricted by the limited number of floating point registers in the
    SPARC architecture.  Applying both optimization techniques provides best
    performance.  A superscalar SPARC processor can provide improved
    floating-point performance, but with signification software and hardware
    development costs.", 
  location     = "https://doi.org/10.1145/106974.106978"
}

@Article{rtbpbriiadwm,
  author       = "Manolis Katevenis and Nestoras Tzartzanis",
  title        = "Reducing the Branch Penalty by Rearranging Instructions in a Double-Width Memory",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "15--27",
  month        = apr,
  keywords     = "pipelined computer architecture, branch penalty, delayed
    branch, delay slot, rearranging instructions into delay slots, super-scalar
    computer architecture, double-width instruction memory",
  abstract     = "In a pipelined processor with an instruction-fetch throughput
    of two (consecutive) instructions per cycle, one method to reduce the
    branch penalty is to rearrange the code by placing (copies of) instructions
    from both targets of a branch in the double-width fetch stream after that
    branch.  This scheme is of interest e.g.  when the number of fetch cycles
    is large, thus making it hard to fill all the delay slots with instructions
    from before the branch, and when the hardware has super-scalar capabilities
    but the compiler does not find enough instructions for parallel execution
    in the basic block where a b ranch is predicted to go.  We study this
    scheme of rearranging instructions, and we evaluate its performance
    (execution time and code size) in the case where no parallel instructions
    are scheduled in the delay slots.", 
  location     = "https://doi.org/10.1145/106974.106977"
}

@Article{avisettva,
  author       = "Andrew Wolfe and John~P. Shen",
  title        = "{A} Variable Instruction Stream Extension to the {VLIW} Architecture",
  journal      = asplos91,
  year         = 1991,
  volume       = 26,
  number       = 4,
  pages        = "2--14",
  month        = apr,
  keywords     = "vliw processors, state-machine models, configurable control
    regimes, sequential programs, barrier synchronization, ",
  abstract     = "A Variable Instruction Stream processor architecture called
    XIMD is proposed.  The XIMD structurally resembles a VLIW and shares many
    of the be@icial characteristics of VLIW; however, the XIMD architecture can
    dynamically partition its resources to support the concurrent execution of
    multiple instruction streams.  The number of streams can vary from cycle to
    cycle to best suit each portion of the application.  The XIMD concept and a
    comparison with other traditional architectures based on state…machine
    models of control paths are presented.  Several program examples further
    illustrate the capabilities of XIMD.  A brief description of an XIMD
    prototype machine is included; details of this implementation are presented
    in another paper.", 
  location     = "https://doi.org/10.1145/106972.106976"
}

@Article{afatuoipl,
  author       = "George~B. Leeman",
  title        = "{A} Formal Approach to Undo Operations in Programming Languages",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 1,
  pages        = "50--87",
  month        = jan,
  keywords     = "checkpoint, language constructs, preprocessors, recovery,
    reverse execution, undo, human interfaces",
  abstract     = "A framework is presented for adding a general Undo facility
    to programming languages.  A discussion of relevant literature is provided
    to show that the idea of Undoing pervades several areas in computer
    science, and even other disciplines.  A simple model of computation is
    introduced, and it is augmented with a minimal amount of additional
    structure needed for recovery and reversal.  Two different interpretations
    of Undo are motivated with examples.  Then, four primitives are defined in
    a language-independent manner; they are sufficient to support a wide range
    of Undo capability.  Two of these primitives carry out state saving, and
    the others mirror the two versions of the Undo operation.  Properties of
    and relationships between these primitives are explored, and there are some
    preliminary remarks on how one could implement a system based on this
    formalism.  The main conclusion is that the notions of recovery and
    reversal of actions can become part of the programming process.", 
  location     = "https://doi.org/10.1145/5001.5005"
}

@Article{ltls,
  author       = "Jens Peter Alfke",
  title        = "Learning to Love {SOM}",
  journal      = "MacTech Journal",
  year         = 1995,
  volume       = 11,
  number       = 1,
  pages        = "12--16",
  month        = jan,
  keywords     = "system object model, fragile base classes, c++, networked
    objects",
  abstract     = "The System Object Model (SOM) provides the object-oriented
    substrate used by OpenDoc and by future versions of the Macintosh Toolbox.
    SOM is fairly complex, relatively new on the Mac, and competes against
    other proprietary object models.  It’s not surprising, then, that there is
    some degree of apprehension and misinformation surrounding it.  This
    article is an introduction to SOM.",
  location     = "http://preserve.mactech.com/articles/mactech/Vol.11/11.01/LearningtoLoveSOM/index.html"
}

@Article{djcdeap4,
  author       = "Eric~E. Allen",
  title        = "Diagnosing {Java} Code:  Designing Extensible Applications, Part 4",
  journal      = "IBM developerWorks",
  year         = 2001,
  month        = dec,
  keywords     = "java, s-expressions, extensibility",
  abstract     = "In this installment of Diagnosing Java Code, author Eric
    Allen illustrates how S-expressions -- syntactic representations of lists
    of elements delimited by parentheses -- can be used to provide a useful and
    lightweight form of black box extensibility.  The advantages of using
    S-expressions are discussed in the context of a particular example.  Also,
    the author details the limitations of S-expressions and notes when they may
    not be the best fit for an application.", 
  location     = "https://www.eecis.udel.edu/~decker/courses/280f07/paper/Java%20Ext%204.pdf"
}

@Article{matmfcs,
  author       = "Ira~W. Cotton",
  title        = "Microeconomics and the Market for Computer Services",
  journal      = surveys,
  year         = 1975,
  volume       = 7,
  number       = 2,
  pages        = "95--111",
  month        = jun,
  keywords     = "billing, charge-back, computer services, economics,
    microeconomics, pricing, supply-demand, elasticity, system management",
  abstract     = "Microeconomics has much to offer the computer services
    manager.  This article reviews some of the traditional topics in
    microeconomics and shows how they can be applied to the computer-services
    market.  The topics covered include supply, demand, costs, and pricing.
    The most significant application of microeconomics is in setting prices--so
    much so that microeconomics is frequently called 'price theory.'
    Accordingly, the thrust of the article is towards providing a sound
    framework for computer-services pricing.",
  location     = "https://doi.org/10.1145/356648.356650"
}

@Article{tfdasisoap,
  author       = "Adrian Cronauer",
  title        = "The Fairness Doctrine:  {A} Solution in Search of a Problem",
  journal      = "Federal Communications Law Journal",
  year         = 1994,
  volume       = 47,
  number       = 1,
  keywords     = "content control, fairness doctrine, narrowcasting",
  abstract     = "The 'Fairness Doctrine' refers to a former policy of the
    Federal Communications Commission wherein a broadcast station which
    presented one viewpoint on a controversial public issue had to afford the
    opposing viewpoint an opportunity to be heard.  The FCC ceased to enforce
    the doctrine in 1987, reasoning that the doctrine actually decreased the
    viewpoints heard by discouraging broadcasters from covering controversial
    issues out of fear of censure by the FCC.  The Author explores the
    historical development of the Fairness Doctrine and examines the flaws with
    the different rationales upon which the doctrine is based.  The Author
    concludes that today's marketplace acts as an alternative to the Fairness
    Doctrine by providing numerous media outlets with specialty formats
    catering to particular viewpoints; therefore, the Fairness Doctrine is
    unnecessary and should not be revived.", 
  location     = "https://www.repository.law.indiana.edu/fclj/vol47/iss1/6"
}

@Article{adusboavcs,
  author       = "G.~W.R.~Luderer and H.~Che and J.~P. Haggerty and P.~A. Kirslis and W.~T. Marshall",
  title        = "{A} Distributed " # unix # " System Based on a Virtual Circuit Switch",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "160--168",
  month        = dec,
  keywords     = "distributed computing, datakit, file servers",
  abstract     = "The popular UNIX operating system provides time-sharing
    service on a single computer.  This paper reports on the design and
    implementation of a distributed UNIX system.  The new operating system
    consists of two components: the S-UNIX subsystem provides a complete UNIX
    process environment enhanced by access to remote files; the F-UNIX
    subsystem is specialized to offer remote file service.  A system can be
    configured out of many computers which operate either under the S-UNIX or
    the F-UNIX operating subsystem.  The file servers together present the view
    of a single global file system.  A single-service view is presented to any
    user terminal connected to one of the S-UNIX subsystems.Computers
    communicate with each other through a high-bandwidth virtual circuit
    switch.  Small front-end processors handle the data and control protocol
    for error and flow-controlled virtual circuits.  Terminals may be connected
    directly to the computers or through the switch.Operational since early
    1980, the system has served as a vehicle to explore virtual circuit
    switching as the basis for distributed system design.  The performance of
    the communication software has been a focus of our work.  Performance
    measurement results are presented for user process level and operating
    system driver level data transfer rates, message exchange times, and system
    capacity benchmarks.  The architecture offers reliability and modularly
    growable configurations.  The communication service offered can serve as
    the foundation for different distributed architectures.", 
  location     = "https://doi.org/10.1145/1067627.806604"
}

@Article{lanthrds,
  author       = "G.~Popek and B.~Walker and J.~Chow and D.~Edwards and C.~Kline and G.~Rudisin and G.~Thiel",
  title        = "{LOCUS}: A Network Transparent, High Reliability Distributed System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "169--177",
  month        = dec,
  keywords     = "LOCUS is a distributed operating system that provides a high
    degree of network transparency while at the same time supporting high
    performance and automatic replication of storage.  By network transparency
    we mean that at the system call interface there is no need to mention
    anything network related.  Knowledge of the network and code to interact
    with foreign sites is below this interface and is thus hidden from both
    users and programs under normal conditions.  LOCUS is application code
    compatible with Unix2, and performance compares favorably with standard,
    single system Unix.  LOCUS runs on a high bandwidth, low delay local
    network.  It is designed to permit both a significant degree of local
    autonomy for each site in the network while still providing a network-wide,
    location independent name structure.  Atomic file operations and extensive
    synchronization are supported.Small, slow sites without local mass store
    can coexist in the same network with much larger and more powerful machines
    without larger machines being slowed down through forced interaction with
    slower ones.  Graceful operation during network topology changes is
    supported.", 
  location     = "https://doi.org/10.1145/800216.806605"
}

@Article{gaeidcs,
  author       = "Andrew~D. Birrell and Roy Levin and Roger~M. Needham and Michael~D. Schroeder",
  title        = "Grapevine:  An Exercise in Distributed Computing (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "178--179",
  month        = dec,
  keywords     = "Grapevine is a distributed, replicated system running on a
    large internet within the Xerox research and development community.  The
    internet extends from coast to coast in the USA, to Canada and to Europe,
    and contains more than 50 Ethernet local networks linked by leased
    telephone lines.  Over 1500 computers are attached to the internet.  Most
    computers are used an personal workstations, but some are used as servers
    providing access to shared facilities such as printers, large-scale
    secondary storage, or data bases.  Computers on the internet are uniformly
    addressable using the PUP family of protocols.", 
  location     = "https://doi.org/10.1145/1067627.806606"
}

@Article{baadsfwmvts,
  author       = "Norman Meyrowitz and Margaret Moser",
  title        = "{BRUWIN}:  An Adaptable Design Strategy for Window Manager\slash Virtual Terminal Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "180--189",
  month        = dec,
  keywords     = "window management, device independence",
  abstract     = "With only one process viewable and operational at any moment,
    the standard terminal forces the user to continually switch between
    contexts.  Yet this is unnatural and counter-intuitive to the normal
    working environment of a desk where the worker is able to view and base
    subsequent actions on multiple pieces of information.The window manager is
    an emerging computing paradigm which allows the user to create multiple
    terminals on the same viewing surface and to display and act upon these
    simultaneous processes without loss of context.  Though several research
    efforts in the past decade have introduced window managers, they have been
    based on the design or major overhaul of a language or operating system;
    the window manager becomes a focus of—rather than a tool of—the system.
    While many of the existing implementations provide wide functionality, most
    implementations and their associated designs are not readily available for
    common use; extensibility is minimal.This paper describes the design and
    implementation of BRUWIN, the BRown University WINdow manager, stressing
    how such a design can be adapted to a variety of computer systems and
    output devices, ranging from alphanumeric terminals to high-resolution
    raster graphics displays.  The paper first gives a brief overview of the
    general window manager paradigm and existing examples.  Next we present an
    explanation of the user-level functions we have chosen to include in our
    general design.  We then describe the structure and design of a window
    manager, outlining the five important parts in detail.  Finally, we
    describe our current implementation and provide a sample session to
    highlight important features.", 
  location     = "https://doi.org/10.1145/1067627.806607"
}

@Article{aadoag,
  author       = "Kourosh Gharachorloo and Madhu Sharma and Simon Steely and Stephen Van Doren",
  title        = "Architecture and Design of {AlphaServer GS320}",
  journal      = asplos00,
  year         = 2000,
  volume       = 35,
  number       = 11,
  pages        = "13--24",
  month        = nov,
  keywords     = "consistency protocols, directories, snoopy cache, multiprocessors",
  abstract     = "This paper describes the architecture and implementation of
    the AlphaServer GS320, a cache-coherent non-uniform memory access
    multiprocessor developed at Compaq.  The AlphaServer GS320 architecture is
    specifically targeted at medium-scale multiprocessing with 32 to 64
    processors.  Each node in the design consists of four Alpha 21264
    processors, up to 32GB of coherent memory, and an aggressive IO subsystem.
    The current implementation supports up to 8 such nodes for a total of 32
    processors.  While snoopy-based designs have been stretched to medium-scale
    multiprocessors by some vendors, providing sufficient snoop bandwidth
    remains a major challenge especially in systems with aggressive processors.
    At the same time, directory protocols targeted at larger scale designs lead
    to a number of inherent inefficiencies relative to snoopy designs.  A key
    goal of the AlphaServer GS320 architecture has been to achieve the
    best-of-both-worlds, partly by exploiting the bounded scale of the target
    systems.This paper focuses on the unique design features used in the
    AlphaServer GS320 to efficiently implement coherence and consistency.  The
    guiding principle for our directory-based protocol is to address
    correctness issues related to rare protocol races without burdening the
    common transaction flows.  Our protocol exhibits lower occupancy and lower
    message counts compared to previous designs, and provides more efficient
    handling of 3-hop transactions.  Furthermore, our design naturally lends
    itself to elegant solutions for deadlock, livelock, starvation, and
    fairness.  The AlphaServer GS320 architecture also incorporates a couple of
    innovative techniques that extend previous approaches for efficiently
    implementing memory consistency models.  These techniques allow us to
    generate commit events (which are used for ordering purposes) well in
    advance of formulating the reply to a transaction.  Furthermore, the
    separation of the commit event allows time-critical replies to by-pass
    inbound requests without violating ordering properties.  Even though our
    design specifically targets medium-scale servers, many of the same
    techniques can be applied to larger-scale directory-based and smaller-scale
    snoopy-based designs.  Finally, we evaluate the performance impact of some
    of the above optimizations and present a few competitive benchmark
    results.", 
  location     = "https://doi.org/10.1145/356989.356991"
}

@Article{taotes,
  author       = "Edward~D. Lazowska and Henry~M. Levy and Guy~T. Almes and Michael~J. Fischer and Robert~J. Fowler and Stephen~C. Vestal",
  title        = "The Architecture of the {Eden} System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "148--159",
  month        = dec,
  keywords     = "object-based systems, distributed systems, location
    independent objects",
  abstract     = "The University of Washington's Eden project is a five-year
    research effort to design, build and use an “integrated distributed”
    computing environment.  The underlying philosophy of Eden involves a fresh
    approach to the tension between these two adjectives.  In briefest form,
    Eden attempts to support both good personal computing and good multi-user
    integration by combining a node machine / local network hardware base with
    a software environment that encourages a high degree of sharing and
    cooperation among its users.The hardware architecture of Eden involves an
    Ethernet local area network interconnecting a number of node machines with
    bit-map displays, based upon the Intel iAPX 432 processor.  The software
    architecture is object-based, allowing each user access to the information
    and resources of the entire system through a simple interface.This paper
    states the philosophy and goals of Eden, describes the programming
    methodology that we have chosen to support, and discusses the hardware and
    kernel architecture of the system.", 
  location     = "https://doi.org/10.1145/800216.806603"
}

@Article{witb,
  author       = "Robert Schmidt",
  title        = "What is the {BIOS}?",
  journal      = "Smart Computing",
  year         = 1994,
  volume       = 5,
  number       = 7,
  month        = jul,
  keywords     = "bios, operating systems, system initialization, libraries"
}

@Article{asofsafl,
  author       = "M.~Satyanarayanan",
  title        = "{A} Study of File Sizes and Functional Lifetimes",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "96--108",
  month        = dec,
  keywords     = "file systems, statistics",
  abstract     = "The performance of a file system depends strongly on the
    characteristics of the files stored in it.  This paper discusses the
    collection, analysis and interpretation of data pertaining to files in the
    computing environment of the Computer Science Department at Carnegie-Mellon
    University (CMU-CSD).  The information gathered from this work will be used
    in a variety of ways:1.  As a data point in the body of information
    available on file systems.2.  As input to a simulation or analytic model of
    a file system for a local network, being designed and imlemented at CMU-CSD
    3. As the basis of implementation decisions and parameters for the file
    system just mentioned.4.  As a step toward understanding how a user
    community creates, maintains and uses files.",  
  location     = "https://doi.org/10.1145/800216.806597",  
  location     = "https://www.cs.cmu.edu/~satya/docdir/satya-sosp-1981.pdf"
}

@Article{htgps,
  author       = "Matt Bishop",
  title        = "Hierarchical Take-Grant Protection Systems",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "109--122",
  month        = dec,
  keywords     = "de jure rules, de facto rules, information flow,
    authorization flow, hierarchical protection",
  abstract     = "The application of the Take-Grant Protection Model to
    hierarchical protection systems is explored.  The proposed model extends
    the results of Wu and applies the results of Bishop and Snyder to obtain
    necessary and sufficient conditions for a hierarchical protection graph to
    be secure.  In addition, restrictions on the take and grant rules are
    developed that ensure the security of all graphs generated by these
    restricted rules.", 
  location     = "https://doi.org/10.1145/800216.806598"
}

@Article{csfisaa,
  author       = "David~K. Gifford",
  title        = "Cryptographic Sealing for Information Secrecy and Authentication (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "123--124",
  month        = dec,
  keywords     = "public-key cryptography, capabilities, access control lists,
    information flow control",
  abstract     = "The problem of computer security can be considered to consist
    of four distinct components: secrecy (ensuring that information is only
    disclosed to authorized users), authentication (ensuring that information
    is not forged), integrity (ensuring that information is not destroyed), and
    availability (ensuring that access to information can not be maliciously
    interrupted).The paper describes a new protection mechanism called
    cryptographic sealing that provides primitives for secrecy and
    authentication.  The mechanism is enforced with a synthesis of classical
    cryptography, public-key cryptography, and a threshold scheme.", 
  location     = "https://doi.org/10.1145/800216.806599"
}

@Article{aumaificiame,
  author       = "George~W. Cox and William~M. Corwin and Konrad~K. Lai and Fred~J. Pollack",
  title        = "{A} Unified Model and Implementation for Interprocess Communication in a Multiprocessor Environment (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "125--126",
  month        = dec,
  keywords     = "ipc, process scheduling, ",
  abstract     = "This paper describes interprocess communication and process
    dispatching on the Intel 432.  The primary assets of the facility are its
    generality and its usefulness in a wide range of applications.  The
    conceptual model, supporting mechanisms, available interfaces, current
    implementations, and absolute and comparative performance are described.", 
  location     = "https://doi.org/10.1145/1067627.806600"
}

@Article{iamosfaobc,
  author       = "Kevin~C. Kahn and William~M. Corwin and T.~Don Dennis and Herman D'Hooge and David~E. Hubka and Linda~A. Hutchins and John~T. Montague and Fred~J. Pollack",
  title        = "{iMAX}:  {A} Multiprocessor Operating System for an Object-Based Computer",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "127--136",
  month        = dec,
  keywords     = "432, ada, architecture design, processor-memory model, system
    configurability, process management, memory management, device independence",
  abstract     = "The Intel iAPX 432 is an object-based microcomputer which,
    together with its operating system iMAX, provides a multiprocessor computer
    system designed around the ideas of data abstraction.  iMAX is implemented
    in Ada and provides, through its interface and facilities, an Ada view of
    the 432 system.  Of paramount concern in this system is the uniformity of
    approach among the architecture, the operating system, and the language.
    Some interesting aspects of both the external and internal views of iMAX
    are discussed to illustrate this uniform approach.", 
  location     = "https://doi.org/10.1145/1067627.806601"
}

@Article{ti4ofs,
  author       = "Fred~J. Pollack and Kevin~C. Kahn and Roy~M. Wilkinson",
  title        = "The {iMAX}-432 Object Filing System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "137--147",
  month        = dec,
  abstract     = "iMAX is the operating system for Intel's iAPX-432 computer
    system. The iAPX-4321 is an object-oriented multiprocessor architecture
    that supports capability-based addressing.  The object filing system is
    that part of iMAX that implements a permanent reliable object store.In this
    paper we describe the key elements of the iMAX object filing system design.
    We first contrast the concept of an object filing system with that of a
    conventional file system.  We then describe the iMAX design paying
    particular attention to five problems that other object filing designs have
    either solved inadequately or failed to address.  Finally, we discuss an
    effect of object filing on the programming semantics of Ada.", 
  keywords     = "type management, object spaces, linkage systems, synchronization", 
  location     = "https://doi.org/10.1145/1067627.806602"
}

@Article{tmattrapl,
  author       = "C.~R. Spooner",
  title        = "The {ML} Approach to the Readable All-Purpose Language",
  journal      = toplas,
  year         = 1986,
  volume       = 8,
  number       = 2,
  pages        = "215--243",
  month        = apr,
  keywords     = "adaptability, compile-time procedures, context, environment,
    extensibility, language-creation systems, language design, macro expansion,
    readability, textual domains",
  abstract     = "The ideal computer language is seen as one that would be as
    readable as natural language, and so adaptable that it could serve as the
    only language a user need ever know.  An approach to language design has
    emerged that shows promise of allowing one to come much closer to that
    ideal than might reasonably have been expected.  Using this approach, a
    language referred to as ML has been developed, and has been implemented as
    a language-creation system in which user-defined procedures invoked at
    translation time translate the source to some object code.  In this way the
    user can define both the syntax and the semantics of the source language.
    Both language and implementation are capable of further development.  This
    paper describes the approach, the language, and the implementation and
    recommends areas for further work.", 
  location     = "https://doi.org/10.1145/5397.5918"
}

@Article{aroodrfadcs,
  author       = "Liba Svobodova",
  title        = "{A} Reliable Object-Oriented Data Repository for a Distributed Computer System",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "47--57",
  month        = dec,
  keywords     = "distributed data storage system, server, atomic update,
    stable storage, optical disk, memory management, crash recovery",
  abstract     = "The repository described in this paper is a component of a
    distributed data storage system for a network of many autonomous machines
    that might run diverse applications.  The repository is a server machine
    that provides very large, very reliable long-term storage for both private
    and shared data objects.  The repository can handle both very small and
    very large data objects, and it supports atomic update of groups of objects
    that might be distributed over several repositories.  Each object is
    represented as a history of its states; in the actual implementation, an
    object is a list of immutable versions.The core of the repository is stable
    append-only storage called Version Storage (VS).  VS contains the histories
    of all data objects in the repository as well as all information needed for
    crash recovery.  To maintain the current versions of objects online, a
    copying scheme was adopted that resembles techniques of real-time garbage
    collection.  VS can be implemented with optical disks.", 
  location     = "https://doi.org/10.1145/1067627.806591"
}

@Article{acotnbfs,
  author       = "James~G. Mitchell and Jeremy Dion",
  title        = "{A} Comparison of Two Network-Based File Servers (Summary)",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "45--46",
  month        = dec,
  keywords     = "rings, buses, pup, file servers",
  abstract     = "This paper compares two working network-based file servers,
    the Xerox Distributed File System (XDFS) implemented at the Xerox Palo Alto
    Research Center, and the Cambridge File Server (CFS) implemented at the
    Cambridge University Computer Laboratory.  Both servers support concurrent
    random access to files using atomic transactions, both are connected to
    local area networks, and both have been in service long enough to enable us
    to draw lessons from them for future file servers.We compare the servers in
    terms of design goals, implementation issues, performance, and their
    relative successes and failures, and discuss what we would do differently
    next time.", 
  location     = "https://doi.org/10.1145/358468.358475"
}

@Article{otcowsp,
  author       = "Niklaus Wirth",
  title        = "On the Composition of Well-Structured Programs",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 4,
  pages        = "247--259",
  month        = dec,
  keywords     = "programming methods, systematic programming, program schemas,
    goto-free programs, well-structured programs, pascal",
  abstract     = "A professional programmer's know-how used to consist of the
    mastery of a set of techniques applicable to specific problems and to some
    specific problems and to some specific computer.  With the increase of
    computer power, the programmer's tasks grew more complex, and hence the
    need for a systematic approach became evident.  Recently, the subject of
    programming methods, generally applicable rules and patterns of
    development, received considerable attention.  Structured programming is
    the formulation of programs as hierarchical, nested structures of
    statements and objects of computation.  We give brief examples of
    structured programs, show the essence of this approach, discusses its
    relationship with program verification, and comment on the role of
    structured languages.", 
  location     = "https://doi.org/10.1145/356635.356639"
}

@Article{scsian,
  author       = "A.~J. Herbert and R.~M. Needham",
  title        = "Sequencing Computation Steps in a Network",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "59--63",
  month        = dec,
  keywords     = "distributed computations, synchronization, coordination",
  abstract     = "It is sometimes necessary in the course of a distributed
    computation to arrange that a certain set of operations is carried out in
    the correct order and the correct number of times (typically once).  If
    several sets of operations are performed on different machines on the
    network there is no obvious mechanism for enforcing such ordering
    constraints in a fully distributed way.  This lack basically stems from the
    difficulty of preventing copying and repetition of messages by machines and
    from the impossibility of constraining externally the actions of machines
    in response to messages that come into their hands.This paper presents a
    possible method for ensuring the integrity of sequences of operations on
    different machines.  The technique may be thought of as a means of enabling
    machines to ensure that requests made of them are valid and timely, not as
    means of centralized control of services.", 
  location     = "https://doi.org/10.1145/800216.806592"
}

@Article{aaconosk,
  author       = "Richard~F. Rashid and George~G. Robertson",
  title        = "Accent:  {A} Communication Oriented Network Operating System Kernel",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "64--75",
  month        = dec,
  keywords     = "ipc, virtual storage, distributed systems, modularity,
    location transparency, message passing",
  abstract     = "Accent is a communication oriented operating system kernel
    being built at Carnegie-Mellon University to support the distributed
    personal computing project, Spice, and the development of a fault-tolerant
    distributed sensor network (DSN).  Accent is built around a single,
    powerful abstraction of communication between processes, with all kernel
    functions, such as device access and virtual memory management accessible
    through messages and distributable throughout a network.  In this paper,
    specific attention is given to system supplied facilities which support
    transparent network access and fault-tolerant behavior.  Many of these
    facilities are already being provided under a modified version of VAX/UNIX.
    The Accent system itself is currently being implemented on the Three Rivers
    Corp.  PERQ.", 
  location     = "https://doi.org/10.1145/800216.806593"
}

@Article{casbstdpiaalprb,
  author       = "{\" O}zalp {\u B}abaoglu and William Joy",
  title        = "Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Referenced Bits",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "78--86",
  month        = dec,
  keywords     = "page replacement, vaxen, clock page replacement,
    multiprogramming, paging",
  abstract     = "This paper discusses the modifications made to the UNIX
    operating system for the VAX-11/780 to convert it from a swap-based
    segmented system to a paging-based virtual memory system.  Of particular
    interest is that the host machine architecture does not include
    page-referenced bits.  We discuss considerations in the design of
    page-replacement and load-control policies for such an architecture, and
    outline current work in modeling the policies employed by the system.  We
    describe our experience with the chosen algorithms based on
    benchmark-driven studies and production system use.", 
  location     = "https://doi.org/10.1145/800216.806595"
}

@Article{tmpe,
  author       = "Jan Heering and Paul Klint",
  title        = "Towards Monolingual Programming Environments",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 2,
  pages        = "183--213",
  month        = apr,
  keywords     = "monolingual system, language integration, language design,
  debugging languages, type checking, event expectation, side-effect recovery",
  abstract     = "Most programming environments are much too complex.  One way
    of simplifying them is to reduce the number of mode-dependent languages the
    user has to be familiar with.  As a first step towards this end, the
    feasibility of unified command/programming/debugging languages, and the
    concepts on which such languages have to be based, are investigated.  The
    unification process is accomplished in two phases.  First, a unified
    command/programming framework is defined and, second, this framework is
    extended by adding an integrated debugging capability to it.  Strict rules
    are laid down by which to judge language concepts presenting themselves as
    candidates for inclusion in the framework during each phase.  On the basis
    of these rules many of the language design questions that have hitherto
    been resolved this way or that, depending on the taste of the designer,
    lose their vagueness and can be decided in an unambiguous manner.", 
  location     = "https://doi.org/10.1145/3318.3321"
}

@Article{wasaeafvmm,
  author       = "Richard~W. Carr and John~L. Hennessy",
  title        = "{WSClock} --- {A} Simple and Effective Algorithm for Virtual Memory Management",
  journal      = sosp81,
  year         = 1981,
  volume       = 15,
  number       = 5,
  pages        = "87--95",
  month        = dec,
  keywords     = "page replacement algorithms, virtual storage, working set,
    load control, multiprogramming",
  abstract     = "A new virtual memory management algorithm WSCLOCK has been
    synthesized from the local working set (WS) algorithm, the global CLOCK
    algorithm, and a new load control mechanism for auxiliary memory access.
    The new algorithm combines the most useful feature of WS—a natural and
    effective load control that prevents thrashing—with the simplicity and
    efficiency of CLOCK.  Studies are presented to show that the performance of
    WS and WSCLOCK are equivalent, even if the savings in overhead are
    ignored.", 
  location     = "https://doi.org/10.1145/800216.806596"
}

@Article{asaflscs,
  author       = "Bruno Blanchet and Patrick Cousot and Radhia Cousot and Jérome Feret and Laurent Mauborgne and Antoine Miné and David Monniaux and Xavier Rival",
  title        = "{A} Static Analyzer for Large Safety-Critical Software (Extended Abstract)",
  journal      = notices ,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "196--201",
  month        = may,
  keywords     = "abstract analysis, abstract domains",
  abstract     = "We show that abstract interpretation-based static program
    analysis can be made efficient and precise enough to formally verify a
    class of properties for a family of large programs with few or no false
    alarms.  This is achieved by refinement of a general purpose static
    analyzer and later adaptation to particular programs of the family by the
    end-user through parametrization.  This is applied to the proof of
    soundness of data manipulation operations at the machine level for periodic
    synchronous safety critical embedded software.The main novelties are the
    design principle of static analyzers by refinement and adaptation through
    parametrization (Sect.  3 and 7), the symbolic manipulation of expressions
    to improve the precision of abstract transfer functions (Sect.  6.3), the
    octagon (Sect.  6.2.2), ellipsoid (Sect.  6.2.3), and decision tree (Sect.
    6.2.4) abstract domains, all with sound handling of rounding errors in
    oating point computations, widening strategies (with thresholds: Sect.
    7.1.2, delayed: Sect.  7.1.3) and the automatic determination of the
    parameters (parametrized packing: Sect.  7.2).", 
  location     = "https://doi.org/10.1145/780822.781153"
}

@Article{papdo,
  author       = "Malcolm~P. Atkinson and Ronald Morrison",
  title        = "Procedures as Persistent Data Objects",
  journal      = toplas,
  year         = 1985,
  volume       = 7,
  number       = 4,
  pages        = "539--559",
  month        = oct,
  keywords     = "persistent storage, first-class functions, scoping,
    polymorphism, closures, partial evaluation, views, separate compilation,
    binding, static and dynamic typing", 
  abstract     = "A persistent programming environment, together with a
    language supporting first class procedures, may be used to provide the
    semantic features of other object modeling languages.  In particular, the
    two concepts may be combined to implement abstract data types, modules,
    separate compilation, views, and data protection.  Furthermore, the ideas
    may be used in system construction and version control, as demonstrated
    here.", 
  location     = "https://doi.org/10.1145/4472.4477"
}

@Article{pasecat,
  author       = "Brad Appleton",
  title        = "Patterns and Software:  Essential Concepts and Terminology",
  journal      = "Object Magazine Online",
  year         = 1997,
  volume       = 3,
  number       = 5,
  month        = may,
  keywords     = "origins, history, pattern types, pattern components,
    qualities", 
  abstract     = "Fundamental to any science or engineering discipline is a
    common vocabulary for expressing its concepts, and a language for relating
    them together.  The goal of patterns within the software community is to
    create a body of literature to help software developers resolve recurring
    problems encountered throughout all of software development.  Patterns help
    create a shared language for communicating insight and experience about
    these problems and their solutions.  Formally codifying these solutions and
    their relationships lets us successfully capture the body of knowledge
    which defines our understanding of good architectures that meet the needs
    of their users.  Forming a common pattern language for conveying the
    structures and mechanisms of our architectures allows us to intelligibly
    reason about them.  The primary focus is not so much on technology as it is
    on creating a culture to document and support sound engineering
    architecture and design.", 
  location     = "https://www.bradapp.net/docs/patterns-intro.html"
}

@Article{cmfiaaitdoftcs,
  author       = "Aadrew~M. Tyrrell and Geof~F. Carpenter",
  title        = "{CSP} Methods for Identifying Atomic Actions in the Design of Fault Tolerant Concurrent Systems",
  journal      = tse,
  year         = 1995,
  volume       = 21,
  number       = 7,
  pages        = "629--639",
  month        = jul,
  keywords     = "CSP methods, atomic actions, fault tolerant concurrent
    systems design, error propagation, error recovery, fault tolerant parallel
    processing systems, fault tolerance mechanisms, underlying atomic actions,
    explicit trace evaluation, interprocess communications, CSP descriptions,
    structural arguments, full trace sets, communicating sequential processes", 
  abstract     = "Limiting the extent of error propagation when faults occur
    and localizing the subsequent error recovery are common concerns in the
    design of fault tolerant parallel processing systems.  Both activities are
    made easier if the designer associates fault tolerance mechanisms with the
    underlying atomic actions of the system.  With this in mind, the paper has
    investigated two methods for the identification of atomic actions in
    parallel processing systems described using CSP.  Explicit trace evaluation
    forms the basis of the first algorithm, which enables a designer to analyze
    interprocess communications and thereby locate atomic action boundaries in
    a hierarchical fashion.  The second method takes CSP descriptions of the
    parallel processes and uses structural arguments to infer the atomic action
    boundaries.  This method avoids the difficulties involved with producing
    full trace sets, but does incur the penalty of a more complex algorithm.", 
  location     = "https://doi.org/10.1109/32.392983"
}

@Article{abntir,
  author       = "Robert Fung and Brendan Del Favero",
  title        = "Applying Bayesian Networks to Information Retrieval",
  journal      = cacm,
  year         = 1995,
  volume       = 38,
  number       = 3,
  pages        = "42--57",
  month        = mar,
  keywords     = "information retrieval, baysian networks, search",
  abstract     = "Information retrieval (IR) is the identification of documents
    or other units of information in a collection that are relevant to a
    particular information need.  An information need is a set of questions to
    which someone would like to find an answer.  Here are some examples of IR
    tasks: finding articles in the New York Times that discuss the Iran-Contra
    affair; searching the recent postings in a Usenet newsgroup for references
    to a particular model of personal computer; finding the entries referring
    to butterflies in an online CD-ROM encyclopedia.", 
  location     = "https://doi.org/10.1145/203330.203340"
}

@Article{anlrfnc,
  author       = "Wei Li and Keshav Pingali",
  title        = "Access Normalization:  Loop Resturcturing for {NUMA} Compilers",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "285--295",
  month        = sep,
  keywords     = "numa compilation, data access matrices, loop transformations,
    data dependencies, numa code generation",
  abstract     = "In scalable parallel machines, processors can make local
    memory accesses much faster than they can make remote memory accesses.  In
    addition, when a number of remote accesses must be made, it is usually more
    efficient to use block transfers of data rather than to use many small
    messages.  To run well on such machines, software must exploit these
    features.  We believe it is too onerous for a programmer to do this by
    hand, so we have been exploring the use of restructuring compiler technology
    for this purpose.  In this paper, we start with a language like FORTRAN-D
    with user-specified data distribution and develop a systematic loop
    transformation strategy called access normalization that restructures loop
    nests to exploit locality and block transfers.  We demonstrate the power of
    our techniques using routines from the BLAS (Basic Linear Algebra
    Subprograms) library.  An important feature of our approach is that we
    model loop transformations using invertible matrices and integer lattice
    theory, thereby generalizing Banerjee's framework of unimodular matrices.", 
  location     = "https://doi.org/10.1145/143371.143541"
}

@Article{ctwofimmt,
  author       = "John Kubiatowicz and David Chaiken and Anant Agarwal",
  title        = "Closing the Window of Fulnerability in Multiphase Memory Transactions",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "274--284",
  month        = sep,
  keywords     = "deadlock, livelock, caching, context switching, cache coherence",
  abstract     = "Multiprocessor architects have begun to explore several
    mechanisms such as prefetching, context-switching and software-assisted
    dynamic cache-coherence, which transform single-phase memory transactions
    in conventional memory systems into multiphase operations.  Multiphase
    operations introduce a window of vulnerability in which data can be
    invalidated before it is used.  Losing data due to invalidations introduces
    damaging livelock situations.  This paper discusses the origins of the
    window of vulnerability and proposes an architectural framework that closes
    it.  The framework is implemented in Alewife, a large-scale multi-processor
    being built at MIT.", 
  location     = "https://doi.org/10.1145/143371.143540", 
  location     = "http://groups.csail.mit.edu/cag/papers/pdf/window-of-vulnerability.pdf"
}

@Article{csmsahfsm,
  author       = "Mark~D. Hill and James~R. Larus and Steven~K. Reinhardt and David~A. Wood",
  title        = "Cooperative Shared Memory:  Software and Hardware for Scalable Multiprocessors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "262--273",
  month        = sep,
  keywords     = "annotations, check-in/check-out, synchronization",
  abstract     = "We believe the paucity of massively parallel, shared-memory
    machines follows from the lack of a shared-memory programming performance
    model that can inform programmers of the cost of operations (so they can
    avoid expensive ones) and can tell hardware designers which cases are
    common (so they can build simple hardware to optimize them).  Cooperative
    shared memory, our approach to shared-memory design, addresses this
    problem.Our initial implementation of cooperative shared memory uses a
    simple programming model, called Check-In/Check-Out (CICO), in conjunction
    with even simpler hardware, called Dir1SW.  In CICO, programs bracket uses
    of shared data with a check_in directive terminating the expected use of
    the data.  A cooperative prefetch directive helps hide communication
    latency.  Dir1SW is a minimal directory protocol that adds little
    complexity to message-passing hardware, but efficiently supports programs
    written within the CICO model.", 
  location     = "https://doi.org/10.1145/161541.161544", 
  location     = "ftp://ftp.cs.wisc.edu/wwt/tocs93_csm.pdf"
}

@Article{esptb,
  author       = "Michael~D. Smith and Mark Horowitz and Monica~S. Lam",
  title        = "Efficient Superscalar Perfformance Through Boosting",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "248--259",
  month        = sep,
  keywords     = "speculative execution, instruction reordering, boosting,
    instruction scheduling, trace scheduling, hardware support, superscalar
    architectures",
  abstract     = "The foremost goal of superscalar processor design is to
    increase performance through the exploitation of instruction-level
    parallelism (ILP).  Previous studies have shown that speculative execution
    is required for high instruction per cycle (IPC) rates in non-numerical
    applications.  The general trend has been toward supporting speculative
    execution in complicated, dynamically-scheduled processors.  Performance,
    though, is more than just a high IPC rate; it also depends upon instruction
    count and cycle time.  Boosting is an architectural technique that supports
    general speculative execution in simpler, statically-scheduled processors.
    Boosting labels speculative instructions with their control dependence
    information.  This labelling eliminates control dependence constraints on
    instruction scheduling while still providng full dependence information to
    the hardware.  We have incorporated boosting into a trace-based, global
    scheduling algorithm that exploits ILP without adversely affecting the
    instruction count of a program.  We use this algorithm and estimates of the
    boosting hardware involved to evaluate how much speculative execution
    support is really necessary to achieve good performance.  We find that a
    statically-scheduled superscalar processor using a minimal implementation
    of boosting can easily reach the performance of a much more complex
    dynamically-scheduled superscalar processor.", 
  location     = "https://doi.org/10.1145/143371.143534"
}

@Article{ssfvasp,
  author       = "Scott~A. Mahlke and William~Y. Chen and Wen-mei~W. Hwu and B.~Ramakrishna Rau and Michael~S. Schlansker",
  title        = "Sentinel Scheduling for {VLIW} and Superscalar Processors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "238--247",
  month        = sep,
  keywords     = "superblock scheduling, percolation scheduling, instruction
    boosting, exception handling, sentinel scheduling, speculative stores",
  abstract     = "Speculative execution is an important source of parallelism
    for VLIW and superscalar processors.  A serious challenge with
    compiler-controlled speculative execution is to accurately detect and
    report all program execution errors at the time of occurrence.  In this
    paper, a set of architectural features and compile-time scheduling support
    referred to as sentinel scheduling is introduced.  Sentinel scheduling
    provides an effective framework for compiler-controlled speculative
    execution that accurately detects and reports all exceptions.  Sentinel
    scheduling also supports speculative execution of store instructions by
    providing a store buffer which allows probationary entries.  Experimental
    results show that sentinel scheduling is highly effective for a wide range
    of VLIW and superscalar processors.", 
  location     = "https://doi.org/10.1145/143371.143529"
}

@Article{fmefu,
  author       = "Brian~N. Bershad and David~D. Redell and John~R. Ellis",
  title        = "Fast Mutual Exclusion for Uniprocessors",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "223--233",
  month        = sep,
  keywords     = "mutual exclusion, memory interlocking, restartable
    instructions",
  abstract     = "In this paper we describe restartable atomic sequences, an
    optimistic mechanism for implementing simple atomic operations (such as
    Test-And-Set) on a uniprocessor.  A thread that is suspended within a
    restartable atomic sequence is resumed by the operating system at the
    beginning of the sequence, rather than at the point of suspension.  This
    guarantees that the thread eventually executes the sequence atomically.  A
    restartable atomic sequence has significantly less overhead than other
    software-based synchronization mechanisms, such as kernel emulation or
    software reservation.  Consequently, it is an attractive alternative for
    use on uniprocessors that do no support atomic operations.  Even on
    processors that do support atomic operations in hardware, restartable
    atomic sequences can have lower overhead.We describe different
    implementations of restartable atomic sequences for the Mach 3.0 and Taos
    operating systems.  These systems' thread management packages rely on
    atomic operations to implement higher-level mutual exclusion facilities.
    We show that improving the performance of low-level atomic operations, and
    therefore mutual exclusion mechanisms, improves application performance.", 
  location     = "https://doi.org/10.1145/143371.143523"
}

@Article{acoeamdo,
  author       = "Kamen Yotov and Xiaoming Li and Gang Ren and Michael Cibulskis and Gerald DeJong and Maria Garzaran and David Padua and Keshav Pingali and Paul Stodghill and Peng Wu",
  title        = "{A} Comparison of Empirical and Model-Deiven Optimization",
  journal      = sigplan # " (" # pot # "ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation, PLDAI '03)",
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "63--76",
  month        = may,
  keywords     = "compilers, memory hierarchy, tiling, blocking, unrolling,
    program transformation, code generation, empirical optimization, model-driven
    optimization, blas",
  abstract     = "Empirical program optimizers estimate the values of key
    optimization parameters by generating different program versions and
    running them on the actual hardware to determine which values give the best
    performance.  In contrast, conventional compilers use models of programs
    and machines to choose these parameters.  It is widely believed that
    model-driven optimization does not compete with empirical optimization, but
    few quantitative comparisons have been done to date.  To make such a
    comparison, we replaced the empirical optimization engine in ATLAS (a
    system for generating a dense numerical linear algebra library called the
    BLAS) with a model-driven optimization engine that used detailed models to
    estimate values for optimization parameters, and then measured the relative
    performance of the two systems on three different hardware platforms.  Our
    experiments show that model-driven optimization can be surprisingly
    effective, and can generate code whose performance is comparable to that of
    code generated by empirical optimizers for the BLAS.", 
  location     = "https://dl.acm.org/doi/10.1145/781131.781140"
}

@Article{maccforvoct,
  author       = "Kristy Andrews and Duane Sand",
  title        = "Migrating a {CISC} Computer Family onto {RISC} via Object Code Translation",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "213--222",
  month        = sep,
  keywords     = "portability, interpretation",
  abstract     = "A minicomputer/mainframe family (Tandem NonStop Systems) and
    all of its machine-dependent vendor and user software has been moved from a
    proprietary CISC instruction set onto a generic RISC instruction set
    Translation of programs' CISC object code into optimized RISC object code
    is a migration path that is easy, gives greatly improved performance,and
    provides all the benefits of traditional object code compatibility.  These
    benefits include no reprogramming, no retraining, fast time to market, and
    debugging of optimized programs as if they were still running on the CISC
    platform.  This paper shares our experience in implementing this migration
    scheme, with measurements of the resulting performance and code size.", 
  location     = "https://doi.org/10.1145/143371.143520",
  location     = "https://www.hpl.hp.com/techreports/tandem/TR-92.1.pdf"
}

@Article{edb,
  author       = "Robert Wahbe",
  title        = "Efficient Data Breakpoints",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "200--212",
  month        = sep,
  keywords     = "debugging, data breakpoints, virtual memory protection",
  abstract     = "Breakpoints are user-specified rules that trigger debugging
    actions when certain conditions arise in an executing program.  To support
    source-level debugging, programmers should be able to specify breakpoints
    conditions in terms of programming language control and data abstractions.
    Support for breakpoints specified in terms of control conditions, known as
    control breakpoints, is ubiquitous.  The analogous data breakpoints, a
    breakpoints specified in terms of a data condition, is difficult to
    implement efficiently and has only limited support in most current
    debuggers.  A number of authors have speculated that efficient data
    breakpoints require hardware support.  In this paper we examine hardware
    and software strategies for implementing data breakpoints.  We use a
    simulation experiment to estimate the performance of four representative
    implementations.  We conclude that while hardware-based solutions are able
    to deliver the best overall performance, they are expensive and can
    simultaneously support only a limited number of breakpoints.  In contrast,
    a software solution based on modifying the code of the program being
    debugged to monitor all instructions that might affect the data breakpoints
    condition is simple and portable, and provides for any number of
    breakpoints.  Further, we show that its expected performance is acceptable
    for most debugging applications.", 
  location     = "https://doi.org/10.1145/143371.143518"
}

@Article{acpmuepcm,
  author       = "Kieran Harty and David~R. Cheriton",
  title        = "Application-Controlled Physical Memory using External Page-Cache Management",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "187--197",
  month        = sep,
  keywords     = "storage management, v++, external page-cache management, ",
  abstract     = "Next generation computer systems will have gigabytes of
    physical memory and processors in the 100 MIPS range or higher.  Contrary
    to some conjectures, this trend requires more sophisticated memory
    management support for memory-bound computations such as scientific
    simulations and systems such as large-scale database systems, even though
    memory management for most programs will be less of a concern.  We describe
    the design, implementation and evaluation of a virtual memory system that
    provides application control of physical memory using external page-cache
    management.  In this approach, a sophisticated application is able to
    monitor and control the amount of physical memory it has available for
    execution, the exact contents of this memory, and the scheduling and nature
    of page-in and page-out using the abstraction of a physical page cache
    provided by the kernel.  We claim that this approach can significantly
    improve performance for many memory-bound applications while reducing
    kernel complexity, yet does not complicate other applications or reduce
    their performance.", 
  location     = "https://doi.org/10.1145/143365.143511",
  location     = "http://i.stanford.edu/pub/cstr/reports/cs/tr/91/1394/CS-TR-91-1394.pdf"
}

@Article{atcpni,
  author       = "Dana~S. Henry and Christopher~F. Joerg",
  title        = "{A} Tightly-Coupled Processor-Network Interface",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "111--122",
  month        = sep,
  keywords     = "nics, register access, message-passing architectures, memory
    mapping, performance, optimizations",
  abstract     = "Careful design of the processor-network interface can
    dramatically reduce the software overhead of interprocessor communication.
    Our interface architecture reduces communication overhead five fold in our
    benchmarks.  Most of our performance gain comes from simple, low cost
    hardware mechanisms for fast dispatching on, forwarding of, and replying to
    messages.  The remaining improvement can be gained by implementing the
    network interface as part of the processor's register file.  For example,
    using our hardware mechanisms a register-mapped interface can receive,
    process, and reply to a remote read request in a total of two RISC
    instructions.  We have implemented an RTL model of an off-chip
    memory-mapped interface which provides our hardware mechanisms.  Our
    industrial partner, Motorola, is implementing a similar network interface
    on-chip in an experimental version of the 88110 processor.", 
  location     = "https://doi.org/10.1145/143371.143497",
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-342/memo-342.pdf"
}

@Article{cmfvic,
  author       = "Bob Wheeler and Brian~N. Bershad",
  title        = "Consistency Management for Virtually Indexed Caches",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "124--136",
  month        = sep,
  keywords     = "aliasing, virtual-physical mapping, state machines,
    consistency models, dma",
  abstract     = "A virtually indexed cache can improve performance by allowing
    cache lookup and address translation to occur in parallel, thus reducing
    processor cycle time.  Unlike physically indexed caches, virtually indexed
    caches create consistency problems because a physical address may be
    represented in more than one cache line when it has been accessed through
    more than one virtual address.  Write-back virtually indexed caches create
    additional inconsistencies because memory may become stale with respect…to
    the cache.  In this paper we examine the problem of consistency management
    for a virtually indexed write-back cache.  We assume that the hardware does
    not support intracache consistency.  We present model and software
    implementation strategy for maintaining consistency with virtually indexed
    caches.  We present measurements from an implementation of this model on
    the HP 9000 Series 7000 in the context of the Mach operating system.  Our
    measurements show that a virtually indexed cache can be managed with nearly
    the same cost as the required to manage a physically indexed one, even when
    used by a virtual memory system that encourages and exploits sharing.",
  location     = "https://doi.org/10.1145/143371.143499"
}

@Article{etatbfpac,
  author       = "Tzi-cker Chiueh and Randy~H. Katz",
  title        = "Eliminating the Address Translation Bottleneck for Physical Address Cache",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "137--148",
  month        = sep,
  keywords     = "cache architecture, lazy address translation, pipelining",
  abstract     = "Two architectural techniques are presented and analyzed in
    this paper that aim at eliminating the Translation Lcookaside Buffer (TLB)
    access delay from the critical path of physical address cache-based scalar
    processors.  The first technique, parallel address translation, masks the
    TLB access delay by using a set-associative virtual memory map to extend
    the cache size beyond the product of the cache associativity and the
    virtual memory page size.  The second technique, lazy address translation,
    bypasses the TLB access completely by using the base register and offset in
    a memory reference as a caching mechanism for its corresponding physical
    page.  Consequently the TLB access is needed only when this caching scheme
    fails.  A trace-driven simulation study is conducted and the experimental
    results show that under the given workload the parallel address translation
    scheme works best when the virtual memory is 16-way set associative, and
    the penalty on the average cycle-per-instruction due to lazy address
    translation is less than 1.3%.", 
  location     = "https://doi.org/10.1145/143371.143501"
}

@Article{acothsa,
  author       = "Ivan~E. Sutherland and Robert~F. Sproull and Robert~A. Schumacker",
  title        = "{A} Characterization of Ten Hidden-Surface Algorithms",
  journal      = surveys,
  year         = 1974,
  volume       = 6,
  number       = 1,
  pages        = "1--55",
  month        = mar,
  keywords     = "sorting, graphics rendering, hidden-line elimination,
    hidden-surface elimination, coherence, computer graphics, raster graphics,
    perspective transformation, algorithm analysis",
  abstract     = "The paper asserts that the hidden-surface problem is mainly
    one of sorting.  The various surfaces of an object to be shown in
    hidden-surface or hidden-line form must be sorted to find out which ones
    are visible at various places on the screen.  Surfaces may be sorted by
    lateral position in the picture (XY), by depth (Z), or by other criteria.
    The paper shows that the order of sorting and the types of sorting used
    form differences among the existing hidden-surface algorithms.  To reduce
    the work of sorting, each algorithm capitalizes on some coherence property
    of the objects represented.  'San-line coherence,' the fact that one TV
    scan line of output is likely to be nearly the same as the previous TV scan
    line, is one commonly used kind of coherence.  'Frame coherence,' the fact
    that the entire picture does not change very much between successive frames
    of a motion picture can be very helpful if it is applicable.  By
    systematically looking for additional kinds of coherence and untried
    sorting orders and sorting types, the paper is able to suggest two
    promising new approaches to the hidden-surface problem.  The first, a
    combination of three existing algorithms, is promising because it would
    capitalize on the frame and scan-line coherence.  The second new approach
    would sort in order Y, Z, X, the only sorting order for which an existing
    algorithm could not be found.", 
  location     = "https://dl.acm.org/doi/abs/10.1145/356625.356626"
}

@Article{apeoohccp,
  author       = "Jack~E. Veenstra and Robert~J. Fowler",
  title        = "{A} Performance Evaluation of Optimal Hybrid Cache Coherency Protocols",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "149--160",
  month        = sep,
  keywords     = "coherency protocols, cost models, trace-driven simulations",
  abstract     = "The caches within a multiprocessor typically use either a 
    write-invalidate protocol or a write-update protocol to maintain
    consistency.  The recently introduced MIPS R4000 processor allows operating
    system software to select, on a per-page basis, which multiprocessor cache
    coherence protocol (write-invalidate versus write-update) the hardware will
    use.  The availability of the R4000 and the prospect of even more flexible
    hardware motivated us to examine the potential performance advantages of
    allowing user-level control over the choice of coherence protocol on a
    per-page basis and to ask whether more powerful hybrid protocols provide
    substantially more benefit.  We examine the potential benefits of three
    classes of hybrid protocols: (1) hybrid protocols that choose statically,
    at the beginning of the program, between write-invalidate (WI) or
    write-update (WU) on a per-page basis, (2) hybrid protocols that choose
    statically between WI or WU for each cache block, and (3) dynamic hybrid
    protocols that can choose between WI or WU at each write.  In order to
    determine how much potential benefit could be obtained by each of these
    protocol classes, we used trace-driven simulations to evaluate the optimal
    off-line protocol for each class.  We found that the use of a hybrid
    protocol can substantially reduce the cost of memory references for most of
    the programs studied.  A few programs can also realize large additional
    benefits from a per-block static hybrid protocol compared to a per-page
    static hybrid protocol.  None of the programs, however, receive a
    significant additional benefit from using a dynamic hybrid protocol
    compared to the per-block static hybrid protocol, unless cache block sizes
    are larger than 16 words (64 bytes).", 
  location     = "https://doi.org/10.1145/143371.143503"
}

@Article{ctcaspoamos,
  author       = "Josep Torrellas and Anoop Gupta and John Hennessy",
  title        = "Charactrizing the Caching and Synchronization Performance of a Multiprocessor Operating System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "162--174",
  month        = sep,
  keywords     = "workloads, cache behavior, os cache activity, cache misses",
  abstract     = "Good cache memory performance is essential to achieving high
    CPU utilization in shared memory multiprocessors.  While the performance of
    caches is determined by both application and operating system (OS)
    references, most research has focused on the cache performance of the OS is
    largely unknown.  In this paper we characterize the cache performance of a
    commercial System V UNIX running on a four-CPU multiprocessor.  The related
    issue of the performance impact of the OS synchronization activity is also
    studied.  For our study, we use a hardware monitor that records the cache
    misses in the machine without perturbing it.  We study three multiprocessor
    workloads: a parallel compile, a multiprogrammed load, and a commercial
    database.  Our results show that OS misses occur frequently enough to stall
    CPUs for 17--21% of their non-idle time.  Further, if we include
    application misses induced by OS interference in the cache, then the stall
    time reaches 25%.  A detailed analysis reveals three major sources of OS
    misses: instruction fetches, process migration and data accesses in block
    operations.  As for synchronization behavior, we find that OS
    synchronization has low overhead if supported correctly and that OS locks
    show good locality and low contention.", 
  location     = "https://doi.org/10.1145/143371.143506"
}

@Article{hsssflan,
  author       = "Thomas~E. Anderson and Susan~S. Owicki and James~B. Saxe and Charles~P. Thacker",
  title        = "High-Speed Switch Scheduling for Local-Area Networks",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "98--110",
  month        = sep,
  keywords     = "scheduling, data forwarding, port buffering, cross-bar
    switches, optical networks, high-speed networking, bipartite-graph
    matching, fabric scheduling",
  abstract     = "Current technology trends make it possible to build
    communication networks that can support high-performance distributed
    computing.  This paper describes issues in the design of a prototype switch
    for an arbitrary topology point-to-point network with link speeds of up to
    1 Gbit/s.  The switch deals in fixed-length ATM-style cells, which it can
    process at a rate of 37 million cells per second.  It provides high
    bandwidth and low latency for datagram traffic.  In addition, it supports
    real-time traffic by providing bandwidth reservations with guaranteed
    latency bounds.  The key to the switch's operation is a technique called
    parallel iterative matching, which can quickly identify a set of
    conflict-free cells for transmission in a time slot.  Bandwidth
    reservations are accommodated in the switch by building a fixed schedule
    for transporting cells from reserved flows across the switch; parallel
    iterative matching can fill unused slots with datagram traffic.  Finally,
    we note that parallel iterative matching may not allocate bandwidth fairly
    among flows of datagram traffic.  We describe a technique called
    statistical matching, which can be used to ensure fairness at the switch
    and to support applications with rapidly changing needs for guaranteed
    bandwidth.", 
  location     = "https://doi.org/10.1145/161541.161736", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-99.pdf"
}

@Article{lfosipac,
  author       = "Jan Newmarch",
  title        = "Lessons from Open Source:  Intellectual Property and Courseware",
  journal      = "First Monday",
  year         = 2001,
  volume       = 6,
  number       = 6,
  month        = apr,
  keywords     = "intellectual property, secrecy, courseware, the commons,
    copyright",
  abstract     = "In this competitive age, universities are seeking ways to
    protect their intellectual property, for fear that it might be stolen or
    used by others without financial benefit coming back to the university.
    Increasingly, universities are using mechanisms of secrecy to secure their
    property.  This paper argues that this approach is wrong on both moral and
    business grounds, and that a better model can be found in the Open Source
    movement of the software industry.", 
  location     = ""
}

@Article{asfsasos,
  author       = "Eric~J. Koldinger and Jeffrey~S. Chase and Susan~J. Eggers",
  title        = "Architectural Support for Single Address Space Operating System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "175--186",
  month        = sep,
  keywords     = "single address space operating systems, single global virtual
    address space, memory system architecture, protection, domains, system
    organization, sharing, wide-address architectures, 32-bit address spaces,
    virtually indexed caches, protection lookaside buffer", 
  abstract     = "Recent microprocessor announcements show a trend toward
    wide-address computers: architectures that support 64 bits of virtual
    address space.  Such architectures facilitate fundamentally new operating
    system organizations that promote efficient data sharing and cooperation,
    both between complex applications and between parts of the operating system
    itself.  One such organization is the single address space operating
    system, in which all processes run within a single global virtual address
    space; protection is provided not through conventional address space
    boundaries, but through protection domains that dictate which pages of the
    global address space a process can reference.  This paper focuses on the
    architectural implications of single address space operating systems,
    specifically the interaction between the memory system architecture and the
    operating system's use of addressing and protection.  Our purpose is to
    explore certain architectural opportunities created by single address space
    by evaluating two protection models that support them.  The first provides
    protection on a per-page, per-domain basis; we define the protection
    lookaside buffer, a hardware structure that implements this model.  The
    second provides protection on a page-group basis; this model is implemented
    in the Hewlett-Packard PA-RISC architecture.", 
  location     = "https://doi.org/10.1145/143371.143508"
}

@Article{nvmffrfs,
  author       = "Mary Baker and Satoshi Asami and Etienne Deprit and John Ouseterhout and Margo Seltzer",
  title        = "Non-Volatile Memory for Fast, Reliable File Systems",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "10--22",
  month        = sep,
  keywords     = "nvram, write file-system performance, client-side caching,
    server-side caching, log-structured file systems",
  abstract     = "Given the decreasing cost of non-volatile RAM (NVRAM), by the
    late 1990’s it will be feasible for most workstations to include a megabyte
    or more of NVRAM, enabling the design of higher-performance, more reliable
    systems.  We present the trace-driven simulation and analysis of two uses
    of NVRAM to improve I/O performance in distributed file systems:
    non-volatile file caches on client workstations to reduce write traffic to
    file servers, and write buffers for write-optimized file systems to reduce
    server disk accesses.  Our results show that a megabyte of NVRAM on
    diskless clients reduces the amount of file data written to the server by
    40 to 50%.  Increasing the amount of NVRAM shows rapidly diminishing
    returns, and the particular NVRAM block replacement policy makes little
    difference to write traffic.  Closely integrating the NVRAM with the
    volatile cache provides the best total traffic reduction.  At today’s
    prices, volatile memory provides a better performance improvement per
    dollar than NVRAM for client caching, but as volatile cache sizes increase
    and NVRAM becomes cheaper, NVRAM will become cost effective.  On the server
    side, providing a one-half megabyte write-buffer per file system reduces
    disk accesses by about 20% on most of the measured logstructured file
    systems (LFS), and by 90% on one heavilyused file system that includes
    transaction-processing workloads.", 
  location     = "https://doi.org/10.1145/143371.143380"
}

@Article{pdfcoirda,
  author       = "Mark Holland and Garth~A. Gibson",
  title        = "Parity Declustering for Continuous Operation in Redundant Disk Arrays",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "23--35",
  month        = sep,
  keywords     = "raid disks, error recovery, declustering, data layout, data
    reconstruction",
  abstract     = "We describe and evaluate a strategy for declustering the
    parity encoding in a redundant disk array.  This declustered parity
    organization balances cost against data reliability and performance during
    failure recovery.  It is targeted at highly-available parity-based arrays
    for use in continuousoperation systems.  It improves on standard parity
    organizations by reducing the additional load on surviving disks during the
    reconstruction of a failed disk’s contents.  This yields higher user
    throughput during recovery, and/or shorter recovery time.  We first address
    the generalized parity layout problem, basing our solution on balanced
    incomplete and complete block designs.  A software implementation of
    declustering is then evaluated using a disk array simulator under a highly
    concurrent workload comprised of small user accesses.  We show that
    declustered parity penalizes user response time while a disk is being
    repaired (before and during its recovery) less than comparable
    non-declustered (RAID5) organizations without any penalty to user response
    time in the fault-free state.  We then show that previously proposed
    modifications to a simple, single-sweep reconstruction algorithm further
    decrease user response times during recovery, but, contrary to previous
    suggestions, the inclusion of these modifications may, for many
    configurations, also slow the reconstruction process.  This result arises
    from the simple model of disk access performance used in previous work,
    which did not consider throughput variations due to positioning delays.", 
  location     = "https://doi.org/10.1145/143371.143383",
  location     = "http://www.pdl.cmu.edu/PDL-FTP/Declustering/ASPLOS.pdf"
}

@Article{ssfsl,
  author       = "Anne Rogers and Kai Li",
  title        = "Software Support for Speculative Loads",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "38--50",
  month        = sep,
  keywords     = "speculative loads, instruction lifting, loop unrolling,
    basic-block lifting",
  abstract     = "This paper describes a very simple mechanism and related
    compiler support for software--controlled speculative loads.  The compiler
    issues speculative load instructions based on anticipated data references
    and the ability of the memory system to hide memory latency in
    high--performance processors.  The architectural support for such a
    mechanism is simple and minimal, yet handles faults gracefully.  We have
    simulated the speculative load mechanism based on a MIPS processor and a
    detailed memory system.  The results of scientific kernel loops indicate
    that the speculative load techniques are effective approaches to hiding
    memory latency.", 
  location     = "https://doi.org/10.1145/143365.143484"
}

@Article{aupocp,
  author       = "Jeremy Jacob",
  title        = "{A} Uniform Presentation of Confidentiality Properties",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1186--1194",
  month        = nov,
  keywords     = "theoretical foundations of security, security models,
    information flow, formal methods, measures of confidentiality, shared
    systems, program correctness, program specification",
  abstract     = "Security (in the sense of confidentiality) properties are
    properties of shared systems.  A suitable model of shared systems, in which
    one can formally define the term security property and then proceed to
    catalog several security properties, is presented.  The purpose is to
    present various information-flow properties in a manner that exposes their
    differences and similarities.  Abstraction is the main tool, and everything
    that is not central to the purpose is discarded.  The presentation is
    generic in the model of computation.  The abstraction lays bare a regular
    structure into which many interesting information-flow properties fall.  A
    shared system is represented by a relation.  How this model lets one reason
    about information flow is discussed and the term information flow property
    is formally defined.  Various information-flow properties are described.
    Composability and probabilistic security properties are addressed.", 
  location     = "https://doi.org/10.1109/32.106973"
}

@Article{rmlvnbapc,
  author       = "Tien-Fu Chen and Jean-Loup Baer",
  title        = "Reducing Memory Latency via Non-Blocking and Prefetching Caches",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "51--61",
  month        = sep,
  keywords     = "non-blocking caches, prefetching caches, memory models,
    architectural variations, latency, compiler manipulations",
  abstract     = "Non-blocking caches and prefetching caches are two techniques
    for hiding memory latency by exploiting the overlap of processor
    computations with data accesses.  A non-blocking cache allows execution to
    proceed concurrently with cache misses as long as dependency constraints
    are observed, thus exploiting post-miss operations.  A prefetching cache
    generates prefetch requests to bring data in the cache before it is
    actually needed, thus allowing overlap with pre-miss computations.  In this
    paper, we evaluate the effectiveness of these two hardware-based schemes.
    We propose a hybrid design based on the combination of these approaches.
    We also consider compiler-based optimizations to enhance the effectiveness
    of non-blocking caches.  Results from instruction level simulations on the
    SPEC benchmarks show that the hardware prefetching caches generally
    outperform non-blocking caches.  Also, the relative effectiveness of
    non-blocking caches is more adversely affected by an increase in memory
    latency than that of prefetching caches.  However, the performance of
    non-blocking caches can be improved substantially by compiler optimizations
    such as instruction scheduling and register renaming.  The hybrid design
    can be effective in reducing the memory latency penalty for many applications.", 
  location     = "https://doi.org/10.1145/143371.143486",
  location     = "https://dada.cs.washington.edu/research/tr/1992/06/UW-CSE-92-06-03.pdf"
}

@Article{daeoacafp,
  author       = "Todd~C. Mowry and Monica~S. Lam and Anoop Gupta",
  title        = "Design and Evaluation of a Compiler Algorithm for Prefetching",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "62--73",
  month        = sep,
  keywords     = "memory hierarchy, access optimization, prefetching, locality
    analysis, loop splitting, prefetch scheduling, software pipelining, cpu
    stalls",
  abstract     = "Software-controlled data prefetching is a promising technique
    for improving the performance of the memory subsystem to match today’s
    high-performance processors.  While prefetching is useful in hiding the
    latency, issuing prefetches incurs an instruction overhead and can increase
    the load on the memory subsystem.  As a result, care must be taken to
    ensure that such overheads do not exceed the benefits.  This paper proposes
    a compiler algorithm to insert prefetch instructions into code that
    operates on dense matrices.  Our algorithm identifies those references that
    are likely to be cache misses, and issues prefetches only for them.  We
    have implemented our algorithm in the SUIF (Stanford University
    Intermediate Form) optimizing compiler.  By generating fully functional
    code, we have been able to measure not only the improvements in cache miss
    rates, but also the overall performance of a simulated system.  We show
    that our algorithm significantly improves the execution speed of our
    benchmark programs—some of the programs improve by as much as a factor of
    two.  When compared to an algorithm that indiscriminately prefetches all
    array accesses, our algorithm can eliminate many of the unnecessary
    prefetches without any significant decrease in the coverage of the cache
    misses.", 
  location     = "https://doi.org/10.1145/143371.143488",
  location     = "https://suif.stanford.edu/papers/mowry92.pdf"
}

@Article{itaodbpubc,
  author       = "Shien-Tai Pan and Kimming So and Joseph~T. Rahmeh",
  title        = "Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "76--84",
  month        = sep,
  keywords     = "dynamic branch prediction, counter-based prediction,
    correlation-based prediction",
  abstract     = "Long branch delay is a well–known problem in today’s high
    performance superscalar and supetpipeline processor designs.  A common
    technique used to alleviate this problem is to predict the direction of
    branches during the instruction fetch.  Counter-based branch prediction, in
    particular, has been reported as an effective scheme for predicting the
    direction of branches.  However, its accuracy is generally limited by
    branches whose future behavior is also dependent upon the history of other
    branches.  To enhance branch prediction accuracy with a minimum increase in
    hardware cost, we propose a correlation-based scheme and show how the
    prediction accuracy can be improved by incorporating information, not only
    from the history of a specific branch, but also from the history of other
    branches.  Specifically, we use the information provided by a proper
    subhistory of a branch to predict the outcome of that branch.  The proper
    subhistory is selected based on the outcomes of the most recently executed
    M branches.  The new scheme is evaluated using traces collected from
    running the spec benchmark suite on an IBM RISC System/60000 workstation.
    The results show that, as compared with the 2-bit counter-based prediction
    scheme, the correlation-based branch prediction achieves up to 11%
    additional accuracy at the extra hardware cost of one shift register.  The
    results also show that the accuracy of the new scheme surpasses that of the
    counter-based branch prediction at saturation.", 
  location     = "https://doi.org/10.1145/143371.143490"
}

@Article{pcbdfproap,
  author       = "Joseph~A. Fisher and Stefan~M. Freudenberger",
  title        = "Prediction Conditional Branch Directions from Previous Runs of a Program",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "85--95",
  month        = sep,
  keywords     = "conditional branch predictions, instructions per mispredicted
    branch, instruction-level parallelism",
  abstract     = "There are several important reasons for predicting which way
    the flow of control of a program is going to go: first, in
    instruction-level parallel architectures, code motions can produce more
    data-ready candidate instructions at once than there are resources to
    execute them.  Some of these are speculative (executed ahead of a
    conditional branch that might otherwise have prevented their execution), so
    one must sensibly pick among them, and one must avoid issuing low
    probability speculative instructions when the system overhead associated
    with canceling them most of the time outweighs the gain of their infrequent
    success; second, important classes of compiler optimizations depend upon
    this information; and finally, branch prediction can help optimize
    pipelined fetch and execute, icache fill, etc.  If substantial code motions
    are desired, it is probably impractical to expect the hardware to make
    them, and a compiler must instead.  Thus, the compiler must have access to
    branch predictions made before the program runs.  In this paper we consider
    the question of how predictable branches are when previous runs of a
    program are used to feed back information to the compiler.  We propose new
    measures which we believe more clearly capture the predictability of
    branches in programs.  We find that even code with a complex flow of
    control, including systems utilities and and language processors written in
    C, are dominated by branches which go in one way, and that this direction
    usually varies little when one changes the data used as the predictor and
    target.", 
  location     = "https://doi.org/10.1145/143371.143493",
  location     = "https://www.hpl.hp.com/techreports/92/HPL-92-98.html"
}

@Article{ttinp,
  author       = "Lal George and Matthias Blume",
  title        = "Taming the {IXP} Network Processor",
  journal      = pldi03,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "26--34",
  month        = may,
  keywords     = "network processors, intel ixa, integer linear programming,
    register allocation, bank assignment, programming languages, code generation",
  abstract     = "We compile Nova, a new language designed for writing network
    processing applications, using a back end based on integer-linear
    programming (ILP) for register allocation, optimal bank assignment, and
    spills.  The compiler's optimizer employs CPS as its intermediate
    representation; some of the invariants that this IR guarantees are
    essential for the formulation of a practical ILP model.Appel and George
    used a similar ILP-based technique for the IA32 to decide which variables
    reside in registers but deferred the actual assignment of colors to a later
    phase.  We demonstrate how to carry over their idea to an architecture with
    many more banks, register aggregates, variables with multiple simultaneous
    register assignments, and, very importantly, one where bank- and
    register-assignment cannot be done in isolation from each other.  Our
    approach performs well in practise---without causing an explosion in size
    or solve time of the generated integer linear programs.", 
  location     = "https://doi.org/10.1145/781131.781135"
}

@Article{cpcpbh,
  author       = "Per Brinch Henson",
  title        = "Concurrent Programming Concepts",
  journal      = surveys,
  year         = 1973,
  volume       = 5,
  number       = 4,
  pages        = "223--245",
  month        = dec,
  keywords     = "structured multiprogramming, programming languages, operating
    systems, programming errors, resource protection, compile-time checking,
    correctness proofs, sequential and concurrent processes, synchronizing
    events, semaphores, shared data, mutual exclusion, critical regions, monitors",
  abstract     = "This paper describes the evolution of language features for
    multiprogramming from event queues and semaphores to critical regions and
    monitors.  It suggests that the choice of language concepts should be
    guided by two simple principles: First, it should be possible to understand
    a concurrent program in time-independent terms by an effort proportional to
    its size; secondly, it should be possible to state assumptions about
    invarmnt relationships among program components and have these assumptions
    checked automatically.  The central problems of multiprogramming are
    illustrated by annotated algorithms written in a weU-structured programming
    language.", 
  location     = "https://doi.org/10.1145/356622.356624"
}

@Article{oldcialsfs,
  author       = "Michael Burrows and Charles Jerian and Butler Lampson and Timothy Mann",
  title        = "On-Line Data Compression in an Log-Structured File System",
  journal      = asplos92,
  year         = 1992,
  volume       = 27,
  number       = 9,
  pages        = "2--9",
  month        = sep,
  keywords     = "sprite, log-structured file systems, data compression,
    hardware compression, write-log management, i-o performance",
  abstract     = "We have incorporated on-line data compression into the low
    levels of a log-structured file system (Rosenblum’s Sprite LFS).  Each
    block of data or meta-data is compressed as it is written to the disk and
    decompressed as it is read.  The log-structuring overcomes the problems of
    allocation and fragmentation for variable-sized blocks.  We observe
    compression factors ranging from 1.6 to 2.2, using algorithms running from
    1.7 to 0.4 MBytes per second in software on a DECstation 5000/200.  System
    performance is degraded by a few percent for normal activities (such as
    compiling or editing), and as much as a factor of 1.6 for file system
    intensive operations (such as copying multi-megabyte files).  Hardware
    compression devices mesh well with this design.  Chips are already
    available that operate at speeds exceeding disk transfer rates, which
    indicates that hardware compression would not only remove the performance
    degradation we observed, but might well increase the effective disk
    transfer rate beyond that obtainable from a system without compression.",
  location     = "https://www.hpl.hp.com/techreports/92/HPL-92-85.html"
}

@Article{laaoosp,
  author       = "Andrew~A. Lamb and William Thies and Saman Amarasinghe",
  title        = "Linear Analysis and Optimization of Stream Programs",
  journal      = pldi03,
  year         = 2003,
  volume       = 38,
  number       = 5,
  pages        = "26--34",
  month        = may,
  keywords     = "stream programming, streamit, optimization, embedded systems,
    linear systems, algebraic simplification, dsp, fft",
  abstract     = "As more complex DSP algorithms are realized in practice,
    there is an increasing need for high-level stream abstractions that can be
    compiled without sacrificing efficiency.  Toward this end, we present a set
    of aggressive optimizations that target linear sections of a stream
    program.  Our input language is StreamIt, which represents programs as a
    hierarchical graph of autonomous filters.  A filter is linear if each of
    its outputs can be represented as an affine combination of its inputs.
    Linearity is common in DSP components; examples include FIR filters,
    expanders, compressors, FFTs and DCTs.We demonstrate that several
    algorithmic transformations, traditionally hand-tuned by DSP experts, can
    be completely automated by the compiler.  First, we present a linear
    extraction analysis that automatically detects linear filters from the
    C-like code in their work function.  Then, we give a procedure for
    combining adjacent linear filters into a single filter, as well as for
    translating a linear filter to operate in the frequency domain.  We also
    present an optimization selection algorithm, which finds the sequence of
    combination and frequency transformations that will give the maximal
    benefit.We have completed a fully-automatic implementation of the above
    techniques as part of the StreamIt compiler, and we demonstrate a 450%
    performance improvement over our benchmark suite.", 
  location     = "https://doi.org/10.1145/780822.781134", 
  location     = "https://dspace.mit.edu/handle/1721.1/29668"
}

@Article{cpocptieftl,
  author       = "Marty Ossefort",
  title        = "Correctness Proofs of Communicating Processes:  Three Illustrative Examples from the Literature",
  journal      = toplas,
  year         = 1983,
  volume       = 5,
  number       = 4,
  pages        = "620--640",
  month        = oct,
  keywords     = "message-passing system, program proofs, rebound sorting,
    communicating sequential processes, csp, pre- and post-conditions,
    hierarchy, local and global reasoning",
  abstract     = "The proof method for process networks proposed by Misra and
    Chandy is applied to three examples from the literature.  The proof method
    is easy to use, preserves process autonomy in the network proof, and
    conforms naturally to the hierarchical structure of the network.  Two very
    large-scale integration algorithms and a sorting network are presented in a
    variant Hoare's communicating sequential processes model, specified
    completely, and formally proved.",
  location     = "https://doi.org/10.1145/69575.2083"
}

@Article{adofssfid,
  author       = "Kanth Miriyala and Mehdi~T. Harandi",
  title        = "Automatic Derivation of Formal Software Specifications from Informal Descriptions",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 10,
  pages        = "1126--1142",
  month        = oct,
  keywords     = "approximate analogy, formal specification, informal
    specification, difference-based reasoning, schemas, specification assistance",
  abstract     = "SPECIFIER, an interactive system which derives formal
    specifications of data types and programs from their informal descriptions,
    is described.  The process of deriving formal specifications is viewed as a
    problem-solving process.  The system uses common problem-solving techniques
    such as schemas, analogy, and difference-based reasoning to derive formal
    specifications.  If an informal description is a commonly occurring
    operation for which the system has a schema, then the formal specification
    is derived by instantiating the schema.  If there is a no such schema,
    SPECIFIER tries to find a previously solved problem which is analogous to
    the current problem.  If the problem found is directly analogous to the
    current problem, it applies an analogy mapping to obtain a formal
    specification.  On the other hand, if the analogy found is only
    approximate, it solves the directly analogous part of the problem by
    analogy and performs difference-based reasoning using the remaining
    (unmatched) parts to transform the formal specification obtained by analogy
    to a formal specification for the entire original problem.",
  location     = "https://doi.org/10.1109/32.99198"
}

@Article{arotvvsm,
  author       = "Paul~A. Karger and Mary Ellen Zurko and Douglas~W. Bonin and Andrew~H. Mason and Clifford~E. Kahn",
  title        = "{A} Retrospective on the {VAX} {VMM} Security Model",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 11,
  pages        = "1147--1165",
  month        = nov,
  keywords     = "computer security, virtual machines, covert channels,
    mandatory security, discretionary security, layered design, security kernels,
    protection rights",
  abstract     = "The development of a virtual-machine monitor (VMM) security
    kernel for the VAX architecture is described.  The focus is on how the
    system's hardware, microcode, and software are aimed at meeting A1-level
    security requirements while maintaining the standard interfaces and
    applications of the VMS and ULTRIX-32 operating systems.  The VAX security
    kernel supports multiple concurrent virtual machines on a single VAX
    system, providing isolation and controlled sharing of sensitive data.
    Rigorous engineering standards were applied during development to comply
    with the assurance requirements for verification and configuration
    management.  The VAX security kernel has been developed with a heavy
    emphasis on performance and system management tools.  The kernel performs
    sufficiently well that much of its development was carried out in virtual
    machines running on the kernel itself, rather than in a conventional
    time-sharing system.", 
  location     = ""
}

@Article{tpwsdmtc,
  author       = "Mike Olson and Uche Ogbuji",
  title        = "The {Python Web} Services Developer:  Messaging Technologies Compared",
  journal      = "IBM Developer Works",
  year         = 2002,
  month        = jul,
  keywords     = "soap, web services, corba, performance",
  abstract     = "Choosing between technologies always involves trade-offs;
    often you sacrifice performance to gain ease of programming.  Perhaps the
    realm of most interest to Web services developers is messaging technology.
    How can you balance speedy performance with human readability? Mike Olson
    and Uche Ogbuji don't claim to have the answer to this question, but they
    do offer some hard data to help you make the decision that best fits your
    needs.  In this article, they help you compare some of the different
    messaging protocols available.  You will write a simple application for
    each protocol and compare various measurements of speed, message overhead,
    and relative development time." 
}

@Article{alafgrnwagd,
  author       = "Michael~D. Vose",
  title        = "{A} Linear Algorithm For Generating Random Numbers With a Given Distribution",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "972--975",
  month        = sep,
  keywords     = "random, random numbers, random variables",
  abstract     = "Let xi be a random variable over a finite set with an
    arbitrary probability distribution.  Improvements to a fast method of
    generating sample values for xi in constant time are suggested.", 
  location     = "https://doi.org/10.1109/32.92917"
}

@TechReport{aqmafd30,
  author       = "Greg White",
  title        = "Active Queue Management Algorithms for {DOCSIS} 3.0",
  subtitle     = "A Simulation Study of CoDel, SfQ-CoDel and PIE in DOCSIS 3.0 Networks",
  institution  = "Access Network Technologies, CableLabs",
  year         = 2013,
  month        = apr,
  keywords     = "latency, buffering, buffer bloat, queue drop algorithms, pie,
    codel, sfq-codel, packet loss, tcp, performance",
  abstract     = "This paper describes the results of a simulation study of
    three active queue management algorithms applied to the upstream
    transmission buffer in a DOCSIS 3.  cable modem.  This paper is a follow-on
    to an earlier study which examined the Controlled Delay (CoDel) active
    queue management algorithm in a simulated DOCSIS 3.  cable modem.  This
    expanded study looks at CoDel in more depth, and compares it to two other
    promising active queue management algorithms, Stochastic Flow Queue - CoDel
    (SFQ- CoDel) and Proportional Integral Enhanced (PIE).  These three queue
    management algorithms are compared to existing (tail drop) buffering
    implementations that exist in current cable modems across a range of
    latency-sensitive applications.  It is demonstrated that current cable
    modem implementations result in severe degradation of user experience for
    latency-sensitive applications in situations where the user is
    simultaneously uploading a file via TCP.  The goal of the active queue
    managers in this study is to prevent the degradation of latency sensitive
    applications, while not impacting the TCP upload performance.  The
    Stochastic Flow Queue - Controlled Delay active queue manager displays
    extremely good performance in most traffic scenarios, enabling up to 2x
    reduction in latency for gaming traffic, x reduction in web page load time,
    and pristine VoIP quality, all while minimally impacting TCP upload
    performance.  The Proportional Integral Enhanced active queue manager
    similarly provided good performance, and is optimized for efficient
    implementation in existing cable modems.", 
  location     = "https://www.cablelabs.com/wp-content/uploads/2014/05/Active_Queue_Management_Algorithms_DOCSIS_3_0.pdf"
}

@Article{fgacfdsm,
  author       = "Ioannis Schoinas and Babak Falsafi and Alvin~R. Lebeck and Steven~K. Reinhardt and James~R. Larus and David~A. Wood",
  title        = "Fine-Grain Access Control for Distributed Shared Memory",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "297--306",
  month        = nov,
  keywords     = "access control, distributed shared store, hardware-software
    trade-off, connection machines",
  abstract     = "This paper discusses implementations of fine-grain memory
    access control, which selectively restricts reads and writes to
    cache-block-sized memory regions.  Fine-grain access control forms the
    basis of efficient cache-coherent shared memory.  This paper focuses on
    low-cost implementations that require little or no additional hardware.
    These techniques permit efficient implementation of shared memory on a wide
    range of parallel systems, thereby providing shared-memory codes with a
    portability previously limited to message passing.This paper categorizes
    techniques based on where access control is enforced and where access
    conflicts are handled.  We incorporated three techniques that require no
    additional hardware into Blizzard, a system that supports distributed
    shared memory on the CM-5.  The first adds a software lookup before each
    shared-memory reference by modifying the program's executable.  The second
    uses the memory's error correcting code (ECC) as cache-block valid bits.
    The third is a hybrid.  The software technique ranged from slightly faster
    to two times slower than the ECC approach.  Blizzard's performance is
    roughly comparable to a hardware shared-memory machine.  These results
    argue that clusters of workstations or personal computers with networks
    comparable to the CM-5's will be able to support the same shared-memory
    interfaces as supercomputers.", 
  location     = "https://minds.wisconsin.edu/handle/1793/59904", 
  location     = "https://doi.org/10.1145/381792.195575"
}

@Article{iamttmaw,
  author       = "James Laudon and Anoop Gupta and Mark Horowitz",
  title        = "Interleaving:  {A} Multithreading Technique Targeting Multiprocessors and Workstations",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "308--318",
  month        = nov,
  keywords     = "multiple context processors, fine granularity, blocking,
    context switching, threaded computations, latency hiding",
  abstract     = "There is an increasing trend to use commodity microprocessors
    as the compute engines in large-scale multiprocessors.  However, given that
    the majority of the microprocessors are sold in the workstation market, not
    in the multiprocessor market, it is only natural that architectural
    features that benefit only multiprocessors are less likely to be adopted in
    commodity microprocessors.  In this paper, we explore multiple-context
    processors, an architectural technique proposed to hide the large memory
    latency in multiprocessors.  We show that while current multiple-context
    designs work reasonably well for multiprocessors, they are ineffective in
    hiding the much shorter uniprocessor latencies using the limited
    parallelism found in workstation environments.  We propose an alternative
    design that combines the best features of two existing approaches, and
    present simulation results that show it yields better performance for both
    multiprogrammed workloads on a workstation and parallel applications on a
    multiprocessor.  By addressing the needs of the workstation environment,
    our proposal makes multiple contexts more attractive for commodity
    microprocessors.", 
  location     = "https://doi.org/10.1145/381792.195576"
}

@Article{hsffcba,
  author       = "Nicholas~P. Carter and Stephen~W. Keckler and William~J. Dally",
  title        = "Hardware Support for Fast Capability-Based Addressing",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "319--327",
  month        = nov,
  keywords     = "guarded pointers, capability, single-address space systems",
  abstract     = "Traditional methods of providing protection in memory systems
    do so at the cost of increased context switch time and/or increased storage
    to record access permissions for processes.  With the advent of computers
    that supported cycle-by-cycle multithreading, protection schemes that
    increase the time to perform a context switch are unacceptable, but
    protecting unrelated processes from each other is still necessary if such
    machines are to be used in non-trusting environments.This paper examines
    guarded pointers, a hardware technique which uses tagged 64-bit pointer
    objects to implement capability-based addressing.  Guarded pointers encode
    a segment descriptor into the upper bits of every pointer, eliminating the
    indirection and related performance penalties associated with traditional
    implementations of capabilities.  All processes share a single 54-bit
    virtual address space, and access is limited to the data that can be
    referenced through the pointers that a process has been issued.  Only one
    level of address translation is required to perform a memory reference.
    Sharing data between processes is efficient, and protection states are
    defined to allow fast protected subsystem calls and create unforgeable data
    keys.", 
  location     = "https://doi.org/10.1145/195470.195579"
}

@Article{teomhc,
  author       = "Radhika Thekkath and Susan~J. Eggers",
  title        = "The Effectiveness of Multiple Hardware Contexts",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "328--337",
  month        = nov,
  keywords     = "multi-context processors, threaded computations, caching",
  abstract     = "Multithreaded processors are used to tolerate long memory
  latencies.  By executing threads loaded in multiple hardware contexts, an
  otherwise idle processor can keep busy, thus increasing its utilization.
  However, the larger size of a multi-thread working set can have a negative
  effect on cache conflict misses.  In this paper we evaluate the two phenomena
  together, examining their combined effect on execution time.The usefulness of
  multiple hardware contexts depends on: program data locality, cache
  organization and degree of multiprocessing.  Multiple hardware contexts are
  most effective on programs that have been optimized for data locality.  For
  these programs, execution time dropped with increasing contexts, over widely
  varying architectures.  With unoptimized applications, multiple contexts had
  limited value.  The best performance was seen with only two contexts, and
  only on uniprocessors and small multiprocessors.  The behavior of the
  unoptimized applications changed more noticeably with variations in cache
  associativity and cache hierarchy, unlike the optimized programs.As a
  mechanism for exploiting program parallelism, an additional processor is
  clearly better than another context.  However, there were many configurations
  for which the addition of a few hardware contexts brought as much or greater
  performance than a larger multiprocessor with fewer than the optimal number
  of contexts.", 
  location     = "https://doi.org/10.1145/195470.195583"
}

@Article{aaotfppc,
  author       = "Bernard Elspas and Karl~N. Levitt and Richard~J. Waldinger and Abraham Waksman",
  title        = "An Assessment of Techniques for Proving Program Correctness",
  journal      = surveys,
  year         = 1972,
  volume       = 4,
  number       = 2,
  pages        = "97--147",
  month        = jun,
  keywords     = "automatic program verification, theorem proving, program
    schema, first-order predicate calculus",
  abstract     = "The purpose of this paper is to point out the significant
    quantity of work is progress on technique that will enable programmers to
    prove their programs correct.  This work has included: investigation in the
    theory of program schemas or abstract programs, development of the art of
    the informal or manual proof of correctness; and development of mechanical
    or semi-mechanical approaches to proving correctness.  At present, these
    mechanical approaches rely upon the availability of powerful
    theorem-provers, envelopment of which is being actively pursued.  All of
    these technical areas are here surveyed in detail, and recommendations are
    made concerning the direction of future research toward producing a
    semi-mechanical program verifier.", 
  location     = "https://doi.org/10.1145/356599.356602"
}

@Article{tpaoibdticcm,
  author       = "Steven Cameron Woo and Jaswinder Pal Singh and John~L. Hennessy",
  title        = "The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "219--229",
  month        = nov,
  keywords     = "block data transfer, load-store communication, system
    architecture, scientific programming, shared-store multiprocessor
    programs",
  abstract     = "Integrating support for block data transfer has become an
    important emphasis in recent cache-coherent shared address space
    multiprocessors.  This paper examines the potential performance benefits of
    adding this support.  A set of ambitious hardware mechanisms is used to
    study performance gains in five important scientific computations that
    appear to be good candidates for using block transfer.  Our conclusion is
    that the benefits of block transfer are not substantial for hardware
    cache-coherent multiprocessors.  The main reasons for this are (i) the
    relatively modest fraction of time applications spend in communication
    amenable to block transfer, (ii) the difficulty of finding enough
    independent computation to overlap with the communication latency that
    remains after block transfer, and (iii) long cache lines often capture many
    of the benefits of block transfer in efficient cache-coherent machines.  In
    the cases where block transfer improves performance, prefetching can often
    provide comparable, if not superior, performance benefits.  We also examine
    the impact of varying important communication parameters and processor
    speed on the effectiveness of block transfer, and comment on useful
    features that a block transfer facility should support for real
    applications.", 
  location     = "https://doi.org/10.1145/381792.195547",
  location     = "https://www.cs.princeton.edu/~jps/papers/bulk-transfer-conf.ps"
}

@Article{itaosbpubc,
  author       = "Cliff Young and Michael~D. Smith",
  title        = "Improving the Accuracy of Static Branch Prediction Using Branch Correlation",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "232--241",
  month        = nov,
  keywords     = "branch prediction, branch correlation, code duplication",
  abstract     = "Recent work in history-based branch prediction uses novel
    hardware structures to capture branch correlation and increase branch
    prediction accuracy.  We present a profile-based code transformation that
    exploits branch correlation to improve the accuracy of static branch
    prediction schemes.  Our general method encodes branch history information
    in the program counter through the duplication and placement of program
    basic blocks.  For correlation histories of eight branches, our
    experimental results achieve up to a 14.7% improvement in prediction
    accuracy over conventional profile-based prediction without any increase in
    the dynamic instruction count of our benchmark applications.  In the
    majority of these applications, ode duplication increases code size by less
    than 30%.  For the few applications with code segments that exhibit
    exponential branching paths and no branch correlation, simple compile-time
    heuristics can eliminate these branches as code-transformation
    candidates.", 
  location     = "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-S09/handouts/papers/msmith_asplos94.ps"
}

@Article{rbcvba,
  author       = "Brad Calder and Dirk Grunwald",
  title        = "Reducing Branch Costs via Branch Alignment",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "242--251",
  month        = nov,
  keywords     = "branch prediction, branch alignment",
  abstract     = "Several researchers have proposed algorithms for basic block 
    reordering.  We call these branch alignment algorithms.  The primary
    emphasis of these algorithms has been on improving instruction cache
    locality, and the few studies concerned with branch prediction reported
    small or minimal improvements.  As wide-issue architectures become
    increasingly popular the importance of reducing branch costs will increase,
    and branch alignment is one mechanism which can effectively reduce these
    costs.In this paper, we propose an improved branch alignment algorithm that
    takes into consideration the architectural cost model and the branch
    prediction architecture when performing the basic block reordering.  We
    show that branch alignment algorithms can improve a broad range of static
    and dynamic branch prediction architectures.  We also show that a program
    performance can be improved by approximately 5% even when using recently
    proposed, highly accurate branch prediction architectures.  The programs
    are compiled by any existing compiler and then transformed via binary
    transformations.  When implementing these algorithms on a Alpha AXP 21604
    up to a 16% reduction in total execution time is achieved.", 
  location     = "https://doi.org/10.1145/381792.195553"
}

@Article{cofidl,
  author       = "Steve Carr and Kathryn~S. McKinley and Chau-Wen Tseng",
  title        = "Compiler Optimizations for Improving Data Locality",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "252--262",
  month        = nov,
  keywords     = "loop reversals, loop fusion, loop permutations, compiler
    optimizations, compound transformations, tiling",
  abstract     = "In the past decade, processor speed has become significantly
    faster than memory speed.  Small, fast cache memories are designed to
    overcome this discrepancy, but they are only effective when programs
    exhibit data locality.  In this paper, we present compiler optimizations to
    improve data locality based on a simple yet accurate cost model.  The model
    computes both temporal and spatial reuse of cache lines to find desirable
    loop organizations.  The cost model drives the application of compound
    transformations consisting of loop permutation, loop fusion, loop
    distribution, and loop reversal.  We demonstrate that these program
    transformations are useful for optimizing many programs.To validate our
    optimization strategy, we implemented our algorithms and ran experiments on
    a large collection of scientific programs and kernels.  Experiments with
    kernels illustrate that our model and algorithm can select and achieve the
    best performance.  For over thirty complete applications, we executed the
    original and transformed versions and simulated cache hit rates.  We
    collected statistics about the inherent characteristics of these programs
    and our ability to improve their data locality.  To our knowledge, these
    studies are the first of such breadth and depth.  We found performance
    improvements were difficult to achieve because benchmark programs typically
    have high hit rates even for small data caches; however, our optimizations
    significantly improved several programs.", 
  location     = "https://doi.org/10.1145/381792.195557"
}

@Article{daerdcgs,
  author       = "Dawson~R. Engler and Todd~A. Proebsting",
  title        = "{DCG}:  An Efficient, Retargetable Dynamic Code Generation System",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "263--272",
  month        = nov,
  keywords     = "dynamic code generation, interfaces, intermediate
    representation, run-time optimization",
  abstract     = "Dynamic code generation allows aggressive optimization
    through the use of runtime information.  Previous systems typically relied
    on ad hoc code generators that were not designed for retargetability, and
    did not shield the client from machine-specific details.  We present a
    system, dcg, that allows clients to specify dynamically generated code in a
    machine-independent manner.  Our one-pass code generator is easily
    retargeted and extremely efficient (code generation costs approximately 350
    instructions per generated instruction).  Experiments show that dynamic
    code generation increases some application speeds by over an order of
    magnitude.", 
  location     = "https://doi.org/10.1145/381792.195567"
}

@Article{tpiofitsfm,
  author       = "Mark Heinrich and Jeffrey Kuskin and David Ofelt and John Heinlein and Joel Baxter and Jaswinder Pal Singh and Richard Simoni and Kourosh Gharachorloo and David Nakahira and Mark Horowitz and Anoop Gupta and Mendel Rosenblum and John Hennessy",
  title        = "The Performance Impact of Flexibility in the {Stanford FLASH} Multiprocessor",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "274--285",
  month        = nov,
  keywords     = "flash shared-memory multiprocessor, network processor,
    programmability",
  abstract     = "A flexible communication mechanism is a desirable feature in
    multiprocessors because it allows support for multiple communication
    protocols, expands performance monitoring capabilities, and leads to a
    simpler design and debug process.  In the Stanford FLASH multiprocessor,
    flexibility is obtained by requiring all transactions in a node to pass
    through a programmable node controller, called MAGIC.  In this paper, we
    evaluate the performance costs of flexibility by comparing the performance
    of FLASH to that of an idealized hardwired machine on representative
    parallel applications and a multiprogramming workload.  To measure the
    performance of FLASH, we use a detailed simulator of the FLASH and MAGIC
    designs, together with the code sequences that implement the
    cache-coherence protocol.  We find that for a range of optimized parallel
    applications the performance differences between the idealized machine and
    FLASH are small.  For these programs, either the miss rates are small or
    the latency of the programmable protocol can be hidden behind the memory
    access time.  For applications that incur a large number of remote misses
    or exhibit substantial hot-spotting, performance is poor for both machines,
    though the increased remote access latencies or the occupancy of MAGIC lead
    to lower performance for the flexible design.  In most cases, however,
    FLASH is only 2%–12% slower than the idealized machine.",
  location     = "https://doi.org/10.1145/195470.195569",
  location     = "http://csl.cs.ucf.edu/~heinrich/papers/FlashPerf.pdf"
}

@Article{scatrooiacp,
  author       = "Jonas Skeppstedt and Per Stenstr{\" o}m",
  title        = "Simple Compiler Algorithms to Reduce Ownership Overhead in Cache Coherence Protocols",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "286--296",
  month        = nov,
  keywords     = "compilers, adaptive ache coherence protocols",
  abstract     = "We study in this paper the design and efficiency of compiler
    algorithms that remove ownership overhead in shared-memory multiprocessors
    with write-invalidate protocols.  These algorithms detect loads followed by
    stores to the same address.  Such loads are marked and constitute a hint to
    the cache to obtain an exclusive copy of the block.  We consider three
    algorithms where the first one focuses on load-store sequences within each
    basic block of code and the other two analyse the existence of load-store
    sequences across basic blocks at the intra-procedural level.  Since the
    dataflow analysis we adopt is a trivial variation of live-variable
    analysis, the algorithms are easily incorporated into a compiler.Through
    detailed simulations of a cache-coherent NUMA architecture using five
    scientific parallel benchmark programs, we find that the algorithms are
    capable of removing over 95% of the separate ownership acquisitions.
    Moreover, we also find that even the simplest algorithm is comparable in
    efficiency with previously proposed hardware-based adaptive cache coherence
    protocols to attack the same problem.", 
  location     = "https://doi.org/10.1145/195470.195572"
}

@Article{ccacpotamuow,
  author       = "Ann Marie Grizzaffi Maynard and Colette~M. Donnelly and Bret~R. Olszewski",
  title        = "Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "145--156",
  month        = nov,
  keywords     = "workloads, cache performance, memory subsystems, operating
    system activity, technical applications, commercial workloads",
  abstract     = "Experience has shown that many widely used benchmarks are
    poor predictors of the performance of systems running commercial
    applications.  Research into this anomaly has long been hampered by a lack
    of address traces from representative multi-user commercial workloads.
    This paper presents research, using traces of industry-standard commercial
    benchmarks, which examines the characteristic differences between technical
    and commercial workloads and illustrates how those differences affect cache
    performance.Commercial and technical environments differ in their
    respective branch behavior, operating system activity, I/O, and dispatching
    characteristics.  A wide range of uniprocessor instruction and data cache
    geometries were studied.  The instruction cache results for commercial
    workloads demonstrate that instruction cache performance can no longer be
    neglected because these workloads have much larger code working sets than
    technical applications.  For database workloads, a breakdown of kernel and
    user behavior reveals that the application component can exhibit behavior
    similar to the operating system and therefore, can experience miss rates
    equally high.  This paper also indicates that “dispatching” or process
    switching characteristics must be considered when designing level-two
    caches.  The data presented shows that increasing the associativity of
    second-level caches can reduce miss rates significantly.  Overall, the
    results of this research should help system designers choose a cache
    configuration that will perform well in commercial markets.", 
  location     = "https://doi.org/10.1145/381792.195524"
}

@Article{acmdildmc,
  author       = "Brian~N. Bershad and Dennis Lee and Theodore~H. Romer and J.~Bradley Chen",
  title        = "Avoiding Conflict Misses Dynamically in Large Direct-Mapped Caches",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "158--170",
  month        = nov,
  keywords     = "cache miss look-aside buffer, page remapping, page coloring,
    adaptive page mapping",
  abstract     = "This paper describes a method for improving the performance
    of a large direct-mapped cache by reducing the number of conflict misses.
    Our solution consists of two components: an inexpensive hardware device
    called a Cache Miss Lookaside (CML) buffer that detects conflicts by
    recording and summarizing a history of cache misses, and a software policy
    within the operating system's virtual memory system that removes conflicts
    by dynamically remapping pages whenever large numbers of conflict misses
    are detected.  Using trace-driven simulation of applications and the
    operating system, we show that a CML buffer enables a large direct-mapped
    cache to perform nearly as well as a two-way set associative cache of
    equivalent size and speed, although with lower hardware cost and
    complexity.", 
  location     = "https://doi.org/10.1145/381792.195527"
}

@Article{sttposwloss,
  author       = "Madhusudhan Talluri and Mark~D. Hill",
  title        = "Surpassing the {TLB} Performance of Superpages with Less Operating System Support",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "171--182",
  month        = nov,
  keywords     = "superpages, subblock tlbs, physical memory allocation, page
    reservation, tlb replacement, tlb performance",
  abstract     = "Many commercial microprocessor architectures have added
    translation lookaside buffer (TLB) support for superpages.  Superpages
    differ from segments because their size must be a power of two multiple of
    the base page size and they must be aligned in both virtual and physical
    address spaces.  Very large superpages (e.g., 1MB) are clearly useful for
    mapping special structures, such as kernel data or frame buffers.  This
    paper considers the architectural and operating system support required to
    exploit medium-sized superpages (e.g., 64KB, i.e., sixteen times a 4KB base
    page size).  First, we show that superpages improve TLB performance only
    after invasive operating system modifications that introduce considerable
    overhead.We then propose two subblock TLB designs as alternate ways to
    improve TLB performance.  Analogous to a subblock cache, a
    complete-subblock TLB associates a tag with a superpage-sized region but
    has valid bits, physical page number, attributes, etc., for each possible
    base page mapping.  A partial-subblock TLB entry is much smaller than a
    complete-subblock TLB entry, because it shares physical page number and
    attribute fields across base page mappings.  A drawback of a
    partial-subblock TLB is that base page mappings can share a TLB entry only
    if they map to consecutive physical pages and have the same attributes.  We
    propose a physical memory allocation algorithm, page reservation, that
    makes this sharing more likely.  When page reservation is used,
    experimental results show partial-subblock TLBs perform better than
    superpage TLBs, while requiring simpler operating system changes.  If
    operating system changes are inappropriate, however, complete-subblock TLBs
    perform best.", 
  location     = "https://doi.org/10.1145/195470.195531",
  location     = "http://www.cs.wisc.edu/~markhill/papers/asplos6_superpages.pdf"
}

@Article{dmdutmcb,
  author       = "David~M. Gallagher and William~Y. Chen and Scott~A. Mahlke and John~C. Gyllenhaal and Wen-mei~W. Hwu",
  title        = "Dynamic Memory Disambiguation Using the Memory Conflict Buffer",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "183--193",
  month        = nov,
  keywords     = "memory conflict buffer, address hashing, speculative
    execution, context switches, conflict correction code",
  abstract     = "To exploit instruction level parallelism, compilers for VLIW
    and superscalar processors often employ static code scheduling.  However,
    the available code reordering may be severely restricted due to ambiguous
    dependences between memory instructions.  This paper introduces a simple
    hardware mechanism, referred to as the memory conflict buffer, which
    facilitates static code scheduling in the presence of memory store/load
    dependences.  Correct program execution is ensured by the memory conflict
    buffer and repair code provided by the compiler.  With this addition,
    significant speedup over an aggressive code scheduling model can be
    achieved for both non-numerical and numerical programs.", 
  location     = "https://doi.org/10.1145/381792.195534",
  location     = "http://impact.crhc.illinois.edu/shared/papers/asplos-94-buffer.pdf"
}

@Article{aasopgiopc,
  author       = "Kenichi Hayashi and Tsunehisa Doi and Takeshi Horie and Yoichi Koyanagi and Osamu Shiraki and Nobutaka Imamura and Toshiyuki Shimizu and Hiroaki Ishihata and Tatsuya Shindo",
  title        = "{AP1000}+: Architectural Support of {PUT/GET} Interface for Parallelizing Compiler",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "196--207",
  month        = nov,
  keywords     = "communication mechanisms, data transfer, vpp fortran,
    interprocessor communication, flag updates",
  abstract     = "The scalability of distributed-memory parallel computers
    makes them attractive candidates for solving large-scale problems.  New
    languages, such as HPF, FortranD, and VPP Fortran, have been developed to
    enable existing software to be easily ported to such machines.  Many
    distributed-memory parallel computers have been built, but none of them
    support the mechanisms required by such languages.  We studied the
    mechanisms required by parallelizing compilers and proposed a new
    architecture to support them.  Based on this proposed architecture, we
    developed a new distributed-memory parallel computer, the AP1000+, which is
    an enhanced version of the AP1000.  Using scientific applications in VPP
    Fortran and C, such as NAS parallel benchmarks, we simulated the
    performance of the AP1000+.", 
  location     = "https://doi.org/10.1145/381792.195538"
}

@Article{lmssfpli,
  author       = "James~R. Larus and Brad Richards and Guhan Viswanathan",
  title        = "{LCM}:  Memory System Support for Parallel Language Implementation",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "208--218",
  month        = nov,
  keywords     = "recoverable shared memory, c**, parallel function semantics,
    reductions, data-race detection, false sharing",
  abstract     = "Higher-level parallel programming languages can be difficult
    to implement efficiently on parallel machines.  This paper shows how a
    flexible, compiler-controlled memory system can help achieve good
    performance for language constructs that previously appeared too costly to
    be practical.Our compiler-controlled memory system is called Loosely
    Coherent Memory (LCM).  It is an example of a larger class of Reconcilable
    Shared Memory (RSM) systems, which generalize the replication and merge
    policies of cache-coherent shared-memory.  RSM protocols differ in the
    action taken by a processor in response to a request for a location and the
    way in which a processor reconciles multiple outstanding copies of a
    location.  LCM memory becomes temporarily inconsistent to implement the
    semantics of C** parallel functions efficiently.  RSM provides a compiler
    with control over memory-system policies, which it can use to implement a
    language's semantics, improve performance, or detect errors.  We illustrate
    the first two points with LCM and our compiler for the data-parallel
    language C**.", 
  location     = "https://doi.org/10.1145/195473.195545"
}

@Article{raiahcrm,
  author       = "Michael Upton and Thomas Huff and Trevor Mudge and Richard Brown",
  title        = "Resource Allocation in a High Clock Rate Microprocessor",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "98--109",
  month        = nov,
  keywords     = "pipelining, decoupled architecture, prefetching, non-blocking
    cache, resource allocation, superscalar, floating point latencies",
  abstract     = "This paper discusses the design of a high clock rate (300MHz)
    processor.  The architecture is described, and the goals for the design are
    explained.  The performance of three processor models is evaluated using
    trace-driven simulation.  A cost model is used to estimate the resources
    required to build processors with varying sizes of on-chip memories, in
    both single and dual issue models.  Recommendations are then made to
    increase the effectiveness of each of the models.", 
  location     = "https://doi.org/10.1145/381792.195510"
}

@Article{hassfeeh,
  author       = "Chandramohan~A. Thekkath and Henry~M. Levy",
  title        = "Hardware and Software Support for Efficient Execption Handling",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "110--119",
  month        = nov,
  keywords     = "user-level exceptions, fast exception handling",
  abstract     = "Program-synchronous exceptions, for example, breakpoints,
    watchpoints, illegal opcodes, and memory access violations, provide
    information about exceptional conditions, interrupting the program and
    vectoring to an operating system handler.  Over the last decade, however,
    programs and run-time systems have increasingly employed these mechanisms
    as a performance optimization to detect normal and expected conditions.
    Unfortunately, current architecture and operating system structures are
    designed for exceptional or erroneous conditions, where performance is of
    secondary importance, rather than normal conditions.  Consequently, this
    has limited the practicality of such hardware-based detection mechanisms.We
    propose both hardware and software structures that permit efficient
    handling of synchronous exceptions by user-level code.  We demonstrate a
    software implementation that reduces exception-delivery cost by an
    order-of-magnitude on current RISC processors, and show the performance
    benefits of that mechanism for several example applications.", 
  location     = "https://doi.org/10.1145/381792.195515"
}

@Article{atfmrtdoaosaameua,
  author       = "Pramod~V. Argade and David~K. Charles and Craig Taylor",
  title        = "{A} Technique for Monitoring Run-Time Dynamics of an Operating System and a Microprocessor Executing User Applications",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "122--131",
  month        = nov,
  keywords     = "trace-based simulations, operating system tuning, system
    design, instruction set analysis, run-time analysis",
  abstract     = "In this paper, we present a non-invasive and efficient
    technique for simulating applications complete with their operating system
    interaction.  The technique involves booting and initiating an application
    on a hardware development system, capturing the entire state of the
    application and the microprocessor at a well defined point in execution and
    then simulating the application on microprocessor simulators.  Extensive
    statistics generated from the simulators on run-time dynamics of the
    application, the operating system as well as the microprocessor enabled us
    to tune the operating system and the microprocessor architecture and
    implementation.  The results also enabled us to optimize system level
    design choices by anticipating/predicting the performance of the target
    system.  Lastly, the results were used to adjust and refocus the evolution
    of the architecture of both the operating system and the microprocessor.", 
  location     = "https://doi.org/10.1145/381792.195518"
}

@Article{tdswti,
  author       = "Richard Uhlig and David Nagle and Trevor Mudge and Stuart Sechrest",
  title        = "Trap-Driven Simulation with {Tapeworm II}",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "132--144",
  month        = nov,
  keywords     = "workloads, measurement variations, measurement bias,
    portability",
  abstract     = "Tapeworm II is a software-based simulation tool that
    evaluates the cache and TLB performance of multiple-task and operating
    system intensive workloads.  Tapeworm resides in an OS kernel and causes a
    host machine's hardware to drive simulations with kernel traps instead of
    with address traces, as is conventionally done.  This allows Tapeworm to
    quickly and accurately capture complete memory referencing behavior with a
    limited degradation in overall system performance.  This paper compares
    trap-driven simulation, as implemented in Tapeworm, with the more common
    technique of trace-driven memory simulation with respect to speed,
    accuracy, portability and flexibility.", 
  location     = "https://dl.acm.org/doi/10.1145/195473.195521"
}

@Article{sdactidos,
  author       = "Chandramohan~A. Thekkath and Henry~M. Levy and Edward~D. Lazowska",
  title        = "Separating Data and Control Transfer in Distributed Operating Systems",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "2--11",
  month        = nov,
  keywords     = "distributed storage, ipc, atm networks, data-control
    separation, client-server models, communication co-processors",
  abstract     = "Advances in processor architecture and technology have
    resulted in workstations in the 100+ MIPS range.  As well, newer local-area
    networks such as ATM promise a ten- to hundred-fold increase in throughput,
    much reduced latency, greater scalability, and greatly increased
    reliability, when compared to current LANs such as Ethernet.We believe that
    these new network and processor technologies will permit tighter coupling
    of distributed systems at the hardware level, and that distributed systems
    software should be designed to benefit from that tighter coupling.  In this
    paper, we propose an alternative way of structuring distributed systems
    that takes advantage of a communication model based on remote network
    access (reads and writes) to protected memory segments.A key feature of the
    new structure, directly supported by the communication model, is the
    separation of data transfer and control transfer.  This is in contrast to
    the structure of traditional distributed systems, which are typically
    organized using message passing or remote procedure call (RPC).  In
    RPC-style systems, data and control are inextricably linked—all RPCs must
    transfer both data and control, even if the control transfer is
    unnecessary.We have implemented our model on DECstation hardware connected
    by an ATM network.  We demonstrate how separating data transfer and control
    transfer can eliminate unnecessary control transfers and facilitate tighter
    coupling of the client and server.  This has the potential to increase
    performance and reduce server load, which supports scaling in the face of
    an increasing number of clients.  For example, for a small set of file
    server operations, our analysis shows a 50% decrease in server load when we
    switched from a communications mechanism requiring both control transfer
    and data transfer, to an alternative structure based on pure data
    transfer.", 
  location     = "https://doi.org/10.1145/381792.195481"
}

@Article{sapmfmcs,
  author       = "Rohit Chandra and Scott Devine and Ben Verghese and Anoop Gupta and Mendel Rosenblum",
  title        = "Scheduling and Page Migration for Multiprocessor Compute Servers",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "12--24",
  month        = nov,
  keywords     = "cc-numa systems, page migration, scheduling policies,
    affinity scheduling, gang scheduling, processor sets, activations, dash,
    sequential workloads, performance, data locality, synchronization",
  abstract     = "Several cache-coherent shared-memory multiprocessors have
    been developed that are scalable and offer a very tight coupling between
    the processing resources.  They are therefore quite attractive for use as
    compute servers for multiprogramming and parallel application workloads.
    Process scheduling and memory management, however, remain challenging due
    to the distributed main memory found on such machines.  This paper examines
    the effects of OS scheduling and page migration policies on the performance
    of such compute servers.  Our experiments are done on the Stanford DASH, a
    distributed-memory cache-coherent multiprocessor.  We show that for our
    multiprogramming workloads consisting of sequential jobs, the traditional
    Unix scheduling policy does very poorly.  In contrast, a policy
    incorporating cluster and cache affinity along with a simple page-migration
    algorithm offers up to two-fold performance improvement.  For our workloads
    consisting of multiple parallel applications, we compare space-sharing
    policies that divide the processors among the applications to time-slicing
    policies such as standard Unix or gang scheduling.  We show that
    space-sharing policies can achieve better processor utilization due to the
    operating point effect, but time-slicing policies benefit strongly from
    user-level data distribution.  Our initial experience with automatic page
    migration suggests that policies based only on TLB miss information can be
    quite effective, and useful for addressing the data distribution problems
    of space-sharing schedulers.", 
  location     = "https://doi.org/10.1145/195473.195485"
}

@Article{rsafm,
  author       = "Beng-Hong Lim and Anant Agarwal",
  title        = "Reactive Synchronization Algorithms for Multiprocessors",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "25--35",
  month        = nov,
  keywords     = "synchronization, spin locks, test & set, combining trees,
    adaptive algorithms, consensus objects, multiprocessor systems",
  abstract     = "Synchronization algorithms that are efficient across a wide
    range of applications and operating conditions are hard to design because
    their performance depends on unpredictable run-time factors.  The designer
    of a synchronization algorithm has a choice of protocols to use for
    implementing the synchronization operation.  For example, candidate
    protocols for locks include test-and-set protocols and queueing protocols.
    Frequently, the best choice of protocols depends on the level of
    contention: previous research has shown that test-and-set protocols for
    locks outperform queueing protocols at low contention, while the opposite
    is true at high contention.This paper investigates reactive synchronization
    algorithms that dynamically choose protocols in response to the level of
    contention.  We describe reactive algorithms for spin locks and
    fetch-and-op that choose among several shared-memory and message-passing
    protocols.  Dynamically choosing protocols presents a challenge: a reactive
    algorithm needs to select and change protocols efficiently, and has to
    allow for the possibility that multiple processes may be executing
    different protocols at the same time.  We describe the notion of consensus
    objects that the reactive algorithms use to preserve correctness in the
    face of dynamic protocol changes.Experimental measurements demonstrate that
    reactive algorithms perform close to the best static choice of protocols at
    all levels of contention.  Furthermore, with mixed levels of contention,
    reactive algorithms outperform passive algorithms with fixed protocols,
    provided that contention levels do not change too frequently.  Measurements
    of several parallel applications show that reactive algorithms result in
    modest performance gains for spin locks and significant gains for
    fetch-and-op.", 
  location     = "https://doi.org/10.1145/381792.195490"
}

@Article{iompasmitsfm,
  author       = "John Heinlein and Kourosh Gharachorloo and Scott Dresser and Anoop Gupta",
  title        = "Integration of Message Passing and Shared Memory in the Stanford {FLASH} Multiprocessor",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "38--50",
  month        = nov,
  keywords     = "shared-store multiprocessor systems, message passing,
    programmable communication co-processor, virtual memory, software tlb,
    cache coherence",
  abstract     = "The advantages of using message passing over shared memory
    for certain types of communication and synchronization have provided an
    incentive to integrate both models within a single architecture.  A key
    goal of the FLASH (FLexible Architecture for SHared memory) project at
    Stanford is to achieve this integration while maintaining a simple and
    efficient design.  This paper presents the hardware and software mechanisms
    in FLASH to support various message passing protocols.  We achieve low
    overhead message passing by delegating protocol functionality to the
    programmable node controllers in FLASH and by providing direct user-level
    access to this messaging subsystem.  In contrast to most earlier work, we
    provide an integrated solution that handles the interaction of the
    messaging protocols with virtual memory, protected multiprogramming, and
    cache coherence.  Detailed simulation studies indicate that this system can
    sustain message-transfers rates of several hundred megabytes per second,
    effectively utilizing projected network bandwidths for next generation
    multiprocessors.", 
  location     = "https://doi.org/10.1145/381792.195494"
}

@Article{soimlwdttg,
  author       = "Vijay Karamcheti and Andrew~A. Chien",
  title        = "Software Overhead in Messaging Layers:  Where Does the Time Go?",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "51--60",
  month        = nov,
  keywords     = "communication services, protocol layering, reliability,
    sequencing, who does what, ",
  abstract     = "Despite improvements in network interfaces and software
    messaging layers, software communication overhead still dominates the
    hardware routing cost in most systems.  In this study, we identify the
    sources of this overhead by analyzing software costs of typical
    communication protocols built atop the active messages layer on the CM-5.
    We show that up to 50–70% of the software messaging costs are a direct
    consequence of the gap between specific network features such as arbitrary
    delivery order, finite buffering, and limited fault-handling, and the user
    communication requirements of in-order delivery, end-to-end flow control,
    and reliable transmission.  However, virtually all of these costs can be
    eliminated if routing networks provide higher-level services such as
    in-order delivery, end-to-end flow control, and packet-level
    fault-tolerance.  We conclude that significant cost reductions require
    changing the constraints on messaging layers: we propose designing networks
    and network interfaces which simplify or replace software for implementing
    user communication requirements.", 
  location     = "https://doi.org/10.1145/381792.195499"
}

@Article{witsimpasmp,
  author       = "Satish Chandra and James~R. Larus and Anne Rogers",
  title        = "Where is Time Spend in Message-Passing and Shared-Memory Programs?",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "61--73",
  month        = nov,
  keywords     = "message passing, shared memory, performance, parallel
    programming", 
  abstract     = "Message passing and shared memory are two techniques parallel
    programs use for coordination and communication.  This paper studies the
    strengths and weaknesses of these two mechanisms by comparing equivalent,
    well-written message-passing and shared-memory programs running on similar
    hardware.  To ensure that our measurements are comparable, we produced two
    carefully tuned versions of each program and measured them on
    closely-related simulators of a message-passing and a shared-memory
    machine, both of which are based on same underlying hardware assumptions.We
    examined the behavior and performance of each program carefully.  Although
    the cost of computation in each pair of programs was similar,
    synchronization and communication differed greatly.  We found that
    message-passing's advantage over shared-memory is not clear-cut.  Three of
    the four shared-memory programs ran at roughly the same speed as their
    message-passing equivalent, even though their communication patterns were
    different.", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-463-94", 
  location     = "https://doi.org/10.1145/381792.195501"
}

@Article{poahar,
  author       = "William~J. Schmidt and Kelvin~D. Nilsen",
  title        = "Performance of a Hardware-Assisted Real-Time Garbage Collector",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "76--85",
  month        = nov,
  keywords     = "garbage collection, co-processors, baker's collector, heap
    storage, dynamic storage management",
  abstract     = "Hardware-assisted real-time garbage collection offers high
    throughput and small worst-case bounds on the times required to allocate
    dynamic objects and to access the memory contained within previously
    allocated objects.  Whether the proposed technology is cost effective
    depends on various choices between configuration alternatives.  This paper
    reports the performance of several different configurations of the
    hardware-assisted real-time garbage collection system subjected to several
    different workloads.  Reported measurements demonstrate that
    hardware-assisted real-time garbage collection is a viable alternative to
    traditional explicit memory management techniques, even for low-level
    languages like C++.", 
  location     = "https://doi.org/10.1145/381792.195504"
}

@Article{eanvmmss,
  author       = "Michael Wu and Willy Zwaenepoel",
  title        = "{eNVy}:  {A} Non-Volatile, Main Memory Storage System",
  journal      = asplos94,
  year         = 1994,
  volume       = 29,
  number       = 11,
  pages        = "86--97",
  month        = nov,
  keywords     = "flash storage, sram, write buffering, write performance, page
    remapping, cleaning, locality",
  abstract     = "This paper describes the architecture of eNVy, a large
    non-volatile main memory storage system built primarily with Flash memory.
    eNVy presents its storage space as a linear, memory mapped array rather
    than as an emulated disk in order to provide an efficient and easy to use
    software interface.Flash memories provide persistent storage with
    solid-state memory access times at a lower cost than other solid-state
    technologies.  However, they have a number of drawbacks.  Flash chips are
    write-once, bulk-erase devices whose contents cannot be updated in-place.
    They also suffer from slow program times and a limit on the number of
    program/erase cycles.  eNVy uses a copy-on-write scheme, page remapping, a
    small amount of battery backed SRAM, and high bandwidth parallel data
    transfers to provide low latency, in-place update semantics.  A cleaning
    algorithm optimized for large Flash arrays is used to reclaim space.  The
    algorithm is designed to evenly wear the array, thereby extending its
    lifetime.Software simulations of a 2 gigabyte eNVy system show that it can
    support I/O rates corresponding to approximately 30,000 transactions per
    second on the TPC-A database benchmark.  Despite the added work done to
    overcome the deficiencies associated with Flash memories, average latencies
    to the storage system are as low as 180ns for reads and 200ns for writes.
    The estimated lifetime of this type of storage system is in the 10 year
    range when exposed to a workload of 10,000 transactions per second.", 
  location     = "https://doi.org/10.1145/195470.195506", 
  location     = "https://scholarship.rice.edu/handle/1911/17039"
}

@Article{patpcanrfsm,
  author       = "Robert~A. Kahn",
  title        = "Public Access to Personal Computing: {A} New Role for Science Museums",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 4,
  pages        = "56--66",
  month        = apr,
  keywords     = "public access, science museums, education, taxonomy",
  location     = "https://doi.org/10.1109/C-M.1977.217714"
}

@Article{pdm,
  author       = "Alan Kay and Adele Goldberg",
  title        = "Personal Dynamic Media",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 3,
  pages        = "31--44",
  month        = apr,
  keywords     = "dynabook, smalltalk",
  location     = "https://doi.org/10.1109/C-M.1977.217672"
}

@Article{miaac,
  author       = "Ware Myers",
  title        = "Microprocessors in Automation and Communications",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 12,
  pages        = "",
  month        = dec,
  keywords     = "japan, industrial policy, microprocessor development",
  location     = "https://dl.acm.org/doi/10.5555/1300779.1302407"
}

@Article{tmitbl,
  author       = "Robert~L. Schoenfeld and William~A. Kocsis and Norman Milkman and Gordon Silverman",
  title        = "The Microprocessor in the Biological Laboratory",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 5,
  pages        = "56--67",
  month        = may,
  keywords     = "laboratory systems, biological research, microprocessor
    systems, data collection",
  location     = "https://dl.acm.org/doi/10.5555/1300742.1301505"
}

@Article{fdoms,
  author       = "Vason~P. Srini",
  title        = "Fault Diagnosis of Microprocessor Systems",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 1,
  pages        = "60--65",
  month        = jan,
  keywords     = "diagnostic systems, resident diagnostics, diagnostic supervisors",
  location     = "https://doi.org/10.1109/C-M.1977.217500"
}

@Article{aimds,
  author       = "Larry Krummel and Gaymond Schultz",
  title        = "Advances in Microcomputer Development Systems",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 2,
  pages        = "13--19",
  month        = feb,
  keywords     = "system design, software development, in-circuit emulation",
  location     = "https://doi.org/10.1109/C-M.1977.217641"
}

@Article{auatmsd,
  author       = "Tomlinson~G. Rauscher",
  title        = "{A} Unified Approach to Microcomputer Software Development",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 6,
  pages        = "44-54",
  month        = jun,
  keywords     = "software development, requirements, design, development,
    testing, maintenance",
  location     = "https://doi.org/10.1109/C-M.1978.218226"
}

@Article{mpipm,
  author       = "William Brown",
  title        = "Modular Programming in {PL/M}",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 3,
  pages        = "40--46",
  month        = mar,
  keywords     = "modularity, pl/m, interfaces, implementation",
  location     = "https://doi.org/10.1109/C-M.1978.218095"
}

@Article{msdimp,
  author       = "Celeste~S. Magers",
  title        = "Managing Software Development in Microprocessor Projects",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 6,
  pages        = "34--42",
  month        = jun,
  keywords     = "software development, programmers, productivity, projects,
    development tools, ",
  location     = "https://dblp.org/rec/journals/computer/Magers78"
}

@Article{mmafbsm,
  author       = "V.~Michael Powers and Jos{\' e}~H. Hernandez",
  title        = "Microsystems Microprogram Assemblers for Bit Slice Microprocessors",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 7,
  pages        = "108--120",
  month        = jul,
  keywords     = "microprogramming, bit-sliced microprocessors, assemblers",
  location     = "https://doi.org/10.1109/C-M.1978.218277"
}

@Article{maamblimsd,
  author       = "Harvey~A. Cohen and Rhys~s. Francis",
  title        = "Macro-Assemblers and Macro-Based Languages in Microprocessor Software Development",
  journal      = ieeec,
  year         = 1979,
  volume       = 12,
  number       = 2,
  pages        = "53--64",
  month        = feb,
  keywords     = "assemblers, macros, cross assembly, language design, dsls,
    modularity, universal cross-assemblers",
  location     = "https://doi.org/10.1109/MC.1979.1658618"
}

@Article{pahcao,
  author       = "Jim Warren",
  title        = "Personal and Hobby Computing: An Overview",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 3,
  pages        = "10--22",
  month        = mar,
  keywords     = "personal computing, hobbyist computing, social organizations,
    hardware, software", 
  location     = "https://doi.org/10.1109/C-M.1977.217667"
}

@Article{pcpi,
  author       = "Portia Isaacson and Robert~C. Gammill and Richard~S. Heiser and Adam Osborne and Larry Tesler and Jim~C. Warren",
  title        = "Personal Computing",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 9,
  pages        = "86--97",
  month        = sep,
  keywords     = "consumer computers, hardware technologies, social impacts",
  location     = "https://dl.acm.org/doi/abs/10.1145/1041571.1041576"
}

@Article{ti8ma1beot8,
  author       = "Stephen~P. Morse and William~B. Pohlman and Bruce~W. Ravenel",
  title        = "The {Intel} 8086 Microprocessor:  {A} 16-bit Evolution of the 8080",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 6,
  pages        = "18--27",
  month        = jun,
  keywords     = "instruction sets, storage spaces, segmentation, cpu architecture",
  abstract     = "",
  location     = "https://dl.acm.org/doi/10.1109/C-M.1978.218219"
}

@Article{tim4mfac,
  author       = "John~F. Wakerly",
  title        = "The {Intel} {MCS}-48 Microcomputer Family: {A} Critique",
  journal      = ieeec,
  year         = 1979,
  volume       = 12,
  number       = 2,
  pages        = "22--31",
  month        = feb,
  keywords     = "cpu architecture, program store, program control, instruction
    sets, system design",
  abstract     = "",
  location     = "https://dl.acm.org/doi/10.1109/MC.1979.1658613"
}

@Article{aoanm,
  author       = "Bernard~L. Peuto",
  title        = "Architecture of a New Microprocessor",
  journal      = ieeec,
  year         = 1979,
  volume       = 12,
  number       = 2,
  pages        = "10--21",
  month        = feb,
  keywords     = "z8000, system architectures, registers, instruction sets,
    addressing modes",
  abstract     = "",
  location     = "https://dl.acm.org/doi/abs/10.1109/MC.1979.1658612"
}

@Article{cotfm,
  author       = "Dave Caulkins",
  title        = "Critique of the {F8} Microprocessor",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 8,
  pages        = "83--86",
  month        = aug,
  keywords     = "cpu architecture, multi-chip architectures, instruction sets",
  abstract     = "The Fairchild F8 is a control microprocessor whose
    architecture is considerably different from most machines in the same price
    and performance class.  Good computer architecture is consistent,
    symmetrical, and coherent; the programmer is provided with the maximum
    amount of information possible after each operation and his freedom of
    action is limited as little as possible.  A machine with these qualities
    behaves in the way one expects it to behave; it is free of special cases
    and peculiar quirks.  The F8 falls considerably short of meeting these
    goals.  It predates most equivalent micros; its designers seem to have had
    little contact with others doing similar things.  This background has
    resulted in a machine combining brilliant design concepts with ugly
    flaws.", 
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217830"
}

@Article{amafacwtm6,
  author       = "Edward Stritter and Tom Gunter",
  title        = "A Microprocessor Architecture for A Changing World: The {Motorola} 68000",
  journal      = ieeec,
  year         = 1979,
  volume       = 12,
  number       = 2,
  pages        = "43--52",
  month        = feb,
  keywords     = "microprocessor architectures, instruction sets",
  abstract     = "The first implementation of a new microprocessor architecture
    promises to narrow the gap between the power of very small and very large
    computers.", 
  location     = "https://dl.acm.org/doi/10.1109/MC.1979.1658617"
}

@Article{mnwls,
  author       = "William~L. Spetz",
  title        = "Microprocessor Networks",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 7,
  pages        = "64--70",
  month        = jul,
  keywords     = "multiprocessor systems, hierarchical networks, ring networks,
    star networks, distributed hardware",
  abstract     = "The phenomenal growth of the microprocessor business over the
    last two years has been well covered in the technical literature.
    Extrapolating from this growth, typical predictions of future trends have
    suggested that by 1980, microprocessor complexity will increase five to ten
    times while the speed of the chip will triple.1It seems reasonable to
    expect this performance at approximately today's microprocessor prices.  If
    instruction set efficiency is assumed to show a direct correspondence with
    chip complexity, these chips can be expected to be 15 to 30 times as
    capable in a given application.",
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217786"
}

@Article{icfmms,
  author       = "Paul~M. Russo",
  title        = "Interprocessor Communication for Multi-Microcomputer System",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 4,
  pages        = "67--76",
  month        = apr,
  keywords     = "multiprocessor systems, multi-microcomputer structures,
    cosmac microprocessor",
  location     = "https://doi.org/10.1109/C-M.1977.217786"
}

@Article{bsma,
  author       = "Nikitas~A. Alexandridis",
  title        = "Bit-Sliced Microprocessor Architecture",
  journal      = ieeec,
  year         = 1978,
  volume       = 11,
  number       = 6,
  pages        = "56--80",
  month        = jun,
  keywords     = "bit-sliced architecture, microprogramming, custom design",
  abstract     = "With the high speed of bipolar technologoy and the
    flexibility of user-microprogrammability, bit-sliced microprocessors are
    finding a place in critical applications.",
  location     = "https://dl.acm.org/doi/10.1109/C-M.1978.218226"
}

@Article{ssfs1bid,
  author       = "Kells~A. Elmquist and Howard Fullmer and David~B. Gustavson and George Morrow",
  title        = "Standard Specification for {S}-100 Bus Interface Devices",
  journal      = ieeec,
  year         = 1979,
  volume       = 12,
  number       = 7,
  pages        = "28--52",
  month        = jul,
  keywords     = "bus interfaces, standards, addressing, arbitration,
    interrupts, dma",
  abstract     = "This proposed standard eliminates many of the problems in the
    S-100 bus and upgrades it for 16-bit microprocessors.  It is offered here
    for public comment before submission to the IEEE Standards Board.",
  location     = "https://dl.acm.org/doi/10.1109/MC.1979.1658813"
}

@Article{teopdims,
  author       = "Ken McKenzie",
  title        = "The Evolution of Peripheral Devices in Microprocessor Systems",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 6,
  pages        = "12--17",
  month        = jun,
  keywords     = "system development, performance, user programming, multi-busses",
  abstract     = "LSI I/O controllers solve system problems by efficiently
    interfacing the microcomputer to external units"
}

@Article{ciitaom,
  author       = "Bernard~L. Peuto and Leonard~J. Shustek",
  title        = "Current Issues in the Architecture of Microprocessors",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 2,
  pages        = "13--19",
  month        = feb,
  keywords     = "technological constraints, architectures, instruction metrics",
  abstract     = "Despite the fact that microcomputers have existed
    commercially for only five years, microcomputer architecture is not an
    entirely new field.  It is, rather, the application of the general
    principles of computer architecture to microcomputers.  In 'Planning a
    Computer System,' Frederick P. Brooks Jr. defines computer architecture
    as being, like other architecture, 'the art of determining the needs of the
    user of the structure and then designing to meet those needs as effectively
    as possible within economic and technological constraints.", 
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217642"
}

@Article{misoa,
  author       = "John~F. Wakerly",
  title        = "Microprocessor Input\slash Output Architecture",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 2,
  pages        = "26--33",
  month        = feb,
  keywords     = "organization, system bus, addressing, interrupts, dma",
  abstract     = "There are at least as many different microprocessor
    input/output organizations, circuit configurations, and device types as
    there are microprocessor families and manufacturers.  Due to the diversity
    of competing approaches and devices, their similarities and differences are
    not always evident.  The fact that each manufacturer describes his
    approaches and devices differently, and the relative scarcity of formal
    text material on microprocessors, do not make the situation any better.  In
    this short space there is no hope of comprehensively surveying all
    available approaches and devices (nor would I want to, even if space were
    available!).  On the other hand, I can present an overview of basic
    microprocessor I/O organization and typical circuit configurations and
    devices.  Using these concepts as a basis, one may be able to make
    comparisons between current and future approaches available commercially.", 
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217643"
}

@Article{dcpaktp,
  author       = "Mark {Shepherd, Jr.}",
  title        = "Distributed Computing Power:  {A} Key to Productivity",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 11,
  pages        = "66--74",
  month        = nov,
  keywords     = "application fields, programming bottleneck, hard-wired programs",
  abstract     = "Productivity is the driving force of the economic growth that
    leads to improved living standards.  However, if the United States
    continues the past pattern of 2% productivity growth per year, the gross
    national product per capita will grow at only 2.5% per year and
    unemployment will still be at 5.7% in 1990.  On the other hand, a single
    percentage point increase in productivity–from 2% to 3%–could accelerate
    economic growth sufficiently to drive unemployment down to 3.4% by 1990;
    two percentage points could drive it down to 1%, if other factors do not
    intrude.", 
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217568"
}

@Article{adpfma,
  author       = "Dennis~R. Allison",
  title        = "{A} Design Philosophy for Microcomputer Architectures",
  journal      = ieeec,
  year         = 1977,
  volume       = 10,
  number       = 2,
  pages        = "35--41",
  month        = feb,
  keywords     = "computer design, abstract machines, programming languages, compilers",
  abstract     = "Microcomputers, as a class of programmable machines, are
    uncomfortable and difficult to program, and are even more unpleasant as the
    target machines for programming languages.  This need not be so.",
  location     = "https://dl.acm.org/doi/10.1109/C-M.1977.217645"
}

@Article{frtms,
  author       = "Robert~N. Noyce",
  title        = "From Relays to {MPU}'s",
  journal      = ieeec,
  year         = 1976,
  volume       = 9,
  number       = 12,
  pages        = "26--29",
  month        = dec,
  keywords     = "costs, performance, technological trends, reliability",
  abstract     = "When the IEEE Computer Society was founded twentyfive years
    ago, the transistor was a laboratory curiosity, and operating computers
    were assembled from relays or vacuum tubes.  Today, a single integrated
    circuit far surpasses the capability of those early computers, and further
    progress seems inevitable.  The development of semiconductor devices has
    depended upon a synergism with computers.  This is particularly true for
    integrated circuits, whose development was motivated by the computer
    applications.  With each advance in components, the computers resulting
    from their use reached a wider market, motivating further advances in the
    semiconductor technology.", 
  location     = "https://dl.acm.org/doi/10.1109/C-M.1976.218465"
}

@Article{tvdl,
  author       = "Peter Wegner",
  title        = "The {Vienna Definition Language}",
  journal      = surveys,
  year         = 1972,
  volume       = 4,
  number       = 1,
  pages        = "5--63",
  month        = mar,
  keywords     = "metalanguages, language definition, interpreters, semantics,
    program proofs, interpreter equivalence, operational semantics",
  abstract     = "The Vienna Definition Language (VDL) is a programming
    language for defining programming languages.  It describes precisely the
    execution of the set of all programs in a programming language.  However,
    the Vienna Definition Language is important not only as one definition
    techniques among many others but as an illustration of a new information
    structure-oriented approach to programming language study .  The paper may
    be regarded as a case study in the information structure modeling of
    programming languages, as well as an introduction to a specific modeling
    technique.  Part 1 includes a brief review and comparison of techniques of
    language definition, and then introduces the basic data structures and data
    structure manipulation operations used to define programming languages.
    Part 2 considers the definition of sets of data structures.  Part 3
    introduces the notion for specifying VDL instructions, and illustrates the
    definition of a simple language for arithmetic expression evaluation by
    giving the complete sequence of states (snapshots) arising during
    expression evaluation.  Part 4 defines a single block structured language
    in VDL and considers certain basic design issues for such languages more
    generally.  Part 5 introduces an alternative definition of block structure
    language defined in Part 4, proves the equivalence of the two definitions,
    and briefly reviews recent work on proving the interpreter equivalence.  An
    Appendix considers the definition of VDL in VDL.  This paper may be read at
    a number of different levels.  The reader interested in a quick
    introduction to VDL's basic definition techniques need only read section
    1.3 and Part 3.  The reader interested in a deeper understanding of block
    structure languages should read Part 4, while the reader concerned with
    proofs of equivalence of interpreters should read Part 5.", 
  location     = "https://dl.acm.org/doi/10.1145/356596.356598"
}

@Article{ossfidlocncs,
  author       = "Ben Verghese and Scott Devine and Anoop Gupta and Mendel Rosenblum",
  title        = "Operating System Support for Improving Data Locality on {CC}-{NUMA} Compute Servers",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "279--289",
  month        = sep,
  keywords     = "kernel migration, kernel replication, network latency",
  abstract     = "The dominant architecture for the next generation of
    shared-memory multiprocessors is CC-NUMA (cache-coherent non-uniform memory
    architecture).  These machines are attractive as compute servers because
    they provide transparent access to local and remote memory.  However, the
    access latency to remote memory is 3 to 5 times the latency to local
    memory.  CC-NOW machines provide the benefits of cache coherence to
    networks of workstations, at the cost of even higher remote access latency.
    Given the large remote access latencies of these architectures, data
    locality is potentially the most important performance issue.  Using
    realistic workloads, we study the performance improvements provided by OS
    supported dynamic page migration and replication.  Analyzing our
    kernel-based implementation, we provide a detailed breakdown of the costs.
    We show that sampling of cache misses can be used to reduce cost without
    compromising performance, and that TLB misses may not be a consistent
    approximation for cache misses.  Finally, our experiments show that dynamic
    page migration and replication can substantially increase application
    performance, as much as 30%, and reduce contention for resources in the
    NUMA memory system.", 
  location     = ""
}

@Article{icpwbtadp,
  author       = "Jih-Kwon Peir and Windsor~W. Hsu and Honesty Young and Shauchi Ong",
  title        = "Improving Cache Performance with Balanced Tag and Data Paths",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "268--278",
  month        = sep,
  keywords     = "caching, path balancing, parallelism",
  abstract     = "There are two concurrent paths in a typical cache access ---
    one through the data array and the other through the tag array.  The path
    through the data array drives the selected set out of the array.  The path
    through the tag array determines cache hit/miss and, for set-associative
    caches, selects the appropriate line from within the selected set.  In both
    direct-mapped and set-associative caches, the path through the tag array is
    significantly longer than that through the data array.  In this paper, we
    propose a path balancing technique help match the delays of the tag and
    data paths.  The basic idea behind this technique is to employ a separate
    subset of the tag array to decouple the one-to-one relationship between
    address tags and cache lines so as to achieve a design that provides higher
    performance.  Performance evaluation using both TPC-C and SPEC92 benchmarks
    shows that this path balancing technique offers impressive improvements in
    overall system performance over conventional cache designs.  For TPC-C,
    improvements in the range of 6% to 28% are possible.", 
  location     = "https://doi.org/10.1145/237090.237202"
}

@Article{rnlusiagme,
  author       = "Hervé~A. Jamrozik and Michael~J. Feeley and Geoffrey~M. Voelker and James Evans and Anna~R. Karlin and Henry~M. Levy and Mary~K. Vernon",
  title        = "Reducing Network Latency Using Subpages in an Global Memory Environment",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "258--267",
  month        = sep,
  keywords     = "subpages, trace-driven simulations, eager fetches, lazy
    fetches, pipeline fetches",
  abstract     = "New high-speed networks greatly encourage the use of network
    memory as a cache for virtual memory and file pages, thereby reducing the
    need for disk access.  Because pages are the fundamental transfer and
    access units in remote memory systems, page size is a key performance
    factor.  Recently, page sizes of modern processors have been increasing in
    order to provide more TLB coverage and amortize disk access costs.
    Unfortunately, for high-speed networks, small transfers are needed to
    provide low latency.  This trend in page size is thus at odds with the use
    of network memory on high-speed networks.This paper studies the use of
    subpages as a means of reducing transfer size and latency in a
    remote-memory environment.  Using trace-driven simulation, we show how and
    why subpages reduce latency and improve performance of programs using
    network memory.  Our results show that memory-intensive applications
    execute up to 1.8 times faster when executing with 1K-byte subpages, when
    compared to the same applications using full 8K-byte pages in the global
    memory system.  Those same applications using 1K-byte subpages execute up
    to 4 times faster than they would using the disk for backing store.  Using
    a prototype implementation on the DEC Alpha and AN2 network, we demonstrate
    how subpages can reduce remote-memory fault time; e.g., our prototype is
    able to satisfy a fault on a 1K subpage stored in remote memory in 0.5
    milliseconds, one third the time of a full page.", 
  location     = "https://doi.org/10.1145/237090.237198"
}

@Article{cdpcfm,
  author       = "Edouard Bugnion and Jennifer~M. Anderson and Todd~C. Mowry and Mendel Rosenblum and Monica~S. Lam",
  title        = "Compiler-Directed Page Coloring for Multiprocessors",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "244--255",
  month        = sep,
  keywords     = "paging, cache conflicts, adaptive algorithms, access
    patterns, compile-time analysis, page coloring, prefetching",
  abstract     = "This paper presents a new technique, compiler-directed page
    coloring, that eliminates conflict misses in multiprocessor applications.
    It enables applications to make better use of the increased aggregate cache
    size available in a multiprocessor.  This technique uses the compiler's
    knowledge of the access patterns of the parallelized applications to direct
    the operating system's virtual memory page mapping strategy.  We
    demonstrate that this technique can lead to significant performance
    improvements over two commonly used page mapping strategies for machines
    with either direct-mapped or two-way set-associative caches.  We also show
    that it is complementary to latency-hiding techniques such as
    prefetching.We implemented compiler-directed page coloring in the SUIF
    parallelizing compiler and on two commercial operating systems.  We applied
    the technique to the SPEC95fp benchmark suite, a representative set of
    numeric programs.  We used the SimOS machine simulator to analyze the
    applications and isolate their performance bottlenecks.  We also validated
    these results on a real machine, an eight-processor 350MHz Digital
    AlphaServer.  Compiler-directed page coloring leads to significant
    performance improvements for several applications.  Overall, our technique
    improves the SPEC95fp rating for eight processors by 8% over Digital UNIX's
    page mapping policy and by 20% over a page coloring, a standard page
    mapping policy.  The SUIF compiler achieves a SPEC95fp ratio of 57.4, the
    highest ratio to date.", 
  location     = "https://doi.org/10.1145/237090.237195"
}

@Article{eddmbidsp,
  author       = "Mazen A.~R. Saghir and Paul Chow and Corinna~G. Lee",
  title        = "Exploiting Dual Data-Memory Banks in Digital Signal Processors",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "234--243",
  month        = sep,
  keywords     = "dsp, data partitioning, data duplication",
  abstract     = "Over the past decade, digital signal processors (DSPs) have
    emerged as the processors of choice for implementing embedded applications
    in high-volume consumer products.  Through their use of specialized
    hardware features and small chip areas, DSPs provide the high performance
    necessary for embedded applications at the low costs demanded by the
    high-volume consumer market.  One feature commonly found in DSPs is the use
    of dual data-memory banks to double the memory system's bandwidth.  When
    coupled with high-order data interleaving, dual memory banks provide the
    same bandwidth as more costly memory organizations such as a dual-ported
    memory.  However, making effective use of dual memory banks remains
    difficult, especially for high-level language (HLL) DSP compilers.In this
    paper, we describe two algorithms --- compaction-based (CB) data
    partitioning and partial data duplication --- that we developed as part of
    our research into the effective exploitation of dual data-memory banks in
    HLL DSP compilers.  We show that CB partitioning is an effective technique
    for exploiting dual data-memory banks, and that partial data duplication
    can augment CB partitioning in improving execution performance.  Our
    results show that CB partitioning improves the performance of our kernel
    benchmarks by 13%-40% and the performance of our application benchmarks by
    3%-15%.  For one of the application benchmarks, partial data duplication
    boosts performance from 3% to 34%.", 
  location     = "https://dl.acm.org/doi/10.1145/248209.237193", 
  location     = "https://www.eecg.utoronto.ca/~corinna/publications/saghir.asplos96.ps"
}

@Article{cbpfrds,
  author       = "Chi-Keung Luk and Todd~C. Mowry",
  title        = "Compiler-Based Prefetching for Recursive Data Structures",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "222--233",
  month        = sep,
  keywords     = "memory latency, prefetching, pointer chasing, greedy
    prefetching, history-based prefetching, linearized prefetching, caching",
  abstract     = "Software-controlled data prefetching offers the potential for
    bridging the ever-increasing speed gap between the memory subsystem and
    today's high-performance processors.  While prefetching has enjoyed
    considerable success in array-based numeric codes, its potential in
    pointer-based applications has remained largely unexplored.  This paper
    investigates compiler-based prefetching for pointer-based applications---in
    particular, those containing recursive data structures.  We identify the
    fundamental problem in prefetching pointer-based data structures and
    propose a guideline for devising successful prefetching schemes.  Based on
    this guideline, we design three prefetching schemes, we automate the most
    widely applicable scheme (greedy prefetching) in an optimizing research
    compiler, and we evaluate the performance of all three schemes on a modern
    superscalar processor similar to the MIPS R10000.  Our results demonstrate
    that compiler-inserted prefetching can significantly improve the execution
    speed of pointer-based codes---as much as 45% for the applications we
    study.  In addition, the more sophisticated algorithms (which we currently
    perform by hand, but which might be implemented in future compilers) can
    improve performance by as much as twofold.  Compared with the only other
    compiler-based pointer prefetching scheme in the literature, our algorithms
    offer substantially better performance by avoiding unnecessary overhead and
    hiding more latency.", 
  location     = "https://doi.org/10.1145/237090.237190"
}

@Article{amfiemap,
  author       = "Richard~C. Holt and David~B. Wortman",
  title        = "{A} Model for Implementing {EUCLID} Modules and Prototypes",
  journal      = toplas,
  year         = 1982,
  volume       = 4,
  number       = 4,
  pages        = "552--562",
  month        = oct,
  keywords     = "modules, packages, system implementation language, type
    descriptors, macros, environment displays",
  abstract     = "The PASCAL-based programming language EUCLID was designed for
    implementing verifiable system software.  EUCLID's design includes many
    novel extensions, including a module mechanism and a substantial
    generalization of the PASCAL type mechanism.  This paper presents an
    implementation model for two of these extensions: modules and parameterized
    type definitions (prototypes).", 
  location     = "https://dl.acm.org/doi/abs/10.1145/69622.357183"
}

@Article{hclaooisd,
  author       = "R.~Bianchini and L.~I. Kontothanassis and R.~Pinto and M.~De Maria and M.~Abud and C.~L. Amorim",
  title        = "Hiding Communication Latency and Coherence Overhead in Software DSMs",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "198--209",
  month        = sep,
  keywords     = "protocol controllers, automatic updates",
  abstract     = "In this paper we propose the use of a PCI-based programmable
    protocol controller for hiding communication and coherence overheads in
    software DSMs.  Our protocol controller provides three different types of
    overhead tolerance: a) moving basic communication and coherence tasks away
    from computation processors; b) prefetching of diffs; and c) generating and
    applying diffs with hardware assistance.  We evaluate the isolated and
    combined impact of these features on the performance of TreadMarks.  We
    also compare performance against two versions of the Shrimp-based AURC
    protocol.  Using detailed execution-driven simulations of a 16-node network
    of workstations, we show that the greatest performance benefits provided by
    our protocol controller come from our hardware-supported diffs.  Reducing
    the burden of communication and coherence transactions on the computation
    processor is also beneficial but to a smaller extent.  Prefetching is not
    always profitable.  Our results show that our protocol controller can
    improve running time performance by up to 50% for TreadMarks, which means
    that it can double the TreadMarks speedups.  The overlapping implementation
    of TreadMarks performs as well or better than AURC for 5 of our 6
    applications.  We conclude that the simple hardware support we propose
    allows for the implementation of high-performance software DSMs at low
    cost.  Based on this conclusion, we are building the NCP2 parallel system
    at COPPE/UFRJ.", 
  location     = "https://doi.org/10.1145/237090.237185", 
  location     = "https://www.cos.ufrj.br/uploadfile/es35695.pdf"
}

@Article{aictsrtsdsms,
  author       = "Sandhya Dwarkadas and Alan~L. Cox and Willy Zwaenepoel",
  title        = "{A} Integrated Compile-Time\slash Run-Time Software Distribution Shared Memory System",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "186--197",
  month        = sep,
  keywords     = "compiler analysis, consistency primitives, access analysis",
  abstract     = "On a distributed memory machine, hand-coded message passing
    leads to the most efficient execution, but it is difficult to use.
    Parallelizing compilers can approach the performance of hand-coded message
    passing by translating data-parallel programs into message passing
    programs, but efficient execution is limited to those programs for which
    precise analysis can be carried out.  Shared memory is easier to program
    than message passing and its domain is not constrained by the limitations
    of parallelizing compilers, but it lags in performance.  Our goal is to
    close that performance gap while retaining the benefits of shared memory.
    In other words, our goal is (1) to make shared memory as efficient as
    message passing, whether hand-coded or compiler-generated, (2) to retain
    its ease of programming, and (3) to retain the broader class of
    applications it supports.To this end we have designed and implemented an
    integrated compile-time and run-time software DSM system.  The programming
    model remains identical to the original pure run-time DSM system.  No user
    intervention is required to obtain the benefits of our system.  The
    compiler computes data access patterns for the individual processors.  It
    then performs a source-to-source transformation, inserting in the program
    calls to inform the run-time system of the computed data access patterns.
    The run-time system uses this information to aggregate communication, to
    aggregate data and synchronization into a single message, to eliminate
    consistency overhead, and to replace global synchronization with
    point-to-point synchronization wherever possible.We extended the Parascope
    programming environment to perform the required analysis, and we augmented
    the TreadMarks run-time DSM library to take advantage of the analysis.  We
    used six Fortran programs to assess the performance benefits: Jacobi,
    3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with
    two different data set sizes.  The experiments were run on an 8-node IBM
    SP/2 using user-space communication.  Compiler optimization in conjunction
    with the augmented run-time system achieves substantial execution time
    improvements in comparison to the base TreadMarks, ranging from 4% to 59%
    on 8 processors.  Relative to message passing implementations of the same
    applications, the compile-time run-time system is 0-29% slower than message
    passing, while the base run-time system is 5-212% slower.  For the five
    programs that XHPF could parallelize (all except IS), the execution times
    achieved by the compiler optimized shared memory programs are within 9% of
    XHPF.", 
  location     = "https://dl.acm.org/doi/abs/10.1145/237090.237181", 
  location     = "http://www.cs.rochester.edu/u/sandhya/papers/asplos96.ps"
}

@Article{atnacvvoddd,
  author       = "Armando Fox and Steven~D. Gribble and Eric~A. Brewer and Elan Amir",
  title        = "Adapting to Network and Client Variability via On-Demand Dynamic Distillation",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "160--170",
  month        = sep,
  keywords     = "variance, semantic-directed compression, smart networks,
    transcoding", 
  abstract     = "The explosive growth of the Internet and the proliferation of
    smart cellular phones and handheld wireless devices is widening an already
    large gap between Internet clients.  Clients vary in their hardware
    resources, software sophistication, and quality of connectivity, yet server
    support for client variation ranges from relatively poor to none at all.
    In this paper we introduce some design principles that we believe are
    fundamental to providing 'meaningful' Internet access for the entire range
    of clients.  In particular, we show how to perform on-demand
    datatype-specific lossy compression on semantically typed data, tailoring
    content to the specific constraints of the client.  We instantiate our
    design principles in a proxy architecture that further exploits typed data
    to enable application-level management of scarce network resources.  Our
    proxy architecture generalizes previous work addressing all three aspects
    of client variation by applying well-understood techniques in a novel way,
    resulting in quantitatively better end-to-end performance, higher quality
    display output, and new capabilities for low-end clients.", 
  location     = "https://dl.acm.org/doi/10.1145/248209.237177"
}

@Article{salosoafsfgsm,
  author       = "Daniel~J. Scales and Kourosh Gharachorloo and Chandramohan~A. Thekkath",
  title        = "Shasta:  {A} Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "174--185",
  month        = sep,
  keywords     = "cache coherence, polling, instruction scheduling, relaxed
    memory models, batching",
  abstract     = "This paper describes Shasta, a system that supports a shared
    address space in software on clusters of computers with physically
    distributed memory.  A unique aspect of Shasta compared to most other
    software distributed shared memory systems is that shared data can be kept
    coherent at a fine granularity.  In addition, the system allows the
    coherence granularity to vary across different shared data structures in a
    single application.  Shasta implements the shared address space by
    transparently rewriting the application executable to intercept loads and
    stores.  For each shared load or store, the inserted code checks to see if
    the data is available locally and communicates with other processors if
    necessary.  The system uses numerous techniques to reduce the run-time
    overhead of these checks.  Since Shasta is implemented entirely in
    software, it also provides tremendous flexibility in supporting different
    types of cache coherence protocols.  We have implemented an efficient cache
    coherence protocol that incorporates a number of optimizations, including
    support for multiple communication granularities and use of relaxed memory
    models.  This system is fully functional and runs on a cluster of Alpha
    workstations.The primary focus of this paper is to describe the techniques
    used in Shasta to reduce the checking overhead for supporting fine
    granularity sharing in software.  These techniques include careful layout
    of the shared address space, scheduling the checking code for efficient
    execution on modern processors, using a simple method that checks loads
    using only the value loaded, reducing the extra cache misses caused by the
    checking code, and combining the checks for multiple loads and stores.  To
    characterize the effect of these techniques, we present detailed
    performance results for the SPLASH-2 applications running on an Alpha
    processor.  Without our optimizations, the checking overheads are
    excessively high, exceeding 100% for several applications.  However, our
    techniques are effective in reducing these overheads to a range of 5% to
    35% for almost all of the applications.  We also describe our coherence
    protocol and present some preliminary results on the parallel performance
    of several applications running on our workstation cluster.  Our experience
    so far indicates that once the cost of checking memory accesses is reduced
    using our techniques, the Shasta approach is an attractive software
    solution for supporting a shared address space with fine-grain access to
    data.", 
  location     = "https://dl.acm.org/doi/10.1145/248209.237179", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-96-2.html"
}

@Article{mbabp,
  author       = "André Seznec and Stéphan Jourdan and Pascal Sainrat and Pierre Michaud",
  title        = "Multiple-Block Ahead Branch Predictors",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "116--127",
  month        = sep,
  keywords     = "branch prediction, branch prediction tables, instruction paths",
  abstract     = "A basic rule in computer architecture is that a processor
    cannot execute an application faster than it fetches its instructions.
    This paper presents a novel cost-effective mechanism called the two-block
    ahead branch predictor.  Information from the current instruction block is
    not used for predicting the address of the next instruction block, but
    rather for predicting the block following the next instruction block.This
    approach overcomes the instruction fetch bottle-neck exhibited by
    wide-dispatch 'brainiac' processors by enabling them to efficiently predict
    addresses of two instruction blocks in a single cycle.  Furthermore,
    pipelining the branch prediction process can also be done by means of our
    predictor for 'speed demon' processors to achieve higher clock rate or to
    improve the prediction accuracy by means of bigger prediction
    structures.Moreover, and unlike the previously-proposed multiple predictor
    schemes, multiple-block ahead branch predictors can use any of the branch
    prediction schemes to perform the very accurate predictions required to
    achieve high-performance on superscalar processors.", 
  location     = "https://doi.org/10.1145/248208.237169", 
  location     = "https://hal.inria.fr/inria-00073867/document"
}

@Article{aobpvdc,
  author       = "I-Cheng~K. Chen and John~T. Coffey and Trevor~N. Mudge",
  title        = "Analysis of Branch Prediction via Data Compression",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "128--137",
  month        = sep,
  keywords     = "branch prediction, data compression, markov predictors",
  abstract     = "Branch prediction is an important mechanism in modern
    microprocessor design.  The focus of research in this area has been on
    designing new branch prediction schemes.  In contrast, very few studies
    address the theoretical basis behind these prediction schemes.  Knowing
    this theoretical basis helps us to evaluate how good a prediction scheme is
    and how much we can expect to improve its accuracy.In this paper, we apply
    techniques from data compression to establish a theoretical basis for
    branch prediction, and to illustrate alternatives for further improvement.
    To establish a theoretical basis, we first introduce a conceptual model to
    characterize each component in a branch prediction process.  Then we show
    that current 'two-level' or correlation based predictors are, in fact,
    simplifications of an optimal predictor in data compression, Prediction by
    Partial Matching (PPM).If the information provided to the predictor remains
    the same, it is unlikely that significant improvements can be expected
    (asymptotically) from two-level predictors, since PPM is optimal.  However,
    there are a rich set of predictors available from data compression, several
    of which can still yield some improvement in cases where resources are
    limited.  To illustrate this, we conduct trace-driven simulation running
    the Instruction Benchmark Suite and the SPEC CINT95 benchmarks.  The
    results show that PPM can outperform a two-level predictor for modest sized
    branch target buffers.", 
  location     = "https://doi.org/10.1145/248209.237171", 
  location     = "http://tnm.engin.umich.edu/wp-content/uploads/sites/353/2017/12/1996.10.Analysis-of-Branch-Prediction-via-Data-Compression.pdf"
}

@Article{vlalvp,
  author       = "Mikko~H. Lipasti and Christopher~B. Wilkerson and John Paul Shen",
  title        = "Value Locality and Load Value Prediction",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "138--147",
  month        = sep,
  keywords     = "value locality, microarchitecture models",
  abstract     = "Since the introduction of virtual memory demand-paging and
    cache memories, computer systems have been exploiting spatial and temporal
    locality to reduce the average latency of a memory reference.  In this
    paper, we introduce the notion of value locality, a third facet of locality
    that is frequently present in real-world programs, and describe how to
    effectively capture and exploit it in order to perform load value
    prediction.  Temporal and spatial locality are attributes of storage
    locations, and describe the future likelihood of references to those
    locations or their close neighbors.  In a similar vein, value locality
    describes the likelihood of the recurrence of a previously-seen value
    within a storage location.  Modern processors already exploit value
    locality in a very restricted sense through the use of control speculation
    (i.e.  branch prediction), which seeks to predict the future value of a
    single condition bit based on previously-seen values.  Our work extends
    this to predict entire 32- and 64-bit register values based on
    previously-seen values.  We find that, just as condition bits are fairly
    predictable on a per-static-branch basis, full register values being loaded
    from memory are frequently predictable as well.  Furthermore, we show that
    simple microarchitectural enhancements to two modern microprocessor
    implementations (based on the PowerPC 620 and Alpha 21164) that enable load
    value prediction can effectively exploit value locality to collapse true
    dependencies, reduce average memory latency and bandwidth requirements, and
    provide measurable performance gains.", 
  location     = "https://doi.org/10.1145/248209.237173"
}

@Article{satpocdvsm,
  author       = "Mikko~H. Lipasti and Christopher~B. Wilkerson and John Paul Shen",
  title        = "{SoftFLASH}:  Analyzing the Performance of Clustered Distributed Virtual Shared Memory",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "210--220",
  month        = sep,
  keywords     = "shared address space, relaxed memory consistency, network
    requests",
  abstract     = "One potentially attractive way to build large-scale
    shared-memory machines is to use small-scale to medium-scale shared-memory
    machines as clusters that are interconnected with an off-the-shelf network.
    To create a shared-memory programming environment across the clusters, it
    is possible to use a virtual shared-memory software layer.  Because of the
    low latency and high bandwidth of the interconnect available within each
    cluster, there are clear advantages in making the clusters as large as
    possible.  The critical question then becomes whether the latency and
    bandwidth of the top-level network and the software system are sufficient
    to support the communication demands generated by the clusters.To explore
    these questions, we have built an aggressive kernel implementation of a
    virtual shared-memory system using SGI multiprocessors and 100Mbyte/sec
    HIPPI interconnects.  The system obtains speedups on 32 processors (four
    nodes, eight processors per node plus additional reserved protocol
    processors) that range from 6.9 on the communication-intensive FFT program
    to 21.6 on Ocean (both from the SPLASH 2 suite).  In general, clustering is
    effective in reducing internode miss rates, but as the cluster size
    increases, increases in the remote latency, mostly due to increased TLB
    synchronization cost, offset the advantages.  For communication-intensive
    applications, such as FFT, the overhead of sending out network requests,
    the limited network bandwidth, and the long network latency prevent the
    achievement of good performance.  Overall, this approach still appears
    promising, but our results indicate that large low latency networks may be
    needed to make cluster-based virtual shared-memory machines broadly useful
    as large-scale shared-memory multiprocessors.", 
  location     = "https://doi.org/10.1145/248209.237187"
}

@Article{tsapoi,
  author       = "Theodore~H. Romer and Dennis Lee and Geoffrey~M. Voelker and Alec Wolman and Wayne~A. Wong and Jean-Loup Baer and Brian~N. Bershad and Henry~M. Levy",
  title        = "The Structure and Peformance of Interpreters",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "150--159",
  month        = sep,
  keywords     = "interpreter performance, virtual machines, architectural
    effects, instruction caches, performance",
  abstract     = "Interpreted languages have become increasingly popular due to
    demands for rapid program development, ease of use, portability, and
    safety.  Beyond the general impression that they are 'slow,' however,
    little has been documented about the performance of interpreters as a class
    of applications.This paper examines interpreter performance by measuring
    and analyzing interpreters from both software and hardware perspectives.
    As examples, we measure the MIPSI, Java, Perl, and Tcl interpreters running
    an array of micro and macro benchmarks on a DEC Alpha platform.  Our
    measurements of these interpreters relate performance to the complexity of
    the interpreter's virtual machine and demonstrate that native runtime
    libraries can play a key role in providing good performance.  From an
    architectural perspective, we show that interpreter performance is
    primarily a function of the interpreter itself and is relatively
    independent of the application being interpreted.  We also demonstrate that
    high-level interpreters' demands on processor resources are comparable to
    those of other complex compiled programs, such as gcc.  We conclude that
    interpreters, as a class of applications, do not currently motivate special
    hardware support for increased performance.", 
  location     = "https://doi.org/10.1145/237090.237175"
}

@Article{eoasfgabcilspm,
  author       = "Arvind Krishnamurthy and Klaus~E. Schauser and Chris~J. Scheiman and Randolph~Y. Wang and David~E. Culler and Katherine Yelick",
  title        = "Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "37--48",
  month        = sep,
  keywords     = "massively parallel machines, global memory architectures,
    network processors, network interfaces",
  abstract     = "Large-scale parallel machines are incorporating increasingly
    sophisticated architectural support for user-level messaging and global
    memory access.  We provide a systematic evaluation of a broad spectrum of
    current design alternatives based on our implementations of a global
    address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2,
    Cray T3D, and Berkeley NOW.  This evaluation includes a range of
    compilation strategies that make varying use of the network processor; each
    is optimized for the target architecture and the particular strategy.  We
    analyze a family of interacting issues that determine the performance
    trade-offs in each implementation, quantify the resulting latency,
    overhead, and bandwidth of the global access operations, and demonstrate
    the effects on application performance.", 
  location     = "https://doi.org/10.1145/248209.237147", 
  location     = "https://people.eecs.berkeley.edu/~culler/papers/asplos96-coproc.ps"
}

@Article{wpoftaset,
  author       = "Dirk Grunwald and Richard Neves",
  title        = "Whole-Pogram Optimization for Time and Space Efficient Threads",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "50--59",
  month        = sep,
  keywords     = "threaded programs, context switching, register save and
    restore, whole-program analysis, link-time analysis",
  abstract     = "Modern languages and operating systems often encourage
    programmers to use threads, or independent control streams, to mask the
    overhead of some operations and simplify program structure.  Multitasking
    operating systems use threads to mask communication latency, either with
    hardwares devices or users.  Client-server applications typically use
    threads to simplify the complex control-flow that arises when multiple
    clients are used.  Recently, the scientific computing community has started
    using threads to mask network communication latency in massively parallel
    architectures, allowing computation and communication to be overlapped.
    Lastly, some architectures implement threads in hardware, using those
    threads to tolerate memory latency.In general, it would be desirable if
    threaded programs could be written to expose the largest degree of
    parallelism possible, or to simplify the program design.  However, threads
    incur time and space overheads, and programmers often compromise simple
    designs for performance.  In this paper, we show how to reduce time and
    space thread overhead using control flow and register liveness information
    inferred after compilation.  Our techniques work on binaries, are not
    specific to a particular compiler or thread library and reduce the 
    overall execution time of fine-grain threaded programs by &amp;asymp;
    15-30%.  We use execution-driven analysis and an instrumented operating
    system to show why the execution time is reduced and to indicate areas for
    future work.", 
  location     = "https://doi.org/10.1145/248209.237149"
}

@Article{tsfcl,
  author       = "James Philbin and Jan Edler and Otto~J. Anshus and Craig~C. Douglas and Kai Li",
  title        = "Thread Scheduling for Cache Locality",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "60--71",
  month        = sep,
  keywords     = "find-grained threading, thread scheduling, cache-conflict
    management, data partitioning",
  abstract     = "This paper describes a method to improve the cache locality
    of sequential programs by scheduling fine-grained threads.  The algorithm
    relies upon hints provided at the time of thread creation to determine a
    thread execution order likely to reduce cache misses.  This technique may
    be particularly valuable when compiler-directed tiling is not feasible.
    Experiments with several application programs, on two systems with
    different cache structures, show that our thread scheduling method can
    improve program performance by reducing second-level cache misses.", 
  location     = "https://doi.org/10.1145/237090.237151", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-569-96"
}

@Article{trfcsosc,
  author       = "Peter~M. Chen and Wee Teck Ng and Subhachandra Chandra and Christopher Aycock and Gurushankar Rajamani and David Lowell",
  title        = "The {Rio File Cache}:  Surviving Operating System Crashes",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "74--83",
  month        = sep,
  keywords     = "non-volatile storage, disk-i/o reliability, file caching, ",
  abstract     = "One of the fundamental limits to high-performance,
    high-reliability file systems is memory's vulnerability to system crashes.
    Because memory is viewed as unsafe, systems periodically write data back to
    disk.  The extra disk traffic lowers performance, and the delay period
    before data is safe lowers reliability.  The goal of the Rio (RAM I/O) file
    cache is to make ordinary main memory safe for persistent storage by
    enabling memory to survive operating system crashes.  Reliable memory
    enables a system to achieve the best of both worlds: reliability equivalent
    to a write-through file cache, where every write is instantly safe, and
    performance equivalent to a pure write-back cache, with no
    reliability-induced writes to disk.  To achieve reliability, we protect
    memory during a crash and restore it during a reboot (a 'warm' reboot).
    Extensive crash tests show that even without protection, warm reboot
    enables memory to achieve reliability close to that of a write-through file
    system.  Adding protection makes memory even safer than a write-through
    file system while adding essentially no overhead.  By eliminating
    reliability-induced disk writes, Rio performs 4-22 times as fast as a
    write-through file system, 2-14 times as fast as a standard Unix file
    system, and 1-3 times as fast as an optimized system that risks losing 30
    seconds of data and metadata.", 
  location     = "https://doi.org/10.1145/237090.237154",
  location     = "https://web.eecs.umich.edu/~pmchen/papers/chen96.pdf"
}

@Article{pdvd,
  author       = "Edward~K. Lee and Chandramohan~A. Thekkath",
  title        = "Petal:  Distributed Virtual Disks",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "84--92",
  month        = sep,
  keywords     = "remote disk systems, recoverable disk systems, incremental
    reconfiguration, distributed disk systems",
  abstract     = "The ideal storage system is globally accessible, always
    available, provides unlimited performance and capacity for a large number
    of clients, and requires no management.  This paper describes the design,
    implementation, and performance of Petal, a system that attempts to
    approximate this ideal in practice through a novel combination of features.
    Petal consists of a collection of network-connected servers that
    cooperatively manage a pool of physical disks.  To a Petal client, this
    collection appears as a highly available block-level storage system that
    provides large abstract containers called virtual disks.  A virtual disk is
    globally accessible to all Petal clients on the network.  A client can
    create a virtual disk on demand to tap the entire capacity and performance
    of the underlying physical resources.  Furthermore, additional resources,
    such as servers and disks, can be automatically incorporated into Petal.We
    have an initial Petal prototype consisting of four 225 MHz DEC 3000/700
    workstations running Digital Unix and connected by a 155 Mbit/s ATM
    network.  The prototype provides clients with virtual disks that tolerate
    and recover from disk, server, and network failures.  Latency is comparable
    to a locally attached disk, and throughput scales with the number of
    servers.  The prototype can achieve I/O rates of up to 3150 requests/sec
    and bandwidth up to 43.1 Mbytes/sec.", 
  location     = "https://dl.acm.org/doi/10.1145/248209.237157",
  location     = "https://www.scs.stanford.edu/nyu/01fa/sched/petal.pdf"
}

@Article{aqaolnl,
  author       = "Kathryn~S. McKinley and Olivier Temam",
  title        = "A Quantitative Analysis of Loop Nest Locality",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "94--104",
  month        = sep,
  keywords     = "loop nests, data locality, cache pollution, capacity and
    conflict misses",
  abstract     = "This paper analyzes and quantifies the locality
    characteristics of numerical loop nests in order to suggest future
    directions for architecture and software cache optimizations.  Since most
    programs spend the majority of their time in nests, the vast majority of
    cache optimization techniques target loop nests.  In contrast, the locality
    characteristics that drive these optimizations are usually collected across
    the entire application rather than the nest level.  Indeed, researchers
    have studied numerical codes for so long that a number of commonly held
    assertions have emerged on their locality characteristics.  In light of
    these assertions, we use the Perfect Benchmarks to take a new look at
    measuring locality on numerical codes based on references, loop nests, and
    program locality properties.  Our results show that several popular
    assertions are at best overstatements.  For example, we find that temporal
    and spatial reuse have balanced roles within a loop nest and most reuse
    across nests and the entire program is temporal.  These results are
    consistent with high hit rates, but go against the commonly held assumption
    that spatial reuse dominates.  Another result contrary to popular
    assumption is that misses within a nest are overwhelmingly conflict misses
    rather than capacity misses.  Capacity misses are a significant source of
    misses for the entire program, but mostly correspond to potential reuse
    between different loop nests.  Our locality measurements reveal important
    differences between loop nests and programs; refute some popular
    assertions; and provide new insights for the compiler writer and the
    architect.", 
  location     = "https://doi.org/10.1145/248208.237161", 
  location     = "https://www.cs.utexas.edu/users/mckinley/papers/quant-asplos-1996.pdf"
}

@Article{tibroop,
  author       = "Andrew~S. Huang and John Paul Shen",
  title        = "The Intrinsic Bandwidth Requirements of Ordinary Programs",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "105--114",
  month        = sep,
  keywords     = "memory efficiency, bandwidth spectrum",
  abstract     = "While there has been an abundance of recent papers on
    hardware and software approaches to improving the performance of memory
    accesses, few papers have addressed the problem from the program's point of
    view.  There is a general notion that certain programs have larger working
    sets than others.  However, there is no quantitative method for evaluating
    and comparing the memory requirements of programs.This paper introduces the
    bandwidth spectrum for characterizing the memory requirements of a
    program's instruction and data stream.  The bandwidth spectrum measures the
    average bandwidth requirement of a program as a function of available local
    memory.  These measurements are performed under the most idealized
    conditions of perfect knowledge and perfect memory management.  As such,
    they represent the lower bounds on the memory requirements of programs.  We
    present the bandwidth spectrums for a set of 22 benchmarks and show how
    they can be used in the comparison of memory requirements and I/O
    requirement.  The bandwidth spectrums also offer a convenient method to
    weigh the trade-off amongst instruction issue rate, local memory capacity
    and bandwidth into local memory.Using the bandwidth spectrum, we show that
    at issue rates of four or less, bandwidth usually scales linearly with the
    issue rate.  At higher issue rates, bandwidth can often scale superlinearly
    with respect to issue rate.  Finally, we also investigate the effects of
    varying the input sets on the bandwidth spectrums.", 
  location     = "https://doi.org/10.1145/237090.237163"
}

@Article{sdooc,
  author       = "John Hennessy",
  title        = "Symbolic Debugging of Optimized Code",
  journal      = toplas,
  year         = 1982,
  volume       = 4,
  number       = 3,
  pages        = "323--344",
  month        = jul,
  keywords     = "symbolic debugging, code optimization, directed acyclic
    graphs, flow graphs", 
  abstract     = "The long standing conflict between the optimization of code
    and the ability to symbolically debug the code is examined.  The effects of
    local and global optimizations on the variables of a program are
    categorized and models for representing the effect of optimizations are
    given.  These models are used by algorithms which determine the subset of
    variables whose values do not correspond to those in the original program.
    Algorithms for restoring these variables to their correct values are also
    developed.  Empirical results from the application of these algorithms to
    local optimization are presented.", 
  location     = "https://doi.org/10.1145/357172.357173",
  location     = "http://i.stanford.edu/pub/cstr/reports/csl/tr/79/175/CSL-TR-79-175.pdf"
}

@Article{sacittm,
  author       = "Steven~L. Scott",
  title        = "Syncnronization and Communication in the {T3E} Multiprocessor",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "26--36",
  month        = sep,
  keywords     = "vector processors, system architecture, global communication,
    atomic storage operations, messaging, synchronization, performance",
  abstract     = "This paper describes the synchronization and communication
    primitives of the Cray T3E multiprocessor, a shared memory system scalable
    to 2048 processors.  We discuss what we have learned from the T3D project
    (the predecessor to the T3E) and the rationale behind changes made for the
    T3E.  We include performance measurements for various aspects of
    communication and synchronization.The T3E augments the memory interface of
    the DEC 21164 microprocessor with a large set of explicitly-managed,
    external registers (E-registers).  E-registers are used as the source or
    target for all remote communication.  They provide a highly pipelined
    interface to global memory that allows dozens of requests per processor to
    be outstanding.  Through E-registers, the T3E provides a rich set of atomic
    memory operations and a flexible, user-level messaging facility.  The T3E
    also provides a set of virtual hardware barrier/eureka networks that can be
    arbitrarily embedded into the 3D torus interconnect.", 
  location     = "https://doi.org/10.1145/248209.237144"
}

@Article{aeomcmfasmswip,
  author       = "Vijay~S. Pai and Parthasarathy Ranganathan and Sarita~V. Adve and Tracy Harton",
  title        = "An Evaluation of Memory Consistency Models for Shared-Memory Systems with {ILP} Processors",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "12--23",
  month        = sep,
  keywords     = "consistency models, sequential consistency, speculative
    loads, write-through caches, write-back caches, release consistency, latency
    toleration, fuzzy acquires, control dependencies",
  abstract     = "Relaxed consistency models have been shown to significantly
    outperform sequential consistency for single-issue, statically scheduled
    processors with blocking reads.  However, current microprocessors
    aggressively exploit instruction-level parallelism (ILP) using methods such
    as multiple issue, dynamic scheduling, and non-blocking reads.  Researchers
    have conjectured that two techniques, hardware-controlled non-binding
    prefetching and speculative loads, have the potential to equalize the
    hardware performance of memory consistency models on such processors.This
    paper performs the first detailed quantitative comparison of several
    implementations of sequential consistency and release consistency optimized
    for aggressive ILP processors.  Our results indicate that hardware
    prefetching and speculative loads dramatically improve the performance of
    sequential consistency.  However, the gap between sequential consistency
    and release consistency depends on the cache write policy and the
    complexity of the cache-coherence protocol implementation.  In most cases,
    release consistency significantly outperforms sequential consistency, but
    for two applications, the use of a write-back primary cache and a more
    complex cache-coherence protocol nearly equalizes the performance of the
    two models.We also observe that the existing techniques, which require
    on-chip hardware modifications, enhance the performance of release
    consistency only to a small extent.  We propose two new software techniques
    --- fuzzy acquires and selective acquires --- to achieve more overlap than
    allowed by the previous implementations of release consistency.  To enhance
    methods for overlapping acquires, we also propose a technique to eliminate
    control dependences caused by an acquire loop, using a small amount of
    off-chip hardware called the synchronization buffer.", 
  location     = "https://doi.org/10.1145/248209.237142"
}

@Article{tcfascm,
  author       = "Kunle Olukotun and Basem~A. Nayfeh and Lance Hammond and Ken Wilson and Kunyung Chang",
  title        = "The Case for a Single-Chop Multiprocessor",
  journal      = asplos96,
  year         = 1996,
  volume       = 31,
  number       = 9,
  pages        = "2--11",
  month        = sep,
  keywords     = "superscalar architectures, single-chip multiprocessors,
    microarchitectures, architectural trade-offs",
  abstract     = "Advances in IC processing allow for more microprocessor
    design options. The increasing gate density and cost of wires in advanced
    integrated circuit technologies require that we look for new ways to use
    their capabilities effectively.  This paper shows that in advanced
    technologies it is possible to implement a single-chip multiprocessor in
    the same area as a wide issue superscalar processor.  We find that for
    applications with little parallelism the performance of the two
    microarchitectures is comparable.  For applications with large amounts of
    parallelism at both the fine and coarse grained levels, the multiprocessor
    microarchitecture outperforms the superscalar architecture by a significant
    margin.  Single-chip multiprocessor architectures have the advantage in
    that they offer localized implementation of a high-clock rate processor for
    inherently sequential applications and low latency interprocessor
    communication for parallel applications.", 
  location     = "https://doi.org/10.1145/248209.237140"
}

@Article{vssfhpp,
  author       = "Chao-Ying Fu and Matthew~D. Jennings and Sergei~Y. Larin and Thomas~M. Conte",
  title        = "Value Speculation Scheduling for High Performance Processors",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "262--271",
  month        = oct,
  keywords     = "value prediction, speculative scheduling, ilp",
  abstract     = "Recent research in value prediction shows a surprising amount
    of predictability for the values produced by register-writing instructions.
    Several hardware based value predictor designs have been proposed to
    exploit this predictability by eliminating flow dependencies for highly
    predictable values.  This paper proposed a hardware and software based
    scheme for value speculation scheduling (VSS).  Static VLIW scheduling
    techniques are used to speculate value dependent instructions by scheduling
    them above the instructions whose results they are dependent on.
    Prediction hardware is used to provide value predictions for allowing the
    execution of speculated instructions to continue.  In the case of
    miss-predicted values, control flow is redirected to patch-up code so that
    execution can proceed with the correct results.  In this paper, experiments
    in VSS for load operations in the SPECint95 benchmarks are performed.
    Speedup of up to 17% has been shown for using VSS.  Empirical results on
    the value predictability of loads, based on value profiling data, are also
    provided.", 
  location     = "https://doi.org/10.1145/384265.291058",
  location     = "https://people.engr.ncsu.edu/hzhou/VS.pdf"
}

@Article{aesodiem,
  author       = "Narayan Ranganathan and Manoj Franklin",
  title        = "An Empirical Study of Decentralized {ILP} Execution Models",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 1,
  pages        = "272--281",
  month        = oct,
  keywords     = "decentralized ilp, dependencies, partitioning",
  abstract     = "Recent fascination for dynamic scheduling as a means for
    exploiting instruction-level parallelism has introduced significant
    interest in the scalability aspects of dynamic scheduling hardware.  In
    order to overcome the scalability problems of centralized hardware
    schedulers, many decentralized execution models are being proposed and
    investigated recently.  The crux of all these models is to split the
    instruction window across multiple processing elements (PEs) that do
    independent, scheduling of instructions.  The decentralized execution
    models proposed so far can be grouped under 3 categories, based on the
    criterion used for assigning an instruction to a particular PE.  They are:
    (i) execution unit dependence based decentralization (EDD), (ii) control
    dependence based decentralization (CDD), and (iii) data dependence based
    decentralization (DDD).  This paper investigates the performance aspects of
    these three decentralization approaches.  Using a suite of important
    benchmarks and realistic system parameters, we examine performance
    differences resulting from the type of partitioning as well as from
    specific implementation issues such as the type of PE interconnect.We found
    that with a ring-type PE interconnect, the DDD approach performs the best
    when the number of PEs is moderate, and that the CDD approach performs best
    when the number of PEs is large.  The currently used approach---EDD---does
    not perform well for any configuration.  With a realistic crossbar,
    performance does not increase with the number of PEs for any of the
    partitioning approaches.  The results give insight into the best way to use
    the transistor budget available for implementing the instruction window.", 
  location     = "https://doi.org/10.1145/291006.291061"
}

@Article{fooopsum,
  author       = "Eric Schnarr and James~R. Larus",
  title        = "Fast Out-of-Order Processor Simulation Using Memoization",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "283--294",
  month        = oct,
  keywords     = "out-of-order processor simulation, direct execution,
    memoization, simulating speculative execution, fast fowarding, ",
  abstract     = "Our new out-of-order processor simulatol; FastSim, uses two
    innovations to speed up simulation 8--15 times (vs.  Wisconsin
    SimpleScalar) with no loss in simulation accuracy.  First, FastSim uses
    speculative direct-execution to accelerate the functional emulation of
    speculatively executed program code.  Second, it uses a variation on
    memoization---a well-known technique in programming language
    implementation---to cache microarchitecture states and the resulting
    simulator actions, and then 'fast forwards' the simulation the next time a
    cached state is reached.  Fast-forwarding accelerates simulation by an
    order of magnitude, while producing exactly the same, cycle-accurate result
    as conventional simulation.", 
  location     = "https://doi.org/10.1145/384265.291063",
  location     = "ftp://ftp.cs.wisc.edu/wwt/asplos98_fastsim.pdf"
}

@Article{podwosmswooop,
  author       = "Parthasarathy Ranganathan and Kourosh Gharachorloo and Sarita~V. Adve and Luiz André Barroso",
  title        = "Performance of Database Workloads on Shared-Memory Systems with Out-of-Order Processors",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "307--318",
  month        = oct,
  keywords     = "workload variation, aggressive ilp, multi- vs uni-processors,
    performance bottlenecks",
  abstract     = "Database applications such as online transaction processing
    (OLTP) and decision support systems (DSS) constitute the largest and
    fastest-growing segment of the market for multiprocessor servers.  However,
    most current system designs have been optimized to perform well on
    scientific and engineering workloads.  Given the radically different
    behavior of database workloads (especially OLTP), it is important to
    re-evaluate key system design decisions in the context of this important
    class of applications.This paper examines the behavior of database
    workloads on shared-memory multiprocessors with aggressive out-of-order
    processors, and considers simple optimizations that can provide further
    performance improvements.  Our study is based on detailed simulations of
    the Oracle commercial database engine.  The results show that the
    combination of out-of-order execution and multiple instruction issue is
    indeed effective in improving performance of database workloads, providing
    gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP
    and DSS, respectively.  In addition, speculative techniques enable
    optimized implementations of memory consistency models that significantly
    improve the performance of stricter consistency models, bringing the
    performance to within 10--15% of the performance of more relaxed models.The
    second part of our study focuses on the more challenging OLTP workload.  We
    show that an instruction stream buffer is effective in reducing the
    remaining instruction stalls in OLTP, providing a 17% reduction in
    execution time (approaching a perfect instruction cache to within 15%).
    Furthermore, our characterization shows that a large fraction of the data
    communication misses in OLTP exhibit migratory behavior; our preliminary
    results show that software prefetch and writeback/flush hints can be used
    for this data to further reduce execution time by 12%.", 
  location     = "https://doi.org/10.1145/291069.291067"
}

@Article{alasmmutrmapto,
  author       = "Bruce~L. Jacob and Trevor~N. Mudge",
  title        = "{A} Look at Several Memory Management Units, {TLB}-Refill Mechanisms and Page Table Organizations",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "295--306",
  month        = oct,
  keywords     = "ultrix, mach, intel, pa-risc",
  abstract     = "Virtual memory is a staple in modem systems, though there is
    little agreement on how its functionality is to be implemented on either
    the hardware or software side of the interface.  The myriad of design
    choices and incompatible hardware mechanisms suggests potential performance
    problems, especially since increasing numbers of systems (even embedded
    systems) are using memory management.  A comparative study of the
    implementation choices in virtual memory should therefore aid system-level
    designers.This paper compares several virtual memory designs, including
    combinations of hierarchical and inverted page tables on hardware-managed
    and software-managed translation lookaside buffers (TLBs).  The simulations
    show that systems are fairly sensitive to TLB size; that interrupts already
    account for a large portion of memory-management overhead and can become a
    significant factor as processors execute more concurrent instructions; and
    that if one includes the cache misses inflicted on applications by the VM
    system, the total VM overhead is roughly twice what was thought (10--20%
    rather than 5--10%).", 
  location     = "https://doi.org/10.1145/291069.291065",
  location     = "https://user.eng.umd.edu/~blj/papers/asplos98.pdf"
}

@Article{ammpbimimadu,
  author       = "Daniel Citron and Dror Feitelson and Larry Rudolph",
  title        = "Accelerating Multi-Media Processing by Implementing Memoing in Multiplication and Division Units",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "252--259",
  month        = oct,
  keywords     = "computational acceleration, caching",
  abstract     = "This paper proposes a technique that enables performing
    multi-cycle (multiplication, division, square-root...) computations in a
    single cycle.  The technique is based on the notion of memoing: saving the
    input and output of previous calculations and using the output if the input
    is encountered again.  This technique is especially suitable for
    Multi-Media (MM) processing.  In MM applications the local entropy of the
    data tends to be low which results in repeated operations on the same
    datum.The inputs and outputs of assembly level operations are stored in
    cache-like lookup tables and accessed in parallel to the conventional
    computation.  A successful lookup gives the result of a multi-cycle
    computation in a single cycle, and a failed lookup doesn't necessitate a
    penalty in computation time.Results of simulations have shown that on the
    average, for a modestly sized memo-table, about 40% of the floating point
    multiplications and 50% of the floating point divisions, in Multi-Media
    applications, can be avoided by using the values within the memo-table,
    leading to an average computational speedup of more than 20%.",
  location     = "https://doi.org/10.1145/291069.291056",
  location     = "http://people.csail.mit.edu/rudolph/Autobiography/papers/MemoingMM98ASPLOS.ps"
}

@Article{aoooetfrbt,
  author       = "Bich~C. Le",
  title        = "An Out-of-Order Execution Technique for Runtime Binary Translators",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "151--158",
  month        = oct,
  keywords     = "out-of-order execution, exact exception handling, emulation",
  abstract     = "A dynamic translator emulates an instruction set architccturc
    by translating source instructions to native code during execution.  On
    statically-scheduled hardware, higher performance can potentially be
    achieved by reordering the translated instructions; however, this is a
    challenging transformation if the source architecture supports precise
    exception semantics, and the user-level program is allowed to register
    exception handlers.  This paper presents a software technique which allows
    a translator to achieve the out-of-order execution of user-level programs,
    while preserving all sequential semantics.  The design combines a
    translator, an interpreter, and a set of operating system services.  Using
    the proposed techniques, a dynamic translator can optimistically reorder
    instructions and speculate them across branch boundaries.  If a
    mispeculated operation causes an exception, the recovery algorithm reverts
    the application state to a safe point, then retranslates the faulty code
    without reordering to disable further exceptions.", 
  location     = "https://doi.org/10.1145/291069.291039"
}

@Article{oewtunsefmp,
  author       = "Chandra Krintz and Brad Calder and Han Bok Lee and Benjamin~G. Zorn",
  title        = "Overlapping Execution with Transfer Using Non-Strict Execution for Mobile Programs",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "159--169",
  month        = oct,
  keywords     = "non-strict execution, code re-ordering, low-latency
    downloading",
  abstract     = "In order to execute a program on a remote computer, it
    must first be transferred over a network.  This transmission incurs the
    over-head of network latency before execution can begin.  This latency can
    vary greatly depending upon the size of the program., where it is located
    (e.g., on a local network or across the Internet), and the bandwidth
    available to retrieve the program.  Existing technologies, like Java,
    require that a jle be filly transferred before it can start executing.  For
    large files and low bandwidth lines, this delay can be significant.In this
    paper we propose and evaluate a non-strict form of mobile program
    execution.  A mobile program is any program that is transferred to a
    different machine and executed.  The goal of nonstrict execution is to
    overlap execution with transfer; allowing the program to start executing as
    soon as possible.  Non-strict execution allows a procedure in the program
    to start executing as soon as its code and data have transferred.  To
    enable this technology, we examine several techniques for rearranging
    procedures and reorganizing the data inside Java classjles.  Our results
    show that nonstrict execution decreases the initial transfer delay between
    31% and 56% on average, with an average reduction in overall execution time
    between 25% and 40%.", 
  location     = "https://doi.org/10.1145/291006.291040"
}

@Article{vlpbp,
  author       = "Jared Stark and Marius Evers and Yale~N. Patt",
  title        = "Variable Length Path Branch Prediction",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "170--179",
  month        = oct,
  keywords     = "branch prediction, dynamic path length, profiling,
    pipelining",
  abstract     = "Accurate branch prediction is required to achieve high
    performance in deeply pipelined, wide-issue processors.  Recent studies
    have shown that conditional and indirect (or computed) branch targets can
    be accuratelypredicted by recording the path, which consists of the target
    addresses of recent branches, leading up to the branch.  In current path
    based branch predictors, the N most recent target addresses are hashed
    together to form an index into a table, where N is some fixed integer.  The
    indexed table entry isused to make a prediction for the current branch.This
    paper introduces a new branch predictor in which the value of N is allowed
    to vary.  By constructing the index into the table using the last N target
    addresses, and using profiling information to select the proper value of N
    for each branch, extremely accurate branch prediction is achieved.  For the
    SPECint95 gee benchmark, this new predictor has a conditional branch
    misprediction rate of 4.3% given a 4K byte hardware budget.  For
    comparison, the gshare predictor, a predictor known for its high accuracy,
    has a conditional branch misprediction rate of 8.8% given the same hardware
    budget.  For the indirect branches in gee, the new predictor achieves a
    misprediction rate of 27.7% when given a hardware budget of 512 bytes,
    whereas the best competingpredictor achieves a misprediction rate of 44.2%
    when given the same hardware budget.", 
  location     = "https://doi.org/10.1145/291006.291042"
}

@Article{pisaiismm,
  author       = "Ben Verghese and Anoop Gupta and Mendel Rosenblum",
  title        = "Performance Isolation:  Sharing and Isolation in Shared-Memory Multiprocessors",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "181--192",
  month        = oct,
  keywords     = "performance isolation, resource sharing, group resource
    management", 
  abstract     = "Shared-memory multiprocessors (SMPs) are being extensively
    used as general-purpose servers.  The tight coupling of multiple
    processors, memory, and I/O provides enormous computing power in a single
    system, and enables the efficient sharing of these resources.The operating
    systems for these machines (UNIX or Windows NT) provide very few controls
    for sharing the resources of the system among the active tasks or users.
    This unconstrained sharing model is a serious limitation for a server
    because the load placed by one user can adversely affect other users'
    performance in an unpredictable manner.  We show that this lack of
    isolation is caused by the resource allocation scheme (or lack thereof)
    carried over from singleuser workstations.  Multi-user multiprocessor
    systems require more sophisticated resource management, and we show how the
    proposed 'performance isolation' scheme can address the current weaknesses
    of these systems.  We have implemented performance isolation in the Silicon
    Graphics IRIX operating system for three important system resources: CPU
    time, memory, and disk bandwidth.  Running a number of workloads we show
    that our proposed scheme is successful at providing workstation-like
    isolation under heavy load, SMP-like latency under light load, and SMP-like
    throughput in all cases.", 
  location     = "https://doi.org/10.1145/291006.291044"
}

@Article{uamfatoni,
  author       = "Yuqun Chen and Angelos Bilas and Stefanos~N. Damianakis and Cezary Dubnicki and Kai Li",
  title        = "{UTLB}: {A} Mechanism for Address Translation on Network Interfaces",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "193--204",
  month        = oct,
  keywords     = "dma, caching",
  abstract     = "An important aspect of a high-speed network system is the
    ability to transfer data directly between the network interface and
    application buffers.  Such a direct data path requires the network
    interface to 'know' the virtual-to-physical address translation of a user
    buffer, i.e., the physical memory location of the buffer.  This paper
    presents an efficient address translation architecture, User-managed TLB
    (UTLB), which eliminates system calls and device interrupts from the common
    communication path.  UTLB also supports application-specific policies to
    pin and unpin application memory.  We report micro-benchmark results for an
    implementation on Myrinet PC clusters.  A trace-driven analysis is used to
    compare the UTLB approach with the interrupt-based approach.  It is also
    used to study the effects of UTLB cache size, associativity, and
    prefetching.  Our results show that the UTLB approach delivers robust
    performance with relatively small translation cache sizes.", 
  location     = "https://doi.org/10.1145/291006.291046", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-580-98"
}

@Article{lardicbns,
  author       = "Vivek~S. Pai and Mohit Aron and Gaurov Banga and Michael Svendsen and Peter Druschel and Willy Zwaenepoel and Erich Nahum",
  title        = "Locality-Aware Request Distribution in Cluster-Based Network Servers",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "205--216",
  month        = oct,
  keywords     = "load balancing, task assignment, cache locality, tcp
    connection hand-offs",
  abstract     = "We consider cluster-based network servers in which a
    front-end directs incoming requests to one of a number of back-ends.
    Specifically, we consider content-based request distribution: the front-end
    uses the content requested, in addition to information about the load on
    the back-end nodes, to choose which back-end will handle this request.
    Content-based request distribution can improve locality in the back-ends'
    main memory caches, increase secondary storage scalability by partitioning
    the server's database, and provide the ability to employ back-end nodes
    that are specialized for certain types of requests.As a specific policy for
    content-based request distribution, we introduce a simple, practical
    strategy for locality-aware request distribution (LARD).  With LARD, the
    front-end distributes incoming requests in a manner that achieves high
    locality in the back-ends' main memory caches as well as load balancing.
    Locality is increased by dynamically subdividing the server's working set
    over the back-ends.  Trace-based simulation results and measurements on a
    prototype implementation demonstrate substantial performance improvements
    over state-of-the-art approaches that use only load information to
    distribute requests.  On workloads with working sets that do not fit in a
    single server node's main memory cache, the achieved throughput exceeds
    that of the state-of-the-art approach by a factor of two to four.With
    content-based distribution, incoming requests must be handed off to a
    back-end in a manner transparent to the client, after the front-end has
    inspected the content of the request.  To this end, we introduce an
    efficient TCP handoflprotocol that can hand off an established TCP
    connection in a client-transparent manner.", 
  location     = "https://doi.org/10.1145/291006.291048", 
  location     = "http://www.cs.rice.edu/~alc/comp520/papers/p205-pai.pdf"
}

@Article{iolmp,
  author       = "Olivier Temam",
  title        = "Investigating Optimal Local Memory Performance",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "218--227",
  month        = oct,
  keywords     = "locality, register managementb",
  abstract     = "Recent work has demonstrated that, cache space is often
    poorly utilized.  However, no previous work has yet demonstrated upper
    bounds on what a cache or local memory could achieve when exploiting both
    spatial and temporal locality.  Belady's MIN algorithm does yield an upper
    bound, but exploits only temporal locality.  In this article, we present an
    optimal replacement algorithm for local memory that exploits temporal
    locality and spatial locality simultaneously.  This algorithm is an
    extension of Belady's algorithm.  We prove the optimality of this new
    algorithm with respect to minimizing misses, and we show experimentally
    that the algorithm produces nearly minimum memory traffic on the SPEC95
    benchmarks.  Like Belady's algorithm, our algorithm requires the entire
    program trace.  It selects replacement victims and the number of words it
    fetches at once based on future accesses.  Many different spatial locality
    strategies can be implemented with this algorithm.  With an optimal
    strategy, the algorithm yields an upper bound that enables us to evaluate
    alternative implementations to today's caches.  We further demonstrate the
    utility of this algorithm as an analysis tool by evaluating several
    intermediate strategies between cache and optimal to highlight the
    limitations of the cache line paradigm using the SPEC95 benchmarks.", 
  location     = "https://doi.org/10.1145/291069.291050", 
  location     = "http://pages.saclay.inria.fr/olivier.temam/files/eval/Tem98.pdf"
}

@Article{ccdp,
  author       = "Brad Calder and Chandra Krintz and Simmi John and Todd Austin",
  title        = "Cache-Conscious Data Placement",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "139--149",
  month        = oct,
  keywords     = "data placement, cache conflicts, data caches",
  abstract     = "As the gap between memory and processor speeds continues to
    widen, cache eficiency is an increasingly important component of processor
    performance.  Compiler techniques have been used to improve instruction
    cache pet$ormance by mapping code with temporal locality to different cache
    blocks in the virtual address space eliminating cache conflicts.  These
    code placement techniques can be applied directly to the problem of placing
    data for improved data cache pedormance.In this paper we present a general
    framework for Cache Conscious Data Placement.  This is a compiler directed
    approach that creates an address placement for the stack (local variables),
    global variables, heap objects, and constants in order to reduce data cache
    misses.  The placement of data objects is guided by a temporal relationship
    graph between objects generated via profiling.  Our results show that
    profile driven data placement significantly reduces the data miss rate by
    24% on average.", 
  location     = "https://doi.org/10.1145/384265.291036", 
  location     = "https://sites.cs.ucsb.edu/~ckrintz/abstracts/ASPLOS-98-CCDP.html"
}

@Article{cdmrbwact,
  author       = "Jih-Kwon Peir and Yongjoon Lee and Windsor~W. Hsu",
  title        = "Capturing Dynamic Memory Reference Behavior with Adaptive Cache Topology",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "240--250",
  month        = oct,
  keywords     = "caching, adaptive caching, data prefetch, group-associative
    caches", 
  abstract     = "Memory references exhibit locality and are therefore not
    uniformly distributed across the sets of a cache.  This skew reduces the
    effectiveness of a cache because it results in the caching of a
    considerable number of less-recently-used lines which are less likely to be
    re-referenced before they are replaced.  In this paper, we describe a
    technique that dynamically identifies these less-recently-used lines and
    effectively utilizes the cache frames they occupy to more accurately
    approximate the global least-recently-used replacement policy while
    maintaining the fast access time of a direct-mapped cache.  We also explore
    the idea of using these underutilized cache frames to reduce cache misses
    through data prefetching.  In the proposed design, the possible locations
    that a line can reside in is not predetermined.  Instead, the cache is
    dynamically partitioned into groups of cache lines.  Because both the total
    number of groups and the individual group associativity adapt to the
    dynamic reference pattern, we call this design the adaptive
    group-associative cache.  Performance evaluation using trace-driven
    simulations of the TPC-C benchmark and selected programs from the SPEC95
    benchmark suite shows that the group-associative cache is able to achieve a
    hit ratio that is consistently better than that of a 4-way set-associative
    cache.  For some of the workloads, the hit ratio approaches that of a
    fully-associative cache.", 
  location     = "https://doi.org/10.1145/291006.291053"
}

@Article{pmafptwcoaa,
  author       = "Somnath Ghosh and Margaret Martonosi and Sharad Malik",
  title        = "Precise Miss Analysis for Program Transformations with Caches of Arbitrary Associativity",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "228--239",
  month        = oct,
  keywords     = "the cache miss equation, cold misses, set-associative caches,
    parametric solutions",
  abstract     = "Analyzing and optimizing program memory performance is a
    pressing problem in high-performance computer architectures.  Currently,
    software solutions addressing the processor-memory performance gap include
    compiler-or programmer-applied optimizations like data structure padding,
    matrix blocking, and other program transformations.  Compiler optimization
    can be effective, but the lack of precise analysis and optimization
    frameworks makes it impossible to confidently make optimal, rather than
    heuristic-based, program transformations.  Imprecision is most problematic
    in situations where hard-to-predict cache conflicts foil heuristic
    approaches.  Furthermore, the lack of a general framework for compiler
    memory performance analysis makes it impossible to understand the combined
    effects of several program transformations.The Cache Miss Equation (CME)
    framework discussed in this paper addresses these issues.  We express
    memory reference and cache conflict behavior in terms of sets of equations.
    The mathematical precision of CMEs allows us to find true optimal solutions
    for transformations like blocking or padding.  The generality of CMEs also
    allows us to reason about interactions between transformations applied in
    concert.  Unlike our prior work, this framework applies to caches of
    arbitrary associativity.  This paper also demonstrates the utility of CMEs
    by presenting precise algorithms for intra-variable padding, inter-variable
    padding, and selecting tile sizes.  Our experiences with CMEs implemented
    in the SUIF system show that they are a unifying mathematical framework
    offering the generality and precision imperative for compiler optimizations
    on current high-performance architectures.", 
  location     = "http://www.princeton.edu/~mrm/papers/ghosh_asplos98.ps", 
  location     = "https://doi.org/10.1145/291006.291051"
}

@Article{pcassaauattl,
  author       = "Boris Weissman",
  title        = "Performance Counters and State Sharing Annotations:  a Unified Approach to Thread Locality",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "127--138",
  month        = oct,
  keywords     = "shared state, caching, scheduling policies, cache reload
    ratio",
  abstract     = "This paper describes a combined approach for improving thread
    locality that uses the bardware performance monitors of modem processors
    and program-centric code annotations to guide thread scheduling on SMPs.
    The approach relies on a shared state cache model to compute expected
    thread footprints in the cache on-line.  The accuracy of the model has been
    analyzed by simmations involving a set of parallel applications.  We
    demonstrate how the cache model can be used to implement several practical
    locality-based thread scheduling policies with little overhead.  Active
    Threads, a portable, high-performance thread system, has been built and
    used to investigate the performance impact of locality scheduling for
    several applications.", 
  location     = "https://doi.org/10.1145/291069.291035"
}

@Article{dbpflds,
  author       = "Amir Roth and Andreas Moshovos and Gurindar~S. Sohi",
  title        = "Dependence Based Prefetching for Linked Data Structures",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "115--126",
  month        = oct,
  keywords     = "https://doi.org/10.1145/291069.291034",
  abstract     = "We introduce a dynamic scheme that captures the
    accesspat-terns of linked data structures and can be used to predict future
    accesses with high accuracy.  Our technique exploits the dependence
    relationships that exist between loads that produce addresses and loads
    that consume these addresses.  By identzj+ing producer-consumer pairs, we
    construct a compact internal representation for the associated structure
    and its traversal.  To achieve a prefetching eflect, a small prefetch
    engine speculatively traverses this representation ahead of the executing
    program.  Dependence-based prefetching achieves speedups of up to 25% on a
    suite of pointer-intensive programs.", 
  location     = "https://doi.org/10.1145/291069.291034"
}

@Article{hstoiadriotrmh,
  author       = "Philip Machanick and Pierre Salverda and Lance Pompe",
  title        = "Hardware-Software Trade-Offs in a Direct {Rambus} Implementation of the {RAMpage} Memory Hierarchy",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "105--114",
  month        = oct,
  keywords     = "sram, dram, system architectures, caching, context switches, ",
  abstract     = "The RAMpage memory hierarchy is an alternative to the
    traditional division between cache and main memory: main memory is moved up
    a level and DRAM is used as a paging device.  The idea behind RAMpage is to
    reduce hardware complexity, if at the cost of software complexity, with a
    view to allowing more flexible memory system design.  This paper
    investigates some issues in choosing between RAMpage and a
    conventionalcache architecture, with a view to illustrating trade-offs
    which can be made in choosing whether to place complexity in the memory
    system in hardware or in software.  Performance results in this paper are
    based on a simple Rambus implementation of DRAM, with performance
    characteristics of Direct Rambus, which should be available in 1999.  This
    paper explores the conditions under which it becomes feasible to perform a
    context switch on a miss in the RAMpage model, and the conditions under
    which RAMpage is a win over a conventional cache architecture: as the
    CPU-DRAM speed gap grows, RAMpage becomes more viable.", 
  location     = "https://doi.org/10.1145/384265.291032"
}

@Article{ccmkdc,
  author       = "Keith~D. Cooper and Timothy~J. Harvey",
  title        = "Compiler-Controlled Memory",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "2--11",
  month        = oct,
  keywords     = "register spilling, dsp architectures, register coloring,
    scratch storage, ",
  abstract     = "Optimizations aimed at reducing the impact of memory
    operations on execution speed have long concentrated on improving cache
    performance.  These efforts achieve a.  reasonable level of success.  The
    primary limit on the compiler's ability to improve memory behavior is its
    imperfect knowledge about the run-time behavior of the program.  The
    compiler cannot completely predict runtime access patterns.There is an
    exception to this rule.  During the register allocation phase, the compiler
    often must insert substantial amounts of spill code; that is, instructions
    that move values from registers to memory and back again.  Because the
    compiler itself inserts these memory instructions, it has more knowledge
    about them than other memory operations in the program.Spill-code
    operations are disjoint from the memory manipulations required by the
    semantics of the program being compiled, and, indeed, the two can interfere
    in the cache.  This paper proposes a hardware solution to the problem of
    increased spill costs---a small compiler-controlled memory (CCM) to hold
    spilled values.  This small random-access memory can (and should) be placed
    in a distinct address space from the main memory hierarchy.  The compiler
    can target spill instructions to use the CCM, moving most compiler-inserted
    memory traffic out of the pathway to main memory and eliminating any impact
    that those spill instructions would have on the state of the main memory
    hierarchy.  Such memories already exist on some DSP microprocessors.  Our
    techniques can be applied directly on those chips.This paper presents two
    compiler-based methods to exploit such a memory, along with experimental
    results showing that speedups from using CCM may be sizable.  It shows that
    using the register allocation's coloring paradigm to assign spilled values
    to memory can greatly reduce the amount of memory required by a program.", 
  location     = "https://doi.org/10.1145/291069.291010"
}

@Article{shobrbal,
  author       = "Matthew~L. Seidl and Benjamin~G. Zorn",
  title        = "Segregating Heap Objects by Reference Behavior and Lifetime",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "12--23",
  month        = oct,
  keywords     = "heap storage, use prediction, virtual storage, access",
  abstract     = "Dynamic storage allocation has become increasingly important
    in many applications, in part due to the use of the object-oriented
    paradigm.  At the same time, processor speeds are increasing faster than
    memory speeds and programs are increasing in size faster than memories.  In
    this paper, we investigate efforts to predict heap object reference and
    lifetime behavior at the time objects are allocated.  Our approach uses
    profile-based optimization, and considers a variety of different
    information sources present at the time of object allocation to predict the
    object's reference frequency and lifetime.  Our results, based on
    measurements of six allocation intensive programs, show that program
    references to heap objects are highly predictable and that our prediction
    methods can successfully predict the behavior of these heap objects.  We
    show that our methods can decrease the page fault rate of the programs
    measured, sometimes dramatically, in cases where the physical memory
    available to the program is constrained.", 
  location     = "https://doi.org/10.1145/384265.291012", 
  location     = "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/asplos8-seg-objects.pdf"
}

@Article{sismfl,
  author       = "Michelle Mills Strout and Larry Carter and Jeanne Ferrante and Beth Simon",
  title        = "Schedule-Independent Storage Mapping for Loops",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "24--33",
  month        = oct,
  keywords     = "storage reuse, universal occupancy vectors, branch and bound
    search, storage mapping",
  abstract     = "This paper studies the relationship between storage
    requirements and performance.  Storage-related dependences inhibit
    optimizations for locality and parallelism.  Techniques such as renaming
    and array expansion can eliminate all storage-related dependences, but do
    so at the expense of increased storage.  This paper introduces the
    universal occupancy vector (UOV) for loops with a regular stencil of
    dependences.  The UOV provides a schedule-independent storage reuse pattern
    that introduces no further dependences (other than those implied by true
    flow dependences).  OV-mapped code requires less storage than full array
    expansion and only slightly more storage than schedule-dependent minimal
    storage.We show that determine if a vector is a UOV is NPcomplete.
    However, an easily constructed but possibly nonminimal UOV can be used.  We
    also present a branch and bound algorithm which finds the minimal UOV,
    while still maintaining a legal UOV at all times.Our experimental results
    show that the use of OV-mapped storage, coupled with tiling for locality,
    achieves better performance than tiling after array expansion, and
    accommodates larger problem sizes than untilable, storage-optimized code.
    Furthermore, storage mapping based on the UOV introduces negligible
    runtime overhead.", 
  location     = "https://doi.org/10.1145/291006.291015",
  location     = "http://cgi.cs.arizona.edu/~mstrout/Papers/Papers98-03/asplos98pp.pdf"
}

@Article{aeaoir,
  author       = "Avinash Sodani and Gurindar~S. Sohi",
  title        = "An Empirical Analysis of Instruction Repetition",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "35--45",
  month        = oct,
  keywords     = "instruction repetition, function level analysis, local
    analysis",
  abstract     = "We study the phenomenon of instruction repetition, where the
    inputs and outputs of multiple dynamic instances of a static instruction
    are repeated.  We observe that over 80% of the dynamic instructions
    executed in several programs are repeated and most of the repetition is due
    to a small number of static instructions.  We attempt understand the source
    of this repetitive behavior by categorizing dynamic program instructions
    into dynamic program slices at both a global level and a local (within
    function) level.  We observe that repeatability is more an artifact of how
    computation is expressed, and less of program inputs.  Function-level
    analysis suggests that many functions are called with repeated arguments,
    though almost all of them have side effects.  We provide commentary on
    exploiting the observed phenomenon and its sources in both software and
    hardware.", 
  location     = "https://doi.org/10.1145/291069.291016", 
  location     = "ftp://ftp.cs.wisc.edu/sohi/papers/1998/asplos-inst-repeat.pdf"
}

@Article{stsoilpoarm,
  author       = "Walter Lee and Rajeev Barua and Matthew Frank and Devabhaktuni Srikrishna and Jonathan Babb and Vivek Sarkar and Saman Amarasinghe",
  title        = "Space-Time Scheduling of Instruction-Level Parallelism on a Raw Machine",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "46--57",
  month        = oct,
  keywords     = "scalability, space-time scheduling, basic blocks, control
  orchestration",
  abstract     = "Increasing demand for both greater parallelism and faster
    clocks dictate that future generation architectures will need to
    decentralize their resources and eliminate primitives that require single
    cycle global communication.  A Raw microprocessor distributes all of its
    resources, including instruction streams, register files, memory ports, and
    ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them
    fully to the compiler.  Because communication in Raw machines is
    distributed, compiling for instruction-level parallelism (ILP) requires
    both spatial instruction partitioning as well as traditional temporal
    instruction scheduling.  In addition, the compiler must explicitly manage
    all communication through the interconnect, including the global
    synchronization required at branch points.  This paper describes RAWCC, the
    compiler we have developed for compiling general-purpose sequential
    programs to the distributed Raw architecture.  We present performance
    results that demonstrate that although Raw machines provide no mechanisms
    for global communication the Raw compiler can schedule to achieve speedups
    that scale with the number of available functional units.", 
  location     = "https://doi.org/10.1145/291069.291018",
  location     = "https://groups.csail.mit.edu/cag/raw/documents/Lee-ASPLOS-1998.pdf"
}

@Article{dssfacm,
  author       = "Lance Hammond and Mark Willey and Kunle Olukotun",
  title        = "Data Speculation Support for a Chip Multiprocessor",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "58--69",
  month        = oct,
  keywords     = "chip multiprocessors, data speculation, speculative threads,
    loop iterations, precise exceptions, caching",
  abstract     = "Thread-level speculation is a technique that enables parallel
    execution of sequential applications on a multiprocessor.  This paper
    describes the complete implementation of the support for thread-level
    speculation on the Hydra chip multiprocessor (CMP).  The support consists
    of a number of software speculation control handlers and modifications to
    the shared secondary cache memory system of the CMP This support is
    evaluated using five representative integer applications.  Our results show
    that the speculative support is only able to improve performance when there
    is a substantial amount of medium--grained loop-level parallelism in the
    application.  When the granularity of parallelism is too small or there is
    little inherent parallelism in the application, the overhead of the
    software handlers overwhelms any potential performance benefits from
    speculative-thread parallelism.  Overall, thread-level speculation still
    appears to be a promising approach for expanding the class of applications
    that can be automatically parallelized, but more hardware intensive
    implementations for managing speculation control are required to achieve
    performance improvements on a wide class of integer applications.", 
  location     = "https://doi.org/10.1145/291006.291020", 
  location     = "http://arsenalfc.stanford.edu/kunle/publications/hydra_ASPLOS98.pdf"
}

@Article{vnsvisa,
  author       = "Rodney Van Meter and Gregory~G. Finn and Steve Hotz",
  title        = "{VISA}: Netstation's Virtual Internet {SCSI} Adapter",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "71--80",
  month        = oct,
  keywords     = "network area peripherals, networking protocols",
  abstract     = "In this paper we describe the implementation of VISA, our
    Virtual Internet SCSI Adapter.  VISA was built to evaluate the performance
    impact on the host operating system of using IP to communicate with
    peripherals, especially storage devices.  We have built and benchmarked
    file systems on VISA-attached emulated disk drives using UDP/IP.  By using
    IP, we expect to take advantage of its scaling characteristics and support
    for heterogeneous media to build large, long-lived systems.  Detailed file
    system and network CPU utilization and performance data indicate that it is
    possible for UDP/IP to reach more than 80% of SCSI's maximum throughput
    without the use of network coprocessors.  We conclude that IP is a viable
    alternative to special-purpose storage network protocols, and presents
    numerous advantages.", 
  location     = "https://doi.org/10.1145/291006.291023", 
  location     = "http://www.isi.edu/division7/netstation/visa.share.ps"
}

@Article{adpmaae,
  author       = "Anurag Acharya and Mustafa Uysal and Joel Saltz",
  title        = "Active Disks:  Programming Model, Algorithms and Evaluation",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "81--91",
  month        = oct,
  keywords     = "distributed disks, active disks, scalability",
  abstract     = "Several application and technology trends indicate that it
    might be both profitable and feasible to move computation closer to the
    data that it processes.  In this paper, we evaluate Active Disk
    architectures which integrate significant processing power and memory into
    a disk drive and allow application-specific code to be downloaded and
    executed on the data that is being read from (written to) disk.  The key
    idea is to offload bulk of the processing to the diskresident processors
    and to use the host processor primarily for coordination, scheduling and
    combination of results from individual disks.  To program Active Disks, we
    propose a stream-based programming model which allows disklets to be
    executed efficiently and safely.  Simulation results for a suite of six
    algorithms from three application domains (commercial data warehouses,
    image processing and satellite data processing) indicate that for these
    algorithms, Active Disks outperform conventional-disk architectures.", 
  location     = "https://doi.org/10.1145/384265.291026", 
  location     = "http://www.cs.umd.edu/~hollings/cs818z/s99/papers/activeDisks.pdf"
}

@Article{acehbsa,
  author       = "Garth~A. Gibson and David~F. Nagle and Khalil Amiri and Jeff Butler and Fay~W. Chang and Howard Gobioff and Charles Hardin and Erik Riedel and David Rochberg and Jim Zelenka",
  title        = "{A} Cost-Effective, High-Bandwidth Storage Architecture",
  journal      = asplos98,
  year         = 1998,
  volume       = 33,
  number       = 11,
  pages        = "92--103",
  month        = oct,
  keywords     = "network attached disks, scalability, file systems, parallel
    file systems",
  abstract     = "This paper describes the Network-Attached Secure Disk (NASD)
    storage architecture, prototype implementations oj NASD drives, array
    management for our architecture, and three, filesystems built on our
    prototype.  NASD provides scalable storage bandwidth without the cost of
    servers used primarily, for transferring data from peripheral networks
    (e.g.  SCSI) to client networks (e.g.  ethernet).  Increasing datuset
    sizes, new attachment technologies, the convergence of peripheral and
    interprocessor switched networks, and the increased availability of
    on-drive transistors motivate and enable this new architecture.  NASD is
    based on four main principles: direct transfer to clients, secure
    interfaces via cryptographic support, asynchronous non-critical-path
    oversight, and variably-sized data objects.  Measurements of our prototype
    system show that these services can be cost-effectively integrated into a
    next generation disk drive ASK.  End-to-end measurements of our prototype
    drive andfilesysterns suggest that NASD cun support conventional
    distributed filesystems without performance degradation.  More importantly,
    we show scaluble bandwidth for NASD-specialized filesystems.  Using a
    parallel data mining application, NASD drives deliver u linear scaling of
    6.2 MB/s per clientdrive pair, tested with up to eight pairs in our lab.", 
  location     = "https://doi.org/10.1145/384265.291029",
  location     = "https://www.pdl.cmu.edu/PDL-FTP/NASD/asplos98.pdf"
}

@Article{ioa,
  author       = "Barbara Liskov and Dorothy Curtis and Paul Johnson and Robert Scheifer",
  title        = "Implementation of {Argus}",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  month        = nov,
  pages        = "111--122",
  keywords     = "guardians, actions, nested actions, performance",
  abstract     = "Argus is a programming language and system developed to
    support constructing and executing distributed programs.  This paper
    describes the Argus implementation, emphasizing the way we implement
    atomic actions, because this is where Argus differs most from other
    implemented systems.  The paper also discusses Argus performance.  The cost
    of actions is quite reasonable, indicating that action systems like Argus
    are practical.",
  location     = "https://doi.org/10.1145/41457.37514"
}

@Article{evsids,
  author       = "Kenneth~P. Birman and Thomas~A. Joseph",
  title        = "Exploiting Virtual Synchrony in Distributed Systems",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  month        = nov,
  pages        = "123--138",
  keywords     = "broadcast, group management, distributed systems, virtual
    synchrony, atomic multicast, group rpc, failure, recovery, toolkits",
  abstract     = "We describe applications of a virtually synchronous
    environment for distributed programming, which underlies a collection of
    distributed programming tools in the ISIS2 system.  A virtually synchronous
    environment allows processes to be structured into process groups, and
    makes events like broadcasts to the group as an entity, group membership
    changes, and even migration of an activity from one place to another appear
    to occur instantaneously — in other words, synchronously.  A major
    advantage to this approach is that many aspects of a distributed
    application can be treated independently without compromising correctness.
    Moreover, user code that is designed as if the system were synchronous can
    often be executed concurrently.  We argue that this approach to building
    distributed and fault-tolerant software is more straightforward, more
    flexible, and more likely to yield correct solutions than alternative
    approaches.", 
  location     = "https://doi.org/10.1145/37499.37515", 
  location     = "https://www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf"
}

@Article{lfaefsewos,
  author       = "Ross~S. Finlayson and David~R. Cheriton",
  title        = "Log Files:  An Extended File Service Exploiting Write-Once Storage",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  month        = nov,
  pages        = "139--148",
  keywords     = "log file services, write-once storage, fault tolerance,
    history-based applications",
  abstract     = "A log service provides efficient storage and retrieval of
    data that is written sequentially (append-only) and not subsequently
    modified.  Application programs and subsystems use log services for
    recovery, to record security audit trails, and for performance monitoring.
    Ideally, a log service should accommodate very large, long-lived logs, and
    provide efficient retrieval and low space overhead.In this paper, we
    describe the design and implementation of the Clio log service.  Clio
    provides the abstraction of log files: readable, append-only files that are
    accessed in the same way as conventional files.  The underlying storage
    medium is required only to be append-only; more general types of write
    access are not necessary.  We show how log files can be implemented
    efficiently and robustly on top of such storage media—in particular,
    write-once optical disk.In addition, we describe a general application
    software storage architecture that makes use of log files.", 
  location     = "http://i.stanford.edu/pub/cstr/reports/cs/tr/87/1177/CS-TR-87-1177.pdf", 
  location     = "https://doi.org/10.1145/37499.37516"
}

@Article{asaeifsd,
  author       = "Andrew~D. Birrell and Michael~B. Jones and Edward~P. Wobber",
  title        = "{A} Simple and Efficient Implementation for Small Databases",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  month        = nov,
  pages        = "149--154",
  keywords     = "locking, reliability",
  abstract     = "This paper describes a technique for implementing the sort of
    small databases that frequently occur in the design of operating systems
    and distributed systems.  We take advantage of the existence of very large
    virtual memories, and quite large real memories, to make the technique
    feasible.  We maintain the database as a strongly typed data structure in
    virtual memory, record updates incrementally on disk in a log and
    occasionally make a checkpoint of the entire database.  We recover from
    crashes by restoring the database from an old checkpoint then replaying the
    log.  We use existing packages to convert between strongly typed data
    objects and their disk representations, and to communicate strongly typed
    data across the network (using remote procedure calls).  Our memory is
    managed entirely by a general purpose allocator and garbage collector.This
    scheme has been used to implement a name server for a distributed system.
    The resulting implementation has the desirable property of being
    simultaneously simple, efficient and reliable.", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-24.html"
}

@Article{uiwiasce,
  author       = "David~A. Nichols",
  title        = "Using Idle Workstations in a Shared Computing Environment",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  pages        = "5--12",
  month        = nov,
  keywords     = "process invocation, remote execution, floating servers",
  abstract     = "The Butler system is a set of programs running on Andrew
    workstations at CMU that give users access to idle workstations.  Current
    Andrew users use the system over 300 times per day.  This paper describes
    the implementation of the Butler system and tells of our experience in
    using it.  In addition, it describes an application of the system known as
    gypsy servers, which allow network server programs to be run on idle
    workstations instead of using dedicated server machines.", 
  location     = "https://doi.org/10.1145/41457.37502"
}

@Article{rtcfsulagc,
  author       = "Robert Hagmann",
  title        = "Reimplementing the {Cedar} File System Using Logging and Group Commit",
  journal      = sosp87,
  year         = 1987,
  volume       = 21,
  number       = 5,
  pages        = "155--162",
  month        = nov,
  keywords     = "file systems, metadata management, page allocation,
    robustness",
  abstract     = "The workstation file system for the Cedar programming
    environment was modified to improve its robustness and performance.
    Previously, the file system used hardware-provided labels on disk blocks to
    increase robustness against hardware and software errors.  The new system
    does not require hardware disk labels, yet is more robust than the old
    system.  Recovery is rapid after a crash.  The performance of operations on
    file system metadata, e.g., file creation or open, is greatly improved.The
    new file system has two features that make it atypical.  The system uses a
    log, as do most database systems, to recover metadata about the file
    system.  To gain performance, it uses group commit, a concept derived from
    high performance database systems.  The design of the system used a simple,
    yet detailed and accurate, analytical model to choose between several
    design alternatives in order to provide good disk performance.", 
  location     = "https://doi.org/10.1145/41457.37518"
}

@TechReport{apfdsifcip,
  author       = "Joel Moses",
  title        = "{A} Program for Drilling Students in Freshman Calculus Integration Problems",
  institution  = "Artificial Intelligence Project, Project MAC, " # mit,
  year         = 1968,
  type         = "Memo",
  number       = 158,
  address      = cma,
  month        = mar,
  keywords     = "integration, problem solving, testing",
  abstract     = "The SARGE program is a prototype of a program which is
    intended to be used as an adjacent to regular classroom work in freshman
    calculus.  Using SARGE, students can type their step-by-step solution to an
    indefinite integration problem, and can have the correctness of their
    solution determined by the system.  The syntax for these steps comes quite
    close to normal mathematical notation, given the limitations of typewriter
    input.  The methods of solution is pretty much unrestricted as long as no
    mistakes are made along the way.  If a mistake is made, SARGE will catch it
    and yield an error message.  The student may modify the incorrect step, or
    he may ask the program for advice on how the mistake arose by typing
    'help'.  At present the program is weak in generating explanations for
    mistakes.  Sometimes the 'help' mechanisms will just yield a response which
    will indicate the way in which the erroneous step can be corrected.  In
    order to improve the explanation mechanism one would need a sophisticated
    analysis of students solutions to homework or quiz problems.  Experience
    with the behavior of students with SARGE, which is nil at present, should
    also help in accomplishing this goal.  SARGE is available as SARGE SAVED in
    T302 2517.", 
  location     = "https://dspace.mit.edu/bitstream/handle/1721.1/6163/AIM-158.pdf"
}

@TechReport{atsij,
  author       = "Christian Heinlein",
  title        = "Advanced Thread Synchronization in {Java}",
  institution  = "Dept. of Computer Structures, University of Ulm",
  year         = 2002,
  address      = "Ulm, Germany",
  keywords     = "bounded buffers, interaction expressions, readers-writers
    problem, synchronization, threads, java",
  abstract     = "Thread synchronization in Java using synchronized methods or
    statements is simple and straightforward as long as mutual exclusion of
    threads is sufficient for an application.  Things become less
    straightforward when wait() andnotify() have to be employed to realize more
    flexible synchronization schemes.  Using two well-known examples, the
    bounded buffer and the readers and writers problem, the traps and snares of
    hand-coded synchronization code and its entanglement with the actual
    application code are illustrated.  Following that, interaction expressions
    are introduced as a completely different approach where synchronization
    problems are solved in a declarative way by simply specifying permissible
    execution sequences of methods.  Their integration into the Java
    programming language using a simple precompiler and the basic ideas to
    enforce at run time the synchronization constraints specified that way are
    described.", 
  location     = "https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.1007%2F3-540-36557-5_25"
}

@TechReport{agifc,
  author       = "Lorenzo Alvisi and Fred~B. Schneider",
  title        = "{A} Graphical Interface for {CHIP}",
  institution  = dcs # "Cornell University",
  year         = 1996,
  number       = 1591,
  address      = itny,
  month        = "6 " # jun,
  keywords     = "chip, debugging, hypothetical processors",
  abstract     = "CHIP (Cornell Hypothetical Instructional Processor) [BBDS83]
    is a computer system designed as an educational tool for teaching
    undergraduate courses in operating system and machine architecture.  This
    document describes CHIP's graphical interface and covers in a tutorial how
    the interface is used to debug and execute CHIP programs.  A Graphical
    Interface for CHIP Lorenzo Alvisi The University of Texas at Austin
    Department of Computer Sciences Austin, TX Fred B.  Schneider Cornell
    University Department of Computer Science Ithaca, NY 6 June 1996 Abstract
    CHIP (Cornell Hypothetical Instructional Processor) is a computer system
    designed for use in teaching undergraduate courses in operating system and
    machine architecture.  This document describes CHIP's graphical interface
    and contains a tutorial describing how the interface is used to debug and
    execute CHIP programs.", 
  location     = "https://www.cs.cornell.edu/fbs/publications/96-1591.pdf"
}

@TechReport{rovhcm,
  author       = "Daniel~C. Hyde",
  title        = "Realization of {Verilog HDL} Computation Model",
  institution  = dcs # "Bucknell University",
  year         = 1997,
  month        = oct,
  keywords     = "vhdl, computational model, digital logic circuits"
}

@TechReport{s04d,
  author       = "Michael Dales",
  title        = "{SWARM} 0.44 Documentation",
  institution  = dcs # "University of Glasgow",
  year         = 2000,
  address      = "Glasgow, Scotland",
  month        = nov,
  keywords     = "architecture, compilation",
  location     = "This document gives a brief explanation of the design and
    implementation of SWARM --- the Software ARM.  It explains what SWARM is,
    and what it isn't, along with the design philosophy."
}

@TechReport{soovfs,
  author       = "Timo Lehtinen",
  title        = "Store --- Object-Oriented Virtual File System",
  institution  = "Stream Technologies Inc.",
  year         = 1993,
  address      = "Espoo, Finland",
  month        = "29 " # jul,
  keywords     = "global name spaces, class store, support class"
}

@TechReport{hvavcveaeispp,
  author       = "Paul Hudak and Mark~P. Jones",
  title        = "Haskell vs. {Ada} vs. " # cpp # " vs. \ldots\ An Experiment in Software Prototyping Productivity",
  institution  = dcs # "Yale University",
  year         = 1994,
  number       = 1049,
  address      = nhco,
  month        = "4 " # jul,
  keywords     = "software engineering, software prototyping, software
    development, programming languages",
  abstract     = "We describe the results of an experiment in which several
    conventional programming languages, together with the functional language
    Haskell, were used to prototype a Naval Surface Warfare Center (NSWC)
    requirement for a Geometric Region Server.  The resulting programs and
    development metrics were reviewed by a committee chosen by the Navy.  The
    results indicate that the Haskell prototype took significantly less time to
    develop and was considerably more concise and easier to understand than the
    corresponding prototypes written in several different imperative languages,
    including Ada and C++.", 
  location     = "http://www.cvc.yale.edu/publications/techreports/tr1049.pdf"
}

@TechReport{tbp,
  author       = "Claus Brabrand and Anders Møller and Michael~I. Schwartzbach",
  title        = "The {\tt <bigwig>} Project",
  institution  = dcs # "University of Aarhus",
  year         = 2002,
  number       = "BRICS-RS-00-42",
  address      = "Aarhus, Denmark",
  month        = jan,
  keywords     = "web services, language design, state, session, concurrency
    control, dynamic html, security",
  abstract     = "We present the results of the project, which aims to design
    and implement a high-level domain-specific language for programming
    interactive Web services.  A fundamental aspect of the development of the
    World Wide Web during the last decade is the gradual change from static to
    dynamic generation of Web pages.  Generating Web pages dynamically in
    dialogue with the client has the advantage of providing up-to-date and
    tailor-made information.  The development of systems for constructing such
    dynamic Web services has emerged as a whole new research area.  The
    language is designed by analyzing its application domain and identifying
    fundamental aspects of Web services inspired by problems and solutions in
    existing Web service development languages.  The core of the design
    consists of a session-centered service model together with a flexible
    template-based mechanism for dynamic Web page construction.  Using
    specialized program analyses, certain Web specific properties are verified
    at compile-time, for instance that only valid HTML 4.01 is ever shown to
    the clients.  In addition, the design provides high-level solutions to form
    field validation, caching of dynamic pages, and temporal-logic based
    concurrency control, and it proposes syntax macros for making highly
    domain-specific languages.  The language is implemented via widely
    available Web technologies, such as Apache on the server-side and
    JavaScript and Java Applets on the client-side.  We conclude with
    experience and evaluation of the project", 
  location     = "https://www.brics.dk/RS/02/1/index.html"
}

@TechReport{tppcin,
  author       = "K.~V. Nori and U.~Ammann and K.~Jensen and H.~H. N{\" a}geli",
  title        = "The {PASCAL} `{P}' Compiler:  Implementation Notes",
  institution  = "Institut f{\" u}r Informatik, Technische Hochschule Z{\" u}ich",
  year         = 1974,
  number       = 10,
  address      = "Z{\" u}rich, Switzerland",
  month        = dec,
  keywords     = "pascal, compilation, p-code, code generation",
  abstract     = "The PASCAL 'P' compiler is a portable compiler for a subset
    of 'Standard PASCAL'.  This compiler is written using exactly the subset it
    processes and it generates object code for a hypothetical stack computer.
    This report is a documentation of the stack computer and of the compiler.
    The latter part of the documentation proved to be useful to one of the 
    authors (K. V. Nori) when informally verifying the compiler.", 
  location     = "http://www.standardpascaline.org/p4.html"
}

@TechReport{sttaaas,
  author       = "Philip Greenspun",
  title        = "Scalability, Three-Tiered Architectures, and Application Servers",
  institution  = "ArsDigita Systems Journal",
  keywords     = "three-tiered architectures, scalability, application servers,
    software development",
  abstract     = "Application servers for Web publishing are generally systems
    that let you write database-backed Web pages in Java.  The first problem
    with this idea is that Java, because it must be compiled, is usually a bad
    choice of programming language for Web services.  The second problem is if
    what you really want to do is write some Java code that talks to data in
    your database, you can execute Java in your RDBMS (Oracle 8.1, Informix
    9.x).  Java executing inside the database server's process is always going
    to have faster access to table data than Java running as a client.  In
    fact, at least with Oracle on a Unix box, you could bind Port 80 to a
    program that would call a Java program running in the Oracle RDBMS.  You
    don't even need a Web server, much less an application server.  It is
    possible that you'll get higher performance and easier development by
    adding a thin-layer Web server like AOLserver or Microsoft's IIS/ASP, but
    certainly you can't get higher reliability by adding a bunch of extra
    programs and computers to a system that need only rely on one program and
    one computer.  This document works through some of these issues in greater
    detail, pointing out the grievous flaws in Netscape Application Server
    (formerly 'Kiva') and explaining the situations in which Oracle Application
    Server is useful.", 
  location     = "http://www.eveandersson.com/arsdigita/asj/application-servers"
}

@TechReport{tra,
  author       = "Dale Green",
  title        = "The Reflection {API}",
  institution  = "Oracle",
  year         = 2019,
  keywords     = "java, reflection",
  abstract     = "Reflection is commonly used by programs which require the
    ability to examine or modify the runtime behavior of applications running
    in the Java virtual machine.  This is a relatively advanced feature and
    should be used only by developers who have a strong grasp of the
    fundamentals of the language.  With that caveat in mind, reflection is a
    powerful technique and can enable applications to perform operations which
    would otherwise be impossible.",
  location     = "https://docs.oracle.com/javase/tutorial/reflect/index.html"
}

@TechReport{tfoiw,
  author       = "Jose Nazario and Jeremy Anderson and Rick Wash and Chris Connelly",
  title        = "The Future of Internet Worms",
  institution  = "Crimelabs Research",
  year         = 2001,
  month        = "20 " # jul,
  keywords     = "worms, internet security, ",
  abstract     = "Network worms, simple slang terminology for automated
    intrusion agents, represent a persistent threat to a growing Internet in an
    increasingly networked world.  However, their evolution has been somewhat
    limited, and they still rely on the same basic paradigms, which contain
    fundamental flaws.  We analyze the basic components of a worm and apply
    this analysis to three worms found in the wild on the Internet.  We then
    proceed to analyze the limiting factors of existing worm paradigms and
    outline new ideas which we expect to become prevalent.  These new worms
    will prove to be more diffcult to identify and eradicate.  It is our
    intention in sharing this knowledge to stimulate the development of
    strategies to detect and counteract the threat of smarter network worms.", 
  location     = "http://www.blackhat.com/presentations/bh-usa-01/JoseNazario/bh-usa-01-Joes-Nazario.pdf"
}

@InProceedings{cmultsygbtr,
  author       = "Stanley~P. Hanks",
  title        = "Creating {MAN}s using {LAN} Technology:  Sometimes You Gotta Break the Rules",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "439--451",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "man, lans, bridging, security, encapsulation, fddi, ethernet,
    vpn, standards",
  abstract     = "Commercially available, off-the-shelf internetworking
    products provide good mechanisms for the creation of limited-throughput
    metropolitan and wide area networks.  However, for many applications either
    the throughput obtained from T1 or slower communication circuits is
    inadequate, or the expense of obtaining high throughput using multiple T1
    or whole DS-3 circuits is prohibitive.  Proposed service offerings such as
    802.6, SMDS, frame relay, or SONET promise adequate speed metropolitan area
    network connectivity at a reasonable price.  Today these services arc not
    available, or where they are available are limited to T1 speeds.
    Metropolitan Fiber Systems owns over 17,000 miles of fiber optic cable in
    12 cities.  Most of this capacity is currently devoted to providing leased
    T1 and DS-3 circuits, and a significant portion of those circuits are for
    data transmission.  This provided a unique opportunity to address the
    question of how to provide LAN speed connectivity in and between
    metropolitan areas using commercially available products.  This paper
    discusses the development of the high speed MAN connectivity service
    offerings based on FDDI provided by MFS.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ahotcosalu1tj1,
  author       = "Alan~E. Kaplan",
  title        = "{A} History of the {COSNIX} Operating System: Assembly Language " # unix # " 1971 to {July}, 1991",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "429--437",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "history, fortran, disk performance, database systems",
  abstract     = "From 1971 until July, 1991 a variant of the assembly language
    version of the Unix operating system (Unix-A) was used to run a transaction
    processing system called COSMOS (Computer System for Mainframe Operations)
    in the Regional Bell Operating Companies.  At one time about seven hundred
    such systems were running on PDP 11//45 and PDP 11/70 computers.  This talk
    describes the history and development of that Unix operating system
    variant, called COSNIX, and explains some of the reasons for its success.
    I hope, also, that it gives a feeling of the challenges involved in
    producing a viable system during the days when computing resources were
    much more severely limited than they are today.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{fsfhs,
  author       = "Henry Spencer",
  title        = "Faster String Function",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "419--428",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "string functions, optimization, performance, parallel execution",
  abstract     = "The string functions provided by ANSI C and by traditional
    Unix C libraries are usually not as well-optimized as they could be.
    Careful tuning of inner loops is common, and on some processors it is
    profitable to rewrite them in assembler to exploit special instructions,
    but on most systems operations are still done a character at a time.  Given
    fairly lenient assumptions about the architecture, versions that operate a
    word at a time are possible.  Word-at-a-time processing is superficially
    difficult for C strings, since they arc terminated by a single null that is
    awkward to detect within a word.  However, carefully chosen combinations of
    logical and arithmetic operations can do such detection at a cost of 3-6
    operations per word, depending on data constraints and architecture,
    without relying on any architecture-specific specialized instructions or
    data paths.  This technique has been around as occasionally-heard folklore
    for some time, but does not appear to have been investigated in detail.
    The resulting word-at-a-time string functions are conspicuously faster than
    the usual ones for long strings.  The crossover point is typically 20-30
    characters, and the asymptotic speed advantage can be as much as a factor
    of 5, although a factor of 2-3 is more typical on 100-character operands.
    For specialized requirements where customized interfaces and customized
    code are permissible, rather higher factors are possible.  Certain problems
    occur, notably higher startup overhead, difficulties with unaligned
    strings, and the prevalence of relatively short strings as operands to some
    string functions.  The case for the fast functions is mixed, and an
    adaptive algorithm is needed to maximize overall performance.  It would
    also be useful to package the algorithms for use in custom string code,
    although this is somewhat challenging.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{rsis50,
  author       = "Sandeep Khanna and Michael Sebree and John Zolnowsky",
  title        = "Realtime Scheduling in {SunOS} 5.0",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "375--390",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time, scheduling, kernel threads, interrupts",
  abstract     = "We describe the fundamental mechanisms in SunOS 5.0 to
    provide realtime scheduling functionality.  Our primary goal was to provide
    bounded behavior for dispatching or blocking threads.  To achieve this goal
    we have modified the kernel to be fully preemptive, guaranteeing dispatch
    after both synchronous and asynchronous wakeups.  We have also worked
    toward controlling priority inversion in the kernel.  The result is a
    kernel capable of delivering realtime scheduling and bounded response to a
    large class of user level applications.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cancpmtppl,
  author       = "Sharon Hopkins",
  title        = "Camels and Needles: Computer Poetry Meets the {Perl} Programming Language",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "391--404",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "perl, poetry, free verse",
  abstract     = "Although various forms of literature have been created with
    the assistance of a computer, and even been generated by computer programs,
    it is only recently that literary works have actually been written in a
    computer language.  A computer-language poem need not necessarily produce
    any output, it may succeed merely by fooling the parser into thinking it is
    an ordinary program.  'Ihe Perl programming language has proved well-suited
    to the creation of computer-language poetry.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{3atofs,
  author       = "W.~D. Roome",
  title        = "{3DFS}: {A} Time-Oriented File Server",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "405--418",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, archiving, nfs, backups, optical storage, caching",
  abstract     = "3DFS is a network file server that provides time-oriented
    access to files and directories.  3DFS allows a user to read the version of
    a file as it existed on a particular day in the past, or to list the files
    in a directory on some prior date.  3DFS saves the daily incremental
    backups from other file systems (Sun file servers, Vaxen,...), and creates
    an on-line file system from these dumps.  3DFS uses optical disks in an
    automated jukebox, so no operator intervention is required.  3DFS uses the
    Sun NFS™ protocol, and looks like any other NFS server.  Any UNIX® command
    can read files in 3DFS, and users mount 3DFS just like any file server.
    Because 3DFS provides on-line access to old versions, users can access
    those versions in-place, without copying them to magnetic disk.  This paper
    describes 3DFS, its implementation, and our experience with it.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ntbpnm,
  author       = "Matt Blaze",
  title        = "{NFS} Tracing by Passive Network Monitoring",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "333--343",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "nfs, filesystem performance, network monitoring",
  abstract     = "Traces of filesystem activity have proven to be useful for a
    wide variety of purposes, ranging from quantitative analysis of system
    behavior to trace-driven simulation of filesystem algorithms.  Such traces
    can be difficult to obtain, however, usually entailing modification of the
    filesystems to be monitored and runtime overhead for the period of the
    trace.  Largely because of these difficulties, a surprisingly small number
    of filesystem traces have been conducted, and few sample workloads are
    available to filesystem researchers.  This paper describes a portable
    toolkit for deriving approximate traces of NFS activity by non-intrusively
    monitoring the Ethernet traffic to and from the file server.  The toolkit
    uses a promiscuous Ethernet listener interface (such as the Packetfilter)
    to read and reconstruct NFS-related RPC packets intended for the server.
    It produces traces of the NFS activity as well as a plausible set of
    corresponding client system calls.  The tool is currently in use at
    Princeton and other sites, and is available via anonymous ftp.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{ccfcsius,
  author       = "Joseph~L. Hellerstein",
  title        = "Control Considerations for {CPU} Scheduling in " # unix # " Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "359--374",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "scheduling, priority scheduling, compute-bound jobs",
  abstract     = "Managing UNIX systems often involves setting service rate
    objectives, such as specifying that application A should receive 50% of the
    central processing unit (CPU).  In most UNIX systems, the only way to
    control CPU usage is by adjusting nice values; unfortunately, the
    relationship between nice values and process service rates has been poorly
    understood.  This paper develops an analytic model that relates service
    rate objectives for compute-bound processes to nice values and three
    scheduler parameters: R (the rate at which priority increases for each
    quantum of CPU consumed), D (the decay factor), and T (the number of quanta
    that expire before CPU usages are decayed); the model is evaluated using
    measurements of a workstation running IBM's Advanced Interactive Executive
    (AIX) 3.1 Operating System.  Based on the model, we develop an algorithm
    that calculates nice values that achieve service rate objectives for
    compute-bound processes.  Experiments conducted on a production AIX 3.1
    system suggest that our algorithm works well in practice.  In addition, we
    use the model to obtain insights into the control implications of parameter
    settings.  For example, we show that the nice mechanism is often less
    effective on faster processors since T tends to increase with processor
    speed; this increases the fraction of time during which processes with
    larger nice values execute, and hence limits the extent to which their
    service rates can be controlled.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{atdaonaaciads,
  author       = "Ken~W. Shirriff and John~K. Ousterhout 
",
  title        = "{A} Trace-Driven Analysis of Name and Attribute Caching in a Distributed System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "315--330",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "caching, network file system, name look-up, performance,
    server load, cache coherence, process migration, trace-driven simulation,
    sprite",
  abstract     = "This paper presents the results of simulating file name and
    attribute caching on client machines in a distributed file system.  The
    simulation used trace data gathered on a network of about 40 workstations.
    Caching was found to be advantageous: a cache on each client containing
    just 10 directories had a 91% hit rate on name lookups.  Entry-based name
    caches (holding individual directory entries) had poorer performance for
    several reasons, resulting in a maximum hit rate of about 83%.  File
    attribute caching obtained a 90% hit rate with a cache on each machine of
    the attributes for 30 files.  The simulations show that maintaining cache
    consistency between machines is not a significant problem; only 1 in 400
    name component lookups required invalidation of a remotely cached entry.
    Process migration to remote machines had little effect on caching.  Caching
    was less successful in heavily shared and modified directories such as
    /tmp, but there weren’t enough references to /tmp overall to affect the
    results significantly.  We estimate that adding name and attribute caching
    to the Sprite operating system could reduce server load by 36% and the
    number of network packets by 30%.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/nameUsenix92.ps.Z"
}

@InProceedings{toatfmaaosj,
  author       = "Matt~W. Mutka and Philip~K. McKinley ",
  title        = "The {OPENSIM} Approach:  Tools for Management and Analysis of Simulation Jobs",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "291--304",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, workstation clusters, scheduling, work sharing, guis,
    simulations",
  abstract     = "This paper presents the design, implementation, and usage of
    OpenSim.  OpenSim provides new tools and integrates existing tools into an
    environment in order to establish a comprehensive facility for performing
    simulation work.  First, OpenSim provides a graphical user interface to
    users for creating input files for simulations and managing output files
    produced from simulations.  Second, tools are provided to help a user
    easily generate plots from sets of output files assocated with a simulation
    project.  Third, OpenSim addresses a common problem for many simulation
    users, namely, lack of computing capacity to serve the jobs.  In order to
    solve this problem, OpenSim integrates Condor, an existing system that
    clusters idle workstations into a processor bank, into its environment so
    that users have access to a large amount of computing capacity without
    interfering with the local usage of workstations by their owners.  Finally,
    since users often plan their schedules according to the deadlines required
    for their jobs, OpenSim enhances Condor so that users can request jobs to
    be scheduled within a deadline.  Therefore, a user can expect that the
    amount of computing capacity required for a simulation project will be
    available before a specified deadline.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mlcidfs,
  author       = "D.~Muntz and Peter Honeyman",
  title        = "Multi-level Caching in Distributed File Systems",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "305--313",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "intermediate caches, distributed file systems, performance,
    host caches, hit rates, trace-driven simulations",
  abstract     = "We are investigating the potential for a hierarchy of
    intermediate file servers to address scaling problems in increasingly large
    distributed file systems.  To this end, we have run trace-driven
    simulations based on data from DEC-SRC and our own data collection to
    determine the potential of caching-only intermediate servers.  The degree
    of sharing among clients is central to the effectiveness of an intermediate
    server.  This turns out to be quite low in the traces available to us.  All
    told, fewer than 10% of block accesses are to files shared by more than one
    file system client.  Our simulations show that even with an infinite cache
    at an intermediate server, cache hit rates are disappointingly low.  For
    client caches as small as 20M, we observe hit rates under 19%.  As client
    cache sizes increase, the hit rate at an intermediate server approaches the
    degree of sharing among all clients.  On the other hand, the intermediate
    server does appear to be effective in boosting the performance and
    scalability of upstream file servers by substantially reducing the request
    rate presented to them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "http://www.citi.umich.edu/techreports/reports/citi-tr-91-3.pdf"
}

@InProceedings{obf,
  author       = "Mitch Bradley",
  title        = "{Open Boot} Firmwear",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "223--235",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "forth, firmwear, boot-time configuration",
  abstract     = "Open Boot is a software architecture for the firmware that
    controls a computer before the operating system has begun execution.  The
    Open Boot firmware design is based on a machine-independent interactive
    programming language (Forth).  Open Boot includes features for
    self-identifying plug-in devices with device-resident boot drivers, support
    for disk, tape, and network booting, hardware configuration reporting, and
    debugging tools for hardware, software, and firmware.  Open Boot is the
    basis for the device identification and booting capabilities of SBus.  An
    IEEE standards effort for boot firmware based on Open Boot is underway.
    The Futurebus+ and VME-D bus standards include support for Open Boot.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{scapmotuk,
  author       = "Michael Litzkow and Marvin Solomon ",
  title        = "Supporting Checkpointing and Process Migration Outside the " # Unix # " Kernel",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "281--290",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "condor, checkpointing, process migration, kernel
    interception, state transfer, coredumps",
  abstract     = "We have implemented both checkpointing and migration of
    processes under UNIX as a part of the Condor package.  Checkpointing,
    remote execution, and process migration are different, but closely related
    ideas; the relationship between these ideas is explored.  A unique feature
    of the Condor implementation of these items is that they are accomplished
    entirely at user level.  Costs and benefits of implementing these features
    without kernel support are presented.  Portability issues, and the
    mechanisms we have devised to deal with these issues, are discussed in
    concrete terms.  The limitations of our implementation, and possible
    avenues to relieve some of these limitations, are presented.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcacidce,
  author       = "Douglas Rosenthal and Wayne Allen and Kenneth Fiduk",
  title        = "Process Control and Communication in Distributed {CAD} Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "271--281",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "ipc, load balancing, process control, message passing,
    distributed computing",
  abstract     = "The MCC Computer-Aided Design (CAD) Framework Process Control
    System (PCS) provides distributed process control and communication
    services in heterogeneous network environments.  The PCS also provides
    network-wide load balancing via an efficient and flexible process placement
    mechanism.  The PCS services enable design tools and CAD framework
    components to leverage the resources of distributed computing networks,
    while supporting various degrees of interaction through distributed,
    real-time communication.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lasodc,
  author       = "Robert~M. English and Alexander~A. Stepanov",
  title        = "Loge:  {A} Self-Organizing Disk Controller",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "237--251",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "While the task of organizing data on the disk has
    traditionally been performed by the file system, the disk controller is in
    many respects better suited to the task.  In this paper, we describe Loge,
    a disk controller that uses internal indirection, accurate physical
    information, and reliable metadata storage to improve I/O performance.  Our
    simulations show that Loge improves overall disk performance, doubles write
    performance, and can, in some cases, improve read performance.  The Loge
    disk controller operates through standard device interfaces, enabling it to
    be used on standard systems without software modification.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{hawsibtifn,
  author       = "Bruce Nelson and Yu-Ping Cheng",
  title        = "How and Why {SCSI} is Better than {IPI} for {NFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "253--270",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "disk controllers, traffic patterns",
  abstract     = "Disk drives are often dismissed as mundane devices, but they
    are actually interesting, complicated, and misunderstood.  In traditional
    Unix servers, disk storage subsystems are usually optimized for
    sequential-transfer performance.  Perhaps counter-intuitively, however, NFS
    file servers exhibit marked random-access disk traffic.  This report
    investigates this apparent contradiction and shows that disk-drive
    concurrency not disk transfer rate—is the important factor in disk storage
    performance for most NFS network servers.  The investigation begins with a
    concrete and detailed comparison of both performance-oriented and
    nonperformance-oriented technical specifications of both SCSI and IPI drive
    and interface types.  It offers a thorough empirical evaluation of SCSI
    disk drive performance, varying parameters such as synchronous or
    asynchronous bus transfers, random and sequential access patterns, and
    multiplicity of drives per SCSI channel.  It discusses (nonempirically)
    similar characteristics for IPI-2 drives.  The report concludes with
    benchmarked comparisons of NFS file servers using SCSI-based disk arrays
    and IPI-2 subsystems.  The results show that NFS heavy-load throughput
    using SCSI disk arrays scales linearly with extra drives, whereas IPI-2
    throughput scales less than proportionally with extra drives.  This SCSI
    scalability advantage, combined with SCSI’s appealing price-performance and
    price-capacity, make SCSI disks a superior choice for NFS servers.  IPI-2
    drives, with their optional high transfer rates, remain an excellent choice
    for compute-oriented servers executing large-file applications where high
    sequential throughput is essential.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{iiiocas,
  author       = "Murthy Devarakonda and Arup Mukherjee",
  title        = "Issues in Implementation of Cache-Affinity Scheduling",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "345--357",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "affinity scheduling, cache management, thread scheduling,
    performance", 
  abstract     = "In a shared memory multiprocessor, a thread may have an
    affinity to a processor because of the data remaining in the processor’s
    cache from a previous dispatch.  We show that two basic problems should be
    addressed in a Unix-like system to exploit cache affinity for improved
    performance: First, the limitation of the Unix dispatcher model (“processor
    seeking a thread”); Second, pseudo-affinity caused by low-cost waiting
    techniques used in a threads package such as C Threads.  We demonstrate
    that the affinity scheduling is most effective when used in a threads
    package that supports multiplexing of user threads on kernel threads.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{at,
  author       = "Jay Littman",
  title        = "Applying Threads",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "209--221",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multithreading, synchronization, deadlock, performance",
  abstract     = "Multithreading components of a software system can increase
    performance, but it can also increase complexity.  At Hewlett-Packard, we
    have developed a workstation based medical product, called the Monitoring
    Full Disclosure Review Station, or M1251A, that uses multithreading to
    achieve performance requirements.  The M1251A continuously acquires
    physiological waveforms and arrhythmia information for presentation to a
    clinician in an intensive care unit.  This paper describes the benefits the
    M1251A gains from multithreading, identifies the problems the development
    team had with multithreading, and explains how those problems were
    resolved.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aspmap,
  author       = "Bernhard Wagner and Bruce~K. Haddon",
  title        = "Application Software:  Product Management and Privileges",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "system administration, portability, standards",
  abstract     = "Application programs for UNIX are increasingly making greater
    demands upon the system structure, are adhering less well to admittedly
    implicit guidelines, or, are being inexactly transliterated from the
    paradigms of other systems.  These influences add to the administrative
    problems and load, and, in some cases, are exacerbating security risks.
    The administrative problems and corresponding solutions are presented here
    in a twofold manner: Firstly, by the description of our use of methods that
    separate administration of the programs and files that make up an
    application suite both from system administration and from eacli other.  We
    argue that thus a sort ol modularity takes place in the software
    administration.  The goal is the lack of need for super user privilege, so
    that this separation improves the overall security of the system.
    Secondly, we list a number of features that we support as being essential,
    a list of requirements to be fulfilled by all application programs written
    for UNIX systems.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pcsr,
  author       = "Spencer Rugaber",
  title        = "Program Comprehension",
  booktitle    = "Encyclopedia of Computer Science and Technology",
  year         = 1995,
  editor       = "Allen Kent and James~G. Williams",
  pages        = "341--368",
  publisher    = "Marcel Dekker",
  address      = nyny,
  keywords     = "program comprehension, cognitive models"
}

@InProceedings{rwpfm,
  author       = "Robin Schaufler",
  title        = "Realtime Workstation Performance for {MIDI}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "139--",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "real-time systems, midi, system performance, benchmarking",
  abstract     = "MIDI studio applications require 1 millisecond accuracy in
    timing transmission and receipt of MIDI messages.  Past MIDI
    implementations on UNIX™ have either used the Roland MPU-401 coprocessor to
    do accurate timing, or have not had timing tests published for them.
    Timing MIDI I/O on the host processor allows for more flexible scheduling
    policies than the MPU-401, but many people expressed skepticism that it
    could be done with sufficient accuracy and efficiency because of UNIX™
    virtual memory and pre-emptive scheduling.  This paper describes studies we
    did on providing millisecond accuracy on the host processor of a Silicon
    Graphics Iris Indigo running IRIX, the Silicon Graphics version of UNIX.
    Our measurements show that millisecond accuracy is feasible on IRIX™
    without modifying the kernel.  The paper goes on to describe how the
    studies relate to other time based media.  With a small set of real time
    features, UNIX can really sing.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aafapmt,
  author       = "Sun Wu and Udi Manber",
  title        = "{\tt agrep} --- Fast Approximate Pattern-Matching Tool",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "153--162",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "approximate string matching, boyer-more, knuth-morris-pratt",
  abstract     = "Searching for a pattern in a text file is a common
    operation in many applications ranging from text editors and databases to
    applications in molecular biology.  In many instances the pattern does not
    appear in the text exaedy.  Errors in the text or in the query can result
    from misspelling or from experimental errors (e.g., when the text is a DNA
    sequence).  The use of such approximate pattern matching has been limited
    until now to specific applications.  Most text editors and searching
    programs do not support searching with errors because of the complexity
    involved in implementing it.  In this paper we describe a new tool, called
    agrep, for approximate pattern matching.  Agrep is based on a new efficient
    and flexible algorithm for approximate string matching.  Agrep is also
    competitive with other tools for exact string matching; it include many
    options that make searching more powerful and convenient.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aewbiwacileas,
  author       = "Bill Cheswick",
  title        = "An Evening with {Berferd} In Which a Cracker is Lured, Endured, and Studied",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "163--173",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "cracking, jails, honeypots, law",
  abstract     = "On 7 January 1991 a cracker, believing he had discovered the
    famous sendmail DEBUG hole in our Internet gateway machine, attempted to
    obtain a copy of our password file.  I sent him one.  For several months we
    led this cracker on a merry chase in order to trace his location and learn
    his techniques.  This paper is a chronicle of the cracker’s 'successes' and
    disappointments, the bait and traps used to lure and detect him, and the
    chroot 'Jail' we built to watch his activities.  We concluded that our
    cracker had a lot of time and persistence, and a good list of security
    holes to use once he obtained a login on a machine.  With these holes he
    could often subvert the uucp and bin accounts in short order, and then
    root.  Our cracker was interested in military targets and new machines to
    help launder his connections.  This is a draft of a paper accepted for the
    January 1992 San Francisco Usenix.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{haph,
  author       = "Peter Honeyman and L.~B. Huston and M.~T. Stolarchuk",
  title        = "Hijacking {AFS}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "175--181",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "network security, mitm attack, afs, network protocols,
    challenge/response oracle",
  abstract     = "We have identified several techniques that allow uncontrolled
    access to files managed by AFS 3.0.  One method relies on administrative
    (or root) access to a user’s workstation.  Defending against this sort of
    attack is difficult.  Another class of attacks comes from promiscuous
    access to the physical network.  Stronger cryptographic protocols, such as
    those employed by AFS 3.1, obviate this problem.  These exercises help us
    understand vulnerabilities in the distributed systems that we employ (and
    deploy), and offer guidelines for securing them.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aibaflsdse,
  author       = "Dale Skeen",
  title        = "An Information Bus Architecture for Large-Scale, Decision-Support Environments",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information bus, publish-subscribe",
  abstract     = "Some of the promising industries for commercializing UNIX are
    those requiring real-time decision support, such as trading rooms, factory
    automation, process control, and network management.  These large-scale,
    real-time environments present challenging technical problems of high data
    volumes, split-second response times, and high availability.  Moreover,
    these environments demand flexible architectures that can support a rapidly
    changing set of application requirements.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{xwbstfu,
  author       = "Doug Blewett and Scott Anderson and Meg Kilduff and Susan Udovic and Mike Wish",
  title        = "{X Widget}-Based Software Tools for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "111--123",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "x resources, xtent, ipc, control flow, guis, user interface
    development, x windows",
  abstract     = "This paper describes a small language and IPC protocol that
    can be used for specifying UNIX style, X Toolkit based, graphics software
    tools.  The language is unusual in that it integrates the X Toolkit widget
    world and the UNIX philosophy of creating applications from collections of
    small reusable filters.  Filters can be constructed from old Xt based
    graphics processes or specified directly in the small language.  The system
    is based on an easily reproducible macro interpreter and IPC system that
    can be used with any collection of widgets.  A multi-process application
    builder constructed with the system is used as an example of how the
    software tools philosophy can be effectively used to construct graphics
    applications.  We present data on the use of the system by both research
    organizations and development groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{pfdomlaae,
  author       = "Reed Hastings and Bob Joyce",
  title        = "Purify: Fast Detection of Memory Leaks and Access Errors",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "125--137",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "storage-access errors, storage leaks, detecting storage
    leaks, object-code rewriting",
  abstract     = "This paper describes Purify™, a software testing and quality
    assurance tool that detects memory leaks and access errors.  Purify inserts
    additional checking instructions directly into the object code produced by
    existing compilers.  These instructions check every memory read and write
    performed by the program-under-test and detect several types of access
    errors, such as reading uninitialized memory or writing to freed memory.
    Purify inserts checking logic into all of the code in a program, including
    third-party and vendor object-code libraries, and verifies system call
    interfaces.  In addition, Purify tracks memory usage and identifies
    individual memory leaks using a novel adaptation of garbage collection
    techniques.  Purify produces standard executable files compatible with
    existing debuggers, and currently runs on Sun Microsystems’ SPARC family of
    workstations.  Purify’s nearly-comprehensive memory access checking slows
    the target program down typically by less than a factor of three and has
    resulted in significantly more reliable software for several development
    groups.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aaedsfti,
  author       = "Alan Emtage and Peter Deutsch",
  title        = "{archie} --- An Electronic Directory Service for the {Internet}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "93--110",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "information retrieval, internet crawling, ftp, indexing",
  abstract     = {The huge size and continued rapid growth of the Internet
    offers a particular challenge to systems designers and service providers in
    this new environment.  Before a user can effectively exploit any of the
    services offered by the Internet community or access any information
    provided by such services, that user must be aware of both the existence of
    the service and the host or hosts on which it is available.  Adequately
    addressing this “resource discovery problem” is a central challenge for
    both service providers and users wishing to capitalize on the possibilities
    of the Internet.  This paper describes archie, our attempt at an on-line
    resource directory service for an internetworked environment.  The current
    implementation of archie automatically indexes and makes available all
    filenames stored at known anonymous FTP sites.  The filename information is
    updated automatically ensuring users access to authoritative information.
    The system also makes available the names and descriptions of several
    thousand packages found on the Internet.}, 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{tefs,
  author       = "Sailesh Chutani and Owen~T. Anderson and Michael~L. Kazar and Bruce~W.  Leverett and W.~Anthony Mason and Robert~N. Sidebotham",
  title        = "The {Episode} File System",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "43--60",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, filesets, metadata logging, integrity checking,
    performance",
  abstract     = "We describe the design of Episode,™ a highly portable
    POSIX-compliant file system.  Episode is designed to utilize the disk
    bandwidth efficiently, and to scale well with improvements in disk capacity
    and speed.  It utilizes logging of meta-data to obtain good performance,
    and to restart quickly after a crash.  Episode uses a layered architecture
    and a generalization of files called containers to implement fileseis.  A
    fileset is a logical file system representing a connected subtree.
    Filesets are the unit of administration, replication, and backup in
    Episode.  The system works well, both as a standalone file system and as a
    distributed file system integrated with the OSF's Distributed Computing
    Environment (DCE).  Episode will be shipped with the DCE as the Local File
    System component, and is also exportable by NFS.  As for performance,
    Episode meta-data operations are significantly faster than typical UNIX
    Berkeley Fast File System implementations due to Episode's use of logging,
    while normal I/O operations run near disk capacity.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{aiolffbu,
  author       = "Dave Shaver and Eric Schnoebelen and George Bier",
  title        = "An Implementation of Large Files for {BSD} " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "61--68",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file systems, portability, software maintenance, 64-bit
    address spaces",
  abstract     = "The design of the ConvexOS 1 filesystem, based on the BSD
    Fast File System, allows for a theoretical maximum file size of about 4402G
    2 with a 4K filesystem block size (or about 64T with 8K blocks.)
    Unfortunately, the actual limit of the CONVEX filesystem has been 2G-1
    because key kernel values and file offset pointers are 32-bits in size.
    This is a problem shared by many other UNIX 3 vendors.  This paper
    describes the path CONVEX has taken to implement files and filesystems
    larger than 2G.  The implementation is based on a new set of 64-bit system
    calls and new library interfaces; it requires no changes to the on-disk
    i-node representation.  The large file programming models and the kernel
    and utilities changes are described.  Measurements of read and write I/O
    rates are presented and show that there is little performance penalty for
    manipulating large files using the chosen implementation.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{serf,
  author       = "Walter~A. Burkhard and Petar~D. Stojadinovi{\' c}",
  title        = "Storage-Efficient Reliable Files",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "69--77",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "n-of-m redundancy, reliability, file systems, file dispersal,
    fault tolerance, performance",
  abstract     = "The File Dispersal Shell is a storage-efficient reliable data
    storage prototype facility for local area networks.  Rabin's information
    dispersal algorithm provides an attractive data organization scheme which
    potentially uses less physical storage space than replication while
    obtaining excellent data reliability and access times comparable to those
    obtained for a single disk.  We have constructed Rabin's information
    dispersal algorithm within a UNIX system shell that provides almost all the
    traditional shell facilities augmented with two additional commands to
    create and delete dispersed files.  We present analytical
    mean-time-to-data-loss results, storage requirements, together with our
    prototype implementation and preliminary access-time measurements.  For
    practical purposes, dispersed files are invisible to the user except for
    the improved reliability at modest disk space cost.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{mmftbu,
  author       = "Nathaniel~S. Borenstein",
  title        = "Multimedia Mail From the Bottom Up",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "79--91",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "multimedia, mail, extensions, outboard processing,
    configuration files, usability",
  abstract     = "Multimedia mail systems have exhibited great potential, but
    the widespread use of multimedia mail has so far been inhibited by the lack
    of interchange standards and the heterogeneity of mail-reading software.
    This paper describes a new approach that seeks to break the existing
    log-jam and make multimedia mail a practical reality.  The paper begins
    with a brief summary of the state of the art in multimedia mail systems.
    It then outlines the new, 'bottom-up' approach, and describes the
    configuration mechanism that is central to its operation.  Next, it
    describes a prototype implementation and its deployment on top of over a
    dozen different mailreading programs at Bellcore and elsewhere.  Finally,
    problems in the prototype installation are discussed, along with future
    prospects for multimedia mail using this approach.  The paper ends by
    outlining a vision of a new and better 'lowest common denominator' for
    electronic mail.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{lpmtfu,
  author       = "Margo Seltzer and Michael Olson",
  title        = "{LIBTP}: Portable, Modular Transactions for " # unix,
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "9--25",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "transactions, shared storage, user-space libraries, two-phase
    commit, crash recovery, lock management",
  abstract     = "Transactions provide a useful programming paradigm for
    maintaining logical consistency, arbitrating concurrent access, and
    managing recovery.  In traditional UNIX systems, the only easy way of using
    transactions is to purchase a database system.  Such systems are often
    slow, costly, and may not provide the exact functionality desired.  This
    paper presents the design, implementation, and performance of LIBTP, a
    simple, non-proprietary transaction library using the 4.4BSD database
    access routines (db(3)).  On a conventional transaction processing style
    benchmark, its performance is approximately 85% that of the database access
    routines without transaction protection, 200% that of using fsync(2) to
    commit modifications to disk, and 125% that of a commercial relational
    database system.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt", 
  location     = "https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/1925.html"
}

@InProceedings{etaomffsio,
  author       = "Orran Krieger and Michael Stumm and Ron Unrau",
  title        = "Exploiting the Advantages of Mapped Files for Stream {I}/{O}",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "27--42",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "file-mapped io, std-io, stream io",
  abstract     = "A new approach for providing user level support for fast
    stream I/O is motivated by four factors common to most modern systems: 1)
    the capability of the operating system to support mapped files, 2) the
    increasing number of applications that use threads, 3) the increasing
    discrepancy between processor speed and disk latency, and 4) the increasing
    amount of available main memory.  In this paper, we first describe the
    advantages and disadvantages of using mapped files to support stream access
    to files, and then describe a new interface, the Alloc Stream Interface
    (ASI), that allows for improved performance over existing stream
    interfaces.  A library that supports ASI has been implemented on several
    systems (including IRIX and SunOS).  In addition, the Stdio library has
    been re-implemented to use ASI.  Significant performance advantages are
    demonstrated for Stdio applications that are linked to this new library and
    particularly for applications that are modified to use ASI directly.  For
    example, on typical Unix platforms, some standard I/O intensive utilities
    are shown to run up to twice as fast when re-linked to use this library and
    up to three times as fast when converted to use ASI.", 
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{cco,
  author       = "Eduardo Krell and Balachander Krishnamurthy",
  title        = "{COLA}: Customied Overlaying",
  booktitle    = usenixw92,
  year         = 1991,
  pages        = "3--7",
  organization = "USENIX Association",
  address      = sfca,
  month        = "20--24 " # jan,
  keywords     = "",
  abstract     = "System calls are the basic building blocks for writing
    programs in the UNIX operating system.  From the canonical read, write,
    open, close, seek, ...  to the more obscure ones, programs have been
    written to use system calls in a variety of ways.  Often there is a need to
    intercept a few system calls to perform some special task.  Given that it
    is hard to go below the level of system calls and still write portable
    programs, it is easy to see the need for intercepts at the system call
    level.  A simple example of a useful interception facility is a library
    that watches for file creation and modifications.  In this paper we
    describe COLA, an elegant, customizable and dynamic facility to overlay a
    variety of system call intercepts.  With COLA, users can specify an
    arbitrary number of system call filters, each of which may intercept
    different system calls and perform different actions upon interception.
    The set of overlaying filters can be modified at any time during the
    session.  A program run under COLA will have any of the filtered system
    calls processed at each layer before control is passed on to the next
    layer.  The final layer always is the standard UNIX system call layer.
    System calls not intercepted by any of the overlaying filters will execute
    transparently.  No recompilation of programs or static relinking is
    necessary.  It should be noted that this facility depends on availability
    of shared libraries.",
  location     = "https://archive.org/stream/winter92_usenix_technical_conf/winter92_usenix_technical_conf_djvu.txt"
}

@InProceedings{abp,
  author       = "Susan~L. Graham and Steven Lucco and Robert Wahbe",
  title        = "Adaptable Binary Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "315--325",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "disassembly, control-flow, relocation, register management",
  abstract     = "To accurately and comprehensively monitor a program's
    behavior, many performance measurement tools transform the program's
    executable representation or binary.  By instrumenting binary programs to
    monitor program events, tools can precisely analyze compiler optimization
    effectiveness, memory system performance, pipeline interlocking, and other
    dynamic program characteristics that are fully exposed only at this level.
    Binary transformation has also been used to support software-enforced fault
    isolation, debugging, machine re-targeting, and machine-dependent
    optimization.At present, binary transformation applications face a
    difficult trade-off.  Previous approaches to implementing robust
    transformations result in significant disk space and run-time overhead.  To
    improve efficiency, some current systems sacrifice robustness, relying on
    heuristic assumptions about the program and recognition of
    compiler-dependent code generation idioms.  In this paper we begin by
    investigating the run-time and disk space overhead of transformation
    strategies that do not require assumptions about the program's control flow
    or register usage.  We then detail simple information about the binary
    program that can significantly reduce this overhead.  For each type of
    information, we show how it enables a corresponding type of binary
    transformation.  We call binary programs that contain such enabling
    information adaptable binaries.  Because adaptable binary information is
    simple, any compiler can generate it.  Despite its simplicity, adaptable
    binary information has the necessary and sufficient expressive power to
    support a rich set of binary transformations.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/adaptable-binary-programs"
}

@InProceedings{aafifbhppat95,
  author       = "Eustace, Alan and Srivastava, Amitabh",
  title        = "{ATOM}, {A} Flexible Interface for Building High-Peformance Program Analysis Tools",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "303--314",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cache simulator, instruction profiling, analysis tools,
    compiler auditing",
  abstract     = "ATOM (Analysis Tools with OM) is a single framework for
    building a wide range of customized program analysis tools.  It provides
    the common infrastructure present in all code-instrumenting tools; this is
    the difficult and time-consuming part.  The user simply defines the
    tool-specific details in instrumentation and analysis routines.  Building a
    basic block counting tool like Pixie with ATOM requires only a page of
    code.ATOM, using OM link-time technology, organizes the final executable
    such that the application program and user's analysis routines run in the
    same address space.  Information is directly passed from the application
    program to the analysis routines through simple procedure calls instead of
    inter-process communication or files on disk.  ATOM takes care that
    analysis routines do not interfere with the program's execution, and
    precise information about the program is presented to the analysis routines
    at all times.  ATOM uses no simulation or interpretation.  ATOM has been
    implemented on the Alpha AXP under OSF/1.  It is efficient and has been
    used to build a diverse set of tools for basic block counting, profiling,
    dynamic memory recording, instruction and data cache simulation, pipeline
    simulation, evaluating branch prediction, and instruction scheduling.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/atom-flexible-interface-building-high-performance",
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-44.html"
}

@InProceedings{ltcuu,
  author       = "James~S. Plank and Micah Beck and Gerry Kingsley and Kai Li",
  title        = "Libckpt:  Transparent Checkpointing under " # unix,
  booktitle    = usenix95,
  year         = 1995,
  pages        = "213--224",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "checkpointing, optimizations, fault tolerance",
  abstract     = "Checkpointing is a simple technique for rollback recovery:
    the state of an executing program is periodically saved to a disk file from
    which it can be recovered after a failure.  While recent research has
    developed a collection of powerful techniques for minimizing the overhead
    of writing checkpoint files, checkpointing remains unavailable to most
    application developers.  In this paper we describe libckpt, a portable
    checkpointing tool for Unix that implements all applicable performance
    optimizations which are reported in the literature.  While libckpt can be
    used in a mode which is almost totally transparent to the programmer, it
    also supports the incorporation of user directives into the creation of
    checkpoints.  This user-directed checkpointing is an innovation which is
    unique to our work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/libckpt-transparent-checkpointing-under-unix"
}

@InProceedings{otpodlp,
  author       = "W.~Wilson Ho and Wei-Chau Chang and Lilian~H. Leung",
  title        = "Optimizing the Performance of Dynamically-Linked Programs",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "225--233",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indirect addressing, data structures, procedure
    repositioning",
  abstract     = "Dynamically-linked programs in general do not perform as well
    as statically-linked programs.  This paper identifies three main areas that
    account for the performance loss.  First, symbols are referenced indirectly
    and thus extra instructions are required.  Second, the overhead in run-time
    symbol resolution is significant.  Third, poor locality of functions in
    shared libraries and data structures maintained by the run-time linker may
    result in poor memory utilization.  This paper presents new optimization
    techniques we developed that address these three areas and significantly
    improve the performance of dynamically-linked programs.  Also, we provide
    measurements of the performance improvement achieved.  Most importantly, we
    show that all desirable features of shared libraries can be achieved
    without sacrificing performance.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267430"
}

@InProceedings{dalfbprda,
  author       = "David~M. Arnow",
  title        = "{DP}:  {A} Library for Building Portable, Reliable Distributed Applications",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "235--247",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed programming, libraries, process management,
    communication",
  abstract     = "DP is a library of process management and communication tools
    for writing portable, reliable distributed applications.  It provides
    support for a flexible set of message operations as well as process
    creation and management.  It has been successfully used in developing
    distributed Monte Carlo, disjunctive programming and integer goal
    programming codes.It differs from PVM and similar libraries in its support
    for lightweight, unreliable messages, as well as asynchronous delivery of
    interrupt-generating messages.  In addition, DP supports the development of
    long-running distributed applications tolerant to the failure or loss of a
    subset of its processors.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/arnow.html"
}

@InProceedings{fslvcapc,
  author       = "Margo Seltzer and Keith~A. Smith and Hari Balakrishnan and Jacqueline Chang and Sara McMains and Venkata Padmanabhan",
  title        = "File System Logging Versus Clustering:  {A} Performance Comparison",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "249--264",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "fast file system, log structured file system, i-o
    performance",
  abstract     = "The Log-structured File System (LFS), introduced in 1991 [8],
    has received much attention for its potential order-of-magnitude
    improvement in file system performance.  Early research results [9] showed
    that small file performance could scale with processor speed and that
    cleaning costs could be kept low, allowing LFS to write at an effective
    bandwidth of 62 to 83% of the maximum.  Later work showed that the presence
    of synchronous disk operations could degrade performance by as much as 62%
    and that cleaning overhead could become prohibitive in transaction
    processing workloads, reducing performance by as much as 40% [10].  The
    same work showed that the addition of clustered reads and writes in the
    Berkeley Fast File System [6] (FFS) made it competitive with LFS in
    large-file handling and software development environments as approximated
    by the Andrew benchmark [4].These seemingly inconsistent results have
    caused confusion in the file system research community.  This paper
    presents a detailed performance comparison of the 4.4BSD Log-structured
    File System and the 4.4BSD Fast File System.  Ignoring cleaner overhead,
    our results show that the order-of-magnitude improvement in performance
    claimed for LFS applies only to meta-data intensive activities,
    specifically the creation of files one-kilobyte or less and deletion of
    files 64 kilobytes or less.For small files, both systems provide comparable
    read performance, but LFS offers superior performance on writes.  For large
    files (one megabyte and larger), the performance of the two file systems is
    comparable.  When FFS is tuned for writing, its large-file write
    performance is approximately 15% better than LFS, but its read performance
    is 25% worse.  When FFS is optimized for reading, its large-file read and
    write performance is comparable to LFS.Both LFS and FFS can suffer
    performance degradation, due to cleaning and disk fragmentation
    respectively.  We find that active FFS file systems function at
    approximately 85-95% of their maximum performance after two to three years.
    We examine LFS cleaner performance in a transaction processing environment
    and find that cleaner overhead reduces LFS performance by more than 33%
    when the disk is 50% full.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267432"
}

@InProceedings{mlians,
  author       = "Uresh Vahalia and Cary~G. Gray and Dennis Ting",
  title        = "Metadata Logging in an {NFS} Server",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "265--276",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "log-structured file systems, nfs, file-system metadata, fault
    tolerance, crash recovery, ",
  abstract     = "Over the last few years, there have been several efforts to
    use logging to improve performance, reliability, and recovery times of file
    systems.  The two major techniques are metadata logging, where the log
    records metadata changes and is a supplement to the on-disk file system,
    and log-structured file systems, whose log is their only on-disk
    representation.  When the file system is mainly or wholly accessed through
    the Network File System (NFS) protocol, it adds new considerations to the
    suitability of the logging technique.  NFS requires that all operations be
    updated to stable storage before returning.  As a result, file system
    implementations that were effective for local access may perform poorly on
    an NFS server.  This paper analyzes the issues regarding the use of logging
    on an NFS server, and describes an implementation of a BSD Fast File System
    (FFS) with metadata logging that performs effectively for a dedicated NFS
    server.", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267433", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/metadata-logging-nfs-server"
}

@InProceedings{hcailsfs,
  author       = "Trevor Blackwell and Jeffrey Harris and Margo Seltzer",
  title        = "Heuristic Cleaning Algorithms in Log-Structured File Systems",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "277--288",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "garbage collection, log-structured file systems,
    benchmarking, performance, trace-driven analysis, scheduling",
  abstract     = "Research results show that while Log-Structured File Systems
    (LFS) offer the potential for dramatically improved file system
    performance, the cleaner can seriously degrade performance, by as much as
    40% in transaction processing workloads [9].  Our goal is to examine trace
    data from live file systems and use those to derive simple heuristics that
    will permit the cleaner to run without interfering with normal file access.
    Our results show that trivial heuristics perform very well, allowing 97% of
    all cleaning on the most heavily loaded system we studied to be done in the
    background.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/blackwell.html", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267434"
}

@InProceedings{muuogoos,
  author       = "J.~Mark Stevenson and Daniel~P. Julin",
  title        = "Mach-{US}: " # unix # " on Generic {OS} Object Servers",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "119--130",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "unix, apis, mach, object-oriented design, ipc",
  abstract     = "This paper examines the Mach-US operating system, its unique 
    architecture, and the lessons demonstrated through its implementation.
    Mach-US is an object-oriented multi-server OS which runs on the Mach3.0
    kernel.  Mach-US has a set of separate servers supplying orthogonal OS
    services and a library which is loaded into each user process.  This
    library uses the services to generate the semantics of the Mach2.5/4.3BSD
    application programmers interface (API).  This architecture makes Mach-US a
    flexible research platform and a powerful tool for developing and examining
    various OS service options.  We will briefly describe Mach-US, the
    motivations for its design choices, and its demonstrated strengths and
    weaknesses.  We will then discuss the insights that we've acquired in the
    areas of multi-server architecture, OS remote method invocation, Object
    Oriented technology for OS implementation, API independent OS services,
    UNIX API re-implementation, and smart user-space API emulation libraries.", 
  location     = "https://www.researchgate.net/publication/2811091_Mach-US_UNIX_on_generic_OS_object_servers"
}

@InProceedings{eiarbds,
  author       = "Jim Waldo and Ann Wollrath and Geoff Wyant and Samuel~C. Kendall",
  title        = "Events in an {RPC} Based Distributed System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "131--142",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "events, publish-subscribe, ipc, corba, interface
    polymorphism, third-party servers",
  abstract     = "We show how to build a distributed system allowing objects to
    register interest in and receive notifications of events in other objects.
    The system is built on top of a pair of interfaces that are interesting
    only in their extreme simplicity.  We then present a simple and efﬁcient
    implementation of these interfaces.  We then show how more complex
    functionality can be introduced to the system by adding third-party
    services.  These services can be added without changing the simple
    interfaces, and without changing the objects in the system that do not need
    the functionality of those services.  Finally, we note a number of open
    issues that remain, and attempt to draw some conclusions based on the
    work.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/events-rpc-based-distributed-system"
}

@InProceedings{ttaosiamco,
  author       = "Jacques Talbot",
  title        = "Turning the {AIX} Operating System into an {MP}-capable {OS}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "143--153",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "powerscale, hardware architecture, atomic operations, caches,
  interrupt handling, locks, deadlocks, debugging, affinity scheduling",
  abstract     = "This paper describes those MP features that Bull and IBM
    together introduced into the AIX operating system to support the Symmetric
    Multiprocessor machine marketed by Bull under the Escala name and by IBM
    under the RS/6000 Models G30, J30 and R30 names.  The PowerPC architecture
    and the AIX operating system present some specific challenges.  We present
    the major problems encountered and how they were solved.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/turning-aix-operating-system-mp-capable-os"
}

@InProceedings{afmbfs,
  author       = "Atsuo Kawaguchi and Shingo Nishioka and Hiroshi Motoda",
  title        = "{A} Flash-Memory Based File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "155--164",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "flash memory, file systems, ",
  abstract     = "A flash memory device driver that supports a conventional
    UNIX file system transparently was designed.  To avoid the limitations due
    to flash memory's restricted number of write cycles and its inability to be
    overwritten, this driver writes data to the flash memory system
    sequentially as a Log-structured File System (LFS) does and uses a cleaner
    to collect valid data blocks and reclaim invalid ones by erasing the
    corresponding flash memory regions.  Measurements showed that the overhead
    of the cleaner has little effect on the performance of the prototype when
    utilization is low but that the effect becomes critical as the utilization
    gets higher, reducing the random write throughput from 222 Kbytes/s at 30%
    utilization to 40 Kbytes/s at 90% utilization.  The performance of the
    prototype in the Andrew Benchmark test is roughly equivalent to that of the
    4.4BSD Pageable Memory based File System (MFS).", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flash-memory-based-file-system"
}

@InProceedings{tpsfpftuos,
  author       = "Andrew Berman and Virgil Bourassa and Erik Selberg",
  title        = "{TRON}:  Process-Specific File Protection for the " # unix # " Operating System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "165--175",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "capabilities, protection domains",
  abstract     = "The file protection mechanism provided in UNIX is
    insufficient for current computing environments.  While the UNIX file
    protection system attempts to protect users from attacks by other users, it
    does not directly address the agents of destruction-executing processes.
    As computing environments become more interconnected and interdependent,
    there is increasing pressure and opportunity for users to acquire and test
    non-secure, and possibly malicious, software.We introduce TRON, a
    process-level discretionary access control system for UNIX.  TRON allows
    users to specify capabilities for a process' access to individual files,
    directories, and directory trees.  These capabilities are enforced by
    system call wrappers compiled into the operating system kernel.  No
    privileged system calls, special files, system administrator intervention,
    or changes to the file system are required.  Existing UNIX programs can be
    run without recompilation under TRON-enhanced UNIX.  Thus, TRON improves
    UNIX security while maintaining current standards of flexibility and
    openness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/tron-process-specific-file-protection-unix-operating"
}

@InProceedings{satfwaid,
  author       = "Tak Yan and Hector Garcia-Molina",
  title        = "{SIFT} --- {A} Tool for Wide-Area Information Dissemination",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "177--186",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "indexing, user interest, information filtering",
  abstract     = "The dissemination model is becoming increasingly important in 
    wide-area information system.  In this model, the user subscribes to an
    information dissemination service by submitting profiles that describe his
    interests.  He then passively receives new, filtered information.  The
    Stanford Information Filtering Tool (SIFT) is a tool to help provide such
    service.  It supports full-text filtering using well-known information
    retrieval models.  The SIFT filtering engine implements novel indexing
    techniques, capable of processing large volumes of information against a
    large number of profiles.  It runs on several major Unix platforms and is
    freely available to the public.  In this paper we present SIFT's approach
    to user interest modeling and user-server communication.  We demonstrate
    the processing capability of SIFT by describing a running server that
    disseminates USENET News.  We present an empirical study of SIFT's
    performance, examining its main memory requirement and ability to scale
    with information volume and user population.", 
  location     = "http://ilpubs.stanford.edu/73/1/1994-7.pdf"
}

@InProceedings{tfsbitk,
  author       = "Brent Welch",
  title        = "The File System Belongs in the Kernel",
  booktitle    = pot # "Second USENIX Mach Symposium",
  year         = 1991,
  pages        = "233--250",
  address      = "Monterey, " # CA,
  month        = "20--22 " # nov,
  keywords     = "os architecture, file systems, sprite, mach, naming, name
  spaces, microkernels",
  abstract     = "This paper argues that a shared, distributed name space and
    I/O interface should be implemented inside the operating system kernel.
    The grounding for the argument is a comparison between the Sprite network
    operating system and the Mach microkernel.  Sprite optimizes the common
    case of file and device access, both local and remote, by providing a
    kernel-level implementation.  Sprite also allows for user-level
    extensibility by letting a user-level process implement the naming and I/O
    interfaces of the file system.  Mach, in contrast, provide general
    interprocess communication and does not define a file system protocol in
    the kernel.", 
  location     = "http://ftp.dk.netbsd.org/pub/doc/OS/Sprite/welch.filesys.ps.Z"
}

@InProceedings{piomps,
  author       = "Jeffrey~C. Mogul and Joel~F. Bartlett and Robert~N. Mayo and Amitabh Srivastava",
  title        = "Performance Implications of Multiple Pointer Sizes",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "187--200",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "performance, address spaces, tlb",
  abstract     = "Many users need 64-bit architectures: 32-bit systems cannot
    support the largest applications, and 64-bit systems perform better for
    some applications.  However, performance on some other applications can
    suffer from the use of large pointers; large pointers can also constrain
    feasible problem size.  Such applications are best served by a 64-bit
    machine that supports the use of both 32-bit and 64-bit pointer variables.
    This paper analyzes several programs and programming techniques to
    understand the performance implications of different pointer sizes.  Many
    (but not all) programs show small but definite performance consequences,
    primarily due to cache and paging effects.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/performance-implications-multiple-pointer-sizes"
}

@InProceedings{dcswmbs,
  author       = "Steven~W. Schlosser and John Linwood Griffin and David~F. Nagle and Gregory~R. Ganger",
  title        = "Designing Computer Systems with {MEMS}-based Storage",
  booktitle    = asplos00,
  year         = 2000,
  pages        = "1--12",
  address      = boma,
  month        = "9--13" # oct,
  keywords     = "mems, non-volatile storage, semi-conductor technology",
  abstract     = "For decades the RAM-to-disk memory hierarchy gap has plagued
    computer architects.  An exciting new storage technology based on
    microelectromechanical systems (MEMS) is poised to fill a large portion of
    this performance gap, significantly reduce system power consumption, and
    enable many new applications.  This paper explores the system-level
    implications of integrating MEMS-based storage into the memory hierarchy.
    Results show that standalone MEMS-based storage reduces I/O stall times by
    4-74X over disks and improves overall application runtimes by 1.9-4.4X.
    When used as on-board caches for disks, MEMS-based storage improves I/O
    response time by up to 3.5X.  Further, the energy consumption of MEMS-based
    storage is 10-54X less than that of state-of-the-art low-power disk drives.
    The combination of the high-level physical characteristics of MEMS-based
    storage (small footprints, high shock tolerance) and the ability to
    directly integrate MEMS-based storage with processing leads to such new
    applications as portable gigabit storage systems and ubiquitous active
    storage nodes.",  
  location     = "https://doi.org/10.1145/378993.378996"
}

@InProceedings{iins95,
  author       = "Richard Golding and Peter Bosch and Carl Staelin and Tim Sullivan and John Wilkes",
  title        = "Idleness is not Sloth",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "201--212",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cycle harvesting, idle-time processing, process dispatch,
    idle detection",
  abstract     = "Many people have observed that computer systems spend much of
    their time idle, and various schemes have been proposed to use this idle
    time productively.  The commonest approach is to off-load activity from
    busy periods to less-busy ones in order to improve system responsiveness.
    In addition, speculative work can be performed in idle periods in the hopes
    that it will be needed later at times of higher utilization, or
    non-renewable resource like battery power can be conserved by disabling
    unused resources.We found opportunities to exploit idle time in our work on
    storage systems, and after a few attempts to tackle specific instances of
    it in ad hoc ways, began to investigate general mechanisms that could be
    applied to this problem.  Our results include a taxonomy of idle-time
    detection algorithms, metrics for evaluating them, and an evaluation of a
    number of idleness predictors that we generated from our taxonomy.", 
  location     = "https://john.e-wilkes.com/papers/idleness.pdf"
}

@InProceedings{eodafacfs,
  author       = "Murthy Devarakonda and Ajay Mohindra and Jill Simoneaux and William~H. Tetzlaff",
  title        = "Evaluation of Design Alternatives for a Cluster File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "35--46",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "cluster computing, token-based consistency, disk sharing,
    file-system sharing, journaling file systems",
  abstract     = "Based on implementation experience and measurements, this
    paper presents an evaluation of design alternatives for a cluster file
    system.  The file system is targeted for IBM cluster systems, Scalable
    POWERparallel and AIX HACMP/6000.  We considered a shared disk approach
    where serialized, multiple instances of a single-system file system
    directly access file data as disk blocks, and a shared system approach
    which is the conventional method of distributing file system function
    between client and server.  We conclude the shard disk approach suffers
    form the difficulties serializing metadata, poor write-sharing performance,
    and low read throughput.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/evaluation-design-alternatives-cluster-file-system"
}

@InProceedings{mraaaims,
  author       = "Jonathan~S. Goldick and Kathy Benninger and Christopher Kirby and Christopher Maher and Bill Zumach",
  title        = "Multi-Resident {AFS}: An Adventure in Mass Storage",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "47--58",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "distributed file systems, mass storage, afs",
  abstract     = "The Pittsburgh Supercomputing Center has been working to
    integrate distributed file system technology with hierarchical mass
    storage.  We produced a system utilizing the Andrew File System that can be
    interfaced to many mass storage systems.  We retained the semantics of AFS
    and compatibility with standard clients and servers.  The architecture has
    a logical separation between the facility that provides the user interface
    and access semantics and the management of the storage systems that contain
    user data.  Support for file level replication is provided for high
    availability to data in a fashion that is transparent to users.  This
    system is called Multi-Resident AFS.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/multi-resident-afs-adventure-mass-storage", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267416"
}

@InProceedings{reatahbmpfs,
  author       = "Ethan~L. Miller and Randy~H. Katz",
  title        = "{RAMA}:  Easy Access to a High-Bandwidth Massively Parallel File System",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "59--69",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "",
  abstract     = "Massively parallel file systems must provide high bandwidth
    file access to programs running on their machines.  Most accomplish this
    goal by striping files across arrays of disks attached to a few specialized
    I/O nodes in the massively parallel processor (MPP).  This arrangement
    requires programmers to give the file system many hints on how their data
    is to be laid out on disk if they want to achieve good performance.
    Additionally, the custom interface makes massively parallel file systems
    hard for programmers to use and difficult to seamlessly integrate into an
    environment with workstations and tertiary storage.The RAMA file system
    addresses these problems by providing a massively parallel file system that
    does not need user hints to provide good performance.  RAMA takes advantage
    of the recent decrease in physical disk size by assuming that each
    processor in an MPP has one or more disks attached to it.  Hashing is then
    used to pseudo-randomly distribute data to all of these disks, insuring
    high bandwidth regardless of access pattern.  Since MPP programs often have
    many nodes accessing a single file in parallel, the file system must allow
    access to different parts of the file without relying on a particular node.
    In RAMA, a file request involves only two nodes -- the node making the
    request and the node on whose disk the data is stored.  Thus, RAMA scales
    well to hundreds of processors.  Since RAMA needs no layout hints from
    applications, it fits well into systems where users cannot (or will not)
    provide such hints.  Fortunately, this flexibility does not cause a large
    loss of performance.  RAMA's simulated performance is within 10-15% of the
    optimum performance of a similarly-sized striped file system, and is a
    factor of 4 or more better than a striped file system with poorly laid out
    data.",
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/rama-easy-access-high-bandwidth-massively-parallel-file",
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267417"
}

@InProceedings{irtpfpus,
  author       = "Ian Wakeman and Atanu Ghosh and Jon Crowcroft and Van Jacobson and Sally Floyd",
  title        = "Implementing Real-Time Packet Forwarding Policies Using {Streams}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "71--82",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "link sharing, diff serve, queue scheduling, streams, packet
    forwarding",
  abstract     = "This paper describes an implementation of the class based
    queueing (CBQ) mechanisms proposed by Sally Floyd and Van Jacobson to
    provide real time policies for packet forwarding.  CBQ allows the traffic
    flows sharing a data link to be guaranteed a share of the bandwidth when
    the link is congested, yet allows flexible sharing of the unused bandwidth
    when the link is unloaded.  In addition, CBQ provides mechanisms which give
    flows requiring low delay priority over other flows.  In this way, links
    can be shared by multiple flows yet still meet the policy and Quality of
    Service (QoS) requirements of the flows.We present a brief description of
    the implementation and some preliminary preformance measurements.  The
    problems of packet classification are addressed in a flexible and
    extensible, yet efficient manner, and whilst the Streams implementation
    cannot cope with very high speed interfaces, it can cope with the serial
    link speeds that are likely to be loaded.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/implementing-real-time-packet-forwarding-policies-using", 
  location     = "https://dl.acm.org/doi/10.5555/1267411.1267418"
}

@InProceedings{stwotckaptplsa,
  author       = "Jeffrey I. Schiller and Derek Atkins",
  title        = "Scaling the {Web of Trust}:  Combining {Kerberos} and {PGP} to Provide Large Scale Authentication",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "83--94",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "e-mail, pgp, public key cryptography, kerberos, web of trust,
    authentication",
  abstract     = "Internet Security has become more important recently as the
    Internet grows exponentially and security breaches become more publicized.
    An important area of concern for many Internet users is the privacy and
    integrity of their electronic files and messages.  Phil Zimmermann's Pretty
    Good Privacy (PGP) provides a general purpose utility for file and message
    protection.  However PGP requires that communicating users be 'introduced'
    to each other.  This paper describes a scheme that permits an enterprise
    using Kerberos to create an automated introducer called the PGP Key Signer
    Service.  Using this service people in the enterprise who have no common
    acquaintances to act as introducers can be introduced through the Key
    Signer.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/scaling-web-trust-combining-kerberos-and-pgp-provide"
}

@InProceedings{fasrofc,
  author       = "Puneet Kumar and M.~Satyanarayanan",
  title        = "Flexible and Safe Resolution of File Conflicts",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "95--106",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "coda file system, conflict resolution, consistency",
  abstract     = "In this paper we describe the support provided by the Coda
    File System for transparent resolution of conflicts arising from concurrent
    updates to a file in different network partitions.  Such partitions often
    occur in mobile computing environments.  Coda provides a framework for
    invoking customized pieces of code called application-specific resolvers
    (asrs) that encapsulate the knowledge needed for file resolution.  If
    resolution succeeds, the user notices nothing more than a slight
    performance delay.  Only if resolution fails does the user have to resort
    to manual repair.  Our design combines a rule-based approach to ASR
    selection with transactional encapsulation of ASR execution.  This paper
    shows how such an approach leads to flexible and efficient file resolution
    without loss of security or robustness.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/flexible-and-safe-resolution-file-conflicts", 
  location     = "https://dl.acm.org/citation.cfm?id=1267419"
}

@InProceedings{oacfftodce,
  author       = "John Dilley",
  title        = "{OODCE}: {A} {C}++ Framework for the {OSF Distributed Computing Environment}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "107--118",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "dce, distributed programming",
  abstract     = "This paper presents a method for developing object-oriented
    distributed applications using the C++ and DCE technologies.  The core of
    this package is a DCE IDL-to-C++ compiler and a set of C++ classes
    providing easy access to DCE functionality.  Using this approach we were
    able to develop more object-oriented distributed applications, and saw a
    significant decrease in application code size.  This contributed to an
    increase in developer productivity and code maintainability.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/oodce-c-framework-osf-distributed-computing-environment"
}

@InProceedings{umi44l,
  author       = "Jan-Simon Pendry and Marshall Kirk McKusick",
  title        = "Union Mounts in 4.{4BSD}-Lite",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "25--33",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "file systems, union file mounts, vnodes",
  abstract     = "This paper describes the design and rationale behind union
    mounts, a new filesystem-namespace management tool available in
    4.4BSD-Lite.  Unlike a traditional mount that hides the contents of the
    directory on which it is placed, a union mount presents a view of a merger
    of the two directories.  Although only the filesystem at the top of the
    union stack can be modified, the union filesystem gives the appearance of
    allowing any- thing to be deleted or modified.  Files in the lower layer
    may be deleted with whiteout in the top layer.  Files to be modified are
    automatically copied to the top layer.  This new functionality makes
    possible several new applications including the ability to apply patches to
    a CD-ROM and eliminate symbolic links generated by an automounter.  Also
    possible is the provision of per- user views of the filesystem, allowing
    private views of a shared work area, or local builds from a centrally
    shared read-only source tree.", 
  location     = "https://www.usenix.org/legacy/publications/library/proceedings/neworl/mckusick.html"
}

@InProceedings{pi44,
  author       = "W.~Richard Stevens and Jan-Simon Pendry",
  title        = "Portals in {4.4BSD}",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "1--10",
  organization = "USENIX Association",
  address      = nola,
  month        = "16--20 " # jan,
  keywords     = "remote access, portals",
  abstract     = "Portals were added to 4.4BSD as an experimental feature and
    are in the publicly available 4.4BSD-Lite distribution.  Portals provide
    access to alternate file types or devices using names in the normal
    filesystem that a process just opens.  For example, an open of
    /p/tcp/foo.com/smtp returns a TCP socket descriptor to the calling process
    that is connected to the SMTP server on the specified host.  By providing
    access through the normal filesystem, the calling process need not be aware
    of the special functions necessary to create a TCP socket and establish a
    TCP connection.  This makes TCP connections, for example, available to
    programs such as Awk, Tcl, and shell scripts.  This paper describes the
    implementation of portals in 4.4BSD as another type of filesystem and
    provides some examples.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/portals-44bsd"
}

@InProceedings{dvdai,
  author       = "Aju John",
  title        = "Dynamic {Vnodes} --- Design and Implementation",
  booktitle    = usenix95,
  year         = 1995,
  pages        = "11--23",
  organization = "USENIX Association",
  address      = nola,
  month        = "11--23 " # jan,
  keywords     = "osf, dynamic storage management, coordinated timeouts",
  abstract     = "Dynamic vnodes make the UNIX kernel responsive to a varying
    demand for vnodes, without a need to rebuild the kernel.  It also optimizes
    the usage of memory by deallocating excess vnodes.  This paper describes
    the design and implementation of dynamic vnodes in DEC OSF/1 V3.0.  The
    focus is on the vnode deallocation logic in a Symmetric Multi-Processing
    environment.Deallocation of vnodes differs from the familiar concept of
    dynamically allocated data structures in the following ways: the legacy
    name-cache design implicitly assumes that vnodes are never deallocated, and
    the vnode free-list needs to cache unused vnodes effectively.", 
  location     = "https://www.usenix.org/conference/usenix-1995-technical-conference/dynamic-vnodes-design-and-implementation"
}

@InProceedings{pitccl,
  author       = "Vijay Saraswat and Radha Jagadeesan and Vinheet Gupta",
  title        = "Programming in Timed Concurrent Constraint Languages",
  booktitle    = "Constraint Programming",
  year         = 1994,
  editor       = "B.~Mayoh and E.~Tyugu and J.~Penjam",
  series       = "NATO ASI Series (Series F: Computer and Systems Sciences)",
  volume       = 131,
  publisher    = sv,
  address      = begr,
  keywords     = "synchronous programming, operational semantics, constraint
    systems, negative information, parallel composition",
  abstract     = "The areas of Qualitative Reasoning about physical systems
    (Weld and de Kleer 1989), reasoning about action and state change (Ginsberg
    1987), reactive, realtime computing (Real-time systems 1991) and concurrent
    programming languages (Milner 1980; Hoare 1985) are areas of inquiry that
    are fundamentally about the same subject matter — the representation,
    design and analysis of continuous and discrete dynamical systems.", 
  location     = "https://doi.org/10.1007/978-3-642-85983-0_15"
}

@InProceedings{srmc,
  author       = "Diomidis Spinellis",
  title        = "Software Reliability: Modern Challenges",
  booktitle    = "Proceedings ESRL '99 --- The Tenth European Conference on Safety and Reliability",
  year         = 1999,
  editor       = "G.~I. Schu{\" e}ller and P.~Kafka",
  pages        = "589--592",
  address      = "Munich-Garching, Germany",
  month        = sep,
  keywords     = "hardware, operating systems, software system architecture,
    programming languages, software development",
  abstract     = "The evolution of computer technology is creating for
    safety-critical systems new challenges and different types of failure
    modes.  Modern computer processors are often delivered with errors, while
    intelligent hardware subsystems may exhibit nondeterministic behaviour.
    Operating systems and programming languages are becoming increasingly
    complicated and their implementations less trustworthy.  In addition,
    component-based multi-tier software system architectures exponentially
    increase the number of failure modes, while Internet connectivity exposes
    systems to malicious attacks.  Finally, IT outsourcing and blind reliance
    on standards can provide developers with a false sense of security.
    Planning in advance for the new challenges is as important as embracing the
    new technology.", 
  location     = "https://www2.dmst.aueb.gr/dds/pubs/conf/1999-ESREL-SoftRel/html/chal.html"
}

@InProceedings{daomflfc,
  author       = "Gokul~V. Subramaniam and Eric~J. Byrne",
  title        = "Deriving an Object Model from Legacy {Fortran} Code",
  booktitle    = pot # "1996 International Conference on Software Maintenance (ICSM '96)",
  year         = 1996,
  pages        = "3--12",
  publisher    = "IEEE Press",
  address      = moncal,
  month        = "4--8 " # nov,
  keywords     = "re-engineering, object models, object-oriented design",
  abstract     = "The practice of software development continues to shift
    towards the use of object-oriented approaches.  The motivation for this
    trend is the benefits attributed to object-oriented software, including
    improved maintainability.  As organizations develop new object-oriented
    software, they face the problem of maintaining their older software.  How
    can existing non-objected-oriented software benefit from this new software
    engineering technology? This paper presents a nine step process for
    deriving an object model from existing unstructured FORTRAN source code.
    Both top-down and bottom-up approaches are used to derive objects, classes,
    class attributes and methods, and relationships among classes.  This
    process can be used within a reengineering project to convert legacy
    FORTRAN code into a new object-oriented implementation written in a
    language such as C++.  Experience with using this process is also
    described.", 
  location     = "https://dl.acm.org/doi/10.5555/645544.655855"
}

@InProceedings{aaotdfpfcimc,
  author       = "Geoffrey~H. Kuenning and Gerald~J. Popek and Peter~L. Reiher",
  title        = "An Analysis of Trace Data for Predictive File Caching in Mobile Computing",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "291--303",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file caching, off-line execution, workloads, trace analysis,
    user behavior",
  abstract     = "One way to provide mobile computers with access to the resources of a network, even in the absence of communication, is to predict which information will be used during disconnection and cache the appropriate data while still connected.  To determine the feasibility of this approach, traces of file-access activity for three diverse application domains were collected for periods of over two months.  Analysis of these traces using traditional and new measures reveals that user working sets tend to be small compared to modern disk sizes, that users tend to reference the same files for several days or even weeks at a time, and that different users do not tend to write to the same file except in highly constrained circumstances.  These factors encourage the conclusion that an automated caching system can be built for a wide variety of environments.",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267277",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/analysis-trace-data-predictive-file-caching"
}

@InProceedings{sscrfmi,
  author       = "Trevor Blackwell and Kee Chan and Koling Chang and Thomas Charuhas and James Gwertzman and Brad Karp and H.~T. Kung and W.~David Li and Dong Lin and Robert Morris and Robert Polansky and Diane Tang and Cliff Young and John Zao",
  title        = "Secure Short-Cut Routing for Mobile {IP}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "305--316",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "ip, mobility, routing, security, encapsulation, forwarding,
    short-cut routing",
  abstract     = "This paper describes the architecture and implementation of a mobile IP system.  It allows mobile hosts to roam between cells implemented with 2-Mbps radio base stations, while maintaining Internet connectivity.  The system is being developed as part of a course on wireless networks at Harvard and has been operational since March 1994.The architecture scales well, both geographically and in the number of mobile hosts supported.  It supports secure short-cut routing to mobile hosts using the existing Internet routing system without change.  The implementation demonstrates a robust, low complexity realization of the architecture, and provides trade-off opportunities between efficiency and cost.Measured performance of the mobile system is generally excellent.  The system can handle a high rate of location updates, and routes packets almost as efficiently for mobile hosts as the Internet does for stationary hosts.  We observe reasonable TCP behavior during hand-offs.",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/secure-short-cut-routing-mobile-ip",
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267278"
}

@InProceedings{prtpopwtl,
  author       = "Arthur Bernstein and Paul~K. Harter",
  title        = "Proving Real-Time Properties of Programs with Temporal Logic",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "1--11",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "temporal logic, proof systems, real-time systems",
  abstract     = "Wirth [Wi77] categorized programs into three classes.  The
    most difficult type of program to understand and write is a real-time
    program.  Much work has been done in the formal verification of sequential
    programs, but much remains to be done for concurrent and real-time
    programs.  The critical nature of typical real-time applications makes the
    validity problem for real-time programs particularly important.  Owicki and
    Lamport [OL80] present a relatively new method for verifying concurrent
    programs using temporal logic.  This paper presents an extension of their
    work to the area of real-time programs.  A model and proof system are
    presented and their use demonstrated using examples from the literature.", 
  location     = "https://doi.org/10.1145/1067627.806585"
}

@InProceedings{davoss,
  author       = "J.~M. Rushby",
  title        = "Design and Verifiation of Secure Systems",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "12--21",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed systems, isolation, security, mechanism",
  abstract     = "This paper reviews some difficulties arising when verifying
    kernelized secure systems and suggests new techniques for their resolution.
    It proposes that secure systems be conceived as distributed systems in
    which security is achieved partly through the physical separation of its
    individual components and partly through the mediation of trusted functions
    performed within some of those components. The security kernel lets such a
    'distributed' system run within a single processor; policy enforcement is
    not the concern of a security kernel. This approach decouples component
    verification, which perform trusted functions, from security kernel
    verification.  This latter task may be accomplished by a new verification
    technique called 'proof of separability' which explicitly addresses the
    security relevant aspects of interrupt handling and other issues ignored by
    present methods.", 
  location     = "https://doi.org/10.1145/1067627.806586"
}

@InProceedings{ankjfb,
  author       = "Joel~F. Bartlett",
  title        = "{A} {NonStop} Kernel",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "22--29",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "redundancy, fault models, processes, message passing, error
    recovery", 
  abstract     = "The Tandem NonStop System is a fault-tolerant, expandable,
    and distributed computer system designed for online transaction processing.
    This paper describes the operating system kernel's key primitives.  The
    first section describes the basic hardware building blocks and introduces
    their software analogs: processes and messages.  Using these primitives, a
    mechanism allowing fault-tolerant resource access, the process-pair, is
    described.  The paper concludes with some observations on this type of
    system structure, and on system use.",
  location     = "https://doi.org/10.1145/1067627.806587",
  location     = "https://www.hpl.hp.com/techreports/tandem/TR-81.4.pdf"
}

@InProceedings{ootdoaos,
  author       = "Hugh~C. Lauer",
  title        = "Observations on the Development of an Operating System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "30--36",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "software development, the five-to-seven-year model, os
    classification, pilot, mesa, software management",
  abstract     = "The development of Pilot, an operating system for a personal
    computer, is reviewed, including a brief history and some of the problems
    and lessons encountered during this development.  As part of understanding
    how Pilot and other operating systems come about, an hypothesis is
    presented that systems can be classified into five kinds according to the
    style and direction of their development, independent of their structure.
    A further hypothesis is presented that systems such as Pilot, and many
    others in widespread use, take about five to seven years to reach maturity,
    independent of the quality and quantity of the talent applied to their
    development.  The pressures, constraints, and problems of producing Pilot
    are discussed in the context of these hypotheses.",
  location     = "https://doi.org/10.1145/800216.806588"
}

@InProceedings{tffsmf,
  author       = "Marek Fridrich and William~J. Older",
  title        = "The {Felix} File Server",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "37--44",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "consistency, file sets, transactions, resiliency",
  abstract     = "This paper describes Felix - a File Server for an
    experimental distributed multicomputer system.  Felix is designed to
    support a variety of file systems, virtual memory, and database
    applications with access being provided by a local area network.  Its
    interface combines block oriented data access with a high degree of crash
    resistance and a comprehensive set of primitives for controlling data
    sharing and consistency.  An extended set of access modes allows increased
    concurrency over conventional systems.", 
  location     = "https://doi.org/10.1145/800216.806589"
}

@InProceedings{ewastfdntps,
  author       = "James~D. Guyton and Michael~F. Schwartz",
  title        = "Experiences with a Survey Tool for Discovering {Network Time Protocol} Servers",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "257--265",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "clock synchronization, ntp, remote monitoring, surveys",
  abstract     = "The Network Time Protocol (NTP) is widely used to synchronize
    computer clocks throughout the Internet.  Existing NTP clients and servers
    form a very large distributed system, and yet the tools available to
    observe and manage this system are fairly primitive.  This paper describes
    our experiences with a prototype tool that attempts to discover relevant
    information about every NTP site on the Internet.  The data produced by
    this tool can be used for a variety of purposes, including locating nearby
    accurate time servers and computing aggregate and long-term evaluations of
    the size and health of the NTP system.  Importantly, our tool provides a
    means by which new NTP server administrators can make informed choices
    among the possible servers with which to synchronize, balancing the need
    for accurate time with the need to distribute server load.  This is an
    important step towards improving global NTP system scalability, since at
    present our measurements indicate that the high-stratum servers are heavily
    overloaded.", 
  location     = "https://www.usenix.org/biblio-4262"
}

@InProceedings{lgccfic,
  author       = "Mummert, Lily~B. and Satyanarayanan, M.",
  title        = "Large Granularity Cache Coherence for Intermittent Connectivity",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "279--289",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "cache coherence, distributed file systems, coda, callbacks,
    cache management",
  abstract     = "To function in mobile computing environments, distributed
    file systems must cope with networks that are slow, intermittent, or both.
    Intermittence vitiates the effectiveness of callback-based cache coherence
    schemes in reducing client-server communication, because clients must
    validate files when connections are reestablished.  In this paper we show
    how maintaining cache coherence at a large granularity alleviates this
    problem.  We report on the implementation and performance of large
    granularity cache coherence for the Coda File System.  Our measurements
    confirm the value of this technique.  At 9.6 Kbps, this technique takes
    only 4 -- 20% of the time required by two other strategies to validate the
    cache for a sample of Coda users.  Even at this speed, the network is
    effectively eliminated as the bottleneck for cache validation.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/large-granularity-cache-coherence-intermittent",
  location     = "https://www.cs.cmu.edu/~satya/docdir/mummert-usenix-large-granularity-1994.pdf"
}

@InProceedings{patdluvi,
  author       = "Timothy~W. Curry",
  title        = "Profiling and Tracing Dynamic Library Usage Via Interposition",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "267--278",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "profiling, tracing, dynamic libraries, dynamic linking,
    interposition",
  abstract     = "Run-time resolution of library functions provides a rich and
    powerful opportunity to collect workload profiles and function/parameter
    trace information without source, special compilation, or special linking.
    This can be accomplished by having the linker resolve library functions to
    special wrapper functions that collect statistics before and after calling
    the real library function, leaving both the application and real library
    unaltered.  The set of dynamic libraries is quite large including
    interesting libraries like libc (the C library and Operating System
    interface), graphics, database, network interface, and many more.  Coupling
    this with the ability to simultaneously trace multiple processes on
    multiple processors covering both client and server processes yields
    tremendous feedback.  We have found the amount of detailed information that
    can be gathered has been useful in many stages of the project life-cycle
    including the design, development, tuning, and sustaining of hardware,
    libraries, and applications.This paper first contrasts our extended view of
    interposition to other profiling, tracing, and interposing techniques.
    This is followed by a description and sample output of tools developed
    around this view; a discussion of obstacles encountered developing the
    tools; and finally, a discussion of anticipated and unanticipated ways
    those tools have been applied.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267275"
}

@InProceedings{ptidec,
  author       = "Douglas~E. Comer and John~C. Lin",
  title        = "Probing {TCP} Implementations",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "245--255",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "remote monitoring, tcp, measurements, testing",
  abstract     = "In this paper, we demonstrate a technique called active
    probing used to study TCP implementations.  Active probing treats a TCP
    implementation as a black box, and uses a set of procedures to probe the
    black box.  By studying the way TCP responds to the probes, one can deduce
    several characteristics of the implementation.  The technique is
    particularly useful if TCP source code is unavailable.To demonstrate the
    technique, the paper shows example probe procedures that examine three
    aspects of TCP.  The results are informative: they reveal implementation
    flaws, protocol violations, and the details of design decisions in five
    vendor-supported TCP implementations.  The results of our experiment
    suggest that active probing can be used to test TCP implementations.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/probing-tcp-implementations"
}

@InProceedings{ossfdm,
  author       = "Ian~M. Leslie and Derek McAuley and Sape~J. Mullender",
  title        = "Operating-System Support for Distributed Multimedia",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "209--219",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "atm, desk-area network, log-structured file systems",
  abstract     = "Multimedia applications place new demands upon processors,
    networks and operating systems.  While some network designers, through ATM
    for example, have considered revolutionary approaches to supporting
    multimedia, the same cannot be said for operating systems designers.  Most
    work is evolutionary in nature, attempting to identify additional features
    that can be added to existing systems to support multimedia.  Here we
    describe the Pegasus project¿s attempt to build an integrated hardware and
    operating system environment from the ground up specifically targeted
    towards multimedia.", 
  location     = "https://research.utwente.nl/en/publications/pegasus-operating-system-support-for-distributed-multimedia-syste"
}

@InProceedings{suiagml,
  author       = "Lincoln Stein and Andre Marquis and Ert Dredge and Mary Pat Reeve and Mark Daly and Steve Rozen and Nathan Goodman",
  title        = "Splicing " # unix # " into a Genome Mapping Laboratory",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "221--229",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "database access, user interfaces",
  abstract     = "The Whitehead Institute/MIT Center for Genome Research is
    responsible for a number of large genome mapping efforts, the scale of
    which create problems of data and workflow management that dictate reliance
    on computer support.  Two years ago, when we started to design the
    informatics support for the laboratory, we realized that the fluid and
    ever-changing nature of the experimental protocols precluded any effort to
    create a single monolithic piece of software.  Instead we designed a system
    that relied on multiple distributed data analysis and processing tools knit
    together by a centralized database.  The obvious choice of operating
    systems was UNIX.  In order to make this choice palatable to the laboratory
    biologists--who rightly consider it their job to do experiments rather than
    to interact with computers, and who have come to expect all software to be
    as intuitive and responsive as the Apple Macintoshes on their desks--we
    designed a system that runs automatically and essentially invisibly.
    Whenever it is necessary for the informatics system to interact with a
    member of the laboratory we have carefully chosen a user interface paradigm
    that best balances the user's expectations against the system's
    capabilities.  When possible we have chosen to adapt familiar software to
    our user interface needs rather than to write user interfaces from scratch.
    We've managed to hide the power of UNIX behind the innocuous personal
    computer-based front ends our users know and love, using techniques that
    should be applicable in other environments as well.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/splicing-unix-genome-mapping-laboratory"
}

@InProceedings{atrpftuos,
  author       = "Liam R.~E. Quin",
  title        = "{A} Text Retrieval Package for the Unix Operating System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "231--243",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "search, text databases",
  abstract     = "This paper describes lq-text, an inverted index text
    retrieval package written by the author.  Inverted index text retrieval
    provides a fast and effective way of searching large amounts of text.  This
    is implemented by making an index to all of the natural-language words that
    occur in the text.  The actual text remains unaltered in place, or, if
    desired, can be compressed or archived; the index allows rapid searching
    even if the data files have been altogether removed.The design and
    implementation of lq-text are discussed, and performance measurements are
    given for comparison with other text searching programs such as grep and
    agrep.  The functionality provided is compared briefly with other packages
    such as glimpse and zbrowser.The lq-text package is available in source
    form, has been successfully integrated into a number of other systems and
    products, and is in use at over 100 sites.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/text-retrieval-package-unix-operating-system"
}

@InProceedings{aekbiopt,
  author       = "Robert~A. Alfieri",
  title        = "An Efficient Kernel-Based Implementation of {POSIX} Threads",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "59--72",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "kernel-space threading, kernel function calls, address-space
  
		  transitions", 
  abstract     = "This paper describes the kernel-based implementation of
    POSIX Threads (Pthreads) in the DG/UXTM operating system.  The
    implementation achieves time efficiency by using a general-purpose trap
    mechanism, known as a Kernel Function Call (KFC), that carries an order of
    magnitude less overhead than a traditional system call.  On a 50 MHz
    Motorola MC88110, the implementation can create and exit a thread (with the
    associated context switch) in 8.1 microseconds and yield to another thread
    in 4.0 microseconds.  The implementation also achieves space efficiency by
    paging and decoupling bulky data structures.The advantages of a
    kernel-based implementation include design simplicity, less code
    redundancy, optimization of global (interprocess) operations, avoidance of
    inopportune preemption, and global semantic flexibility.  The disadvantage
    is a monolithic design that lacks user-level flexibility.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267262", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/efficient-kernel-based-implementation-posix"
}

@InProceedings{uolstiadaer,
  author       = "Andrea~H. Skarra",
  title        = "Using {OS} Locking Services to Implement a {DBMS}:  An Experience Report",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "73--86",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "lock managers, file locking, two-pahse commit, lock servers",
  abstract     = "The paper describes a black-box analysis of the locking
    facilities in several UNIX-compatible operating systems for their ability
    to support transaction synchronization.  It assesses the facilities for
    their adequacy, flexibiilty, and performance.  Most of the operating
    systems in the study provide adequate support for simple two-phase locking
    transaction systems that don't require customized or priority-based
    scheduling of lock requests.  The performance depends on a variety of
    factors: the average execution time for a lock request varies directly with
    the number of cuncurrent locks in the system and indrectly with the number
    of files locked for a given number of lock requests.  The request time is
    smaller when the locked files are local to the requesting process instead
    of remote, and when a process locks a file's segments in order of adjacency
    rather than randomly.  For the areas in which the OS provides inadequate
    support, the paper proposes several specific remedies.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/using-os-locking-services-implement-dbms"
}

@InProceedings{tsaaokma,
  author       = "Jeff Bonwick",
  title        = "The Slab Allocator:  An Object-Cache Kernel Memory Allocator",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "87--98",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "storage allocation, free lists, constructing, finializing,
    homogeneity",
  abstract     = "This paper presents a comprehensive design overview of the
    SunOS 5.4 kernel memory allocator.  This allocator is based on a set of
    object-caching primitives that reduce the cost of allocating complex
    objects by retaining their state between uses.  These same primitives prove
    equally effective for managing stateless memory (e.g.  data pages and
    temporary buffers) because they are space-efficient and fast.  The
    allocator's object caches respond dynamically to global memory pressure,
    and employ an object-coloring scheme that improves the system's overall
    cache utilization and bus balance.  The allocator also has several
    statistical and debugging features that can detect a wide range of problems
    throughout the system.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267263"
}

@InProceedings{abup,
  author       = "Jeffrey~C. Mogul",
  title        = "{A} Better Update Policy",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "99--111",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file cache management, write back, cache flush, performance",
  abstract     = "Some file systems can delay writing modified data to disk, in
    order to reduce disk traffic and overhead.  Prudence dictates that such
    delays be bounded, in case the system crashes.  We refer to an algorithm
    used to decide when to write delayed data back to disk as an update policy.
    Traditional UNIX® systems use a periodic update policy, writing back all
    delayed-write data once every 30 seconds.  Periodic update is easy to
    implement but performs quite badly in some cases.  This paper describes an
    approximate implementation of an interval periodic update policy, in which
    each individual delayed-write block is written when its age reaches a
    threshold.  Interval periodic update adds little code to the kernel and can
    perform much better than periodic update.  In particular, interval periodic
    update can avoid the huge variances in read response time caused by using
    periodic update with a large buffer cache.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267264", 
  location     = "https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-94-4.html"
}

@InProceedings{tdfs,
  author       = "Morgan Clark and Stephen Rago",
  title        = "The Desktop File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "113--124",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file systems, block allocation, b+ trees, shadow paging",
  abstract     = "This paper describes the structure and performance
    characteristics of a commercial file system designed for use on desktop,
    laptop, and notebook computers running the UNIX operating system.  Such
    systems are characterized by their small disk drives dictated by system
    size and power requirements.  In addition, these systems are often used by
    people who have little or no experience administering Unix systems.  The
    Desktop File System attempts to improve overall system usability by
    transparently compressing files, increasing file system reliability, and
    simplifying administrative interfaces.  The Desktop File System has been in
    production use for over a year, and will be included in future versions of
    the SCO Open Desktop Unix system.  Although originally intended for a
    desktop environment, the file system is also being used on many larger,
    server-style machines.",
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/desktop-file-system"
}

@InProceedings{sahblfs,
  author       = "Ken Shirriff and John Ousterhout",
  title        = "Sawmill:  {A} High-Bandwidth Logging File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "125--136",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "raid, log-structured file systems, read batching, metadata,
    cleaning", 
  abstract     = "This paper describes the implementation of Sawmill, a network
    file system using the RAID-II storage system.  Sawmill takes advantage of
    the direct data path in RAID-II between the disks and the network, which
    bypasses the file server CPU.  The key ideas in the implementation of
    Sawmill are combining logging (LFS) with RAID to obtain fast small writes,
    using new log layout techniques to improve bandwidth, and pipelining
    through the controller memory to reduce latency.  The file system can
    currently read data at 21 MB/s and write data at 15 MB/s, close to the raw
    disk array bandwidth, while running on a relatively slow Sun-4.
    Performance measurements show that LFS improved performance of a stream of
    small writes by over a order of magnitude compared to writing directly to
    the RAID, and this improvement would be even larger with a faster CPU.
    Sawmill demonstrates that by using a storage system with a direct data
    path, a file system can provide data at bandwidths much higher than the
    file server itself could handle.  However, processor speed is still an
    important factor, especially when handling many small requests in
    parallel.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267265", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/sawmill-high-bandwidth-logging-file-system"
}

@InProceedings{nv3dai,
  author       = "Brian Pawlowski and Chet Juszczak and Peter Staubach and Carl Smith and Diane Lebel and Dave Hitz",
  title        = "{NFS} Version 3 Design and Implementation",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "137--152",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "nfs, large files, asynchronous writes, crash recovery, weak
    cache consistency, protocol development",
  abstract     = "This paper describes a new version of the Network File System
    (NFS) that supports access to files larger than 4GB and increases
    sequential write throughput sevenfold when compared to unaccelerated NFS
    Version 2.  NFS Version 3 maintains the stateless server design andsimple
    crash recovery of NFS version 2, and the philosophy of building a
    distrubted file service from cooperating protocols.  We describe the
    protocol and its implementation, and provide initial performance
    measurements.  We then describe the implementation effort.  Finally, we
    contrast this work with other distributed file systems and discuss future
    revisions of NFS.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/nfs-version-3-design-and-implementation"
}

@InProceedings{ctaddbnm,
  author       = "Cheng-Zen Yang and Chih-Chung Chen and Yen-Jen Oyang",
  title        = "Clue Tables:  {A} Distributed, Dynamic-Binding Naming Mechanism",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "153--160",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed naming, remote file systems",
  abstract     = "This paper presents a distributed, dynamic naming mechanism
    called clue tables for building highly scalable, highly available
    distributed file systems.  The clue tables naming mechanism is distinctive
    in three aspects.  First, it is designed to cope well with the hierarchical
    structure of the modern large-scale computer networks.  Second, it
    implicitlycarries out load balancing among servers to improve
    systemscalability.  Third, it supports file replication and dynamically
    designates a primary copy to resolve possible data inconsistency.  This
    paper also reports a performance evaluation of the clue tables mechanism
    when compared with NFS, a popular distributed file system.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/1267257.1267266"
}

@InProceedings{olownpiaso,
  author       = "Dan Duchamp",
  title        = "Optimistic Lookup of Whole {NFS} Paths in a Single Operation",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "161--169",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  abstract     = "VFS lookup code examines and translates path names one
    component at a time, checking for special cases such as mount points and
    symlinks.  VFS calls the NFS lookup operation as necessary.  NFS employs
    caching to reduce the number of lookup operations that go to the server.
    However, when part or all of a path is not cached, NFS lookup operations go
    back to the server.  Although NFS's caching is effective,
    component-by-component translation of an uncached path is inefficient,
    enough so that lookup is typically the operation most commonly processed by
    servers.  We study the effect of augmenting the VFS lookup algorithm and
    the NFS protocol so that a client can ask a server to translate an entire
    path in a single operation.  The preconditions for a successful request are
    usually but not always satisfied, so the algorithm is optimistic.  This
    small change can deliver substantial improvements in client latency and
    server load.", 
  keywords     = "path lookup, mount points, path cache", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267267", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/optimistic-lookup-whole-nfs-paths-single"
}

@InProceedings{acfcp,
  author       = "Pei Cao and Edward~W. Felten and Kai Li",
  title        = "Application-Controlled File Caching Policies",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "171--182",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "kernel allocation, swapping, file sharing, prefetching",
  abstract     = "We consider how to improve the performance of file caching by
    allowing user-level control over file cache replacement decisions.  We use
    two-level cache management: the kernel allocates physical pages to
    individual applications (allocation), and each application is responsible
    for deciding how to use its physical pages (replacement).  Previous work on
    two-level memory management has focused on replacement, largely ignoring
    allocation.The main contribution of this paper is our solution to the
    allocation problem.  Our solution allows processes to manage their own
    cache blocks, while at the same time maintains the dynamic allocation of
    cache blocks among processes.  Our solution makes sure that good user-level
    policies can improve the file cache hit ratios of the entire system over
    the existing replacement approach.  We evaluate our scheme by trace-based
    simulation, demonstrating that it leads to significant improvements in hit
    ratios for a variety of applications.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267268", 
  location     = "https://www.cs.princeton.edu/research/techreps/TR-445-94"
}

@InProceedings{rfcitffs,
  author       = "Peter Reiher and John Heidemann and David Ratner and Greg Skinner and Gerald Popek",
  title        = "Resolving File Conflicts in the {Ficus} File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "183--195",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "distributed file system, conflict resulution",
  abstract     = "Ficus is a flexible replication facility with optimistic
    concurrency control designed to span a wide range of scales and network
    environments.  Optimistic concurrency control provides rapid local access
    and high availability of files for update in the face of disconnection, at
    the cost of occasional conflicts that are only discovered when the system
    is reconnected.  Ficus reliably detects all possible conflicts.  Many
    conflicts can be automatically resolved by recognizing the file type and
    understanding the file's semantics.  This paper describes experiences with
    conflicts and automatic conflict resolution in Ficus.  It presents data on
    the frequency and character of conflicts in our environment.  This paper
    also describes how semantically knowledgeable resolvers are designed and
    implemented, and discusses our experiences with their strengths and
    limitations.  We conclude from our experience that optimistic concurrency
    works well in at least one realistic environment, conflicts are rare, and a
    large proportion of those conflicts that do occur can be automatically
    solved without human intervention.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267269", 
  location     = "https://www.isi.edu/~johnh/PAPERS/Reiher94a.pdf"
}

@InProceedings{rfsluapa,
  author       = "James Griffioen and Randy Appleton",
  title        = "Reducing File System Latency using a Predictive Approach",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "197--207",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "file system caching, prefetching, probabilistic prefetching",
  abstract     = "Despite impressive advances in file system through put
    resulting from technologies such as high-bandwidth networks and disk
    arrays, file system latency has not improved and in many cases has become
    worse.  Consequently, file system I/O remains one of the major bottlenecks
    to operating system performance [10].This paper investigates an automated
    predictive approach towards reducing file latency.  Automatic Prefetching
    uses past file accesses to predict future file systemrequests.  The
    objective is to provide data in advance of the request for the data,
    effectively masking access latencies.  We have designed and implement a
    system to measure the performance benefits of automatic prefetching.  Our
    current results, obtained from a trace-driven simulation, show that
    prefetching results in as much as a 280% improvement over LRU especially
    for smaller caches.  Alternatively, prefetching can reduce cache size by up
    to 50%.", 
  location     = "https://dl.acm.org/doi/10.5555/1267257.1267270"
}

@InProceedings{saifsnp,
  author       = "Thomas~Y.~C. Woo and Raghuram Bindignavle and Shaowen Su and Simon~S. Lam",
  title        = "{SNP}:  An Interface for Secure Network Programming",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "45--58",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "application-layer security, secure data transfer, sockets",
  abstract     = "SNP provides a high-level abstraction for secure end-to-end
    network communications.  It supports both stream and datagram semantics
    with security guarantees (e.g., data origin authenticity, data integrity
    and data confidentiality).  It is designed to resemble the Berkeley sockets
    interface so that security can be easily retrofitted into existing socket
    programs with only minor modifications.  SNP is built on top of GSS-API,
    thus making it relatively portable across different authentication
    mechanisms conforming to GSSAPI.  SNP hides the details of GSS-API (e.g.,
    credentials and contexts management), the communication sublayer as well as
    the cryptographic sublayer from the application programmers.  It also
    encapsulates security sensitive information, thus preventing accidental or
    intentional disclosure by an application program.", 
  location     = "https://www.usenix.org/conference/usenix-summer-1994-technical-conference/snp-interface-secure-network-programming", 
  location     = "https://www.cs.utexas.edu/users/lam/Vita/Cpapers/WBSL94.pdf"
}

@InProceedings{anoopls,
  author       = "Jeffrey~S. Haemer",
  title        = "{A} New Object-Oriented Programming Language: {\it sh}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "1--13",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "shell, object-oriented programming, file systems, process trees",
  abstract     = "Many have frittered away their time on C++, while overlooking
    the new POSIX.2-required, object-oriented language: sh.  As will be clear 
    from the enclosed code, the name may allude to the fact that the author
    would be embarrassed to have anyone ﬁnd out about it.  This paper
    introduces a tinybject-oriented programming system written entirely in
    POSIX-conforming shell scripts.", 
  location     = "https://www.usenix.org/biblio-4241"
}

@InProceedings{tomatc,
  author       = "Evan Adams",
  title        = "The Old Man and the {C}",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "15--26",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "c++, generics",
  abstract     = "'You can't teach an old dog new tricks' goes the old proverb.
    This is a story about a pack of old dogs (C programmers) and their odyssey
    of trying to lean new tricks (C++ programming).  C++ is a large, complex
    language which can easily be abused, but also includes may features to help
    programmers more quickly write higher quality code.  The TeamWare group
    consciously decided which C++ features to use and, just as importantly,
    which features not to use.  We also incrementally adopted those features we
    chose to use.  This resulted in a successful C++ experience." 
}

@InProceedings{kmiaefs,
  author       = "Matt Blaze",
  title        = "Key Management in an Encrypting File System",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "27--35",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "key escrow, smart cards, cryptographic file systems, ",
  abstract     = "As distributed computing systems grow in size, complexity and
    variety of application, the problem of protecting sensitive data from
    unauthorized disclosure and tampering becomes increasingly important.
    Cryptographic techniques can play an important role in protecting
    communication links and file data, since access to data can be limited to
    those who hold the proper key.  In the case of file data, however, the
    routine use of encryption facilities often places the organizational
    requirements of information security in opposition to those of information
    management.  Since strong encryption implies that only the holders of the
    cryptographic key have access to the cleartext data, an organization may be
    denied the use of its own critical business records if the key used to
    encrypt these records becomes unavailable (e.g., through the accidental
    death of the key holder).  This paper describes a system, based on
    cryptographic 'smartcards,' for the temporary 'escrow' of file encryption
    keys for critical files in a cryptographic file system.  Unlike
    conventional escrow schemes, this system is bilaterally auditable, in that
    the holder of an escrowed key can verify that, in fact, he or she holds the
    key to a particular directory and the owner of the key can verify, when the
    escrow period is ended, that the escrow agent has neither used the key nor
    can use it in the future.  We describe a new algorithm, based on the DES
    cipher, for the online encryption of file data in a secure and efficient
    manner that is suitable for use in a smartcard.", 
  location     = "https://www.mattblaze.org/papers/cfskey.pdf"
}

@InProceedings{atamfif,
  author       = "Marcus~J. Ranum and Frederick~M. Avolio",
  title        = "{A} Toolkit and Methods for {Internet} Firewalls",
  booktitle    = usenixs94,
  year         = 1994,
  pages        = "37--44",
  organization = "USENIX Association",
  address      = boma,
  month        = "6--10 " # jun,
  keywords     = "firewalls, application gateways, internet security",
  abstract     = "As the number of businesses and government agencies
    connecting to the Internet continues to increase, the demand for Internet
    firewalls — points of security guarding a private network from intrusion —
    has created a demand for reliable tools from which to build them.  We
    present the TIS Internet Firewall Toolkit, which consists of software
    modules and configuration guidelines developed in the course of a broader
    ARPA sponsored project.  Components of the toolkit, while designed to work
    together, can be used in isolation or can be combined with other firewall
    components.  The Firewall Toolkit software runs on UNIX systems using
    TCP/IP with the Berkeley socket interface.  We describe the Firewall
    Toolkit and the reasoning behind some of its design decisions, discuss some
    of the ways in which it may be configured, and conclude with some
    observations as to how it has served in practice."
}

@InProceedings{reoui,
  author       = "E. Merlo and J.~F. Girard and K. Kontogiannis and P. Panangaden and R. {De Mori}",
  title        = "Reverse Engineering of User Interfaces",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "171--179",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "user interfaces, reverse engineering, behavioral
    representations, interface structure, object oriented approach, process
    algebra, CCS, specification language, target language, representational
    method, COBOL/CICS environment",
  abstract     = "A method for reverse engineering user interfaces based on
    their structural and behavioral representations is presented.  The
    interface structure is represented using an object oriented approach while
    interface behavior is described using Milner's process algebra (CCS).  A
    specification language for user interfaces is designed for the multiple
    purposes of serving as a target language for the reverse engineering
    process, as a working specification language for interface redesign, and as
    a specification language for generating a new user interface for a specific
    platform.  The motivations and advantages of such a representational method
    are discussed together with examples of user interface reverse engineering
    in a common business-oriented language (COBOL)/CICS environment.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287767"
}

@InProceedings{affredlis,
  author       = "Peter Aiken and Alice Muntz and Russ Richards",
  title        = "A Framework for Reverse Engineering {DoD} Legacy Information Systems",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "180--191",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "design, reverse engineering, software and system requirements
    and specifications, data architecture, business rules, data modeling",
  abstract     = "A framework to legacy information systems, which have been
    selected by reverse engineers, in the US Department of Defense
    heterogeneous environment, is reported.  This approach was developed to
    recover business rules, domain information, functional requirements, and
    data architectures, largely in the form of normalized, logical data models.
    In a pilot study, the data from diverse systems (ranging from home grown
    languages and database management systems developed during the late 1960s
    to those using high order languages and commercial network database
    management systems) are reverse engineered.  The pilot study is being used
    to validate and refine the framework with real-life systems to develop a
    baseline approach for reverse engineering existing systems; to scope and
    estimate future system re-engineering costs; and to determine the economic
    viability of re-engineering, reverse, and forward engineering efforts.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287766"
}

@InProceedings{repvda,
  author       = "Herbert Ritsch and Harry~M. Sneed",
  title        = "Reverse Engineering Programs via Dynamic Analysis",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "192--201",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, dynamic analysis, program behavior,
    static analysis, esprit docket project, knowledge acquisition,
    post-documentation, software systems", 
  abstract     = "A tool-supported approach to extracting information about
    programs via a dynamic analysis of the program behavior is described.  The
    information obtained in this fashion is intended to supplement the
    information gained through the static analysis of the program source.  By
    joining the two different views of a program, it is hoped that a more
    complete specification of the program function may be developed.  This
    research has taken place within the scope of the ESPRIT DOCKET project to
    study means of knowledge acquisition from and post-documentation of
    existing software systems.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287765"
}

@InProceedings{drfstooop,
  author       = "C.~H. Kung and J.~Gao and P.~Hsia and J.~Lin and Y.~Yoyoshima",
  title        = "Design Recovery for Software Testing of Object-Oriented Programs",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "202--211",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "software testing, reverse engineering, object-oriented
    programming, functional testing, test strategy, test tool, object relation
    diagrams, block branch diagrams, object state diagrams",
  abstract     = "A reverse engineering approach for software testing of
    object-oriented programs is described.  The approach is based on a graphic
    model which consists of three types of diagrams: object relation diagrams;
    block branch diagrams; and object state diagrams.  These diagrams may be
    used to provide guidance on the order to test the classes and member
    functions; prepare member function test cases; prepare test cases for
    object state dependent behaviors and interaction between such behaviors;
    and provide graphic display of coverage information to a tester.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287764"
}

@InProceedings{apptsfre,
  author       = "M.~P. Ward and K.~H. Bennett",
  title        = "{A} Practical Program Transformation System for Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "212--221",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "semantic refinement, proof-theoretical refinement,
  wide-spectrum language",
  abstract     = "Program transformation systems provide one means of formally
    deriving a program from its specification.  A tool called ReForm is
    described.  It is designed to address the inverse problem to support the
    extraction of a specification from existing program code, using
    transformations.  This is an important activity during software
    maintenance.  One of the problems of transformation systems is the scarcity
    of practical tools which can address industrial scale problems, rather than
    contrived laboratory problems.  An analysis of the important software
    engineering factors that contribute to a successful transformation based
    tool is provided.  Results from using the tool are also presented.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287763"
}

@InProceedings{atmolcpaoaetfr,
  author       = "Philip Newcomb and Lawrence Markosian",
  title        = "Automating the Modularization of Large {COBOL} Programs:  Application of an Enabling Technology for Reengineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "222--230",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "modularization, cobol programs, reverse engineering, user interfaces",
  abstract     = "The development of a tool for modularizing large common
    business-oriented language (COBOL) programs is described.  The motivation
    for modularizing these programs is discussed, together with a manual
    modularization process.  The business motivation for building a tool to
    automate the manual process is indicated.  An enabling technology and its
    use in the development of the tool are discussed.  Experience to date in
    alpha-testing the tool is reported.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287762"
}

@InProceedings{spaairet,
  author       = "Howard~B. Reubenstein, Richard~L. Piazza, Susan~N. Roberts",
  title        = "Separating Parsing and Analysis in Reverse Engineering Tools",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "117--125",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "tool maintenance, language independence, design recovery",
  abstract     = "This paper describes the lessons learned in extending the
    capabilities of a reverse engineering tool to analyze both an additional
    dialect of the language it was initially built to parse and a new embedded
    assembly language.  The effort involved in this extension provides data to
    support the assertion that reverse engineering tools should create a clean
    separation between parsing the source code and analyzing it.  We discuss a
    language-independent modeling approach that lets us achieve this
    separation.  This paper also describes additional advantages to maintaining
    this separation such as support for multiple languages and design
    recovery.",
  location     = "http://doi.ieeecomputersociety.org/10.1109/WCRE.1993.287773"
}

@InProceedings{recsd,
  author       = "James~M. {Cross II}",
  title        = "Reverse Engineering Control Structue Diagrams",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "107--116",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "ada, flow charts, control structure diagrams, guis",
  abstract     = "The GRASP/Ada project (Graphical Representations of
    Algorithms, Structures, and Processes for Ada) has successfully created and
    prototyped a new algorithmic level graphical representation for Ada
    software, the Control Structure Diagram (CSD).  The primary impetus for
    creation of the CSD is to improve the comprehension efficiency of Ada
    software and, as a result, improve reliability and reduce costs.  The
    emphasis is on the automatic generation of the CSD from Ada PDL or source
    code to support reverse engineering and maintenance.  The CSD has the
    potential to replace traditional pretty-printed Ada source code.  The
    current prototype provides the capability for the user to generate CSDs
    from Ada PDL or source code with a level of flexibility suitable for
    practical application.",
  location     = "https://doi.org/10.1109/WCRE.1993.287774"
}

@InProceedings{ahatrpp,
  author       = "Alex Quilici",
  title        = "{A} Hybrid Approach to Recognizing Programming Plans",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "126--133",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "student programmers, program understanding, plan libraries,
    plan indexing, plan recognition",
  abstract     = "Most current models of program understanding are unlikely to
    scale up successfully.  Top-down approaches require advance knowledge of
    what the program is supposed to do, which is rarely available with aging
    software systems.  Bottom-up approaches require complete matching of the
    program against a library of programming plans, which is impractical with
    the large plan libraries needed to understand programs that contain many
    domain-specific plans.  This paper presents a hybrid approach to program
    understanding that uses an indexed, hierarchical organization of the plan
    library to limit the number of candidate plans considered during program
    understanding.  This approach is based on observations made from studying
    student programmers attempt to perform bottom-up understanding on
    geometrically-oriented C functions.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287772"
}

@InProceedings{fcfpr,
  author       = "Linda~M. Wills",
  title        = "Flexible Control for Program Recognition",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "134--143",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "graspr, subgraph parsing, chart parsing, advice, indexing,
    partitioning",
  abstract     = "Recognizing commonly used data structures and algorithms is a
    key activity in reverse engineering.  Systems developed to automate this
    recognition process have been isolated, stand-alone systems, usually
    targeting a specific task.  We are interested in applying recognition to
    multiple tasks requiring reverse engineering, such as inspecting,
    maintaining, and reusing software.  This requires a flexible, adaptable
    recognition architecture, since the tasks vary in the amount and accuracy
    of knowledge available about the program, the requirements on recognition
    power, and the resources available.  We have developed a recognition system
    based on graph parsing.  It has a flexible, adaptable control structure
    that can accept advice from external agents.  Its flexibility arises from
    using a chart parsing algorithm.  We are studying this graph parsing
    approach to determine what types of advice an enhance its capabilities,
    performance, and scalability.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287771",
  location     = "ftp://ftp.cc.gatech.edu/pub/groups/reverse/repository/flexible.ps"
}

@InProceedings{cttfore,
  author       = "Peter~G. Selfridge and Richard~C. Waters and Elliot~J. Chikofsky",
  title        = "Challenges to the Field of Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "144--150",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, realistic data, economic impact,
    communication, explicit goals, standard data sets",
  abstract     = "Driven by the economic importance of maintaining and
    improving the enormous base of existing software systems, the reverse
    engineering of software has been of rapidly growing interest over the past
    decade.  More and more commercial software tools support aspects of reverse
    engineering, and more and more researchers in academic and industrial
    organizations are addressing themselves to the fundamental problems of
    reverse engineering.  In the best of all worlds, we researchers on reverse
    engineering would be working together toward clear goals of great economic
    importance.  Unfortunately, it appears that we are mostly just groping
    around in a swamp, each looking for a bit of dry ground (whether or not it
    actually leads out of the swamp), and running into each other only
    occasionally.  If we are to make rapid and effective joint progress, a
    number of improvements need to be made in the way we are pursuing research.
    This position paper presents ten challenges for improvement in three areas:
    avoiding artificial data, focusing on concrete impact, and facilitation
    researcher communication.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287770"
}

@InProceedings{aafreord,
  author       = "William~J. Premerlani and Michael~R. Blaha",
  title        = "An Approach for Reverse Engineering of Relational Databases",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "151--160",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "omt notation, forward engineering, reverse engineering, data
    dictionaries, rdbms",
  abstract     = "The process of software re-engineering consists of a reverse 
    engineering step followed by a forward engineering step.  During reverse
    engineering, one takes a past design or an implementation that embodies a
    design and extracts the essential problem domain content while discarding
    design optimizations and implementation decisions.  During forward
    engineering, this model of the essence of an application becomes the basis
    for reimplementation in a new medium.  Object-oriented models facilitate
    the re-engineering proceeds because the same modeling paradigm is adept at
    representing abstract conceptual models and models with implementation
    decisions.  Based on our experience with several examples, we propose a
    process for reverse engineering of relational databases.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287769"
}

@InProceedings{ctatodre,
  author       = "Jean-Luc Hainaut and M.~Chandelon and C.~Tonneau and M.~Joris",
  title        = "Contribution to a Theory of Database Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "161--170",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "database design, database reverse engineering, data structure
  modeling, data structure extraction, data structure conceptualization",
  abstract     = "This paper proposes both a general framework and specific
    techniques for file and database reverse engineering, i.e.  recovering its
    conceptual schema.  The framework relies on a process/product model that
    matches formal as well as empirical design procedures.  Based on the
    analysis of database design processes, two major phases are defined, data
    structure extraction and data structure conceptualization.  For each phase,
    a set of activities is proposed.  Most of these activities can be described
    as transformation and integration of specification.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287768"
}

@InProceedings{tcapipu,
  author       = "Ted~J. Biggerstaff and Bharat~G. Mitbander and Dallas Webster",
  title        = "The Concept Assignment Problem in Program Understanding",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "27--43",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "reverse engineering, slicing, knowledge base, domain,
    connectionist, concept recognition, plausible reasoning",
  abstract     = "A person understands a program because he is able to relate
    the structures of the program and its environment to his conceptual
    knowledge about the world.  The problem of discovering individual human
    oriented concepts and assigning them to their implementation oriented
    counterparts for a given program is the concept assignment problem.  We
    argue that the solution to this problem requires methods that have a strong
    plausible reasoning component based on a priori knowledge.  We illustrate
    these ideas through example scenarios using an existing design recovery
    system called DESIRE.  Finally, we will evaluate DESIRE based on its usage
    on real-world problems over the years.", 
  location     = "https://dl.acm.org/doi/10.5555/257572.257679"
}

@InProceedings{rrefctss,
  author       = "Helen~M. Edwards and Malcolm Munro",
  title        = "{RECAST}:  Reverse Engineering from {COBOL} to {SSADM} Specification",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "44--53",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "repositories, ssadm, procedural models, software tools",
  abstract     = "The Reverse Engineering into CASE Technology method (RECAST)
    takes the source code for an existing COBOL system and derives a no-loss
    representation of the system documented in an Structured Systems Analysis
    and Design Method (SSADM) format.  This representation of the system is
    derived through the use of a series of transformations.  This paper
    describes the environment within which recast has been developed, outlines
    the stages and steps of the recast method and discusses the use of software
    support tools.  An overview is given of a case study that has been carried
    out for a live system.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/257572.257681"
}

@InProceedings{paisfre,
  author       = "Jon Beck and David Eichmann",
  title        = "Program and Interface Slicing for Reverse Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "54--63",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program slicing, redoumentation, design recovery, interface
    slicing, tree shaking, dependence graphs, ",
  abstract     = "Reverse engineering involves comprehending a software
    system's implementation and the ways the implementation evolved from the
    original design.  Automated support tools are an integral part of such
    effort.  This paper describes how program slicing techniques can be
    employed to assist in the comprehension of large software systems, through
    traditional slicing techniques at the statement level, and through a new
    technique, interface slicing, at the module level.", 
  location     = "https://dl.acm.org/doi/abs/10.5555/257572.257682"
}

@InProceedings{rrcflsbps,
  author       = "Jim~Q. Ning and Andre Engberts and Wojtek Kozaczynski",
  title        = "Recovering Reusable Components from Legacy Systems by Program Segmentation",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "64--72",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "cobol, data model recovery, program segmentation",
  abstract     = "There are many reasons to retire a legacy system.  But the
    system may contain critical business rules and other reusable assets that
    are not explicitly documented anywhere else.  A software reengineering
    technique called program segmentation is described.  It supports the
    recovery of these reusable assets from old code.  This technique consists
    of a focusing step, which helps the analyst localize, understand, and
    combine functional pieces in large programs, and a factoring step, which
    extracts the focused functional pieces and packages them into independent
    reusable modules.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287778"
}

@InProceedings{remfiradt,
  author       = "G.~Canfora and A.~Cimitile and M.~Munro",
  title        = "Reverse Engineering Method for Identifying Reusable Abstract Data Types",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "73--82",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "abstract data types",
  abstract     = "This paper presents results from an experiment in reuse
    within the RE2 project.  It shows how a particular candidature criterion
    for identifying abstract data types in existing software systems can be
    applied both at the theoretical and practical level.  The RE2 project is
    concerned with the exploration of reverse engineering and reengineering
    techniques to facilitate reuse reengineering by the identification and
    classification of approximate candidature criteria.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287777"
}

@InProceedings{iaeodicilp,
  author       = "Filippo Cutillo and Piernicola Fiore and Giuseppe Visaggio",
  title        = "Idenfification and Extraction of ``Domain Independent'' Components in Large Programs",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "83--92",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program slicing",
  abstract     = "This study addresses the problem of the identification and
    extraction of domain independent components from a large working program
    lacking complete documentation.  A modified version of Weiser's slice,
    which we shall call 'Direct Slice,' is defined which makes it possible to
    individuate and extract particular kinds of code segments, distributed
    among many of the modules composing the structure of the working program.
    This provides the advantage of being able to individuate and reaggregate
    program components coherently according to the principles of Information
    Hiding.  The techniques described are also characterized by a certain
    simplicity and can thus be automized.  In fact, a commercially available
    tool was used for experimenting the process.  The present study is based on
    the results of an experimental project performed on applicative programs
    used in the banking sector, characterized by a high grade of difficulty as
    regards maintenance.",
  location     = "https://doi.org/10.1109/WCRE.1993.287776"
}

@InProceedings{fciahomastau,
  author       = "Jean-Fran{\c c}ois Girard and Rainer Koschke",
  title        = "Finding Components in a Hierarchy of Modules:  a Step Towards Architectural Understanding",
  booktitle    = pot # "International Conference on Software Maintenance, ICSM '97",
  year         = 1997,
  pages        = "58--65",
  address      = "Bari, Italy",
  month        = "1--3 " # oct,
  keywords     = "atomic components, dominance analysis",
  abstract     = "This paper presents a method to view a system as a hierarchy
    of modules according to information hiding concepts and to identify
    architectural component candidates in this hierarchy.  The result of the
    method eases the understanding of a system's underlying software
    architecture.  A prototype tool implementing this method was applied to
    three systems written in C (each over 30 Kloc).  For one of these systems,
    an author of the system created an architectural description.  The
    components generated by our method correspond to those of this
    architectural description in almost all cases.  For the other two systems,
    most of the components resulting from the method correspond to meaningful
    system abstractions.",
  location     = "https://dl.acm.org/doi/10.5555/645545.656026"
}

@InProceedings{psfsadpu,
  author       = "David~P. Olshefski and Alan Cole",
  title        = "Prototype System for Static and Dynamic Program Understanding",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "93--106",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "program understanding, pundit, code browsing, semantic
    analysis, program analysis",
  abstract     = "A tool called PUNDIT (Program Understanding Investigation
    Tool) is described.  It is a prototype intended to serve as a vehicle for
    exploring and testing ideas in the area of program understanding; it
    combines static analysis information with information collected at runtime.
    The architecture of PUNDIT is described, together with its two main
    components (the C source analyzer and a graphical user interface).  Several
    of the views provided by the tool are explained, including a high-level
    structure chart, a dynamic call graph, a control flow graph animated during
    program execution, a type definition window, and others.  By integrating
    static and dynamic information, the tool provides a more comprehensive
    understanding of a program as the first step to reengineering or
    maintaining the application that can be obtained by static analysis
    alone.", 
  location     = "https://doi.org/10.1109/WCRE.1993.287775"
}

@InProceedings{ifasmire,
  author       = "K.~Lano and H.~Haughton",
  title        = "Integrating Formal and Strutured Methods in Reverse-Engineering",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "17--26",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "ssadm, object representations, structured notations, logical
    data structures, normal forms, ",
  abstract     = "This paper discusses the important issue of the interaction
    between structured and formal notations in reverse-engineering and
    maintenance, and describes a process which uses both forms of
    representation to support application understanding, evaluation,
    assessment, redesign, and re-engineering.  This process is supported by
    prototype tools implemented on the REDO toolkit and repository for
    reverse-engineering.  It aims to extend and integrate the processes.", 
  location     = "http://doi.ieeecomputersociety.org/10.1109/WCRE.1993.287782"
}

@InProceedings{acatpu,
  author       = "Koen Bertels and Philip Vanneste and Carlos {De Backer}",
  title        = "{A} Cognitive Approach to Program Understanding",
  booktitle    = pot # "Working Conference on Reverse Engineering",
  year         = 1993,
  editor       = "Richard~C. Waters and Elliot~J. Chikofsky",
  pages        = "1--7",
  organization = "IEEE Computer Society, Technical Committee on Software Engineering, Subcommittee on Reverse Engineering",
  publisher    = "IEEE Computer Society Press",
  address      = bama,
  month        = "21--23 " # may,
  keywords     = "cognitive model, programming knowledge, abstraction, program
    analysis, cliches",
  abstract     = "We present a method of program understanding based on a
    cognitive model of programming knowledge, incorporating the basic concepts,
    structures and strategies as used by human expert programmers.  The
    approach involves the generation of a high-level description of the program
    with as main advantages the use of abstraction and robustness with respect
    to conceptual errors.  The use of abstraction allows to transcends any
    syntactical variation and to focus on the semantics.  This also potentially
    allows the analysis of larger programs.  The method can cope with erroneous
    code, and generate a precise description of the bug(s) involved.  It also
    makes for method useful for integration in a tutoring system for
    programming.", 
  location     = "http://dx.doi.org/10.1109/WCRE.1993.287784"
}

@InProceedings{aacis,
  author       = "Nicholas Zvegintzov",
  title        = "Analyzing and Controlling Installed Software",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "65--67",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "system modeling, real-time systems, configuration
    management", 
  abstract     = "Conclusion:  You have an engineering problem."
}

@InProceedings{pc,
  author       = "Horst Zuse",
  title        = "Program Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "61--64",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "measurement, program comprehension, mccabe metrics",
  abstract     = "Measurement theory can advance program comprehension.
    Empirical relational systems consisting of axioms can support program
    comprehension."
}

@InProceedings{ripcsaad,
  author       = "Norman Wilde",
  title        = "Research in Program Comprehension:  Some Analogies and Directions",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "58--60",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "medicine, standardization, software engineering, analogies",
  abstract     = "The exploding size and complexity of computer software
    undoubtedly create unique problems for its maintainers.  Still, software
    professionals might gain insight by considering how other professions have
    managed to progress despite the great complexity of the systems they deal
    with To develop a research agenda, we might consider some interesting
    analogies from the practice of medicine."
}

@InProceedings{rfptpa,
  author       = "Larry {Van Sickle} and Michael Ballantyne",
  title        = "Reengineering for Porting Transaction Processing Applications",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "51--53",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "user interfaces, control structures, reconstruction",
  abstract     = "The Reverse Engineering group at EDS Research has been
    developing representations and software tools to mechanically assist in
    understanding and reengineering COBOL transaction processing applications.
    An EDS division approached us to help them convert a minicomputer
    application to run under CICS on an IBM mainframe, each platform providing
    a different environment.  The user interacts with the minicomputer on field
    at a time, but interacts with CICS a full screen at a time.  This and other
    differences between the environments demand sophisticated feature
    extraction and restructuring techniques.  This paper describes the nature
    of this large commercial problem and the tools and techniques being applied
    to solve it."  
}

@InProceedings{fccmttc,
  author       = "A.~{von Mayrhauser} and A.~M. Vans",
  title        = "From Code Comprehension Model to Tool Capabilities",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "54--57",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "code comprehension models, knowledge base, situation models,
    program models",
  abstract     = "A major part of the maintenance effort is understanding the
    existing code. If we can define and present the maintenance programmer with
    information that best helps to understand the code, we can significantly
    improve quality and efficiency of program understanding and thus
    maintenance.  Our research goal is to develop a tool capability model based
    on an integrated code comprehension model.  This will lead tools that
    support maintenance tasks more effectively."
}

@InProceedings{ipcooosswood,
  author       = "Johannes Sametinger",
  title        = "Improving Program Comprehension of Object-Oriented Software Systems with Object-Oriented Documentation",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "48--50",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "class libraries, application frameworks, documentation
    inheritance",
  abstract     = "Good (system) documentation should be complete, current, and
    consistent in style.  We apply object-oriented technology to documentation
    in order to improve its quality by better reflecting the logical structure
    of a system.  This way of organizing software documentation erases program
    comprehension of object-oriented systems." 
}

@InProceedings{rebspaads,
  author       = "Spencer Rugaber",
  title        = "Reverse Engineering by Simultaneous Program Analysis and Domain Synthesis",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "45--46",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "synchronized refinement, domain analysis",
  abstract     = "Synchronized refinement consists of the parallel analysis of
    the source code and synthesis of a functional description.  The process is
    driven by the detection of design decisions in the source code.  Each
    decision is annotated in the functional description.  The annotation states
    how the decision contributes to the function accomplished by the
    corresponding code.  After each decision is detected and annotated, the
    corresponding part of the source code is replaced by an abbreviated
    description.  In this way, the source code continually grows shorter while
    the functional description grows more complete." 
}

@InProceedings{rotmisc,
  author       = "Charles Altizer and Valdis Berzins",
  title        = "Role of Translation Mechanisms in Software Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "1--3",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "domains, specification, translation",
  abstract     = "Software reengineering is a reaction to growing software
    costs.  It is often more cost-effective to keep and maintain existing
    software as user requirements or operating environments change.  Forward
    and reverse engineering are significant components of software
    reengineering.  Due to the lack of available documentation for many legacy
    software it is necessary to analyze source code to extract design
    information so software can be modified.  A common point of reference for
    software reengineering is the original system design.  Software maintenance
    requires understanding the design, that is, comprehending software is
    significant to reengineering.  Translation expresses some aspects of
    reverse engineering.  Forward engineering translates system specifications
    into implementation source code.  Sophisticated translator generator tools
    let maintenance programmers develop forward and reverse engineering
    translators which automate much of reengineering and let legacy systems be
    effectively updated and maintained quickly at lower costs." 
}

@InProceedings{dfabfrpu,
  author       = "Cornelia Boldyreff",
  title        = "Design Frameworks:  a Basis for Recording Program Understanding",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "4--5",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "design frameworks, system understanding, change",
  abstract     = "The method of design frameworks developed by the author has
    been developed to support both the recording of program understanding and
    its subsequent reuse in both maintenance and development.  Although the
    initial focus in development has been primarily on describing software
    concepts at the higher levels of abstraction required to facilitate their
    reuse during the conceptual phase of design, this message is equally
    applicable to accommodating other levels of understanding more appropriate
    for maintenance of existing software." 
}

@InProceedings{srut,
  author       = "Frank~H. Calliss",
  title        = "Specification Recovery Using Transformations",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "6--8",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "specifications, program slicing, derivations, combinations",
  abstract     = "Existing revere engineering techniques are not well suited to
    large programs, as they often require that an entire program be analyzed
    before any meaningful information is returned to a programmer.  A form of
    incremental reverse engineering is presented, whereby a programmer can
    derive the specification for sections of the program's code.  These partial
    specifications can be stored in such a way that they can be reused in later
    reverse engineering activities, thereby overcoming one of the problems that
    exist with current reverse engineering techniques, namely that the result
    of a previous reverse engineering activity is lost or recorded in a form
    that does not facilitate reuse." 
}

@InProceedings{uooptd,
  author       = "Frank~W. Calliss and Suzanne~W. Dietrich",
  title        = "Understanding Object-Oriented Programs Through Deduction",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "9--12",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "deductive databases, system understanding, horn clauses",
  abstract     = "Deductive databases allow for the declarative specification
    and efficient evaluation of (recursive) rules expressed in logic.  By
    extending the conceptual simplicity of the relational data model with a
    rule capability, deductive databases offer the flexibility to both retrieve
    and reason about knowledge in the database, which is in the form of both
    facts and rules." 
}

@InProceedings{vvafpsc,
  author       = "G.~Canfora and A.~Cimitile and U.~{De Carlini}",
  title        = "{VAPS}:  Visual Aids for {Pascal} Software Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "13--14",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "graph presentations, pascal software, ux",
  abstract     = "The VAPS environment is being developed in an on-going
    project jointly by the DIS (Dept.  of 'Informatica e Sistemistica') of the
    University of Naples and the DIS of the University of Rome.  VAPS consists
    of a reverse engineering subsystem and a visual environment.  The reverse
    engineering subsystem produces information and documents by static analysis
    of Pascal code, while the visual environment interacts with the user.  Both
    graphical and textual documents are used to provide the user with the
    information abstracted by reverse engineering.  All the documents produced
    are linked together in an hyper-text fashion and a multi-window environment
    is used to show them.  These documents describe a software system according
    to three different levels of abstractions: high level design, low-level
    design and code." 
}

@InProceedings{pspcw,
  author       = "Bill Curtis",
  title        = "Position Statement --- Program Comprehension Workshop",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "18--20",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "segmentation, verification, hypothesis generation, synthesis",
  abstract     = "Program comprehension or its design is a central component of
    many larger software tasks which as coding, debugging, and modification.
    Based on observations from previous research and reports such as that by
    Basili and Mills, we propose a simple model of program comprehension from
    which to derive our dependent variables and operational hypotheses for this
    experiment.  The model will be limited to the performance of tasks
    involving a single module rather than to tasks involving several modules.
    The structure of a model for multi-module tasks might be similar to the one
    proposed here, but would be tailored for processes at a higher level of
    program abstraction." 
}

@InProceedings{upsfsc,
  author       = "Filippo Cutillo and Filippo Lanubile and Giuseppe Visaggio",
  title        = "Using Program Slicing for Software Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "21--22",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "program slicing, functional abstraction",
  abstract     = "We propose program slicing as a technique of software
    comprehension which is coherent with the top-down theory.  Program slicing
    has been applied to debugging, parallel processing, module integration,
    evaluation of module cohesion, testing and modification.  Our objective is
    to extract conceptual function from programs using program slicing.  To
    achieve this goal we use a partial knowledge of the application derived
    from maintainer expertise and reverse engineering data."
}

@InProceedings{asmucas,
  author       = "David~A. Dampier and Luqi",
  title        = "Automated Software Maintenance Using Comprehension and Specification",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "24--26",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "prototypes, psdl, conflict resolution",
  abstract     = "More complex software demands more sophisticated software
    development and maintenance methods.  These methods can also be used when
    dealing with old software systems.  Program comprehension methods extract a
    specification from an old software system, and our methods are then used in
    forward engineering of the new software system.  In the Computer-Aided
    Prototyping System (CAPS), quickly built and iteratively updated prototypes
    of the intended system are demonstrated to the user.  As these updates
    occur, a formal mechanism must be developed to automatically integrate
    these changes into the existing prototype.  This paper formalizes the
    update/change integration process and extends the idea to multiple changes
    to the same base prototype.  Application of this technology include:
    automatic updating of different versions of existing software with changes
    made to the base system; integrating changes made by different design teams
    during development; and performing consistency checking after integration
    of seemingly disjoint changes to the same software system." 
}

@InProceedings{airfpc,
  author       = "Mary Jean Harrold and Brian Malloy",
  title        = "An Integrated Representation for Program Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "27--28",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "program maintenance, unified procedural graph, program
    understanding",
  abstract     = "Our approach is to develop an integrated program
    representation and use it to provide many different maintenance tools to
    assist in program understanding.  Previously, we presented our unified
    interprocedural graph that integrates control- and data-flow along with
    control- and data-dependency information into a single representation.
    However, to get this interprocedural information, we require both the
    control flow graphs and the PDG's of each procedure involved.  We have
    extended this research to provide a single integrated representation for
    each procedure that contains the data- and control-flow information
    provided by the individual graphs.  Our integrated representation is an
    extension of the PDG that eliminates the need for the control flow graph of
    a procedure in most cases.  Thus, we use our extended PDG as our only
    procedure representation from which we abstract information to construct
    the unified interprocedural graph and for tools assist in program
    understanding." 
}

@InProceedings{apcbcr,
  author       = "Wojtek Kozaczynski and Jim Ning",
  title        = "Automating Program Comprehension by Concept Recognition",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "29--31",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "concept recognition, program comprehension",
  abstract     = "Browsing-based tools assist in exploring program properties.
    It's the user's responsibility to reason about the meaning of the program
    under analysis.  As the user builds understanding of individual concepts,
    existing tools don't help generalize the knowledge to automate the
    understanding of similar concepts in other programs.  Providing this type
    of assistance to program maintainers requires automating program
    comprehension." 
}

@InProceedings{tfsc,
  author       = "David~P. Olshefski",
  title        = "Tools Facilitating Software Comprehension",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "32--34",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "architecture, pundit, data presentation",
  abstract     = "Our recent work developed a prototype program understanding
    system. It serves as a vehicle for exploring and testing program
    understanding ideas.  The system, named PUNDIT, combines statically
    collected semantic information with debugging capabilities to help the
    programmer understand both the static nature and dynamic behavior of a
    program.  Our experience has led us to the following conclusions concerning
    program comprehension systems: Static and dynamic information should be
    integrated and presented together.  Presenting large amounts of data to the
    user in a useful manner is a more difficult problem than source code
    analysis.  Integration into current build processes and library systems is
    important.  Program comprehension systems should take advantage of existing
    compiler technology.  Performance is a key to user acceptance and this
    means, for now at least, that the use of a database to contain analysis
    information is unacceptable." 
}

@InProceedings{rakfic,
  author       = "Stephen~B. Ornburn and Richard~J. LeBlanc",
  title        = "Recovering Application Knowledge from Imperative Code",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "35--37",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "application knowledge, knowledge structure, application
  rules, ",
  abstract     = "A model relating rule-based representations of application
    knowledge and imperative programs encoding is introduced.  This model,
    developed over the course of several experiments in reverse engineering,
    provides insight into the technical problems which must be solved when
    making explicit the conditions under which operations are performed.  This
    model is particularly useful in its ability to distinguish between code
    supporting the program's functional behavior from code responsible for
    ensuring that various safety and liveness constraints are satisfied." 
}

@InProceedings{putahiqfoarer,
  author       = "Gary Ostrolenk",
  title        = "Program Understanding Through Ad Hoc, Interactive Query Facilities on a Reverse Engineering Repository",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "38--41",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "database queries, attributes, views, identity",
  abstract     = "In any reverse engineering toolkit, facilities for the
    maintenance engineer to query its repository directly are essential for
    program comprehension.  Along with structured source code browsers and
    static analysis tools, they enable the engineer to gain an initial
    understanding of the code and they support the engineering in abstracting
    to higher level descriptions of the application.  This paper outlines some
    of the requirements such query facilities should satisfy." 
}

@InProceedings{pcaacp,
  author       = "Alex Quilici",
  title        = "Program Comprehension as a Cooperative Process",
  booktitle    = "Workshop Notes on Program Comprehension, CSM-92",
  year         = 1992,
  pages        = "42--44",
  organization = "IEEE Computer Society",
  address      = orfl,
  month        = "9 " # nov,
  keywords     = "program understanding, data collection",
  abstract     = "We've been exploring approaches to automated program
    understanding of realistic C and Fortran programs.  Our experience in this
    endeavor have led to us to the belief that automated program understanding
    is doomed unless it's designed as part of a complete environment to assist
    programmers in understanding software." 
}

@Manual{ai43ict,
  title        = "An Introductory 4.{3BSD} Interprocess Communiation Tutorial",
  author       = "Stuart Sechrest",
  organization = csd # ucb,
  address      = beca,
  keywords     = "pipes, socket pairs",
  location     = "https://docs.freebsd.org/44doc/psd/20.ipctut/paper.pdf"
}

@Manual{prapirpca,
  title        = "{PI}-{RPC}:  {A} Platform-Independent Remote Procedure Call Architecture",
  author       = "Randy~J. Ray",
  year         = 2000,
  keywords     = "transport model, message architecture, rpc",
  abstract     = "This introduction of remote procedure call (RPC) services
    using XML and the encoding scheme simplified what was once a daunting
    aspect of distributed computing.  However, the current work stands in a
    frozen state for the sake of compliance with existing software
    implementations This proposal outlines a new model that builds on the
    existing basis of XML-RPC in a modular fashion, maintaining compatibility
    with the existing specification.", 
  location     = "http://www.blackperl.com/xml/PI-RPC.html"
}

@Manual{dcwlmtldhohs,
  title        = "Deplate --- Convert Wiki-like Markup to Latex, {Docbook, HTML}, or ``{HTML} Slides''",
  author       = "Thomas Link",
  year         = 2004,
  month        = aug,
  keywords     = "latex, html, docbook",
  location     = "http://deplate.sourceforge.net/"
}

@Misc{tfmcdp,
  author       = "Gopalan Suresh Raj",
  OPTtitle     = "The Factory Method (Creational) Design Pattern",
  howpublished = "Web page",
  year         = "2000",
  keywords     = "design patterns, factory, instance creation",
  location     = "http://gsraj.tripod.com/design/creational/factory/factory.html"
}

@Misc{cfsrcai,
  author       = "Avi Rappoport",
  title        = "Checklist for Search Robot Crawling and Indexing",
  howpublished = "http://www.searchtools.com/robots/robot-checklist.html",
  year         = "2003",
  month        = "18 " # jul,
  keywords     = "web crawling, spiders, indexing",
  abstract     = "This document provides both technical information and some
    background and insight into what search engine indexing robots should
    expect to encounter .  Technically, the problems arise from
    misunderstandings and exploitation of anomalies by HTML creators (direct
    tagging, WYSIWYG and automated systems), and the tendency of browser
    applications to be very forgiving in their interpretation of pages and
    links.  Therefore, it's impossible to simply read the HTML and HTTP
    specifications and follow the rules there -- the real world is much messier
    than that.", 
  location     = "http://www.searchtools.com/robots/robot-checklist.html"
}

@Misc{ejag,
  author       = "Alan Griffiths",
  title        = "Exceptional Java",
  howpublished = "web page",
  year         = "2002",
  keywords     = "java, exceptions, programming style",
  location     = "https://accu.org/index.php/journals/406"
}

@Misc{gfmv,
  author       = "Mateusz Viste",
  title        = "Gopher {FAQ}",
  howpublished = "ftp",
  year         = "2015",
  month        = sep,
  keywords     = "gopher, faq",
  abstract     = "Common Questions and Answers about the Internet Gopher, a
    client/server protocol for making a world wide information service, with
    many implementations.",
  location     = "URL:gopher://gopher.viste.fr/1/gopher-faq"
}

@Misc{tropf,
  author       = "Daved Niewert",
  title        = "The Rise of Pseudo Fascism",
  howpublished = "web page",
  year         = 2005,
  month        = "25 " # feb,
  keywords     = "conservatism, fascism, pseudo-fascism, one-party states",
  location     = "https://dneiwert.blogspot.com/The%20Rise%20Of%20Pseudo%20Fascism.pdf"
}

@Misc{wtwtdrekp,
  author       = "Karl~E. Wiegers",
  title        = "When Telepathy Won't Do:  Requirements Engineering Key Practices",
  howpublished = "https://www.processimpact.com",
  year         = 2000,
  month        = jul,
  keywords     = "requirements engineering, planning, elicitation, analysis,
    specification, verification, management",
  abstract     = "The software industry exhibits an increasing interest in
    requirements engineering — that is, understanding what you intend to build
    before building it.  Time spent understanding the business problem is an
    excellent investment.  Clients are getting serious about requirements
    because the pain of building poor products is too great.  Applying
    established good practices on projects best achieves requirements success.
    This paper describes a requirements development process framework
    incorporating key practices.  Some highly exploratory or innovative
    projects can tolerate the excessive rework resulting from informal
    requirements engineering.  Most development efforts will benefit from a
    more deliberate and structured approach.", 
  location     = "https://www.processimpact.com/articles/telepathy.pdf"
}

@Misc{txeadft,
  author       = "Doug Hayes",
  title        = "The {XTANGO} Environment and Differences from {TANGO}",
  howpublished = unpublished,
  year         = 1990,
  month        = "3 " # nov,
  keywords     = "algorithm animation, tango"
}

@InBook{aaiissd,
  author       = "Douglas~E. Comer and David~L. Stevens",
  title        = "Internetworking with {TCP/IP}",
  chapter      = "Chapter 8: Algorithms and Issues in Server Software Design",
  publisher    = ph,
  year         = 2001,
  pages        = "101--123",
  volume       = "3",
  address      = usrnj,
  keywords     = "concurrency, iterative, connection-oriented, connectionless,
    transport protocol, statelessness, reliability, sockets, helper processes",
  location     = "TK 5105.585.C66"
}

@InBook{hpw,
  author       = "Network Associates, Inc",
  title        = "Introduction to Cryptography",
  chapter      = "How {PGP} Works",
  year         = 1999,
  keywords     = "encryption, decryption, public key cryptography, digital
    signatures, digital certificates, validity, trust",
  location     = "https://www.itu.int/en/ITU-D/Cybersecurity/Documents/01-Introduction%20to%20Cryptography.pdf"
}

@PhDThesis{cqtg,
  author       = "Grust, Torsten",
  title        = "Comprehending Queries",
  school       = "University of Konstanz",
  year         = 1999,
  address      = "Konstanz, Germany",
  month        = sep,
  keywords     = "category theory, categorical datatypes, query compilation,
    normalization, combinators, comprehensions, query deforestation, functors,
    catamorphisms",
  abstract     = "There are no compelling reasons why database-internal query
    representations have to be designated by operators.  This text describes a
    world in which datatypes determine the comprehension of queries.  In this
    world, a datatype is characterized by its algebra of value constructors.
    These algebras are principal.  Query operators are secondary in the sense
    that they simply box (recursive) programs that describe how to form a query
    result by application of datatype constructors.  Often, operators will be
    unboxed to inspect and possibly rewrite these programs.  Query optimization
    then means to deal with the transformation of programs.  The predominant
    role of the constructor algebras suggests that this model understands
    queries as mappings between such algebras.  The key observation that makes
    the whole approach viable is that (a) homomorphic mappings are expressive
    enough to cover declarative user query languages like OQL or recent SQL
    dialects, and, at the same time, (b) a single program form suffices to
    express homomorphisms between constructor algebras.  Reliance on a single
    combining form, catamorphisms, renders the query programs susceptible to
    Constructive Algorithmics, an effective and extensive algebraic theory of
    program transformations.  The text then takes a step from catamorphisms
    towards a higher-level query representation based on the categorical notion
    of monads.  In a nutshell, monads are algebras exhibiting exactly the
    structure that is needed to support the interpretation of a query calculus,
    the monad comprehension calculus.  Built on top of the abstract monad
    notion, the calculus maps a variety of query constructs (e.g., bulk
    operations, aggregates, and quantifiers) to few syntactic forms.  The
    uniformity of the calculus facilitates the analysis and transformation,
    especially the normalization, of its expressions.  Few but generic calculus
    rewriting rules suffice to implement query transformations that would
    otherwise require extensive rule sets.  The text rediscovers well-known
    query optimization knowledge on sometimes unusual paths that are more
    practicable to follow for an optimizer, though.  Solutions previously
    proposed by others can be simpliﬁed and generalized mainly due to the clear
    account of the structure of queries that the monad comprehension
    calculus—thanks to its density—provides.  The calculus effectively supports
    query optimization in the presence of grouping, various forms of nesting,
    aggregates, and quantifiers.  Although built on top of abstract concepts
    like homomorphisms and monads, this query model is specific enough to grasp
    implementation issues, such as the generation of stream-based (pipelined)
    query execution plans, whose treatment has traditionally been delayed until
    query runtime.  It is the main objective of this thesis to show that
    catamorphisms and monad comprehensions enable a comprehension of queries
    that is eﬀective and easily exploitable inside a query optimizer.", 
  location     = "http://kops.uni-konstanz.de/urn/urn:nbn:de:bsz:352-opus-3120"
}

@Proceedings{fecotfsd,
  title        = pot # "First European Conference on \TeX\ for Scientific Documentation",
  year         = 1985,
  editor       = "Dario Lucarella",
  publisher    = aw,
  address      = "Como, Italy",
  month        = "16--17 " # may,
  keywords     = "tex, document preparation, metafont"
}

@Proceedings{tugnam88,
  title        = pot # "\TeX\ Users Group Ninth Annual Meeting",
  year         = 1988,
  editor       = "Christina Thiele",
  organization = "\TeX\ Users Group",
  address      = "Montreal, Ontario, Canada",
  month        = "22--24 " # aug,
  keywords     = "tex, document production, typesetting, textbook production,
    multilingual documents, sgml, dvi"
}

@Proceedings{tugnam87,
  title        = pot # "\TeX\ Users Group Eighth Annual Meeting",
  year         = 1987,
  editor       = "Christina Thiele",
  organization = "\TeX\ Users Group",
  address      = sewa,
  month        = "24--26 " # aug,
  keywords     = "tex, document production, typesetting, textbook production,
    multilingual documents, sgml, dvi"
}

% Local Variables:
% eval: (set-register ?b "  journal      = sosp87,\n  year         = 1987,\n  volume       = 21,\n  number       = 5,\n  pages        = \"--\",\n  month        = nov,\n")
% End:
		  
