.so bibtex.header

@string{asplos04 = sigplan # " (" # pot # "Eleventh International Conference on " # asplos # ", ASPLOS XI)"}
@string{asplos06 = sigplan # " (" # pot # "Twelfth International Conference on " # asplos # ", ASPLOS XII)"}
@string{asplos08 = sigplan # " (" # pot # "Thirteenth International Conference on " # asplos # ", ASPLOS XIII)"}
@string{asplos09 = sigplan # " (" # pot # "Fourteenth International Conference on " # asplos # ", ASPLOS XIV)"}
@string{asplos10 = sigplan # " (" # pot # "Fifteenth International Conference on " # asplos # ", ASPLOS XV)"}
@string{lctes08 = sigplan # " (" # pot # "2008 ACM SIGPLAN-SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems, LCTES'08)"}
@string{sosp01    = osr # " (" # pot # "Eighteenth" # sosp # ", SOSP '01)"}
@string{osdi02    = osr # " (" # pot # "Fifth " # osdi # ")"}
@string{oopsla86    = sigplan # " (Conference Proceedings on Object-Oriented Programming Systems, Languages And Applications, OOPSLA '86)"}
@string{sapl80 = sigplan # " (" # pot # "ACM-SIGPLAN Symposium on the Ada Programming Language)"}

@Book{tsbotw,
  author       = "Victor Pelevin",
  title        = "The Sacred Book of the Werewolf",
  publisher    = "Viking",
  year         = 2005,
  address      = nyny,
  keywords     = "philosophy, were-creatures, post-soviet russia",
  location     = "PG 3485.E38 S8713"
}

@Book{hps1984,
  author       = "Pamela Sargent",
  title        = "Homesmind",
  subtitle     = "The Watchstar Trilogy Book III",
  publisher    = "Harper \& Row",
  year         = 1984,
  address      = nyny,
  keywords     = "survival",
  location     = "PZ 7.S2472 Ho 1984"
}

@Book{sewpr,
  author       = "Matthias Felleisen and Robert Bruce Findler and Matthew Flatt",
  title        = "Semantics Engineering with {PLT Redex}",
  publisher    = mitp,
  year         = 2009,
  address      = cama,
  keywords     = "reduction semantics, language semantics",
  location     = "QA 76.73.R227 F45"
}

@Book{dhwj,
  author       = "Joe Bageant",
  title        = "Deer Hunting with Jesus",
  subtitle     = "Dispatches from America's Class War",
  publisher    = "Crown",
  year         = 2007,
  address      = nyny,
  keywords     = "working class, class war, economics, politics, virginia",
  location     = "HN 90.S6 B32 2007"
}

@Book{gsrtp,
  author       = "Thomas Pynchon",
  title        = "Gravity's Rainbow",
  publisher    = "Bantam Books",
  year         = 1974,
  address      = nyny,
  price        = "$2.50",
  keywords     = "wwii, rocketry",
  location     = ""
}

@Book{thhgnr,
  author       = "Gerald~N. Rosenberg",
  title        = "The Hollow Hope",
  subtitle     = "Can the Courts Bring About Social Change?",
  publisher    = ucp,
  year         = 2008,
  address      = chil,
  edition      = "second",
  keywords     = "u.s. courts, judicial power, social change, politics, civil
    rights, women's rights, gay rights, environment, reapportionment, criminal
    law", 
  location     = "KF 8700.R66"
}

@Book{ilitec,
  author       = "Rajiv Chandrasekaran",
  title        = "Imperial Life in the Emerald City",
  subtitle     = "Inside Iraq's Green Zone",
  publisher    = "Vintage",
  year         = 2006,
  address      = nyny,
  keywords     = "occupation, development, iraq",
  location     = "DS 79.769.C53"
}

@Book{gleh,
  author       = "Elizabeth Hand",
  title        = "Generation Loss",
  publisher    = "Small Beer Press",
  year         = 2007,
  address      = noma,
  keywords     = "art photographers, maine, ritual sacrifice",
  location     = "PS 3558.A4619 G46"
}

@Book{tuws,
  author       = "Will Storr",
  title        = "The Unpersuadables",
  subtitle     = "Adventures with the Enemies of Science",
  publisher    = "The Overlook Press",
  year         = 2014,
  address      = nyny,
  keywords     = "skepticism, fundamentalism, psi research, holocaust denial",
  location     = "Q 172.5.H47 S76"
}

@Book{ditfp,
  author       = "Edgar Box",
  title        = "Death in the Fifth Position",
  publisher    = "Dutton",
  year         = 1952,
  address      = nyny,
  keywords     = "ballet, fellow travelers",
  location     = "PS 3543.I26 D4"
}

@Book{atdah,
  author       = "Bethany McLean and Joe Nocera",
  title        = "All the Devils are Here",
  subtitle     = "The Hidden History of the Financial Crisis",
  publisher    = "Portfolio\slash Penguin",
  year         = 2010,
  address      = nyny,
  keywords     = "the mortgage crisis, mortgage-backed securities, subprime
    loans, regulation, the financial crisis",
  location     = "HB 3717 2008.M35"
}

@Book{iadm,
  author       = "Donald MacKenzie",
  title        = "Inventing Accuracy",
  subtitle     = "A Historical Sociology of Nuclear Missile Guidance",
  publisher    = mitp,
  year         = 1990,
  address      = cama,
  keywords     = "ballistic missiles, nuclear weapons, military policy, nuclear
    warfare, politics, guidance systems, charles stark draper",
  location     = "UG 1312.B34 M33"
}

@Book{twohk,
  author       = "Sharon Ghamari-Tabrizi ",
  title        = "The Worlds of Herman Kahn",
  subtitle     = "The Intuitive Science of Thermonuclear War",
  publisher    = hup,
  year         = 2005,
  address      = cma,
  keywords     = "rand, on thermonuclear war, the ol' razzle-dazzle",
  location     = "U 263.G49"
}

@Book{dlih,
  author       = "Edgar Box",
  title        = "Death Likes It Hot",
  publisher    = "Dutton",
  year         = 1954,
  address      = nyny,
  keywords     = "the long island life, murrdaar",
  location     = "PS 3543.I26 D44"
}

@Book{tdossf,
  author       = "Shulamith Firestone",
  title        = "The Dialectic of Sex",
  subtitle     = "The Case for Feminist Revolution",
  publisher    = "Morrow",
  year         = 1970,
  address      = nyny,
  keywords     = "feminism, family, children, sex class, the patriarchy, culture",
  location     = "HQ 1190.F57"
}

@Book{lmma18,
  title        = "Lost Mars",
  subtitle     = "Stories From the Golden Age of the Red Planet",
  publisher    = ucp,
  year         = 2018,
  editor       = "Mike Ashley",
  address      = chil,
  keywords     = "the red planet, science fiction, short stories",
  location     = "PN 3433.6.L68"
}

@Book{tdapr,
  author       = "Philip Roth",
  title        = "The Dying Animal",
  publisher    = "Vintage International",
  year         = 2002,
  address      = nyny,
  keywords     = "teaching, student-teacher relations",
  location     = "PS 3568.O855 D95"
}

@Book{votdjs,
  author       = "Jeff Salyards",
  title        = "Veil of the Deserters",
  publisher    = "Night Shade Books",
  year         = 2014,
  address      = nyny,
  keywords     = "memory witches, betrayal, a questing we will go",
  location     = "PS 3619.A44266 V45"
}

@Book{tpsr2018,
  author       = "Sam Rosenfeld",
  title        = "The Polarizers",
  subtitle     = "Postwar Architects of Our Partisan Era",
  publisher    = ucp,
  year         = 2018,
  address      = chil,
  keywords     = "american political parties, ideologies, polarization,
    issues-oriented politics, political organization",
  location     = "JK 2265.R67"
}

@Book{hres1991,
  author       = "Robert~E. Strom and David~F. Bacon and Arthur~P. Goldberg and
    Andy Lowery and Daniel~M. Yellin and Shaula Alexander Yemini",
  title        = "Hermes",
  subtitle     = "A Language for Distributed Computing",
  publisher    = ph,
  year         = 1991,
  address      = ecnj,
  keywords     = "typestate, distributed systems, high-level programming
    languages, distributed computing",
  location     = "QA 76.73.H47 H47"
}

@Book{tvc1998,
  author       = "Arthur~M. {Schlesinger, Jr}",
  title        = "The Vital Center",
  subtitle     = "The Politics of Freedom",
  publisher    = "Transaction Publishers",
  year         = 1998,
  address      = "New Brunswick, N.J.",
  keywords     = "totalitarianism, communism, liberalism, unites states",
  location     = "JC 481.S38"
}

@Book{rsabl,
  author       = "Bruce Lenthall",
  title        = "Radio's America",
  subtitle     = "The Great Depression and the Rise of Mass Culture",
  publisher    = ucp,
  year         = 2007,
  address      = chil,
  keywords     = "mass culture, public intellectuals, radio audience, politics,
    stardom, social research, artistic pursuits",
  location     = "PN 1991.3.U6 L46"
}

@Book{tpp1997,
  author       = "Cynthia Ozick",
  title        = "The Puttermesser Papers",
  publisher    = "Knopf",
  year         = 1997,
  address      = nyny,
  keywords     = "mysticism, municipal corruption, love and marriage,
    immigrants, the old country",
  location     = "PS 3565.Z5 P8"
}

@Book{cothjs,
  author       = "Jeff Salyards",
  title        = "Chains of the Heretic",
  publisher    = "Night Shade Books",
  year         = 2016,
  address      = nyny,
  keywords     = "everybody's kung-fu fightin'",
  location     = "PS 3619.A44266 C33"
}

@Book{tyjv,
  author       = "Jason Vuic",
  title        = "The Yugo",
  subtitle     = "The Rise and Fall of the Worst Car in History",
  publisher    = "Hill and Wang",
  year         = 2009,
  address      = nyny,
  keywords     = "automobiles, car manufacturing, global trade, yugoslavia,
    bricklin, import/export, auto safety",
  location     = "TL 215.Z32 V58"
}

@Book{tshcb,
  author       = "Christophe Boltanski",
  title        = "The Safe House",
  publisher    = ucp,
  year         = 2017,
  address      = chil,
  keywords     = "russian diaspora, paris, ww ii, jewish life, family history",
  location     = "PQ 2662.O5712 C3313"
}

@Book{uel2017,
  author       = "Eduardo Lalo",
  translator   = "Suzanne Jill Levine",
  title        = "Uslessness",
  publisher    = ucp,
  year         = 2017,
  address      = chil,
  keywords     = "art, ambition, paris, san juan, love",
  location     = "PQ 7440.L29 I8813"
}

@Book{tgsl2014,
  author       = "Sergei Lukyanenko",
  title        = "The Genome",
  publisher    = "Open Road Media",
  year         = 2014,
  address      = sfca,
  month        = dec,
  keywords     = "assassination, detection, genetic modifications",
  location     = "978-1-4976-4396-3"
}

@Book{ahac2009,
  author       = "Andrea Camilleri",
  title        = "August Heat",
  publisher    = "Penguin",
  year         = 2009,
  address      = nyny,
  keywords     = "murrdaar, vacation time, getting played like a chump",
  location     = "PQ 4863.A3894 V3613"
}

@Book{prmg2009,
  author       = "Gessen, Masha",
  title        = "Perfect Rigor",
  subtitle     = "A Genius and the Mathematical Breakthrough of the Century",
  publisher    = "Houghton Mifflin Harcourt",
  year         = 2009,
  address      = boma,
  keywords     = "grigory perelman, poincare conjecture, millennium problem,
    russian mathematics education, mathematical society",
  location     = "QA29.P6727 G47"
}

@Book{wfts,
  author       = "Leonard~L. Richards",
  title        = "Who Freed the Slaves?",
  subtitle     = "The Fight Over the Thirteenth Amendment",
  publisher    = ucp,
  year         = 2015,
  address      = chil,
  keywords     = "slavery, politics, civil war",
  location     = "KF 4545.S5 R53"
}

@Book{votv,
  author       = "Andrea Camilleri",
  title        = "Voice of the Violin",
  publisher    = "Viking",
  year         = 2003,
  address      = nyny,
  keywords     = "murrdaar, debt, crooked cops",
  location     = "PQ 4863.A3894 V613"
}

@Book{wrp2017,
  title        = "Who Reads Poetry?",
  subtitle     = "Fifty Views from Poetry Magazine",
  editor       = "Fred Sasaki and Don Share",
  publisher    = ucp,
  year         = 2017,
  address      = chil,
  keywords     = "poetry",
  location     = "PN 1055.W53"
}

@Book{tmotrm,
  author       = "Jason Fox",
  title        = "The Myth of the Rational Market",
  subtitle     = "A History of Risk, Reward, and Delusion on Wall Street",
  publisher    = "Harper Business",
  year         = 2019,
  address      = nyny,
  keywords     = "finance, mathematics, science, randomness, mass delusions",
  location     = "HB 3731 .F69"
}

@Book{tsota,
  author       = "Herbert~A. Simon",
  title        = "The Sciences of the Artificial",
  publisher    = mitp,
  year         = 1996,
  edition      = "third",
  address      = cma,
  keywords     = "human cognition, design, reasoning, hierarchy, interfaces",
  location     = "Q 175.S564"
}

@Book{fof2012,
  author       = "Levenstein, Harvey~A.",
  title        = "Fear of Food",
  subtitle     = "A History of Why We Worry About What We Eat",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "dietary science, nutrition, fads, health, diets, phobias",
  location     = "TX 360.U6"
}

@Book{sre2013,
  author       = "Toh EnJoe",
  title        = "Self-Reference {ENGINE}",
  publisher    = "Haikasoru",
  year         = 2013,
  address      = sfca,
  keywords     = "science fiction, experimental fiction",
  location     = "PL 869.5.N55 S4513"
}

@Book{ett2005,
  author       = "Andrea Camilleri",
  title        = "Excursion to Tindari",
  publisher    = "Penguin",
  year         = 2005,
  address      = nyny,
  keywords     = "murrdaar, organ trafficking",
  location     = "PQ4 863.A3894 G3713"
}

@Book{tpntf,
  author       = "Thomas Frank",
  title        = "The People, No",
  subtitle     = "A Brief History of Anti-Populism",
  publisher    = "Metropolitan Books",
  year         = 2020,
  address      = nyny,
  keywords     = "populism, political culture, social movements",
  location     = "E 183"
}

@Book{tgcs,
  author       = "Cathleen Schine",
  title        = "The Grammarians",
  publisher    = "Picador",
  year         = 2020,
  address      = nyny,
  keywords     = "twins, family, logophilia",
  location     = "PS 3569.C497 G73"
}

@Book{mrjp2010,
  author       = "Rebecca Jo Plant",
  title        = "Mom",
  subtitle     = "The Transformation of Motherhood in Modern America",
  publisher    = ucp,
  year         = 2010,
  address      = chil,
  keywords     = "motherhood, momism, feminism, betty friedan",
  location     = "HQ 759.P564"
}

@Book{wbcljh,
  author       = "Logan~J. Hunder",
  title        = "Witches Be Crazy",
  publisher    = "Night Shade Books",
  year         = 2015,
  address      = "Jersey City, New Jersey",
  keywords     = "quests, wizards, pirates", 
  location     = "PS 3608.U53 W58"
}

@Book{cre2021,
  author       = "Robert Elder",
  title        = "Calhoun",
  subtitle     = "American Heretic",
  publisher    = "Basic Books",
  year         = 2021,
  address      = nyny,
  keywords     = "john c calhoun, constitutionalism, slavery, politics, governance",
  location     = ""
}

@Book{ddpjb,
  author       = "Peter~J. Bowler",
  title        = "Darwin Deleted",
  subtitle     = "Imagining a World Without Darwin",
  publisher    = ucp,
  year         = 2013,
  address      = chil,
  keywords     = "counterfactual speculation, evolutionary biology, history of
    science, darwinian evolution lamarckian evolution",
  location     = "QH 361.B675"
}

@Book{noitat,
  author       = "Patricia Lockwood",
  title        = "No One Is Talking About This",
  publisher    = "Riverhead Books",
  year         = 2021,
  address      = nyny,
  keywords     = "the internet, twitter, attention",
  location     = "PS 3612.O27 N6"
}

@Book{azatcoc,
  author       = "Tom Schachtman",
  title        = "Absolute Zero and the Conquest of Cold",
  publisher    = "Mariner Books",
  year         = 1999,
  address      = nyny,
  keywords     = "cold, cryogenics, science",
  location     = "QC 278.S48"
}

@Book{mlif,
  author       = "Julia Child and Alex Prud'homme",
  title        = "My Life in France",
  publisher    = "Anchor Books",
  year         = 2006,
  address      = nyny,
  keywords     = "biography, gastronomy, french cuisine",
  location     = "TX 649.C47 A3"
}

@Book{spbs1996,
  author       = "Bruce Sterling",
  title        = "Schismatrix Plus",
  publisher    = "Ace Books",
  year         = 1996,
  address      = nyny,
  keywords     = "neoliberals in space",
  location     = "PS 3569.T3876 S32"
}

@Book{glbe,
  author       = "Edward {de Grazia}",
  title        = "Girls Lean Back Everywhere",
  subtitle     = "The Law of Obscenity and the Assault on Genius",
  publisher    = "Vintage",
  year         = 1993,
  address      = nyny,
  keywords     = "censorship, obscenity prosecution",
  location     = "KF 4775.D44"
}

@Book{tdotd,
  author       = "John~A. Long",
  title        = "The Dawn of the Deed",
  subtitle     = "The Prehistoric Origins of Sex",
  publisher    = ucp,
  year         = 2012,
  address      = chil,
  keywords     = "evolution, sex, animal behavior, animal morphology",
  location     = "QH 481.L66"
}

@Book{tsotn,
  author       = "Andrea Camilleri",
  title        = "The Smell of the Night",
  publisher    = "Penguin",
  year         = 2005,
  address      = nyny,
  keywords     = "murrdaar, the bezzle, give and go",
  location     = "PQ 4863.A3894 O3613"
}

@Book{asitpitr,
  author       = "George Saunders",
  title        = "{A} Swim in a Pond in the Rain",
  publisher    = "Random House",
  year         = 2021,
  address      = nyny,
  keywords     = "literature, 19th century russian short stories, writing, reading",
  location     = "PG 3097.S28"
}

@Book{pelb,
  author       = "Edward~L. Bernays",
  title        = "Propaganda",
  publisher    = "Horace Liveright",
  year         = 1928,
  address      = nyny,
  keywords     = "public relations, the mass mind, propaganda",
  location     = "HM 1231.B47"
}

@Book{rgks,
  author       = "Sakuraba, Kazuki",
  title        = "Red Girls",
  subtitle     = "The Legend of the Akakuchibas",
  publisher    = "Haikasoru",
  year         = 2015,
  address      = sfca,
  keywords     = "japan, 2nd half 20th century, generational stories",
  location     = "PL 875.5.A39 A7713"
}

@Book{tbwpwf,
  author       = "Tom Clynes",
  title        = "The Boy Who Played with Fusion",
  subtitle     = "Extreme Science, Extreme Parenting, and How to Make a Star",
  publisher    = "Houghton Mifflin Harcourt",
  year         = 2015,
  address      = boma,
  keywords     = "curiosity, scientific development, gifted pedagogy",
  location     = "QC 774.W55 C59"
}

@Book{bbma,
  author       = "Mateo Askaripour",
  title        = "Black Buck",
  publisher    = "Houghton Mifflin Harcourt",
  year         = 2021,
  address      = boma,
  keywords     = "the ol' back and forth, sales",
  location     = "PS 3601.S593 B57"
}

@Book{rtmac,
  author       = "Andrea Camilleri",
  title        = "Rounding the Mark",
  publisher    = "Penguin",
  year         = 2006,
  address      = nyny,
  keywords     = "murrdaar, thinking of the children",
  location     = "PQ 4863.A3894 G36813"
}

@Book{tddth,
  author       = "Tim Harford",
  title        = "The Data Detective",
  subtitle     = "Ten Easy Rules to Make Sense of Statistics",
  publisher    = "Riverhead Books",
  year         = 2021,
  address      = nyny,
  keywords     = "statistics, data presentation, informal analysis",
  location     = "HA 29.H2498"
}

@Book{obdjm1990,
  author       = "James Morrow",
  title        = "Only Begotten Daughter",
  publisher    = "W.~Morrow",
  year         = 1990,
  address      = nyny,
  keywords     = "fathers and daughters, christianity, secession",
  location     = "PS 3563.O876 O55"
}

@Book{taraat,
  author       = "W.~J. Rorabaugh",
  title        = "The Alcoholic Republic, An American Tradition",
  publisher    = oup,
  year         = 1979,
  address      = nyny,
  keywords     = "alcohol production, social problems, drinking",
  location     = "HV 5291"
}

@Article{hasfadsafes,
  author       = "John Regehr and Alastair Reid",
  title        = "{HOIST}:  {A} System for Automatically Deriving Static Analyzers for Embedded Systems",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "133--143",
  month        = nov,
  keywords     = "abstract interpretation, static analysis, program
    verification, object code, binary decision diagrams",
  abstract     = "Embedded software must meet conflicting requirements such as
    high reliability, running on resource-constrained platforms, and rapid
    development.  Static program analysis can help meet all of these goals.
    People developing analyzers for embedded object code face a difficult
    problem: writing an abstract version of each instruction in the target
    architecture(s).  This is currently done by hand, resulting in abstract
    operations that are both buggy and imprecise.  We have developed Hoist: a
    novel system that solves these problems by automatically constructing
    abstract operations using a microprocessor (or simulator) as its own
    specification.  With almost no input from a human, Hoist generates a
    collection of C functions ready to be linked into an abstract interpreter.
    We demonstrate that Hoist generates abstract operations that are correct,
    having been extensively tested, sufficiently fast, and substantially more
    precise than manually written abstract operations. Hoist is currently
    limited to eight-bit machines due to costs exponential in the word size of
    the target architecture.  It is essential to be able to analyze software
    running on these small processors: they are important and ubiquitous, with
    many embedded and safety-critical systems being based on them.",
  location     = "https://doi.org/10.1145/1037187.1024410",
  location     = "https://www.cs.utah.edu/flux/papers/hoist-asplos04.pdf"
}

@Article{htvvmoaei2pbp,
  author       = "Perry~H. Wang and Jamison~D. Collins and Hong Wang and Dongkeun Kim and Bill Greene and Kai-Ming Chan and Aamir~B. Yunus and Terry Sych and Stephen~F. Moore and John~P. Shen",
  title        = "Helper Threads via Virtual Multithreading on an Experimental {Itanium} 2 Processor-Based Platform",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "144--155",
  month        = nov,
  keywords     = "helper thread, cache miss prefetching, multithreading,
    switch-on-event, itanium processor, pal, db2 database",
  abstract     = "Helper threading is a technology to accelerate a program by
    exploiting a processor's multithreading capability to run ``assist''
    threads.  Previous experiments on hyper-threaded processors have
    demonstrated significant speedups by using helper threads to prefetch
    hard-to-predict delinquent data accesses.  In order to apply this technique
    to processors that do not have built-in hardware support for
    multithreading, we introduce virtual multithreading (VMT), a novel form of
    switch-on-event user-level multithreading, capable of fly-weight
    multiplexing of event-driven thread executions on a single processor
    without additional operating system support.  The compiler plays a key role
    in minimizing synchronization cost by judiciously partitioning register
    usage among the user-level threads.  The VMT approach makes it possible to
    launch dynamic helper thread instances in response to long-latency cache
    miss events, and to run helper threads in the shadow of cache misses when
    the main thread would be otherwise stalled.The concept of VMT is prototyped
    on an Itanium ® 2 processor using features provided by the Processor
    Abstraction Layer (PAL) firmware mechanism already present in currently
    shipping processors.  On a 4-way MP physical system equipped with
    VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism
    can achieve significant performance gains for a diverse set of real-world
    workloads, ranging from single-threaded workstation benchmarks to heavily
    multithreaded large scale decision support systems (DSS) using the IBM DB2
    Universal Database.  We measure a wall-clock speedup of 5.8% to 38.5% for
    the workstation benchmarks, and 5.0% to 12.7% on DSS workload queries.",
  location     = "https://doi.org/10.1145/1037947.1024411"
}

@Article{lomlduasp,
  author       = "Matthias Hauswirth and Trishul~M. Chilimbi",
  title        = "Low-overhead memory leak detection using adaptive statistical profiling",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "156--164",
  month        = nov,
  keywords     = "low-overhead monitoring, runtime analysis, memory leaks, ",
  abstract     = "Sampling has been successfully used to identify performance
    optimization opportunities.  We would like to apply similar techniques to
    check program correctness.  Unfortunately, sampling provides poor coverage
    of infrequently executed code, where bugs often lurk.  We describe an
    adaptive profiling scheme that addresses this by sampling executions of
    code segments at a rate inversely proportional to their execution
    frequency.  To validate our ideas, we have implemented SWAT, a novel memory
    leak detection tool.  SWAT traces program allocations/ frees to construct a
    heap model and uses our adaptive profiling infrastructure to monitor
    loads/stores to these objects with low overhead.  SWAT reports stale
    objects that have not been accessed for a long time as leaks.  This
    allows it to find all leaks that manifest during the current program
    execution.  Since SWAT has low runtime overhead (‹5%), and low space
    overhead (‹10% in most cases and often less than 5%), it can be used to
    track leaks in production code that take days to manifest.  In addition to
    identifying the allocations that leak memory, SWAT exposes where the
    program last accessed the leaked data, which facilitates debugging and
    fixing the leak.  SWAT has been used by several product groups at Microsoft
    for the past 18 months and has proved effective at detecting leaks with a
    low false positive rate (‹10%).",
  location     = "https://doi.org/10.1145/1037947.1024412",
  location     = "https://www.microsoft.com/en-us/research/publication/low-overhead-memory-leak-detection-using-adaptive-statistical-profiling/"
}

@Article{lpp04,
  author       = "Xipeng Shen and Yutao Zhong and Chen Ding",
  title        = "Locality phase prediction",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "165--176",
  month        = nov,
  keywords     = "program phase analysis and prediction, phrase hierarchy,
    locality analysis and optimization, reconfigurable architecture, dynamic
    optimization",
  abstract     = "As computer memory hierarchy becomes adaptive, its
    performance increasingly depends on forecasting the dynamic program
    locality.  This paper presents a method that predicts the locality phases
    of a program by a combination of locality profiling and run-time
    prediction.  By profiling a training input, it identifies locality phases
    by sifting through all accesses to all data elements using
    variable-distance sampling, wavelet filtering, and optimal phase
    partitioning.  It then constructs a phase hierarchy through grammar
    compression.  Finally, it inserts phase markers into the program using
    binary rewriting.  When the instrumented program runs, it uses the first
    few executions of a phase to predict all its later executions.Compared with
    existing methods based on program code and execution intervals, locality
    phase prediction is unique because it uses locality profiles, and it marks
    phase boundaries in program code.  The second half of the paper presents a
    comprehensive evaluation.  It measures the accuracy and the coverage of the
    new technique and compares it with best known run-time methods.  It
    measures its benefit in adaptive cache resizing and memory remapping.
    Finally, it compares the automatic analysis with manual phase marking.  The
    results show that locality phase prediction is well suited for identifying
    large, recurring phases in complex programs.", 
  location     = "https://doi.org/10.1145/1037947.1024414", 
  location     = "http://www.cs.rochester.edu/~cding/Documents/Publications/asplos04.pdf"
}

@Article{dtopmrcfmm,
  author       = "Pin Zhou and Vivek Pandey and Jagadeesan Sundaresan and Anand Raghuraman and Yuanyuan Zhou and Sanjeev Kumar",
  title        = "Dynamic tracking of page miss ratio curve for memory management",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "177--188",
  month        = nov,
  keywords     = "memory management, power management, resource allocation",
  abstract     = "Memory can be efficiently utilized if the dynamic memory
    demands of applications can be determined and analyzed at run-time.  The
    page miss ratio curve(MRC), i.e.  page miss rate vs.  memory size curve, is
    a good performance-directed metric to serve this purpose.  However,
    dynamically tracking MRC at run time is challenging in systems with virtual
    memory because not every memory reference passes through the operating
    system (OS).This paper proposes two methods to dynamically track MRC of
    applications at run time.  The first method is using a hardware MRC monitor
    that can track MRC at fine time granularity.  Our simulation results show
    that this monitor has negligible performance and energy overheads.  The
    second method is an OS-only implementation that can track MRC at coarse
    time granularity.  Our implementation results on Linux show that it adds
    only 7--10% overhead.We have also used the dynamic MRC to guide both memory
    allocation for multiprogramming systems and memory energy management.  Our
    real system experiments on Linux with applications including Apache Web
    Server show that the MRC-directed memory allocation can speed up the
    applications' execution/response time by up to a factor of 5.86 and reduce
    the number of page faults by up to 63.1%.  Our execution-driven simulation
    results with SPEC2000 benchmarks show that the MRC-directed memory energy
    management can improve the Energy * Delay metric by 27--58% over previously
    proposed static and dynamic schemes.", 
  location     = "https://doi.org/10.1145/1037947.1024415", 
  location     = "http://opera.ucsd.edu/paper/ASPLOS04-Zhou.pdf"
}

@Article{copvsap,
  author       = "Rodric~M. Rabbah and Hariharan Sandanagobalane and Mongkol Ekpanyapong and Weng-Fai Wong",
  title        = "Compiler orchestrated prefetching via speculation and predication",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "189--198",
  month        = nov,
  keywords     = "precomputation, speculation, predicted execution, prefetching",
  abstract     = "This paper introduces a compiler orchestrated prefetching
    system as a unified framework geared toward ameliorating the gap between
    processing speeds and memory access latencies.  We focus the scope of the
    optimization on specific subsets of the program dependence graph that
    succinctly characterize the memory access pattern of both regular
    array-based applications and irregular pointer-intensive programs.  We
    illustrate how program embedded precomputation via speculative execution
    can accurately predict and effectively prefetch future memory references
    with negligible overhead.  The proposed techniques reduce the total running
    time of seven SPEC benchmarks and two OLDEN benchmarks by 27% on an Itanium
    2 processor.  The improvements are in addition to several state-of-the-art
    optimizations including software pipelining and data prefetching.  In
    addition, we use cycle-accurate simulations to identify important and
    lightweight architectural innovations that further mitigate the memory
    system bottleneck.  We focus on the notoriously challenging class of
    pointer-chasing applications, and demonstrate how they may benefit from a
    novel scheme of it sentineled prefetching.  Our results for twelve SPEC
    benchmarks demonstrate that 45% of the processor stalls that are caused by
    the memory system are avoidable.  The techniques in this paper can
    effectively mask long memory latencies with little instruction overhead,
    and can readily contribute to the performance of processors today.", 
  location     = "https://doi.org/10.1145/1037947.1024416", 
  location     = "https://cs.uwaterloo.ca/~brecht/courses/702/Possible-Readings/prefetching-to-cache/compiler-orch-via-spec-and-prefetch-p189-rabbah-apslos-2004.pdf"
}

@Article{spfmsgchaasr,
  author       = "Chen-Yong Cher and Antony~L. Hosking and T.~N. Vijaykumar",
  title        = "Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "199--210",
  month        = nov,
  keywords     = "cache architecture, prefetching, garbage collection,
    mark-sweep collection, prefetch-on-grey, buffered prefetch, depth-first
    sweeps, breadth-first sweeps",
  abstract     = "Tracing garbage collectors traverse references from live
    program variables, transitively tracing out the closure of live objects.
    Memory accesses incurred during tracing are essentially random: a given
    object may contain references to any other object.  Since application heaps
    are typically much larger than hardware caches, tracing results in many
    cache misses.  Technology trends will make cache misses more important, so
    tracing is a prime target for prefetching.Simulation of Java benchmarks
    running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a
    projected hardware platform reveal high tracing overhead (up to 65% of
    elapsed time), and that cache misses are a problem.  Applying Boehm's
    default prefetching strategy yields improvements in execution time (16% on
    average with incremental/generational collection for GC-intensive
    benchmarks), but analysis shows that his strategy suffers from significant
    timing problems: prefetches that occur too early or too late relative to
    their matching loads.  This analysis drives development of a new
    prefetching strategy that yields up to three times the performance
    improvement of Boehm's strategy for GC-intensive benchmark (27% average
    speedup), and achieves performance close to that of perfect timing ie, few
    misses for tracing accesses) on some benchmarks.  Validating these
    simulation results with live runs on current hardware produces average
    speedup of 6% for the new strategy on GC-intensive benchmarks with a GC
    configuration that tightly controls heap growth.  In contrast, Boehm's
    default prefetching strategy is ineffective on this platform.", 
  location     = "https://doi.org/10.1145/1037947.1024417", 
  location     = "ftp://ftp.cs.purdue.edu/pub/hosking/papers/asplos04.pdf"
}

@Article{dsa,
  author       = "Timothy~E. Denehy and John Bent and Florentina~I. Popovici and Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Deconstructing storage arrays",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "59--71",
  month        = nov,
  keywords     = "storage, raid storage, statistical analysis, fingerprinting,
    reverse engineering, shear",
  abstract     = "We introduce Shear, a user-level software tool that
    characterizes RAID storage arrays.  Shear employs a set of controlled
    algorithms combined with statistical techniques to automatically determine
    the important properties of a RAID system, including the number of disks,
    chunk size, level of redundancy, and layout scheme.  We illustrate the
    correctness of Shear by running it upon numerous simulated configurations,
    and then verify its real-world applicability by running Shear on both
    software-based and hardware-based RAID systems.  Finally, we demonstrate
    the utility of Shear through three case studies.  First, we show how Shear
    can be used in a storage management environment to verify RAID construction
    and detect failures.  Second, we demonstrate how Shear can be used to
    extract detailed characteristics about the individual disks within an
    array.  Third, we show how an operating system can use Shear to
    automatically tune its storage subsystems to specific RAID
    configurations.",
  location     = "https://doi.org/10.1145/1037947.1024401",
  location     = "https://research.cs.wisc.edu/wind/Publications/shear-asplos04.ps"
}

@Article{fbdedafcc,
  author       = "Yasushi Saito and Svend Frølund and Alistair Veitch and Arif Merchant and Susan Spence",
  title        = "{FAB}: building distributed enterprise disk arrays from commodity components",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "48--58",
  month        = nov,
  keywords     = "disk storage, paxos voting, load balancing, erasure coding,
    replication, timestamping, dynamic reconfiguration, redundancy",
  abstract     = "This paper describes the design, implementation, and
    evaluation of a Federated Array of Bricks (FAB), a distributed disk array
    that provides the reliability of traditional enterprise arrays with lower
    cost and better scalability.  FAB is built from a collection of bricks,
    small storage appliances containing commodity disks, CPU, NVRAM, and
    network interface cards.  FAB deploys a new majority-voting-based algorithm
    to replicate or erasure-code logical blocks across bricks and a
    reconfiguration algorithm to move data in the background when bricks are
    added or decommissioned.  We argue that voting is practical and necessary
    for reliable, high-throughput storage systems such as FAB.  We have
    implemented a FAB prototype on a 22-node Linux cluster.  This prototype
    sustains 85MB/second of throughput for a database workload, and
    270MB/second for a bulk-read workload.  In addition, it can outperform
    traditional master-slave replication through performance decoupling and can
    handle brick failures and recoveries smoothly without disturbing client
    requests.",
  location     = "https://doi.org/10.1145/1037947.1024400"
}

@Article{haifepilotab,
  author       = "Xiaotong Zhuang and Tao Zhang and Santosh Pande",
  title        = "{HIDE}: an infrastructure for efficiently protecting information leakage on the address bus",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "72--84",
  month        = nov,
  keywords     = "secure processor, address bus leakage protection, random
    permutations, basic blocks, control flow graphs",
  abstract     = "XOM-based secure processor has recently been introduced as a
    mechanism to provide copy and tamper resistant execution.  XOM provides
    support for encryption/decryption and integrity checking.  However, neither
    XOM nor any other current approach adequately addresses the problem of
    information leakage via the address bus.  This paper shows that without
    address bus protection, the XOM model is severely crippled.  Two realistic
    attacks are shown and experiments show that 70% of the code might be
    cracked and sensitive data might be exposed leading to serious security
    breaches.Although the problem of address bus leakage has been widely
    acknowledged both in industry and academia, no practical solution has ever
    been proposed that can provide an adequate security guarantee.  The main
    reason is that the problem is very difficult to solve in practice due to
    severe performance degradation which accompanies most of the solutions.
    This paper presents an infrastructure called HIDE (Hardware-support for
    leakage-Immune Dynamic Execution) which provides a solution consisting of
    chunk-level protection with hardware support and a flexible interface which
    can be orchestrated through the proposed compiler optimization and user
    specifications that allow utilizing underlying hardware solution more
    efficiently to provide better security guarantees.Our results show that
    protecting both data and code with a high level of security guarantee is
    possible with negligible performance penalty (1.3% slowdown).", 
  location     = "https://doi.org/10.1145/1037947.1024403", 
  location     = "http://www.cs.wisc.edu/~rajwar/papers/asplos04.pdf"
}

@Article{spevdift,
  author       = "G.~Edward Suh and Jae~W. Lee and David Zhang and Srinivas Devadas",
  title        = "Secure program execution via dynamic information flow tracking",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "85--96",
  month        = nov,
  keywords     = "buffer overflow, format strings, hardware tagging, stack
    smashing, information flow analysis",
  abstract     = "We present a simple architectural mechanism called dynamic
    information flow tracking that can significantly improve the security of
    computing systems with negligible performance overhead.  Dynamic
    information flow tracking protects programs against malicious software
    attacks by identifying spurious information flows from untrusted I/O and
    restricting the usage of the spurious information.Every security attack to
    take control of a program needs to transfer the program's control to
    malevolent code.  In our approach, the operating system identifies a set of
    input channels as spurious, and the processor tracks all information flows
    from those inputs.  A broad range of attacks are effectively defeated by
    checking the use of the spurious values as instructions and pointers.Our
    protection is transparent to users or application programmers; the
    executables can be used without any modification.  Also, our scheme only
    incurs, on average, a memory overhead of 1.4% and a performance overhead of
    1.1%.", 
  location     = "https://doi.org/10.1145/1037947.1024404", 
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-467/memo-467.pdf"
}

@Article{cdmuoi,
  author       = "Jaehyuk Huh and Jichuan Chang and Doug Burger and Gurindar~S. Sohi",
  title        = "Coherence decoupling: making use of incoherence",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "97--106",
  month        = nov,
  keywords     = "coherence decoupling, speculative cache lookup, coherence
    misses, false sharing, caching protocols",
  abstract     = "This paper explores a new technique called coherence
    decoupling, which breaks a traditional cache coherence protocol into two
    protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing
    coherence protocol.  The SCL protocol produces a speculative load value,
    typically from an invalid cache line, permitting the processor to compute
    with incoherent data.  In parallel, the coherence protocol obtains the
    necessary coherence permissions and the correct value.  Eventually, the
    speculative use of the incoherent data can be verified against the coherent
    data.  Thus, coherence decoupling can greatly reduce --- if not eliminate
    --- the effects of false sharing.  Furthermore, coherence decoupling can
    also reduce latencies incurred by true sharing.  SCL protocols reduce those
    latencies by speculatively writing updates into invalid lines, thereby
    increasing the accuracy of speculation, without complicating the simple,
    underlying coherence protocol that guarantees correctness.The performance
    benefits of coherence decoupling are evaluated using a full-system
    simulator and a mix of commercial and scientific benchmarks.  Our results
    show that 40% to 90% of all coherence misses can be speculated correctly,
    and therefore their latencies partially or fully hidden.  This capability
    results in performance improvements ranging from 3% to over 16%, in most
    cases where the latencies of coherence misses have an effect on
    performance.", 
  location     = "https://doi.org/10.1145/1037947.1024406", 
  location     = "http://pages.cs.wisc.edu/~mscalar/papers/2004/asplos04-coherence-decoupling.pdf"
}

@Article{cfp,
  author       = "Srikanth~T. Srinivasan and Ravi Rajwar and Haitham Akkary and Amit Gandhi and Mike Upton",
  title        = "Continual flow pipelines",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "107--119",
  month        = nov,
  keywords     = "non-blocking, instruction window, latency tolerance, cfp,
    storage latency, instruction scheduling, register assignment, cache
    hierarchies", 
  abstract     = "Increased integration in the form of multiple processor cores
    on a single die, relatively constant die sizes, shrinking power envelopes,
    and emerging applications create a new challenge for processor architects.
    How to build a processor that provides high single-thread performance and
    enables multiple of these to be placed on the same die for high throughput
    while dynamically adapting for future applications? Conventional approaches
    for high single-thread performance rely on large and complex cores to
    sustain a large instruction window for memory tolerance, making them
    unsuitable for multi-core chips.  We present Continual Flow Pipelines (CFP)
    as a new non-blocking processor pipeline architecture that achieves the
    performance of a large instruction window without requiring cycle-critical
    structures such as the scheduler and register file to be large.  We show
    that to achieve benefits of a large instruction window, inefficiencies in
    management of both the scheduler and register file must be addressed, and
    we propose a unified solution.  The non-blocking property of CFP keeps key
    processor structures affecting cycle time and power (scheduler, register
    file), and die size (second level cache) small.  The memory
    latency-tolerant CFP core allows multiple cores on a single die while
    outperforming current processor cores for single-thread applications.", 
  location     = "https://doi.org/10.1145/1037947.1024407"
}

@Article{ssrefea,
  author       = "Rajagopalan Desikan and Simha Sethumadhavan and Doug Burger and Stephen~W. Keckler",
  title        = "Scalable selective re-execution for {EDGE} architectures",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "120--132",
  month        = nov,
  keywords     = "mis-speculation recovery, selective re-execution, dependence
    prediction, edge architectures, microarchitecture, speculation, speculative
    dataflow machines",
  abstract     = "Pipeline flushes are becoming increasingly expensive in
    modern microprocessors with large instruction windows and deep pipelines.
    Selective re-execution is a technique that can reduce the penalty of
    mis-speculations by re-executing only instructions affected by the
    mis-speculation, instead of all instructions.  In this paper we introduce a
    new selective re-execution mechanism that exploits the properties of a
    dataflow-like Explicit Data Graph Execution (EDGE) architecture to support
    efficient mis-speculation recovery, while scaling to window sizes of
    thousands of instructions with high performance.  This distributed
    selective re-execution (DSRE) protocol permits multiple speculative waves
    of computation to be traversing a dataflow graph simultaneously, with a
    commit wave propagating behind them to ensure correct execution.  We
    evaluate one application of this protocol to provide efficient recovery for
    load-store dependence speculation.  Unlike traditional dataflow
    architectures which resorted to single-assignment memory semantics, the
    DSRE protocol combines dataflow execution with speculation to enable high
    performance and conventional sequential memory semantics.  Our experiments
    show that the DSRE protocol results in an average 17% speedup over the best
    dependence predictor proposed to date, and obtains 82% of the performance
    possible with a perfect oracle directing the issue of loads.", 
  location     = "https://doi.org/10.1145/1037947.1024408", 
  location     = "https://www.cs.utexas.edu/~skeckler/pubs/asplos04.pdf"
}

@Article{dsdrdibbss,
  author       = "Christopher~R. Lumb and Richard Golding",
  title        = "{D}-{SPTF}: decentralized request distribution in brick-based storage systems",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "37--47",
  month        = nov,
  keywords     = "storage systems, brick-based storage, distributed systems,
    disk scheduling, decentralized systems, load balancing, global cache
    management",
  abstract     = "Distributed Shortest-Positioning Time First (D-SPTF) is a
    request distribution protocol for decentralized systems of storage servers.
    D-SPTF exploits high-speed interconnects to dynamically select which
    server, among those with a replica, should service each read request.  In
    doing so, it simultaneously balances load, exploits the aggregate cache
    capacity, and reduces positioning times for cache misses.  For network
    latencies expected in storage clusters (e.g., 10--200μs), D-SPTF performs
    as well as would a hypothetical centralized system with the same collection
    of CPU, cache, and disk resources.  Compared to popular decentralized
    approaches, D-SPTF achieves up to 65% higher throughput and adapts more
    cleanly to heterogenous server capabilities.", 
  location     = "https://doi.org/10.1145/1037947.1024399"
}

@Article{aulppfsn,
  author       = "Virantha Ekanayake and Clinton Kelly and Rajit Manohar",
  title        = "An ultra low-power processor for sensor networks",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "27--36",
  month        = nov,
  keywords     = "low-energy hardware, sensor networks, asynchronous circuits,
    wireless, sensor network processor, even-driven execution, picojoule
    computing",
  abstract     = "We present a novel processor architecture designed
    specifically for use in low-power wireless sensor-network nodes.  Our
    sensor network asynchronous processor (SNAP/LE) is based on an asynchronous
    data-driven 16-bit RISC core with an extremely low-power idle state, and a
    wakeup response latency on the order of tens of nanoseconds.  The processor
    instruction set is optimized for sensor-network applications, with support
    for event scheduling, pseudo-random number generation, bitfield operations,
    and radio/sensor interfaces.  SNAP/LE has a hardware event queue and event
    coprocessors, which allow the processor to avoid the overhead of operating
    system software (such as task schedulers and external interrupt servicing),
    while still providing a straightforward programming interface to the
    designer.  The processor can meet performance levels required for data
    monitoring applications while executing instructions with tens of
    picojoules of energy.We evaluate the energy consumption of SNAP/LE with
    several applications representative of the workload found in data-gathering
    wireless sensor networks.  We compare our architecture and software against
    existing platforms for sensor networks, quantifying both the software and
    hardware benefits of our approach.", 
  location     = "https://doi.org/10.1145/1037947.1024397"
}

@Article{sc2004,
  author       = "Mihai Budiu and Girish Venkataramani and Tiberiu Chelcea and Seth Copen Goldstein",
  title        = "Spatial computation",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "14--26",
  month        = nov,
  keywords     = "spatial computation, dataflow machine, application-specific
    hardware, low-power computing, hardware compilation, intermediate
    representation, superscalar architecture",
  abstract     = "This paper describes a computer architecture, Spatial
    Computation (SC), which is based on the translation of high-level language
    programs directly into hardware structures.  SC program implementations are
    completely distributed, with no centralized control.  SC circuits are
    optimized for wires at the expense of computation units.In this paper we
    investigate a particular implementation of SC: ASH (Application-Specific
    Hardware).  Under the assumption that computation is cheaper than
    communication, ASH replicates computation units to simplify interconnect,
    building a system which uses very simple, completely dedicated
    communication channels.  As a consequence, communication on the datapath
    never requires arbitration; the only arbitration required is for accessing
    memory.  ASH relies on very simple hardware primitives, using no
    associative structures, no multiported register files, no scheduling logic,
    no broadcast, and no clocks.  As a consequence, ASH hardware is fast and
    extremely power efficient.In this work we demonstrate three features of
    ASH: (1) that such architectures can be built by automatic compilation of C
    programs; (2) that distributed computation is in some respects
    fundamentally different from monolithic superscalar processors; and (3)
    that ASIC implementations of ASH use three orders of magnitude less energy
    compared to high-end superscalar processors, while being on average only
    33% slower in performance (3.5x worst-case).", 
  location     = "https://doi.org/10.1145/1037947.1024396", 
  location     = "https://acg.media.mit.edu/people/simong/thesis/SpatialComputing.pdf"
}

@Article{pwtcact,
  author       = "Lance Hammond and Brian~D. Carlstrom and Vicky Wong and Ben Hertzberg and Mike Chen and Christos Kozyrakis and Kunle Olukotun",
  title        = "Programming with transactional coherence and consistency ({TCC})",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "1--13",
  month        = nov,
  keywords     = "transactions, feedback optimization, multiprocessor
    architecture, parallelization",
  abstract     = "Transactional Coherence and Consistency (TCC) offers a way to
    simplify parallel programming by executing all code within transactions.
    In TCC systems, transactions serve as the fundamental unit of parallel
    work, communication and coherence.  As each transaction completes, it
    writes all of its newly produced state to shared memory atomically, while
    restarting other processors that have speculatively read stale data.  With
    this mechanism, a TCC-based system automatically handles data
    synchronization correctly, without programmer intervention.  To gain the
    benefits of TCC, programs must be decomposed into transactions.  We
    describe two basic programming language constructs for decomposing programs
    into transactions, a loop conversion syntax and a general
    transaction-forking mechanism.  With these constructs, writing correct
    parallel programs requires only small, incremental changes to correct
    sequential programs.  The performance of these programs may then easily be
    optimized, based on feedback from real program execution, using a few
    simple techniques.", 
  location     = "https://doi.org/10.1145/1037947.1024395", 
  location     = "http://csl.stanford.edu/~christos/publications/2004.tcc.asplos.slides.pdf"
}

@Article{saafwcsis,
  author       = "Matt Welsh and David Culler and Eric Brewer",
  title        = "{SEDA}: an architecture for well-conditioned, scalable internet services",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "230--243",
  month        = dec,
  keywords     = "string of pearls, event-driven architecture, pipeline
    architecture, threading, autonomic scalability",
  abstract     = "We propose a new design for highly concurrent Internet
    services, which we call the staged event-driven architecture (SEDA).  SEDA
    is intended to support massive concurrency demands and simplify the
    construction of well-conditioned services.  In SEDA, applications consist
    of a network of event-driven stages connected by explicit queues.  This
    architecture allows services to be well-conditioned to load, preventing
    resources from being overcommitted when demand exceeds service capacity.
    SEDA makes use of a set of dynamic resource controllers to keep stages
    within their operating regime despite large fluctuations in load.  We
    describe several control mechanisms for automatic tuning and load
    conditioning, including thread pool sizing, event batching, and adaptive
    load shedding.  We present the SEDA design and an implementation of an
    Internet services platform based on this architecture.  We evaluate the use
    of SEDA through two applications: a high-performance HTTP server and a
    packet router for the Gnutella peer-to-peer file sharing network.  These
    results show that SEDA applications exhibit higher performance than
    traditional service designs, and are robust to huge variations in load.", 
  location     = "https://doi.org/10.1145/502034.502057", 
  location     = "http://www.sosp.org/2001/papers/welsh.pdf"
}

@Article{barsbrunp,
  author       = "Tammo Spalink and Scott Karlin and Larry Peterson and Yitzchak Gottlieb",
  title        = "Building a robust software-based router using network processors",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "216--229",
  month        = dec,
  keywords     = "layered architecture, packet processing, network coprocessors",
  abstract     = "Recent efforts to add new services to the Internet have
    increased interest in software-based routers that are easy to extend and
    evolve.  This paper describes our experiences using emerging network
    processors---in particular, the Intel IXP1200---to implement a router.  We
    show it is possible to combine an IXP1200 development board and a PC to
    build an inexpensive router that forwards minimum-sized packets at a rate
    of 3.47Mpps.  This is nearly an order of magnitude faster than existing
    pure PC-based routers, and sufficient to support 1.77Gbps of aggregate link
    bandwidth.  At lesser aggregate line speeds, our design also allows the
    excess resources available on the IXP1200 to be used robustly for extra
    packet processing.  For example, with 8 × 100Mbps links, 240 register
    operations and 96 bytes of state storage are available for each 64-byte
    packet.  Using a hierarchical architecture we can guarantee line-speed
    forwarding rates for simple packets with the IXP1200, and still have extra
    capacity to process exceptional packets with the Pentium.  Up to 310Kpps of
    the traffic can be routed through the Pentium to receive 1510 cycles of
    extra per-packet processing.", 
  location     = "https://doi.org/10.1145/502034.502056", 
  location     = "https://people.eecs.berkeley.edu/~culler/courses/cs252-s05/papers/p216-spalink.pdf"
}

@Article{wacswc,
  author       = "Frank Dabek and M.~Frans Kaashoek and David Karger and Robert Morris and Ion Stoica",
  title        = "Wide-area cooperative storage with {CFS}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "202--215",
  month        = dec,
  keywords     = "peer-to-peer storage, chorus, distributed block storage,
    scalability, reliability, caching, distributed file systems, read-only file
    systems",
  abstract     = "The Cooperative File System (CFS) is a new peer-to-peer
    read-only storage system that provides provable guarantees for the
    efficiency, robustness, and load-balance of file storage and retrieval.
    CFS does this with a completely decentralized architecture that can scale
    to large systems.  CFS servers provide a distributed hash table (DHash) for
    block storage.  CFS clients interpret DHash blocks as a file system.  DHash
    distributes and caches blocks at a fine granularity to achieve load
    balance, uses replication for robustness, and decreases latency with server
    selection.  DHash finds blocks using the Chord location protocol, which
    operates in time logarithmic in the number of servers.CFS is implemented
    using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD.
    Experience on a globally deployed prototype shows that CFS delivers data to
    clients as fast as FTP.  Controlled tests show that CFS is scalable: with
    4,096 servers, looking up a block of data involves contacting only seven
    servers.  The tests also demonstrate nearly perfect robustness and
    unimpaired performance even when as many as half the servers fail.", 
  location     = "https://doi.org/10.1145/502034.502054", 
  location     = "https://pdos.csail.mit.edu/papers/cfs:sosp01/cfs_sosp.pdf"
}

@Article{oopwf,
  author       = "David~A. Moon",
  title        = "Object-Oriented Programming with {Flavors}",
  journal      = oopsla86,
  year         = 1986,
  volume       = 21,
  number       = 11,
  pages        = "1--8",
  month        = nov,
  keywords     = "lisp, symbolics, object-oriented programming, generic
    functions, data structures, modularity",
  abstract     = "This paper describes Symbolics' newly redesigned
    object-oriented programming system, Flavors.  Flavors encourages program
    modularity, eases the development of large, complex programs, and provides
    high efficiency at run time.  Flavors is integrated into Lisp and the
    Symbolics program development environment.  This paper describes the
    philosophy and some of the major characteristics of Symbolics' Flavors and
    shows how the above goals are addressed.  Full details of Flavors are left
    to the programmers' manual, Reference Guide to Symbolics Common Lisp.", 
  location     = "https://doi.org/10.1145/28697.28698", 
  location     = "https://www.cs.tufts.edu/comp/150FP/archive/david-moon/flavors.pdf"
}

@Article{asadsftodiisio,
  author       = "Sitaram Iyer and Peter Druschel",
  title        = "Anticipatory scheduling: a disk scheduling framework to overcome deceptive idleness in synchronous {I}/{O}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "117--130",
  month        = dec,
  keywords     = "",
  abstract     = "Disk schedulers in current operating systems are generally
    work-conserving, i.e., they schedule a request as soon as the previous
    request has finished.  Such schedulers often require multiple outstanding
    requests from each process to meet system-level goals of performance and
    quality of service.  Unfortunately, many common applications issue disk
    read requests in a synchronous manner, interspersing successive requests
    with short periods of computation.  The scheduler chooses the next request
    too early; this induces deceptive idleness, a condition where the scheduler
    incorrectly assumes that the last request issuing process has no further
    requests, and becomes forced to switch to a request from another process.We
    propose the anticipatory disk scheduling framework to solve this problem in
    a simple, general and transparent way, based on the non-work-conserving
    scheduling discipline.  Our FreeBSD implementation is observed to yield
    large benefits on a range of microbenchmarks and real workloads.  The
    Apache webserver delivers between 29% and 71% more throughput on a
    disk-intensive workload.  The Andrew filesystem benchmark runs faster by
    8%, due to a speedup of 54% in its read-intensive phase.  Variants of the
    TPC-B database benchmark exhibit improvements between 2% and 60%.
    Proportional-share schedulers are seen to achieve their contracts
    accurately and efficiently.", 
  location     = "https://doi.org/10.1145/502034.502046", 
  location     = "http://pdos.csail.mit.edu/6.824-2002/papers/iyer-scheduling.pdf"
}

@Article{ron,
  author       = "David Andersen and Hari Balakrishnan and Frans Kaashoek and Robert Morris",
  title        = "Resilient overlay networks",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "131--145",
  month        = dec,
  keywords     = "internet performance, failure detection, application-level
    networking, policy routing",
  abstract     = "A Resilient Overlay Network (RON) is an architecture that
    allows distributed Internet applications to detect and recover from path
    outages and periods of degraded performance within several seconds,
    improving over today's wide-area routing protocols that take at least
    several minutes to recover.  A RON is an application-layer overlay on top
    of the existing Internet routing substrate.  The RON nodes monitor the
    functioning and quality of the Internet paths among themselves, and use
    this information to decide whether to route packets directly over the
    Internet or by way of other RON nodes, optimizing application-specific
    routing metrics.Results from two sets of measurements of a working RON
    deployed at sites scattered across the Internet demonstrate the benefits of
    our architecture.  For instance, over a 64-hour sampling period in March
    2001 across a twelve-node RON, there were 32 significant outages, each
    lasting over thirty minutes, over the 132 measured paths.  RON's routing
    mechanism was able to detect, recover, and route around all of them, in
    less than twenty seconds on average, showing that its methods for fault
    detection and recovery work well at discovering alternate paths in the
    Internet.  Furthermore, RON was able to improve the loss rate, latency, or
    throughput perceived by data transfers; for example, about 5% of the
    transfers doubled their TCP throughput and 5% of our transfers saw their
    loss probability reduced by 0.05.  We found that forwarding packets via at
    most one intermediate RON node is sufficient to overcome faults and improve
    performance in most cases.  These improvements, particularly in the area of
    fault detection and recovery, demonstrate the benefits of moving some of
    the control over routing into the hands of end-systems.", 
  location     = "https://doi.org/10.1145/502034.502048", 
  location     = "http://nms.lcs.mit.edu/papers/ron-sosp2001.html"
}

@Article{measrihc,
  author       = "Jeffrey~S. Chase and Darrell~C. Anderson and Prachi~N. Thakar and Amin~M. Vahdat and Ronald~P. Doyle",
  title        = "Managing energy and server resources in hosting centers",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "103--116",
  month        = dec,
  keywords     = "energy management, resource provisioning, economic modeling,
    pricing",
  abstract     = "Internet hosting centers serve multiple service sites from a
    common hardware base.  This paper presents the design and implementation of
    an architecture for resource management in a hosting center operating
    system, with an emphasis on energy as a driving resource management issue
    for large server clusters.  The goals are to provision server resources for
    co-hosted services in a way that automatically adapts to offered load,
    improve the energy efficiency of server clusters by dynamically resizing
    the active server set, and respond to power supply disruptions or thermal
    events by degrading service in accordance with negotiated Service Level
    Agreements (SLAs).Our system is based on an economic approach to managing
    shared server resources, in which services 'bid' for resources as a
    function of delivered performance.  The system continuously monitors load
    and plans resource allotments by estimating the value of their effects on
    service performance.  A greedy resource allocation algorithm adjusts
    resource prices to balance supply and demand, allocating resources to their
    most efficient use.  A reconfigurable server switching infrastructure
    directs request traffic to the servers assigned to each service.
    Experimental results from a prototype confirm that the system adapts to
    offered load and resource availability, and can reduce server energy usage
    by 29% or more for a typical Web workload.", 
  location     = "https://doi.org/10.1145/502034.502045", 
  location     = "http://www.sosp.org/2001/papers/chase.pdf"
}

@Article{bewsnwlln,
  author       = "John Heidemann and Fabio Silva and Chalermek Intanagonwiwat and Ramesh Govindan and Deborah Estrin and Deepak Ganesan",
  title        = "Building efficient wireless sensor networks with low-level naming",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "145--159",
  month        = dec,
  keywords     = "attribute-based naming, sensor networks, in-network
    processing, directed diffusion, declarative routing",
  abstract     = "In most distributed systems, naming of nodes for low-level
    communication leverages topological location (such as node addresses) and
    is independent of any application.  In this paper, we investigate an
    emerging class of distributed systems where low-level communication does
    not rely on network topological location.  Rather, low-level communication
    is based on attributes that are external to the network topology and
    relevant to the application.  When combined with dense deployment of nodes,
    this kind of named data enables in-network processing for data aggregation,
    collaborative signal processing, and similar problems.  These approaches
    are essential for emerging applications such as sensor networks where
    resources such as bandwidth and energy are limited.  This paper is the
    first description of the software architecture that supports named data and
    in-network processing in an operational, multi-application sensor-network.
    We show that approaches such as in-network aggregation and nested queries
    can significantly affect network traffic.  In one experiment aggregation
    reduces traffic by up to 42% and nested queries reduce loss rates by 30%.
    Although aggregation has been previously studied in simulation, this paper
    demonstrates nested queries as another form of in-network processing, and
    it presents the first evaluation of these approaches over an operational
    testbed.", 
  location     = "https://doi.org/10.1145/502034.502049"
}

@Article{mbcrux,
  author       = "Alex~C. Snoeren and Kenneth Conley and David~K. Gifford",
  title        = "Mesh-based content routing using {XML}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "160--173",
  month        = dec,
  keywords     = "overlay networks, mesh networks, routing, xml",
  abstract     = "We have developed a new approach for reliably multicasting
    time-critical data to heterogeneous clients over mesh-based overlay
    networks.  To facilitate intelligent content pruning, data streams are
    comprised of a sequence of XML packets and forwarded by application-level
    XML routers.  XML routers perform content-based routing of individual XML
    packets to other routers or clients based upon queries that describe the
    information needs of downstream nodes.  Our PC-based XML router prototype
    can route an 18 Mbit per second XML stream.Our routers use a novel
    Diversity Control Protocol (DCP) for router-to-router and router-to-client
    communication.  DCP reassembles a received stream of packets from one or
    more senders using the first copy of a packet to arrive from any sender.
    When each node is connected to n parents, the resulting network is
    resilient to (n − 1) router or independent link failures without repair.
    Associated mesh algorithms permit the system to recover to (n − 1)
    resilience after node and/or link failure.  We have deployed a distributed
    network of XML routers that streams real-time air traffic control data.
    Experimental results show multiple senders improve reliability and latency
    when compared to tree-based networks.", 
  location     = "https://doi.org/10.1145/502034.502050", 
  location     = "http://cseweb.ucsd.edu/~snoeren/papers/xml-sosp01.pdf"
}

@Article{albnfs,
  author       = "Athicha Muthitacharoen and Benjie Chen and David Mazi{\` e}res",
  title        = "{A} low-bandwidth network file system",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "174--187",
  month        = dec,
  keywords     = "redundancy, hash coding, file systems, low-bandwidth networks",
  abstract     = "Users rarely consider running network file systems over slow
    or wide-area networks, as the performance would be unacceptable and the
    bandwidth consumption too high.  Nonetheless, efficient remote file access
    would often be desirable over such networks---particularly when high
    latency makes remote login sessions unresponsive.  Rather than run
    interactive programs such as editors remotely, users could run the programs
    locally and manipulate remote files through the file system.  To do so,
    however, would require a network file system that consumes less bandwidth
    than most current file systems.This paper presents LBFS, a network file
    system designed for low-bandwidth networks.  LBFS exploits similarities
    between files or versions of the same file to save bandwidth.  It avoids
    sending data over the network when the same data can already be found in
    the server's file system or the client's cache.  Using this technique in
    conjunction with conventional compression and caching, LBFS consumes over
    an order of magnitude less bandwidth than traditional network file systems
    on common workloads.", 
  location     = "https://doi.org/10.1145/502034.502052", 
  location     = "https://pdos.csail.mit.edu/papers/lbfs:sosp01/lbfs.pdf"
}

@Article{smacipalspptpsu,
  author       = "Antony Rowstron and Peter Druschel",
  title        = "Storage management and caching in {PAST}, a large-scale, persistent peer-to-peer storage utility",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "188--201",
  month        = dec,
  keywords     = "pastry, storage management, hash routing, replica management,
    caching, peer-to-peer systems",
  abstract     = "This paper presents and evaluates the storage management and
    caching in PAST, a large-scale peer-to-peer persistent storage utility.
    PAST is based on a self-organizing, Internet-based overlay network of
    storage nodes that cooperatively route file queries, store multiple
    replicas of files, and cache additional copies of popular files.In the PAST
    system, storage nodes and files are each assigned uniformly distributed
    identifiers, and replicas of a file are stored at nodes whose identifier
    matches most closely the file's identifier.  This statistical assignment of
    files to storage nodes approximately balances the number of files stored on
    each node.  However, non-uniform storage node capacities and file sizes
    require more explicit storage load balancing to permit graceful behavior
    under high global storage utilization; likewise, non-uniform popularity of
    files requires caching to minimize fetch distance and to balance the query
    load.We present and evaluate PAST, with an emphasis on its storage
    management and caching system.  Extensive trace-driven experiments show
    that the system minimizes fetch distance, that it balances the query load
    for popular files, and that it displays graceful degradation of performance
    as the global storage utilization increases beyond 95%.", 
  location     = "https://doi.org/10.1145/502034.502053", 
  location     = "http://www.cs.cornell.edu/People/egs/615/past.pdf"
}

@Article{rtdvsflpeos,
  author       = "Padmanabhan Pillai and Kang~G. Shin",
  title        = "Real-time dynamic voltage scaling for low-power embedded operating systems",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "89--102",
  month        = dec,
  keywords     = "voltage scaling, power management",
  abstract     = "In recent years, there has been a rapid and wide spread of
    non-traditional computing platforms, especially mobile and portable
    computing devices.  As applications become increasingly sophisticated and
    processing power increases, the most serious limitation on these devices is
    the available battery life.  Dynamic Voltage Scaling (DVS) has been a key
    technique in exploiting the hardware characteristics of processors to
    reduce energy dissipation by lowering the supply voltage and operating
    frequency.  The DVS algorithms are shown to be able to make dramatic energy
    savings while providing the necessary peak computation power in
    general-purpose systems.  However, for a large class of applications in
    embedded real-time systems like cellular phones and camcorders, the
    variable operating frequency interferes with their deadline guarantee
    mechanisms, and DVS in this context, despite its growing importance, is
    largely overlooked/under-developed.  To provide real-time guarantees, DVS
    must consider deadlines and periodicity of real-time tasks, requiring
    integration with the real-time scheduler.  In this paper, we present a
    class of novel algorithms called real-time DVS (RT-DVS) that modify the
    OS's real-time scheduler and task management service to provide significant
    energy savings while maintaining real-time deadline guarantees.  We show
    through simulations and a working prototype implementation that these
    RT-DVS algorithms closely approach the theoretical lower bound on energy
    consumption, and can easily reduce energy consumption 20% to 40% in an
    embedded real-time system.", 
  location     = "https://doi.org/10.1145/502034.502044", 
  location     = "http://www.sosp.org/2001/papers/pillai.pdf"
}

@Article{aesoose,
  author       = "Andy Chou and Junfeng Yang and Benjamin Chelf and Seth Hallem and Dawson Engler",
  title        = "An empirical study of operating systems errors",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "73--88",
  month        = dec,
  keywords     = "error discovery, statistical analysis, error cliches, error
    metrics", 
  abstract     = "We present a study of operating system errors found by
    automatic, static, compiler analysis applied to the Linux and OpenBSD
    kernels.  Our approach differs from previous studies that consider errors
    found by manual inspection of logs, testing, and surveys because static
    analysis is applied uniformly to the entire kernel source, though our
    approach necessarily considers a less comprehensive variety of errors than
    previous studies.  In addition, automation allows us to track errors over
    multiple versions of the kernel source to estimate how long errors remain
    in the system before they are fixed.We found that device drivers have error
    rates up to three to seven times higher than the rest of the kernel.  We
    found that the largest quartile of functions have error rates two to six
    times higher than the smallest quartile.  We found that the newest quartile
    of files have error rates up to twice that of the oldest quartile, which
    provides evidence that code 'hardens' over time.  Finally, we found that
    bugs remain in the Linux kernel an average of 1.8 years before being
    fixed.", 
  location     = "https://doi.org/10.1145/502034.502042", 
  location     = "https://pdos.csail.mit.edu/archive/6.097/readings/osbugs.pdf"
}

@Article{badbagatieisc,
  author       = "Dawson Engler and David Yu Chen and Seth Hallem and Andy Chou and Benjamin Chelf",
  title        = "Bugs as deviant behavior: a general approach to inferring errors in systems code",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "57--72",
  month        = dec,
  keywords     = "static analysis, statistical sorting, error analysis",
  abstract     = "A major obstacle to finding program errors in a real system
    is knowing what correctness rules the system must obey.  These rules are
    often undocumented or specified in an ad hoc manner.  This paper
    demonstrates techniques that automatically extract such checking
    information from the source code itself, rather than the programmer,
    thereby avoiding the need for a priori knowledge of system rules.The
    cornerstone of our approach is inferring programmer 'beliefs' that we then
    cross-check for contradictions.  Beliefs are facts implied by code: a
    dereference of a pointer, p, implies a belief that p is non-null, a call to
    'unlock(1)' implies that 1 was locked, etc.  For beliefs we know the
    programmer must hold, such as the pointer dereference above, we immediately
    flag contradictions as errors.  For beliefs that the programmer may hold,
    we can assume these beliefs hold and use a statistical analysis to rank the
    resulting errors from most to least likely.  For example, a call to
    'spin_lock' followed once by a call to 'spin_unlock' implies that the
    programmer may have paired these calls by coincidence.  If the pairing
    happens 999 out of 1000 times, though, then it is probably a valid belief
    and the sole deviation a probable error.  The key feature of this approach
    is that it requires no a priori knowledge of truth: if two beliefs
    contradict, we know that one is an error without knowing what the correct
    belief is.Conceptually, our checkers extract beliefs by tailoring rule
    'templates' to a system --- for example, finding all functions that fit the
    rule template 'a must be paired with b.' We have developed six checkers
    that follow this conceptual framework.  They find hundreds of bugs in real
    systems such as Linux and OpenBSD.  From our experience, they give a
    dramatic reduction in the manual effort needed to check a large system.
    Compared to our previous work [9], these template checkers find ten to one
    hundred times more rule instances and derive properties we found
    impractical to specify manually.", 
  location     = "https://doi.org/10.1145/502034.502041", 
  location     = "https://web.stanford.edu/~engler/deviant-sosp-01.pdf"
}

@Article{iacigbs,
  author       = "Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Information and control in gray-box systems",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "43--56",
  month        = dec,
  keywords     = "experimentation, file-cache management, disk layouts,
    admission control",
  abstract     = "In modern systems, developers are often unable to modify the
    underlying operating system.  To build services in such an environment, we
    advocate the use of gray-box techniques.  When treating the operating
    system as a gray-box, one recognizes that not changing the OS restricts,
    but does not completely obviate, both the information one can acquire about
    the internal state of the OS and the control one can impose on the OS.  In
    this paper, we develop and investigate three gray-box Information and
    Control Layers (ICLs) for determining the contents of the file-cache,
    controlling the layout of files across local disk, and limiting process
    execution based on available memory.  A gray-box ICL sits between a client
    and the OS and uses a combination of algorithmic knowledge, observations,
    and inferences to garner information about or control the behavior of a
    gray-box system.  We summarize a set of techniques that are helpful in
    building gray-box ICLs and have begun to organize a 'gray toolbox' to ease
    the construction of ICLs.  Through our case studies, we demonstrate the
    utility of gray-box techniques, by implementing three useful 'OS-like'
    services without the modification of a single line of OS source code.", 
  location     = "https://doi.org/10.1145/502034.502040", 
  location     = "http://www.sosp.org/2001/papers/arpacidusseau.pdf"
}

@Article{tcaloafrs,
  author       = "Haifeng Yu and Amin Vahdat",
  title        = "The costs and limits of availability for replicated services",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "29--42",
  month        = dec,
  keywords     = "replication, performance, availability",
  abstract     = "As raw system and network performance continues to improve at
    exponential rates, the utility of many services is increasingly limited by
    availability rather than performance.  A key approach to improving
    availability involves replicating the service across multiple, wide-area
    sites.  However, replication introduces well-known tradeoffs between
    service consistency and availability.  Thus, this paper explores the
    benefits of dynamically trading consistency for availability using a
    continuous consistency model.  In this model, applications specify a
    maximum deviation from strong consistency on a per-replica basis.  In this
    paper, we: i) evaluate availability of a prototype replication system
    running across the Internet as a function of consistency level, consistency
    protocol, and failure characteristics, ii) demonstrate that simple
    optimizations to existing consistency protocols result in significant
    availability improvements (more than an order of magnitude in some
    scenarios), iii) use our experience with these optimizations to prove tight
    upper bounds on the availability of services, and iv) show that maximizing
    availability typically entails remaining as close to strong consistency as
    possible during times of good connectivity, resulting in a communication
    versus availability trade-off.", 
  location     = "https://doi.org/10.1145/502034.502038", 
  location     = "https://users.cs.duke.edu/~vahdat/ps/tr-cs-2001-03.pdf"
}

@Article{buatift,
  author       = "Rodrigo Rodrigues and Miguel Castro and Barbara Liskov",
  title        = "{BASE}: using abstraction to improve fault tolerance",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "15--28",
  month        = dec,
  keywords     = "Byzantine fault tolerance, middleware, abstraction",
  abstract     = "Software errors are a major cause of outages and they are
    increasingly exploited in malicious attacks.  Byzantine fault tolerance
    allows replicated systems to mask some software errors but it is expensive
    to deploy.  This paper describes a replication technique, BASE, which uses
    abstraction to reduce the cost of Byzantine fault tolerance and to improve
    its ability to mask software errors.  BASE reduces cost because it enables
    reuse of off-the-shelf service implementations.  It improves availability
    because each replica can be repaired periodically using an abstract view of
    the state stored by correct replicas, and because each replica can run
    distinct or non-deterministic service implementations, which reduces the
    probability of common mode failures.  We built an NFS service where each
    replica can run a different off-the-shelf file system implementation, and
    an object-oriented database where the replicas ran the same,
    non-deterministic implementation.  These examples suggest that our
    technique can be used in practice --- in both cases, the implementation
    required only a modest amount of new code, and our performance results
    indicate that the replicated services perform comparably to the
    implementations that they reuse.", 
  location     = "https://doi.org/10.1145/502034.502037", 
  location     = "http://www.cs.cornell.edu/People/egs/cornellonly/syslunch/fall01/sosp/rodrigues.pdf"
}

@Article{uhacspp,
  author       = "Steve Zdancewic and Lantian Zheng and Nathaniel Nystrom and Andrew~C. Myers",
  title        = "Untrusted hosts and confidentiality: secure program partitioning",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "1--14",
  month        = dec,
  keywords     = "security annotations, program partitioning, information flow,
    control transfer",
  abstract     = "This paper presents secure program partitioning, a
    language-based technique for protecting confidential data during
    computation in distributed systems containing mutually untrusted hosts.
    Confidentiality and integrity policies can be expressed by annotating
    programs with security types that constrain information flow; these
    programs can then be partitioned automatically to run securely on
    heterogeneously trusted hosts.  The resulting communicating subprograms
    collectively implement the original program, yet the system as a whole
    satisfies the security requirements of participating principals without
    requiring a universally trusted host machine.  The experience in applying
    this methodology and the performance of the resulting distributed code
    suggest that this is a promising way to obtain secure distributed
    computation.", 
  location     = "https://doi.org/10.1145/502034.502036", 
  location     = "https://www.cs.cornell.edu/andru/papers/sosp01/zznm01.pdf"
}

@Article{upbtm,
  author       = "Weihaw Chuang and Satish Narayanasamy and Ganesh Venkatesh and Jack Sampson and Michael Van Biesbrouck and Gilles Pokam and Brad Calder and Osvaldo Colavin",
  title        = "Unbounded Page-Based Transactional Memory",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "347--358",
  month        = oct,
  keywords     = "transactions, transactional memory, parallel programming,
    concurrency, virtual memory, transactional hardware",
  abstract     = "Exploiting thread level parallelism is paramount in the
    multicore era.  Transactions enable programmers to expose such parallelism
    by greatly simplifying the multi-threaded programming model.  Virtualized
    transactions (unbounded in space and time) are desirable, as they can
    increase the scope of transactions' use, and thereby further simplify a
    programmer's job.  However, hardware support is essential to support
    efficient execution of unbounded transactions.  In this paper, we introduce
    Page-based Transactional Memory to support unbounded transactions.  We
    combine transaction bookkeeping with the virtual memory system to support
    fast transaction conflict detection, commit, abort, and to maintain
    transactions' speculative data.", 
  location     = "https://doi.org/10.1145/1168919.1168901",
  location     = "https://cseweb.ucsd.edu/~calder/papers/ASPLOS-06-PTM.pdf"
}

@Article{sntmil,
  author       = "Michelle~J. Moravan and Jayaram Bobba and Kevin~E. Moore and Luke Yen and Mark~D. Hill and Ben Liblit and Michael~M. Swift and David~A. Wood",
  title        = "Supporting nested transactional memory in {LogTM}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "359--370",
  month        = nov,
  keywords     = "nested transactions, transactional storage, composition,
    partial aborts, open nesting, escaping actions, concurrency",
  abstract     = "Nested transactional memory (TM) facilitates software
    composition by letting one module invoke another without either knowing
    whether the other uses transactions.  Closed nested transactions extend
    isolation of an inner transaction until the toplevel transaction commits.
    Implementations may flatten nested transactions into the top-level one,
    resulting in a complete abort on conflict, or allow partial abort of inner
    transactions.  Open nested transactions allow a committing inner
    transaction to immediately release isolation, which increases parallelism
    and expressiveness at the cost of both software and hardware
    complexity.This paper extends the recently-proposed flat Log-based
    Transactional Memory (LogTM) with nested transactions.  Flat LogTM saves
    pre-transaction values in a log, detects conflicts with read (R) and write
    (W) bits per cache block, and, on abort, invokes a software handler to
    unroll the log.  Nested LogTM supports nesting by segmenting the log into a
    stack of activation records and modestly replicating R/W bits.  To
    facilitate composition with nontransactional code, such as language runtime
    and operating system services, we propose escape actions that allow trusted
    code to run outside the confines of the transactional memory system.", 
  location     = "https://doi.org/10.1145/1168857.1168902", 
  location     = "https://research.cs.wisc.edu/multifacet/papers/asplos06_nested_logtm.pdf"
}

@Article{titmv,
  author       = "JaeWoong Chung and Chi Cao Minh and Austen McDonald and Travis Skare and Hassan Chafi and Brian~D. Carlstrom and Christos Kozyrakis and Kunle Olukotun",
  title        = "Tradeoffs in transactional memory virtualization",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "371--381",
  month        = nov,
  keywords     = "chip multi-processors, os support, transactional memory,
    virtualization, virtualization",
  abstract     = "For transactional memory (TM) to achieve widespread
    acceptance, transactions should not be limited to the physical resources of
    any specific hardware implementation.  TM systems should guarantee correct
    execution even when transactions exceed scheduling quanta, overflow the
    capacity of hardware caches and physical memory, or include more
    independent nesting levels than what is supported in hardware.  Existing
    proposals for TM virtualization are either incomplete or rely on complex
    hardware implementations, which are an overkill if virtualization is
    invoked infrequently in the common case.We present eXtended Transactional
    Memory (XTM), the first TM virtualization system that virtualizes all
    aspects of transactional execution (time, space, and nesting depth).  XTM
    is implemented in software using virtual memory support.  It operates at
    page granularity, using private copies of overflowed pages to buffer memory
    updates until the transaction commits and snapshots of pages to detect
    interference between transactions.  We also describe two enhancements to
    XTM that use limited hardware support to address key performance
    bottlenecks.We compare XTM to hardwarebased virtualization using both real
    applications and synthetic microbenchmarks.  We show that despite being
    software-based, XTM and its enhancements are competitive with
    hardware-based alternatives.  Overall, we demonstrate that XTM provides a
    complete, flexible, and low-cost mechanism for practical TM
    virtualization.", 
  location     = "https://doi.org/10.1145/1168857.1168903", 
  location     = "http://csl.stanford.edu/~christos/publications/2006.tm_virtualization.asplos.pdf"
}

@Article{anirffehai,
  author       = "Motohiro Kawahito and Hideaki Komatsu and Takao Moriyama and Hiroshi Inoue and Toshio Nakatani",
  title        = "A new idiom recognition framework for exploiting hardware-assist instructions",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "382--393",
  month        = nov,
  keywords     = "idiom recognition, hardware-assist instructions, vmx,
    topological embedding, java, jits, code improvements, graph matching",
  abstract     = "Modern processors support hardware-assist instructions (such
    as TRT and TROT instructions on IBM zSeries) to accelerate certain
    functions such as delimiter search and character conversion.  Such special
    instructions have often been used in high performance libraries, but they
    have not been exploited well in optimizing compilers except for some
    limited cases.  We propose a new idiom recognition technique derived from a
    topological embedding algorithm [4] to detect idiom patterns in the input
    program more aggressively than in previous approaches.  Our approach can
    detect a pattern even if the code segment does not exactly match the idiom.
    For example, we can detect a code segment that includes additional code
    within the idiom pattern.  We implemented our new idiom recognition
    approach based on the Java Just-In-Time (JIT) compiler that is part of the
    J9 Java Virtual Machine, and we supported several important idioms for
    special hardware-assist instructions on the IBM zSeries and on some models
    of the IBM pSeries.  To demonstrate the effectiveness of our technique, we
    performed two experiments.  The first one is to see how many more patterns
    we can detect compared to the previous approach.  The second one is to see
    how much performance improvement we can achieve over the previous approach.
    For the first experiment, we used the Java Compatibility Kit (JCK) API
    tests.  For the second one we used IBM XML parser, SPECjvm98, and
    SPCjbb2000.  In summary, relative to a baseline implementation using exact
    pattern matching, our algorithm converted 75% more loops in JCK tests.  We
    also observed significant performance improvement of the XML parser by 64%,
    of SPECjvm98 by 1%, and of SPECjbb2000 by 2% on average on a z990.
    Finally, we observed the JIT compilation time increases by only 0.32% to
    0.44%.", 
  location     = "https://doi.org/10.1145/1168857.1168905"
}

@Article{agops,
  author       = "Sorav Bansal and Alex Aiken",
  title        = "Automatic generation of peephole superoptimizers",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "394--403",
  month        = nov,
  keywords     = "superoptimization, peephole optimization, code selection,
    searching, satisfiability solvers",
  abstract     = "Peephole optimizers are typically constructed using
    human-written pattern matching rules, an approach that requires expertise
    and time, as well as being less than systematic at exploiting all
    opportunities for optimization.  We explore fully automatic construction of
    peephole optimizers using brute force superoptimization.  While the
    optimizations discovered by our automatic system may be less general than
    human-written counterparts, our approach has the potential to automatically
    learn a database of thousands to millions of optimizations, in contrast to
    the hundreds found in current peephole optimizers.  We show experimentally
    that our optimizer is able to exploit performance opportunities not found
    by existing compilers; in particular, we show speedups from 1.7 to a factor
    of 10 on some compute intensive kernels over a conventional optimizing
    compiler.", 
  location     = "https://doi.org/10.1145/1168857.1168906", 
  location     = "https://theory.stanford.edu/~aiken/publications/papers/asplos06.pdf"
}

@Article{csffp,
  author       = "Armando Solar-Lezama and Liviu Tancau and Rastislav Bodik and Sanjit Seshia and Vijay Saraswat",
  title        = "Combinatorial sketching for finite programs",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "404--415",
  month        = nov,
  keywords     = "sketching, sat, program generation, template programming,
    code synthesis",
  abstract     = "Sketching is a software synthesis approach where the
    programmer develops a partial implementation - a sketch - and a separate
    specification of the desired functionality.  The synthesizer then completes
    the sketch to behave like the specification.  The correctness of the
    synthesized implementation is guaranteed by the compiler, which allows,
    among other benefits, rapid development of highly tuned implementations
    without the fear of introducing bugs.We develop SKETCH, a language for
    finite programs with linguistic support for sketching.  Finite programs
    include many highperformance kernels, including cryptocodes.  In contrast
    to prior synthesizers, which had to be equipped with domain-specific rules,
    SKETCH completes sketches by means of a combinatorial search based on
    generalized boolean satisfiability.  Consequently, our combinatorial
    synthesizer is complete for the class of finite programs: it is guaranteed
    to complete any sketch in theory, and in practice has scaled to realistic
    programming problems.Freed from domain rules, we can now write sketches as
    simpleto-understand partial programs, which are regular programs in which
    difficult code fragments are replaced with holes to be filled by the
    synthesizer.  Holes may stand for index expressions, lookup tables, or
    bitmasks, but the programmer can easily define new kinds of holes using a
    single versatile synthesis operator.We have used SKETCH to synthesize an
    efficient implementation of the AES cipher standard.  The synthesizer
    produces the most complex part of the implementation and runs in about an
    hour.", 
  location     = "https://doi.org/10.1145/1168857.1168907", 
  location     = "https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/combinatorial_sketching.pdf"
}

@Article{appafso,
  author       = "Jeff {Da Silva} and J.~Gregory Steffan",
  title        = "{A} probabilistic pointer analysis for speculative optimizations",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "416--425",
  month        = nov,
  keywords     = "dependence analysis, pointer analysis, speculative
    optimization",
  abstract     = "Pointer analysis is a critical compiler analysis used to
    disambiguate the indirect memory references that result from the use of
    pointers and pointer-based data structures.  A conventional pointer
    analysis deduces for every pair of pointers, at any program point, whether
    a points-to relation between them (i) definitely exists, (ii) definitely
    does not exist, or (iii) maybe exists.  Many compiler optimizations rely on
    accurate pointer analysis, and to ensure correctness cannot optimize in the
    maybe case.  In contrast, recently-proposed speculative optimizations can
    aggressively exploit the maybe case, especially if the likelihood that two
    pointers alias can be quantified.  This paper proposes a Probabilistic
    Pointer Analysis (PPA) algorithm that statically predicts the probability
    of each points-to relation at every program point.  Building on simple
    control-flow edge profiling, our analysis is both one-level context and
    flow sensitive-yet can still scale to large programs including the SPEC
    2000 integer benchmark suite.  The key to our approach is to compute
    points-to probabilities through the use of linear transfer functions that
    are efficiently encoded as sparse matrices.We demonstrate that our analysis
    can provide accurate probabilities, even without edge-profile information.
    We also find that-even without considering probability information-our
    analysis provides an accurate approach to performing pointer analysis.", 
  location     = "https://doi.org/10.1145/1168857.1168908"
}

@Article{spjfc,
  author       = "Jason~F. Cantin and Mikko~H. Lipasti and James~E. Smith",
  title        = "Stealth prefetching",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "274--282",
  month        = nov,
  keywords     = "prefetching, prefetch policy, exclusive access, precaching",
  abstract     = "Prefetching in shared-memory multiprocessor systems is an
    increasingly difficult problem.  As system designs grow to incorporate
    larger numbers of faster processors, memory latency and interconnect
    traffic increase.  While aggressive prefetching techniques can mitigate the
    increasing memory latency, they can harm performance by wasting precious
    interconnect bandwidth and prematurely accessing shared data, causing state
    downgrades at remote nodes that force later upgrades.This paper
    investigates Stealth Prefetching, a new technique that utilizes information
    from Coarse-Grain Coherence Tracking (CGCT) for prefetching data
    aggressively, stealthily, and efficiently in a broadcast-based
    shared-memory multiprocessor system.  Stealth Prefetching utilizes CGCT to
    identify regions of memory that are not shared by other processors,
    aggressively fetches these lines from DRAM in open-page mode, and moves
    them close to the processor in anticipation of future references.  Our
    analysis with commercial, scientific, and multiprogrammed workloads show
    that Stealth Prefetching provides an average speedup of 20% over an
    aggressive baseline system with conventional prefetching.", 
  location     = "https://doi.org/10.1145/1168857.1168892", 
  location     = "https://pharm.ece.wisc.edu/papers/asplos2006_final.pdf"
}

@Article{csehmtsccotf,
  author       = "Koushik Chakraborty and Philip~M. Wells and Gurindar~S. Sohi",
  title        = "Computation spreading: employing hardware migration to specialize {CMP} cores on-the-fly",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "283--292
    ",
  month        = nov,
  keywords     = "dynamic specialization, cache locality, code reuse, thread
    migration, thread assignment",
  abstract     = "In canonical parallel processing, the operating system (OS)
    assigns a processing core to a single thread from a multithreaded server
    application.  Since different threads from the same application often carry
    out similar computation, albeit at different times, we observe extensive
    code reuse among different processors, causing redundancy (e.g., in our
    server workloads, 45-65% of all instruction blocks are accessed by all
    processors).  Moreover, largely independent fragments of computation
    compete for the same private resources causing destructive interference.
    Together, this redundancy and interference lead to poor utilization of
    private microarchitecture resources such as caches and branch predictors.We
    present Computation Spreading (CSP), which employs hardware migration to
    distribute a thread's dissimilar fragments of computation across the
    multiple processing cores of a chip multiprocessor (CMP), while grouping
    similar computation fragments from different threads together.  This paper
    focuses on a specific example of CSP for OS intensive server applications:
    separating application level (user) computation from the OS calls it
    makes.When performing CSP, each core becomes temporally specialized to
    execute certain computation fragments, and the same core is repeatedly used
    for such fragments.  We examine two specific thread assignment policies for
    CSP, and show that these policies, across four server workloads, are able
    to reduce instruction misses in private L2 caches by 27-58%, private L2
    load misses by 0-19%, and branch mispredictions by 9-25%.", 
  location     = "https://doi.org/10.1145/1168857.1168893", 
  location     = "ftp://ftp.cs.wisc.edu/sohi/papers/2006/asplos2006-comp-spread.pdf"
}

@Article{sbicfep,
  author       = "Jason~E. Miller and Anant Agarwal",
  title        = "Software-based instruction caching for embedded processors",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "293--302",
  month        = nov,
  keywords     = "software caching, instruction caching, chaining,
  software-implemented caches, embedded processors, on-chip store",
  abstract     = "While hardware instruction caches are present in virtually
    all general-purpose and high-performance microprocessors today, many
    embedded processors use SRAM or scratchpad memories instead.  These are
    simple array memory structures that are directly addressed and explicitly
    managed by software.  Compared to hardware caches of the same data
    capacity, they are smaller, have shorter access times and consume less
    energy per access.  Access times are also easier to predict with simple
    memories since there is no possibility of a 'miss.' On the other hand, they
    are more difficult for the programmer to use since they are not
    automatically managed.In this paper, we present a software system that
    allows all or part of an SRAM or scratchpad memory to be automatically
    managed as a cache.  This system provides the programming convenience of a
    cache for processors that lack dedicated caching hardware.  It has been
    implemented for an actual processor and runs on real hardware.  Our results
    show that a software-based instruction cache can be built that provides
    performance within 10% of a traditional hardware cache on many benchmarks
    while using a cheaper, simpler, SRAM memory.  On these same benchmarks,
    energy consumption is up to 3% lower than it would be using a hardware
    cache.", 
  location     = "https://doi.org/10.1145/1168857.1168894", 
  location     = "https://groups.csail.mit.edu/cag/raw/documents/Miller-ASPLOS-2006.ps.Z"
}

@Article{meoamtep,
  author       = "Xin Li and Marian Boldt and Reinhard von Hanxleden",
  title        = "Mapping {Esterel} onto a multi-threaded embedded processor",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "303--314",
  month        = nov,
  keywords     = "reactive systems, concurrency, multi-threading, esterel,
    low-power processing",
  abstract     = "The synchronous language Esterel is well-suited for
    programming control-dominated reactive systems at the system level.  It
    provides non-traditional control structures, in particular concurrency and
    various forms of preemption, which allow to concisely express reactive
    behavior.  As these control structures cannot be mapped easily onto
    traditional, sequential processors, an alternative approach that has
    emerged recently makes use of special-purpose reactive processors.
    However, the designs proposed so far have limitations regarding
    completeness of the language support, and did not really take advantage of
    compile-time knowledge to optimize resource usage.This paper presents a
    reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its
    compiler.  The KEP3a improves on earlier designs in several areas; most
    notable are the support for exception handling and the provision of
    context-dependent preemption handling instructions.  The KEP3a compiler
    presented here is to our knowledge the first for multi-threaded reactive
    processors.  The translation of Esterel's preemption constructs onto KEP3a
    assembler is straightforward; however, a challenge is the correct and
    efficient representation of Esterel's concurrency.  The compiler generates
    code that respects data and control dependencies using the KEP3a
    priority-based scheduling mechanism.  We present a priority assignment
    approach that makes use of a novel concurrent control flow graph and has a
    complexity that in practice tends to be linear in the size of the program.
    Unlike earlier Esterel compilation schemes, this approach avoids
    unnecessary context switches by considering each thread's actual execution
    state at run time.  Furthermore, it avoids code replication present in
    other approaches.", 
  location     = "https://doi.org/10.1145/1168857.1168896"
}

@Article{inifhbti,
  author       = "Nathan~L. Binkert and Ali~G. Saidi and Steven~K. Reinhardt",
  title        = "Integrated network interfaces for high-bandwidth {TCP/IP}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "315--324",
  month        = nov,
  keywords     = "network interfaces, tcp/ip performance, zero-copy transfers,
    on-chip nics",
  abstract     = "This paper proposes new network interface controller (NIC)
    designs that take advantage of integration with the host CPU to provide
    increased flexibility for operating system kernel-based performance
    optimization.We believe that this approach is more likely to meet the needs
    of current and future high-bandwidth TCP/IP networking on end hosts than
    the current trend of putting more complexity in the NIC, while avoiding the
    need to modify applications and protocols.  This paper presents two such
    NICs.  The first, the simple integrated NIC (SINIC), is a minimally complex
    design that moves the responsibility for managing the network FIFOs from
    the NIC to the kernel.  Despite this closer interaction between the kernel
    and the NIC, SINIC provides performance equivalent to a conventional
    DMA-based NIC without increasing CPU overhead.  The second design, V-SINIC,
    adds virtual per-packet registers to SINIC, enabling parallel packet
    processing while maintaining a FIFO model.  V-SINIC allows the kernel to
    decouple examining a packet's header from copying its payload to memory.
    We exploit this capability to implement a true zero-copy receive
    optimization in the Linux 2.6 kernel, providing bandwidth improvements of
    over 50% on unmodified sockets-based receive-intensive benchmarks.", 
  location     = "https://doi.org/10.1145/1168857.1168897", 
  location     = "http://web.eecs.umich.edu/~saidi/pubs/asplos06-nic.pdf"
}

@Article{audptpgfgpu,
  author       = "David Tarditi and Sidd Puri and Jose Oglesby",
  title        = "Accelerator: using data parallelism to program {GPUs} for general-purpose uses",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "325--335",
  month        = nov,
  keywords     = "graphics processing units, data parallelism, jit compilation,
    gpu code generation",
  abstract     = "GPUs are difficult to program for general-purpose uses.
    Programmers can either learn graphics APIs and convert their applications
    to use graphics pipeline operations or they can use stream programming
    abstractions of GPUs.  We describe Accelerator, a system that uses data
    parallelism to program GPUs for general-purpose uses instead.  Programmers
    use a conventional imperative programming language and a library that
    provides only high-level data-parallel operations.  No aspects of GPUs are
    exposed to programmers.  The library implementation compiles the
    data-parallel operations on the fly to optimized GPU pixel shader code and
    API calls.We describe the compilation techniques used to do this.  We
    evaluate the effectiveness of using data parallelism to program GPUs by
    providing results for a set of compute-intensive benchmarks.  We compare
    the performance of Accelerator versions of the benchmarks against
    hand-written pixel shaders.  The speeds of the Accelerator versions are
    typically within 50% of the speeds of hand-written pixel shader code.  Some
    benchmarks significantly outperform C versions on a CPU: they are up to 18
    times faster than C code running on a CPU.",
  location     = "https://doi.org/10.1145/1168857.1168898",
  location     = "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-184.pdf"
}

@Article{htmpd,
  author       = "Peter Damron and Alexandra Fedorova and Yossi Lev and Victor Luchangco and Mark Moir and Daniel Nussbaum",
  title        = "Hybrid transactional memory",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "336--346",
  month        = nov,
  keywords     = "software transactional storage, hardware-assisted software,
    transactions", 
  abstract     = "Transactional memory (TM) promises to substantially reduce
    the difficulty of writing correct, efficient, and scalable concurrent
    programs.  But 'bounded' and 'best-effort' hardware TM proposals impose
    unreasonable constraints on programmers, while more flexible software TM
    implementations are considered too slow.  Proposals for supporting
    'unbounded' transactions in hardware entail significantly higher complexity
    and risk than best-effort designs.We introduce Hybrid Transactional Memory
    (HyTM), an approach to implementing TMin software so that it can use best
    effort hardware TM (HTM) to boost performance but does not depend on HTM.
    Thus programmers can develop and test transactional programs in existing
    systems today, and can enjoy the performance benefits of HTM support when
    it becomes available.We describe our prototype HyTM system, comprising a
    compiler and a library.  The compiler allows a transaction to be attempted
    using best-effort HTM, and retried using the software library if it fails.
    We have used our prototype to 'transactify' part of the Berkeley DB system,
    as well as several benchmarks.  By disabling the optional use of HTM, we
    can run all of these tests on existing systems.  Furthermore, by using a
    simulated multiprocessor with HTM support, we demonstrate the viability of
    the HyTM approach: it can provide performance and scalability approaching
    that of an unbounded HTM implementation, without the need to support all
    transactions with complicated HTM support.", 
  location     = "https://doi.org/10.1145/1168857.1168900", 
  location     = "https://www.ece.ubc.ca/~sasha/papers/asplos165-damron.pdf"
}

@Article{i3csm,
  author       = "Shashidhar Mysore and Banit Agrawal and Navin Srivastava and Sheng-Chih Lin and Kaustav Banerjee and Tim Sherwood",
  title        = "Introspective {3D} chips",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "264--273",
  month        = nov,
  keywords     = "introspection, hardware supported profiling, 3d
    architectures, configurable architectures, chip layout",
  abstract     = "While the number of transistors on a chip increases
    exponentially over time, the productivity that can be realized from these
    systems has not kept pace.  To deal with the complexity of modern systems,
    software developers are increasingly dependent on specialized development
    tools such as security profilers, memory leak identifiers, data flight
    recorders, and dynamic type analysis.  Many of these tools require
    full-system data which covers multiple interacting threads, processes, and
    processors.  Reducing the performance penalty and complexity of these
    software tools is critical to those developing next generation
    applications, and many researchers have proposed adding specialized
    hardware to assist in profiling and introspection.  Unfortunately, while
    this additional hardware would be incredibly beneficial to developers, the
    cost of this hardware must be paid on every single die that is
    manufactured.In this paper, we argue that a new way to attack this problem
    is with the addition of specialized analysis hardware built on separate
    active layers stacked vertically on the processor die using 3D IC
    technology.  This provides a modular 'snap-on' functionality that could be
    included with developer systems, and omitted from consumer systems to keep
    the cost impact to a minimum.  In this paper we describe the advantage of
    using inter-die vias for introspection and we quantify the impact they can
    have in terms of the area, power, temperature, and routability of the
    resulting systems.  We show that hardware stubs could be inserted into
    commodity processors at design time that would allow analysis layers to be
    bonded to development chips, and that these stubs would increase area and
    power by no more than 0.021mm2 and 0.9% respectively.", 
  location     = "https://doi.org/10.1145/1168857.1168890"
}

@Article{aptaasfqu,
  author       = "Ethan Schuchman and T.~N. Vijaykumar",
  title        = "{A} program transformation and architecture support for quantum uncomputation",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "252--263",
  month        = nov,
  keywords     = "quantum computing, uncomputation, qla, fault tolerance,
    quantum garbage management, entanglement and interference",
  abstract     = "Quantum computing's power comes from new algorithms that
    exploit quantum mechanical phenomena for computation.  Quantum algorithms
    are different from their classical counterparts in that quantum algorithms
    rely on algorithmic structures that are simply not present in classical
    computing.  Just as classical program transformations and architectures
    have been designed for common classical algorithm structures, quantum
    program transformations and quantum architectures should be designed with
    quantum algorithms in mind.  Because quantum algorithms come with these new
    algorithmic structures, resultant quantum program transformations and
    architectures may look very different from their classical
    counterparts.This paper focuses on uncomputation, a critical and prevalent
    structure in quantum algorithms, and considers how program transformations,
    and architecture support should be designed to accommodate uncomputation.
    In this paper,we show a simple quantum program transformation that exposes
    independence between uncomputation and later computation.  We then propose
    a multicore architecture tailored to this exposed parallelism and propose a
    scheduling policy that efficiently maps such parallelism to the multicore
    architecture.  Our policy achieves parallelism between uncomputation and
    later computation while reducing cumulative communication distance.  Our
    scheduling and architecture allows significant speedup of quantum programs
    (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while
    reducing cumulative communication distance 26%.", 
  location     = "https://doi.org/10.1145/1168857.1168889", 
  location     = "https://engineering.purdue.edu/~vijay/papers/2006/uncomputation.pdf"
}

@Article{adtsonsa,
  author       = "Jaidev~P. Patwardhan and Vijeta Johri and Chris Dwyer and Alvin~R. Lebeck",
  title        = "{A} defect tolerant self-organizing nanoscale {SIMD} architecture",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "241--251",
  month        = nov,
  keywords     = "self-organizing systems, simd architectures, data parallel
  computations, bit-serial assembly, defect tolerance, dna, nanocomputing",
  abstract     = "The continual decrease in transistor size (through either
    scaled CMOS or emerging nano-technologies) promises to usher in an era of
    tera to peta-scale integration.  However, this decrease in size is also
    likely to increase defect densities, contributing to the exponentially
    increasing cost of top-down lithography.  Bottom-up manufacturing
    techniques, like self assembly, may provide a viable lower-cost alternative
    to top-down lithography, but may also be prone to higher defects.
    Therefore, regardless of fabrication methodology, defect tolerant
    architectures are necessary to exploit the full potential of future
    increased device densities.This paper explores a defect tolerant SIMD
    architecture.  A key feature of our design is the ability of a large number
    of limited capability nodes with high defect rates (up to 30%) to
    self-organize into a set of SIMD processing elements.  Despite node
    simplicity and high defect rates, we show that by supporting the familiar
    data parallel programming model the architecture can execute a variety of
    programs.  The architecture efficiently exploits a large number of nodes
    and higher device densities to keep device switching speeds and power
    density low.  On a medium sized system (~1cm2 area), the performance of the
    proposed architecture on our data parallel programs matches or exceeds the
    performance of an aggressively scaled out-of-order processor (128-wide, 8k
    reorder buffer, perfect memory system).  For larger systems (&gt;1cm2), the
    proposed architecture can match the performance of a chip multiprocessor
    with 16 aggressively scaled out-of-order cores.", 
  location     = "https://doi.org/10.1145/1168857.1168888", 
  location     = "https://users.cs.duke.edu/~alvy/papers/sosa.pdf"
}

@Article{rsmdus,
  author       = "Satish Narayanasamy and Cristiano Pereira and Brad Calder",
  title        = "Recording shared memory dependencies using {Strata}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "229--240",
  month        = nov,
  keywords     = "strata recorder, replay, multi-threaded debugging, logging, shared
    memory dependencies", 
  abstract     = "Significant time is spent by companies trying to reproduce
    and fix bugs.  BugNet and FDR are recent architecture proposals that
    provide architecture support for deterministic replay debugging.  They
    focus on continuously recording information about the program's execution,
    which can be communicated back to the developer.  Using that information,
    the developer can deterministically replay the program's execution to
    reproduce and fix the bugs.In this paper, we propose using Strata to
    efficiently capture the shared memory dependencies.  A stratum creates a
    time layer across all the logs for the running threads, which separates all
    the memory operations executed before and after the stratum.  A strata log
    allows us to determine all the shared memory dependencies during replay and
    thereby supports deterministic replay debugging for multi-threaded
    programs.", 
  location     = "https://doi.org/10.1145/1168857.1168886", 
  location     = "https://cseweb.ucsd.edu/~calder/papers/ASPLOS-06-Strata.pdf"
}

@Article{hihbbuad,
  author       = "Trishul~M. Chilimbi and Vinod Ganapathy",
  title        = "{HeapMD}: identifying heap-based bugs using anomaly detection",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "219--228",
  month        = nov,
  keywords     = "anomaly detection, pointer errors , heap storage modeling, graph
    metrics, debugging", 
  abstract     = "We present the design, implementation, and evaluation of
    HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly
    detection.  HeapMD is based upon the observation that, in spite of the
    evolving nature of the heap, several of its properties remain stable.
    HeapMD uses this observation in a novel way: periodically, during the
    execution of the program, it computes a suite of metrics which are
    sensitive to the state of the heap.  These metrics track heap behavior, and
    the stability of the heap reflects quantitatively in the values of these
    metrics.  The 'normal' ranges of stable metrics, obtained by running a
    program on multiple inputs, are then treated as indicators of correct
    behaviour, and are used in conjunction with an anomaly detector to find
    heap-based bugs.  Using HeapMD, we were able to find 40 heap-based bugs, 31
    of them previously unknown, in 5 large, commercial applications.", 
  location     = "https://doi.org/10.1145/1168857.1168885", 
  location     = "http://pages.cs.wisc.edu/~vg/papers/asplos2006/asplos2006.ps"
}

@Article{caepth,
  author       = "Mazen Kharbutli and Xiaowei Jiang and Yan Solihin and Guru Venkataramani and Milos Prvulovic",
  title        = "Comprehensively and efficiently protecting the heap",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "207--218",
  month        = nov,
  keywords     = "computer security, heap attacks, heap security, heap server",
  abstract     = "The goal of this paper is to propose a scheme that provides
    comprehensive security protection for the heap.  Heap vulnerabilities are
    increasingly being exploited for attacks on computer programs.  In most
    implementations, the heap management library keeps the heap meta-data (heap
    structure information) and the application's heap data in an interleaved
    fashion and does not protect them against each other.  Such implementations
    are inherently unsafe: vulnerabilities in the application can cause the
    heap library to perform unintended actions to achieve control-flow and
    non-control attacks.Unfortunately, current heap protection techniques are
    limited in that they use too many assumptions on how the attacks will be
    performed, require new hardware support, or require too many changes to the
    software developers' toolchain.  We propose Heap Server, a new solution
    that does not have such drawbacks.  Through existing virtual memory and
    inter-process protection mechanisms, Heap Server prevents the heap
    meta-data from being illegally overwritten, and heap data from being
    meaningfully overwritten.  We show that through aggressive optimizations
    and parallelism, Heap Server protects the heap with nearly-negligible
    performance overheads even on heap-intensive applications.  We also verify
    the protection against several real-world exploits and attack kernels.", 
  location     = "https://doi.org/10.1145/1168857.1168884", 
  location     = "https://www.cc.gatech.edu/~milos/kharbutli_asplos06.pdf"
}

@Article{eeadsvpm,
  author       = "Engin \.{I}pek and Sally~A. McKee and Rich Caruana and Bronis~R. {de Supinski} and Martin Schulz",
  title        = "Efficiently exploring architectural design spaces via predictive modeling",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "195--206",
  month        = nov,
  keywords     = "design space exploration, sensitivity studies, artificial
    neural networks, performance prediction, learning",
  abstract     = "Architects use cycle-by-cycle simulation to evaluate design
    choices and understand tradeoffs and interactions among design parameters.
    Efficiently exploring exponential-size design spaces with many interacting
    parameters remains an open problem: the sheer number of experiments renders
    detailed simulation intractable.  We attack this problem via an automated
    approach that builds accurate, confident predictive design-space models.
    We simulate sampled points, using the results to teach our models the
    function describing relationships among design parameters.  The models
    produce highly accurate performance estimates for other points in the
    space, can be queried to predict performance impacts of architectural
    changes, and are very fast compared to simulation, enabling efficient
    discovery of tradeoffs among parameters in different regions.  We validate
    our approach via sensitivity studies on memory hierarchy and CPU design
    spaces: our models generally predict IPC with only 1-2% error and reduce
    required simulation by two orders of magnitude.  We also show the efficacy
    of our technique for exploring chip multiprocessor (CMP) design spaces:
    when trained on a 1% sample drawn from a CMP design space with 250K points
    and up to 55x performance swings among different system configurations, our
    models predict performance with only 4-5% error on average.  Our approach
    combines with techniques to reduce time per simulation, achieving net time
    savings of three-four orders of magnitude.", 
  location     = "https://doi.org/10.1145/1168857.1168882", 
  location     = "http://www.cs.rochester.edu/~ipek/asplos06.pdf"
}

@Article{pu3stteaceecm,
  author       = "Taeho Kgil and Shaun D'Souza and Ali Saidi and Nathan Binkert and Ronald Dreslinski and Trevor Mudge and Steven Reinhardt and Krisztian Flautner",
  title        = "{PicoServer}: using {3D} stacking technology to enable a compact energy efficient chip multiprocessor",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "117--128",
  month        = nov,
  keywords     = "low power, tier 1 servers, web/file/streaming servers, 3d
    stacking technology, chip multiprocessor, full-system simulation, power
    trade-offs, data transport",
  abstract     = "In this paper, we show how 3D stacking technology can be used
    to implement a simple, low-power, high-performance chip multiprocessor
    suitable for throughput processing.  Our proposed architecture, PicoServer,
    employs 3D technology to bond one die containing several simple slow
    processing cores to multiple DRAM dies sufficient for a primary memory.
    The 3D technology also enables wide low-latency buses between processors
    and memory.  These remove the need for an L2 cache allowing its area to be
    re-allocated to additional simple cores.  The additional cores allow the
    clock frequency to be lowered without impairing throughput.  Lower clock
    frequency in turn reduces power and means that thermal constraints, a
    concern with 3D stacking, are easily satisfied.The PicoServer architecture
    specifically targets Tier 1 server applications, which exhibit a high
    degree of thread level parallelism.  An architecture targeted to efficient
    throughput is ideal for this application domain.  We find for a similar
    logic die area, a 12 CPU system with 3D stacking and no L2 cache
    outperforms an 8 CPU system with a large on-chip L2 cache by about 14%
    while consuming 55% less power.  In addition, we show that a PicoServer
    performs comparably to a Pentium 4-like class machine while consuming only
    about 1/10 of the power, even when conservative assumptions are made about
    the power consumption of the PicoServer.", 
  location     = "https://doi.org/10.1145/1168857.1168873", 
  location     = "http://web.eecs.umich.edu/~saidi/pres/picoserver.pdf"
}

@Article{aspsafea,
  author       = "Katherine~E. Coons and Xia Chen and Doug Burger and Kathryn~S. McKinley and Sundeep~K. Kushwaha",
  title        = "{A} spatial path scheduling algorithm for {EDGE} architectures",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "129--140",
  month        = nov,
  keywords     = "instruction  scheduling, path scheduling, simulated
    annealing, edge architecture",
  abstract     = "Growing on-chip wire delays are motivating architectural
    features that expose on-chip communication to the compiler.  EDGE
    architectures are one example of communication-exposed microarchitectures
    in which the compiler forms dataflow graphs that specify how the
    microarchitecture maps instructions onto a distributed execution substrate.
    This paper describes a compiler scheduling algorithm called spatial path
    scheduling that factors in previously fixed locations - called anchor
    points - for each placement.  This algorithm extends easily to different
    spatial topologies.  We augment this basic algorithm with three heuristics:
    (1) local and global ALU and network link contention modeling, (2) global
    critical path estimates, and (3) dependence chain path reservation.  We use
    simulated annealing to explore possible performance improvements and to
    motivate the augmented heuristics and their weighting functions.  We show
    that the spatial path scheduling algorithm augmented with these three
    heuristics achieves a 21% average performance improvement over the best
    prior algorithm and comes within an average of 5% of the annealed
    performance for our benchmarks.", 
  location     = "https://doi.org/10.1145/1168857.1168875", 
  location     = "http://www.cs.utexas.edu/users/mckinley/papers/sps-asplos-2006.pdf"
}

@Article{isfatda,
  author       = "Martha Mercaldi and Steven Swanson and Andrew Petersen and Andrew Putnam and Andrew Schwerin and Mark Oskin and Susan~J. Eggers",
  title        = "Instruction scheduling for a tiled dataflow architecture",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "141--150",
  month        = nov,
  keywords     = "dataflow architecture, instruction scheduling, tiled architectures",
  abstract     = "This paper explores hierarchical instruction scheduling for a
    tiled processor.  Our results show that at the top level of the hierarchy,
    a simple profile-driven algorithm effectively minimizes operand latency.
    After this schedule has been partitioned into large sections, the
    bottom-level algorithm must more carefully analyze program structure when
    producing the final schedule.Our analysis reveals that at this bottom
    level, good scheduling depends upon carefully balancing instruction
    contention for processing elements and operand latency between producer and
    consumer instructions.  We develop a parameterizable instruction scheduler
    that more effectively optimizes this trade-off.  We use this scheduler to
    determine the contention-latency sweet spot that generates the best
    instruction schedule for each application.  To avoid this
    application-specific tuning, we also determine the parameters that produce
    the best performance across all applications.  The result is a
    contention-latency setting that generates instruction schedules for all
    applications in our workload that come within 17% of the best schedule for
    each.", 
  location     = "https://doi.org/10.1145/1168857.1168876", 
  location     = "https://cseweb.ucsd.edu/~swanson/papers/ASPLOS2006WaveScalar.pdf"
}

@Article{ecgtdappisp,
  author       = "Michael~I. Gordon and William Thies and Saman Amarasinghe",
  title        = "Exploiting coarse-grained task, data, and pipeline parallelism in stream programs",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "151--162",
  month        = nov,
  keywords     = "coarse-grained dataflow, multicore, raw, software pipelining,
    streamit, stream processing, exotic compilation",
  abstract     = "As multicore architectures enter the mainstream, there is a
    pressing demand for high-level programming models that can effectively map
    to them.  Stream programming offers an attractive way to expose
    coarse-grained parallelism, as streaming applications (image, video, DSP,
    etc.) are naturally represented by independent filters that communicate
    over explicit data channels.In this paper, we demonstrate an end-to-end
    stream compiler that attains robust multicore performance in the face of
    varying application characteristics.  As benchmarks exhibit different
    amounts of task, data, and pipeline parallelism, we exploit all types of
    parallelism in a unified manner in order to achieve this generality.  Our
    compiler, which maps from the StreamIt language to the 16-core Raw
    architecture, attains a 11.2x mean speedup over a single-core baseline, and
    a 1.84x speedup over our previous work.", 
  location     = "https://doi.org/10.1145/1168857.1168877", 
  location     = "http://groups.csail.mit.edu/commit/papers/06/gordon-asplos06.pdf"
}

@Article{tescfwpe,
  author       = "Mahim Mishra and Timothy~J. Callahan and Tiberiu Chelcea and Girish Venkataramani and Seth~C. Goldstein and Mihai Budiu",
  title        = "Tartan: evaluating spatial computation for whole program execution",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "163--174",
  month        = nov,
  keywords     = "spatial computation, dataflow architectures, reconfigurable
    hardware, asynchronous circuits, low power, defect tolerance",
  abstract     = "Spatial Computing (SC) has been shown to be an
    energy-efficient model for implementing program kernels.  In this paper we
    explore the feasibility of using SC for more than small kernels.  To this
    end, we evaluate the performance and energy efficiency of entire
    applications on Tartan, a general-purpose architecture which integrates a
    reconfigurable fabric (RF) with a superscalar core.  Our compiler
    automatically partitions and compiles an application into an instruction
    stream for the core and a configuration for the RF.  We use a detailed
    simulator to capture both timing and energy numbers for all parts of the
    system.Our results indicate that a hierarchical RF architecture, designed
    around a scalable interconnect, is instrumental in harnessing the benefits
    of spatial computation.  The interconnect uses static configuration and
    routing at the lower levels and a packet-switched, dynamically-routed
    network at the top level.  Tartan is most energyefficient when almost all
    of the application is mapped to the RF, indicating the need for the RF to
    support most general-purpose programming constructs.  Our initial
    investigation reveals that such a system can provide, on average, an order
    of magnitude improvement in energy-delay compared to an aggressive
    superscalar core on single-threaded workloads.", 
  location     = "https://doi.org/10.1145/1168857.1168878", 
  location     = "http://www.cs.cmu.edu/~phoenix/papers/asplos06.pdf"
}

@Article{apcafcacc,
  author       = "Stijn Eyerman and Lieven Eeckhout and Tejas Karkhanis and James~E. Smith",
  title        = "{A} performance counter architecture for computing accurate {CPI} components",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "175--184",
  month        = nov,
  keywords     = "hardware performance counter architecture, superscalar
    processor performance modeling, cpi stacks, miss events",
  abstract     = "A common way of representing processor performance is to use
    Cycles per Instruction (CPI) `stacks' which break performance into a
    baseline CPI plus a number of individual miss event CPI components.  CPI
    stacks can be very helpful in gaining insight into the behavior of an
    application on a given microprocessor; consequently, they are widely used
    by software application developers and computer architects.  However,
    computing CPI stacks on superscalar out-of-order processors is challenging
    because of various overlaps among execution and miss events (cache misses,
    TLB misses, and branch mispredictions).This paper shows that meaningful and
    accurate CPI stacks can be computed for superscalar out-of-order
    processors.  Using interval analysis, a novel method for analyzing
    out-of-order processor performance, we gain understanding into the
    performance impact of the various miss events.  Based on this
    understanding, we propose a novel way of architecting hardware performance
    counters for building accurate CPI stacks.  The additional hardware for
    implementing these counters is limited and comparable to existing hardware
    performance counter architectures while being significantly more accurate
    than previous approaches.", 
  location     = "https://doi.org/10.1145/1168857.1168880", 
  location     = "https://www.elis.ugent.be/~leeckhou/papers/asplos06.pdf"
}

@Article{aaermfmpapp,
  author       = "Benjamin~C. Lee and David~M. Brooks",
  title        = "Accurate and efficient regression modeling for microarchitectural performance and power prediction",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "185--194",
  month        = nov,
  keywords     = "microarchitecture, simulation, statistics, interference,
    regression, prediction, modeling, evaluation",
  abstract     = "We propose regression modeling as an efficient approach for
    accurately predicting performance and power for various applications
    executing on any microprocessor configuration in a large microarchitectural
    design space.  This paper addresses fundamental challenges in
    microarchitectural simulation cost by reducing the number of required
    simulations and using simulated results more effectively via statistical
    modeling and inference.Specifically, we derive and validate regression
    models for performance and power.  Such models enable computationally
    efficient statistical inference, requiring the simulation of only 1 in 5
    million points of a joint microarchitecture-application design space while
    achieving median error rates as low as 4.1 percent for performance and 4.3
    percent for power.  Although both models achieve similar accuracy, the
    sources of accuracy are strikingly different.  We present optimizations for
    a baseline regression model to obtain (1) application-specific models to
    maximize accuracy in performance prediction and (2) regional power models
    leveraging only the most relevant samples from the microarchitectural
    design space to maximize accuracy in power prediction.  Assessing
    sensitivity to the number of samples simulated for model formulation, we
    find fewer than 4,000 samples from a design space of approximately 22
    billion points are sufficient.  Collectively, our results suggest
    significant potential in accurate and efficient statistical inference for
    microarchitectural design space exploration via regression models.", 
  location     = "https://doi.org/10.1145/1168857.1168881", 
  location     = "http://people.duke.edu/~bcl15/documents/lee2006-asplos.pdf"
}

@Article{cbatdg,
  author       = "DeMillo, Richard~A. and Offutt, A.~Jefferson",
  title        = "Constraint-Based Automatic Test Data Generation",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "900--910",
  month        = sep,
  keywords     = "constraint-based data generation, automatic test data
    generation, mutation analysis, relative adequacy, fault-based technique,
    algebraic constraints, Godzilla, module testing, Mothra testing system", 
  abstract     = "A novel technique for automatically generating test data is
    presented. The technique is based on mutation analysis and creates test
    data approximating relative adequacy.  Its  fault-based technique uses
    algebraic constraints to describe test cases designed to find particular
    fault types.  A set of tools collectively called Godzilla automatically
    generates constraints and solves them to create test cases for unit and
    module testing.  Godzilla has been integrated with the Mothra testing
    system and has been used as an effective way to generate test data to kill
    program mutants.  The authors present an initial list of constraints and
    discuss some of the problems solved to develop the complete implementation
    of the technique.",
  location     = "https://doi.org/10.1109/32.92910"
}

@Article{adavvaii,
  author       = "Shan Lu and Joseph Tucek and Feng Qin and Yuanyuan Zhou",
  title        = "{AVIO}: detecting atomicity violations via access interleaving invariants",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "37--48",
  month        = nov,
  keywords     = "concurrent programming, atomicity violation, concurrency bugs,
    bug detection, hardware support, atomicity invariants, invariant discovery,
    cache consistency protocols",
  abstract     = "Concurrency bugs are among the most difficult to test and
    diagnose of all software bugs.  The multicore technology trend worsens this
    problem.  Most previous concurrency bug detection work focuses on one bug
    subclass, data races, and neglects many other important ones such as
    atomicity violations, which will soon become increasingly important due to
    the emerging trend of transactional memory models.This paper proposes an
    innovative, comprehensive, invariantbased approach called AVIO to detect
    atomicity violations.  Our idea is based on a novel observation called
    access interleaving invariant, which is a good indication of programmers'
    assumptions about the atomicity of certain code regions.  By automatically
    extracting such invariants and detecting violations of these invariants at
    run time, AVIO can detect a variety of atomicity violations.Based on this
    idea, we have designed and built two implementations of AVIO and evaluated
    the trade-offs between them.  The first implementation, AVIO-S, is purely
    in software, while the second, AVIO-H, requires some simple extensions to
    the cache coherence hardware.  AVIO-S is cheaper and more accurate but
    incurs much higher overhead and thus more run-time perturbation than AVIOH.
    Therefore, AVIO-S is more suitable for in-house bug detection and
    postmortem bug diagnosis, while AVIO-H can be used for bug detection during
    production runs.We evaluate both implementations of AVIO using large
    realworld server applications (Apache and MySQL) with six representative
    real atomicity violation bugs, and SPLASH-2 benchmarks.  Our results show
    that AVIO detects more tested atomicity violations of various types and has
    25 times fewer false positives than previous solutions on average.", 
  location     = "https://doi.org/10.1145/1168857.1168864", 
  location     = "http://pages.cs.wisc.edu/~shanlu/paper/asplos062-lu.ps"
}

@Article{artrrflmrr,
  author       = "Min Xu and Mark~D. Hill and Rastislav Bodik",
  title        = "{A} regulated transitive reduction ({RTR}) for longer memory race recording",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "49--60",
  month        = nov,
  keywords     = "multithreading, determinism, race recording, data
    compression, approximations, consistency models",
  abstract     = "Multithreaded deterministic replay has important applications
    in cyclic debugging, fault tolerance and intrusion analysis.  Memory race
    recording is a key technology for multithreaded deterministic replay.  In
    this paper, we considerably improve our previous always-on Flight Data
    Recorder (FDR) in four ways: •Longer recording by reducing the log size
    growth rate to approximately one byte per thousand dynamic instructions.
    •Lower hardware cost by reducing the cost to 24 KB per processor core.
    •Simpler design by modifying only the cache coherence protocol, but not the
    cache.  •Broader applicability by supporting both Sequential Consistency
    (SC) and Total Store Order (TSO) memory consistency models (existing
    recorders support only SC).These improvements stem from several ideas: (1)
    a Regulated Transitive Reduction (RTR) recording algorithm that creates
    stricter and vectorizable dependencies to reduce the log growth rate; (2) a
    Set/LRU timestamp approximation method that better approximates timestamps
    of uncached memory locations to reduce the hardware cost; (3) an
    order-value-hybrid recording methodthat explicitly logs the value of
    potential SC-violating load instructions to support multiprocessor systems
    with TSO.", 
  location     = "https://doi.org/10.1145/1168857.1168865", 
  location     = "http://www.cs.wisc.edu/multifacet/papers/asplos06_rtr.pdf"
}

@Article{bbeomld,
  author       = "Michael~D. Bond and Kathryn~S. McKinley",
  title        = "Bell: bit-encoding online memory leak detection",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "61--72",
  month        = nov,
  keywords     = "memory leaks, low-overhead monitoring, probabilistic
    approaches, managed systems, one-bit hashes",
  abstract     = "Memory leaks compromise availability and security by
    crippling performance and crashing programs.  Leaks are difficult to
    diagnose because they have no immediate symptoms.  Online leak detection
    tools benefit from storing and reporting per-object sites (e.g., allocation
    sites) for potentially leaking objects.  In programs with many small
    objects, per-object sites add high space overhead, limiting their use in
    production environments.This paper introduces Bit-Encoding Leak Location
    (Bell), a statistical approach that encodes per-object sites to a single
    bit per object.  A bit loses information about a site, but given sufficient
    objects that use the site and a known, finite set of possible sites, Bell
    uses brute-force decoding to recover the site with high accuracy.We use
    this approach to encode object allocation and last-use sites in Sleigh, a
    new leak detection tool.  Sleigh detects stale objects (objects unused for
    a long time) and uses Bell decoding to report their allocation and last-use
    sites.  Our implementation steals four unused bits in the object header and
    thus incurs no per-object space overhead.  Sleigh's instrumentation adds
    29% execution time overhead, which adaptive profiling reduces to 11%.
    Sleigh's output is directly useful for finding and fixing leaks in SPEC
    JBB2000 and Eclipse, although sufficiently many objects must leak before
    Bell decoding can report sites with confidence.  Bell is suitable for other
    leak detection approaches that store per-object sites, and for other
    problems amenable to statistical per-object metadata.", 
  location     = "https://doi.org/10.1145/1168857.1168866", 
  location     = "https://www.cs.utexas.edu/users/mckinley/papers/bell-asplos-2006.pdf"
}

@Article{ulcdpfmp,
  author       = "Smitha Shyam and Kypros Constantinides and Sujay Phadke and Valeria Bertacco and Todd Austin",
  title        = "Ultra low-cost defect protection for microprocessor pipelines",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "73--82",
  month        = nov,
  keywords     = "reliability, defect protection, low cost, pipelines, hardware
    failures, cheap redundancy",
  abstract     = "The sustained push toward smaller and smaller technology
    sizes has reached a point where device reliability has moved to the
    forefront of concerns for next-generation designs.  Silicon failure
    mechanisms, such as transistor wearout and manufacturing defects, are a
    growing challenge that threatens the yield and product lifetime of future
    systems.  In this paper we introduce the BulletProof pipeline, the first
    ultra low-cost mechanism to protect a microprocessor pipeline and on-chip
    memory system from silicon defects.  To achieve this goal we combine
    area-frugal on-line testing techniques and system-level checkpointing to
    provide the same guarantees of reliability found in traditional solutions,
    but at much lower cost.  Our approach utilizes a microarchitectural
    checkpointing mechanism which creates coarse-grained epochs of execution,
    during which distributed on-line built in self-test (BIST) mechanisms
    validate the integrity of the underlying hardware.  In case a failure is
    detected, we rely on the natural redundancy of instructionlevel parallel
    processors to repair the system so that it can still operate in a degraded
    performance mode.  Using detailed circuit-level and architectural
    simulation, we find that our approach provides very high coverage of
    silicon defects (89%) with little area cost (5.8%).  In addition, when a
    defect occurs, the subsequent degraded mode of operation was found to have
    only moderate performance impacts, (from 4% to 18% slowdown).", 
  location     = "https://doi.org/10.1145/1168857.1168868", 
  location     = "http://www-personal.umich.edu/~kypros/shyam-asplos06.pdf"
}

@Article{upbprtflohcft,
  author       = "Vimal~K. Reddy and Eric Rotenberg and Sailashri Parthasarathy",
  title        = "Understanding prediction-based partial redundant threading for low-overhead, high-coverage fault tolerance",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "83--94",
  month        = nov,
  keywords     = "simultaneous multithreading, smt, chip multiprocessors, cmp,
    slipstream processor, transient faults, time redundancy, redundant
    multithreading, branch prediction, value prediction",
  abstract     = "Redundant threading architectures duplicate all instructions
    to detect and possibly recover from transient faults.  Several lighter
    weight Partial Redundant Threading (PRT) architectures have been proposed
    recently.  (i) Opportunistic Fault Tolerance duplicates instructions only
    during periods of poor single-thread performance.  (ii) ReStore does not
    explicitly duplicate instructions and instead exploits mispredictions among
    highly confident branch predictions as symptoms of faults.  (iii)
    Slipstream creates a reduced alternate thread by replacing many
    instructions with highly confident predictions.  We explore PRT as a
    possible direction for achieving the fault tolerance of full duplication
    with the performance of single-thread execution.  Opportunistic and ReStore
    yield partial coverage since they are restricted to using only partial
    duplication or only confident predictions, respectively.  Previous analysis
    of Slipstream fault tolerance was cursory and concluded that only
    duplicated instructions are covered.  In this paper, we attempt to better
    understand Slipstream's fault tolerance, conjecturing that the mixture of
    partial duplication and confident predictions actually closely approximates
    the coverage of full duplication.  A thorough dissection of prediction
    scenarios confirms that faults in nearly 100% of instructions are
    detectable.  Fewer than 0.1% of faulty instructions are not detectable due
    to coincident faults and mispredictions.  Next we show that the current
    recovery implementation fails to leverage excellent detection capability,
    since recovery sometimes initiates belatedly, after already retiring a
    detected faulty instruction.  We propose and evaluate a suite of simple
    microarchitectural alterations to recovery and checking.  Using the best
    alterations, Slipstream can recover from faults in 99% of instructions,
    compared to only 78% of instructions without alterations.  Both results are
    much higher than predicted by past research, which claims coverage for only
    duplicated instructions, or 65% of instructions.  On an 8-issue SMT
    processor, Slipstream performs within 1.3% of single-thread execution
    whereas full duplication slows performance by 14%.A key byproduct of this
    paper is a novel analysis framework in which every dynamic instruction is
    considered to be hypothetically faulty, thus not requiring explicit fault
    injection.  Fault coverage is measured in terms of the fraction of
    candidate faulty instructions that are directly or indirectly detectable
    before.", 
  location     = "https://doi.org/10.1145/1168857.1168869", 
  location     = "http://people.engr.ncsu.edu/ericro/publications/conference_ASPLOS-12.pdf"
}

@Article{ssbleferm,
  author       = "Angshuman Parashar and Anand Sivasubramaniam and Sudhanva Gurumurthi",
  title        = "{SlicK}: slice-based locality exploitation for efficient redundant multithreading",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "95--105",
  month        = nov,
  keywords     = "transient faults, redundant threading, backward slice
    extraction, microarchitecture",
  abstract     = "Transient faults are expected a be a major design
    consideration in future microprocessors.  Recent proposals for transient
    fault detection in processor cores have revolved around the idea of
    redundant threading, which involves redundant execution of a program across
    multiple execution contexts.  This paper presents a new approach to
    redundant threading by bringing together the concepts of slice-level
    execution and value and control-flow locality into a novel partial
    redundant threading mechanism called SlicK.The purpose of redundant
    execution is to check the integrity of the outputs propagating out of the
    core (typically through stores).  SlicK implements redundancy at the
    granularity of backward-slices of these output instructions and exploits
    value and control-flow locality to avoid redundantly executing slices that
    lead to predictable outputs, thereby avoiding redundant execution of a
    significant fraction of instructions while maintaining extremely low
    vulnerabilities for critical processor structures.We propose the
    microarchitecture of a backward-slice extractor called SliceEM that is able
    to identify backward slices without interrupting the instruction flow, and
    show how this extractor and a set of predictors can be integrated into a
    redundant threading mechanism to form SlicK.  Detailed simulations with
    SPEC CPU2000 benchmarks show that SlicK can provide around 10.2%
    performance improvement over a well known redundant threading mechanism,
    buying back over 50% of the loss suffered due to redundant execution.
    SlicK can keep the Architectural Vulnerability Factors of processor
    structures to typically 0%-2%.  More importantly, SlicK's slice-based
    mechanisms provide future opportunities for exploring interesting points in
    the performance-reliability design space based on market segment needs.", 
  location     = "https://doi.org/10.1145/1168857.1168870", 
  location     = "https://www.cs.virginia.edu/~gurumurthi/papers/asplos06.pdf"
}

@Article{mafteamfss,
  author       = "Taliver Heath and Ana Paula Centeno and Pradeep George and Luiz Ramos and Yogesh Jaluria and Ricardo Bianchini",
  title        = "{Mercury} and {Freon}: temperature emulation and management for server systems",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "106--116",
  month        = nov,
  keywords     = "temperature modeling, thermal management, energy
    conservation, server clusters",
  abstract     = "Power densities have been increasing rapidly at all levels of
    server systems.  To counter the high temperatures resulting from these
    densities, systems researchers have recently started work on softwarebased
    thermal management.  Unfortunately, research in this new area has been
    hindered by the limitations imposed by simulators and real measurements.
    In this paper, we introduce Mercury, a software suite that avoids these
    limitations by accurately emulating temperatures based on simple layout,
    hardware, and componentutilization data.  Most importantly, Mercury runs
    the entire software stack natively, enables repeatable experiments, and
    allows the study of thermal emergencies without harming hardware
    reliability.  We validate Mercury using real measurements and a widely used
    commercial simulator.  We use Mercury to develop Freon, a system that
    manages thermal emergencies in a server cluster without unnecessary
    performance degradation.  Mercury will soon become available from
    http://www.darklab.rutgers.edu.", 
  location     = "https://doi.org/10.1145/1168857.1168872", 
  location     = "https://people.cs.pitt.edu/~kirk/cs3150spring2010/p106-heath.pdf"
}

@Article{tsdhmtwvm,
  author       = "Jedidiah~R. Crandall and Gary Wassermann and Daniela A.~S. {de Oliveira} and Zhendong Su and S.~Felix Wu and Frederic~T. Chong",
  title        = "Temporal search: detecting hidden malware timebombs with virtual machines",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "25--36",
  month        = nov,
  keywords     = "worms, malware, virtual machines, temporal discovery, time
    bombs",
  abstract     = "Worms, viruses, and other malware can be ticking bombs
    counting down to a specific time, when they might, for example, delete
    files or download new instructions from a public web server.  We propose a
    novel virtual-machine-based analysis technique to automatically discover
    the timetable of a piece of malware, or when events will be triggered, so
    that other types of analysis can discern what those events are.  This
    information can be invaluable for responding to rapid malware, and
    automating its discovery can provide more accurate information with less
    delay than careful human analysis.Developing an automated system that
    produces the timetable of a piece of malware is a challenging research
    problem.  In this paper, we describe our implementation of a key component
    of such a system: the discovery of timers without making assumptions about
    the integrity of the infected system's kernel.  Our technique runs a
    virtual machine at slightly different rates of perceived time (time as seen
    by the virtual machine), and identifies time counters by correlating memory
    write frequency to timer interrupt frequency.We also analyze real malware
    to assess the feasibility of using full-system, machine-level symbolic
    execution on these timers to discover predicates.  Because of the
    intricacies of the Gregorian calendar (leap years, different number of days
    in each month, etc.) these predicates will not be direct expressions on the
    timer but instead an annotated trace; so we formalize the calculation of a
    timetable as a weakest precondition calculation.  Our analysis of six real
    worms sheds light on two challenges for future work: 1) time-dependent
    malware behavior often does not follow a linear timetable; and 2) that an
    attacker with knowledge of the analysis technique can evade analysis.  Our
    current results are promising in that with simple symbolic execution we are
    able to discover predicates on the day of the month for four real worms.
    Then through more traditional manual analysis we conclude that a more
    control-flow-sensitive symbolic execution implementation would discover all
    predicates for the malware we analyzed.", 
  location     = "https://doi.org/10.1145/1168857.1168862", 
  location     = "http://web.cs.ucdavis.edu/~su/publications/asplos06.pdf"
}

@Article{gmtbciavme,
  author       = "Stephen~T. Jones and Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Geiger: monitoring the buffer cache in a virtual machine environment",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "14--24",
  month        = nov,
  keywords     = "virtual machines, inferences, grey-box discovery, buffer
    cache management, virtual page management",
  abstract     = "Virtualization is increasingly being used to address server
    management and administration issues like flexible resource allocation,
    service isolation and workload migration.  In a virtualized environment,
    the virtual machine monitor (VMM) is the primary resource manager and is an
    attractive target for implementing system features like scheduling,
    caching, and monitoring.  However, the lackof runtime information within
    the VMM about guest operating systems, sometimes called the semantic gap,
    is a significant obstacle to efficiently implementing some kinds of
    services.In this paper we explore techniques that can be used by a VMM to
    passively infer useful information about a guest operating system's unified
    buffer cache and virtual memory system.  We have created a prototype
    implementation of these techniques inside the Xen VMM called Geiger and
    show that it can accurately infer when pages are inserted into and evicted
    from a system's buffer cache.  We explore several nuances involved in
    passively implementing eviction detection that have not previously been
    addressed, such as the importance of tracking disk block liveness, the
    effect of file system journaling, and the importance of accounting for the
    unified caches found in modern operating systems.Using case studies we show
    that the information provided by Geiger enables a VMM to implement useful
    VMM-level services.  We implement a novel working set size estimator which
    allows the VMM to make more informed memory allocation decisions.  We also
    show that a VMM can be used to drastically improve the hit rate in remote
    storage caches by using eviction-based cache placement without modifying
    the application or operating system storage interface.  Both case studies
    hint at a future where inference techniques enable a broad new class of
    VMM-level functionality.", 
  location     = "https://doi.org/10.1145/1168857.1168861", 
  location     = "https://research.cs.wisc.edu/wind/Publications/geiger-asplos06.pdf"
}

@Article{acosahtfxv,
  author       = "Keith Adams and Ole Agesen",
  title        = "{A} comparison of software and hardware techniques for x86 virtualization",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "2--13",
  month        = nov,
  keywords     = "virtualization, virtual machine monitor, dynamic binary
    translation, x86, vt, svm, mmu, tlb, nested paging",
  abstract     = "Until recently, the x86 architecture has not permitted
    classical trap-and-emulate virtualization.  Virtual Machine Monitors for
    x86, such as VMware ® Workstation and Virtual PC, have instead used binary
    translation of the guest kernel code.  However, both Intel and AMD have now
    introduced architectural extensions to support classical virtualization.We
    compare an existing software VMM with a new VMM designed for the emerging
    hardware support.  Surprisingly, the hardware VMM often suffers lower
    performance than the pure software VMM.  To determine why, we study
    architecture-level events such as page table updates, context switches and
    I/O, and find their costs vastly different among native, software VMM and
    hardware VMM execution.We find that the hardware support fails to provide
    an unambiguous performance advantage for two primary reasons: first, it
    offers no support for MMU virtualization; second, it fails to co-exist with
    existing software techniques for MMU virtualization.  We look ahead to
    emerging techniques for addressing this MMU virtualization problem in the
    context of hardware-assisted virtualization.", 
  location     = "https://doi.org/10.1145/1168857.1168860"
}

@Article{lafjps,
  author       = "Chris Hawblitzel and Thorsten {von Eicken}",
  title        = "Luna: a Flexible {Java} Protection System",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "391--403",
  month        = "Winter",
  keywords     = "type systems, distributed shared storage, remote pointers,
    pointer revocation",
  abstract     = "Extensible Java systems face a difficult trade-off between
    sharing and protection.  On one hand, Java's ability to run different
    protection domains in a single virtual machine enables domains to share
    data easily and communicate without address space switches.  On the other
    hand, unrestricted sharing blurs the boundaries between protection domains,
    making it difficult to terminate domains and enforce restrictions on
    resource usage.  Existing solutions to these problems restrict sharing in
    an ad-hoc fashion, ruling out many desirable programming styles.This paper
    presents an extension to Java's type system that systematically addresses
    the issues of data sharing, revocation, thread control, and resource
    control.  Multiple tasks running in a single virtual machines share data
    using special remote pointers, which have different types from local
    pointers.  The distinction between local and remote pointers allows the
    Java runtime system to mediate the communication between tasks without
    slowing down operations on ordinary pointers.  The extensions to Java are
    implemented by a system called Luna, based on the Guavac and Marmot
    compilers, extended with special optimizations to support both fast
    inter-task communication and dynamic access control.  The paper describes
    two applications written in Luna: a simple extensible web server, and an
    extension of the Squid web cache to support dynamic content generation.", 
  location     = "https://dl.acm.org/doi/abs/10.1145/844128.844164"
}

@Article{otmovc,
  author       = "Constantine~P. Sapuntzakis and Ramesh Chandra and Ben Pfaff and Jim Chow and Monica~S. Lam and Mendel Rosenblum",
  title        = "Optimizing the migration of virtual computers",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "377--390",
  month        = "Winter",
  keywords     = "process migration, state transfer, virtual machines, capsule",
  abstract     = "This paper shows how to quickly move the state of a running
    computer across a network, including the state in its disks, memory, CPU
    registers, and I/O devices.  We call this state a capsule.  Capsule state
    is hardware state, so it includes the entire operating system as well as
    applications and running processes.We have chosen to move x86 computer
    states because x86 computers are common, cheap, run the software we use,
    and have tools for migration.  Unfortunately, x86 capsules can be large,
    containing hundreds of megabytes of memory and gigabytes of disk data.  We
    have developed techniques to reduce the amount of data sent over the
    network: copy-on-write disks track just the updates to capsule disks,
    'ballooning' zeros unused memory, demand paging fetches only needed blocks,
    and hashing avoids sending blocks that already exist at the remote end.  We
    demonstrate these optimizations in a prototype system that uses VMware GSX
    Server virtual, machine monitor to create and run x86 capsules.  The system
    targets networks as slow as 384 kbps.Our experimental results suggest that
    efficient capsule migration can improve user mobility and system
    management.  Software updates or installations on a set of machines can be
    accomplished simply by distributing a capsule with the new changes.
    Assuming the presence of a prior capsule, the amount of traffic incurred is
    commensurate with the size of the update or installation package itself.
    Capsule migration makes it possible for machines to start running an
    application within 20 minutes on a 384 kbps link, without having to first
    install the application or even the underlying operating system.
    Furthermore, users' capsules can be migrated during a commute between home
    and work in even less time.", 
  location     = "https://doi.org/10.1145/844128.844163", 
  location     = "https://benpfaff.org/papers/migration.pdf"
}

@Article{tdaiozasfmce,
  author       = "Steven Osman and Dinesh Subhraveti and Gong Su and Jason Nieh",
  title        = "The design and implementation of {Zap}: a system for migrating computing environments",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "361--376",
  month        = "Winter",
  keywords     = "process migration, portability, system management, pods,
    virtualization",
  abstract     = "We have created Zap, a novel system for transparent migration
    of legacy and networked applications.  Zap provides a thin virtualization
    layer on top of the operating system that introduces pods, which are groups
    of processes that are provided a consistent, virtualized view of the
    system.  This decouples processes in pods from dependencies to the host
    operating system and other processes on the system.  By integrating Zap
    virtualization with a checkpoint-restart mechanism, Zap can migrate a pod
    of processes as a unit among machines running independent operating systems
    without leaving behind any residual state after migration.  We have
    implemented a Zap prototype in Linux that supports transparent migration of
    unmodified applications without any kernel modifications.  We demonstrate
    that our Linux Zap prototype can provide general-purpose process migration
    functionality with low overhead.  Our experimental results for migrating
    pods used for running a standard user's X windows desktop computing
    environment and for running an Apache web server show that these kinds of
    pods can be migrated with subsecond checkpoint and restart latencies.", 
  location     = "https://doi.org/10.1145/844128.844162", 
  location     = "https://www.cs.cmu.edu/~sosman/publications/osdi2002/osdi2002_zap.pdf"
}

@Article{teorrocr,
  author       = "Limin Wang and Vivek Pai and Larry Peterson",
  title        = "The effectiveness of request redirection on {CDN} robustness",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "345--360",
  month        = "Winter",
  keywords     = "content distribution, load balancing, replication, caching,
    address hashing, locality, distribution networks",
  abstract     = "It is becoming increasingly common to construct network
    services using redundant resources geographically distributed across the
    Internet.  Content Distribution Networks are a prime example.  Such systems
    distribute client requests to an appropriate server based on a variety of
    factors---e.g., server load, network proximity, cache locality--in an
    effort to reduce response time and increase the system capacity under load.
    This paper explores the design space of strategies employed to redirect
    requests, and defines a class of new algorithms that carefully balance
    load, locality, and proximity.  We use large-scale detailed simulations to
    evaluate the various strategies.  These simulations clearly demonstrate the
    effectiveness of our new algorithms, which yield a 60--91% improvement in
    system capacity when compared with the best published CDN technology, yet
    user-perceived response latency remains low and the system scales well with
    the number of servers.", 
  location     = "https://doi.org/10.1145/844128.844160"
}

@Article{tnamfbt,
  author       = "Arun Venkataramani and Ravi Kokku and Mike Dahlinabs",
  title        = "{TCP} Nice: a mechanism for background transfers",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "329--343",
  month        = "Winter",
  keywords     = "tcp, background transfers",
  abstract     = "Many distributed applications can make use of large
    background transfers--transfers of data that humans are not waiting for--to
    improve availability, reliability, latency or consistency.  However, given
    the rapid fluctuations of available network bandwidth and changing resource
    costs due to technology trends, hand tuning the aggressiveness of
    background transfers risks (1) complicating applications, (2) being too
    aggressive and interfering with other applications, and (3) being too timid
    and not gaining the benefits of background transfers.  Our goal is for the
    operating system to manage network resources in order to provide a simple
    abstraction of near zero-cost background transfers.  Our system, TCP Nice,
    can provably bound the interference inflicted by background flows on
    foreground flows in a restricted network model.  And our microbenchmarks
    and case study applications suggest that in practice it interferes little
    with foreground flows, reaps a large fraction of spare network bandwidth,
    and simplifies application construction and deployment.  For example, in
    our prefetching case study application, aggressive prefetching improves
    demand performance by a factor of three when Nice manages resources; but
    the same prefetching hurts demand performance by a factor of six under
    standard network congestion control.", 
  location     = "https://doi.org/10.1145/844128.844159", 
  location     = "http://www.cs.umass.edu/~arun/papers/tcp-nice-osdi.pdf"
}

@Article{aaoicds,
  author       = "Stefan Saroiu and Krishna~P. Gummadi and Richard~J. Dunn and Steven~D. Gribble and Henry~M. Levy",
  title        = "An analysis of Internet content delivery systems",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "315--327",
  month        = "Winter",
  keywords     = "content delivery systems, peer-to-peer systems, traffic
    analysis, caching, scalability",
  abstract     = "In the span of only a few years, the Internet has experienced
    an astronomical increase in the use of specialized content delivery
    systems, such as content delivery networks and peer-to-peer file sharing
    systems.  Therefore, an understanding of content delivery on the lnternet
    now requires a detailed understanding of how these systems are used in
    practice.This paper examines content delivery from the point of view of
    four content delivery systems: HTTP web traffic, the Akamai content
    delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic.
    We collected a trace of all incoming and outgoing network traffic at the
    University of Washington, a large university with over 60,000 students,
    faculty, and staff.  From this trace, we isolated and characterized traffic
    belonging to each of these four delivery classes.  Our results (1)
    quantify, the rapidly increasing importance of new content delivery
    systems, particularly peer-to-peer networks, (2) characterize the behavior
    of these systems from the perspectives of clients, objects, and servers,
    and (3) derive implications for caching in these systems.", 
  location     = "https://doi.org/10.1145/844128.844158", 
  location     = "https://www.gribble.org/papers/p2p_osdi.pdf"
}

@Article{srfsptpon,
  author       = "Miguel Castro and Peter Druschel and Ayalvadi Ganesh and Antony Rowstron and Dan~S. Wallach",
  title        = "Secure routing for structured peer-to-peer overlay networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "299--314",
  month        = "Winter",
  keywords     = "overlay routing, pastry, sybil attacks, neighborhood
    management, secure routing",
  abstract     = "Structured peer-to-peer overlay networks provide a substrate
    for the construction of large-scale, decentralized applications, including
    distributed storage, group communication, and content distribution.  These
    overlays are highly resilient; they can route messages correctly even when
    a large fraction of the nodes crash or the network partitions.  But current
    overlays are not secure; even a small fraction of malicious nodes can
    prevent correct message delivery throughout the overlay.  This problem is
    particularly serious in open peer-to-peer systems, where many diverse,
    autonomous parties without preexisting trust relationships wish to pool
    their resources.  This paper studies attacks aimed at preventing correct
    message delivery in structured peer-to-peer overlays and presents defenses
    to these attacks.  We describe and evaluate techniques that allow nodes to
    join the overlay, to maintain routing state, and to forward messages
    securely in the presence of malicious nodes.", 
  location     = "https://doi.org/10.1145/844128.844156", 
  location     = "https://www.cs.rice.edu/~dwallach/pub/osdi2002.pdf"
}

@Article{pmbcae,
  author       = "Landon~P. Cox and Christopher~D. Murray and Brian~D. Noble",
  title        = "Pastiche: making backup cheap and easy",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "285--298",
  month        = "Winter",
  keywords     = "backup, peer-to-peer systems, content-based indexing,
    fingerprinting, compression",
  abstract     = "Backup is cumbersome and expensive.  Individual users almost
    never back up their data, and backup is a significant cost in large
    organizations.  This paper presents Pastiche, a simple and inexpensive
    backup system.  Pastiche exploits excess disk capacity to perform
    peer-to-peer backup with no administrative costs.  Each node minimizes
    storage overhead by selecting peers that share a significant amount of
    data.  It is easy for common installations to find suitable peers, and
    peers with high overlap can be identified with only hundreds of bytes.
    Pastiche provides mechanisms for confidentiality, integrity, and detection
    of failed or malicious peers.  A Pastiche prototype suffers only 7.4%
    overhead for a modified Andrew Benchmark, and restore performance is
    comparable to cross-machine copy.", 
  location     = "https://doi.org/10.1145/844128.844155", 
  location     = "http://www.cs.fsu.edu/~awang/courses/cop5611_s2009/pastiche.pdf"
}

@Article{saaialsne,
  author       = "Amin Vahdat and Ken Yocum and Kevin Walsh and Priya Mahadevan and Dejan Kostić and Jeff Chase and David Becker",
  title        = "Scalability and accuracy in a large-scale network emulator",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "271--284",
  month        = "Winter",
  keywords     = "network emulation, scalability, virtual networking",
  abstract     = "This paper presents ModelNet, a scalable Internet emulation
    environment that enables researchers to deploy unmodified software
    prototypes in a configurable Internet-like environment and subject them to
    faults and varying network conditions.  Edge nodes running user-specified
    OS and application software are configured to route their packets through a
    set of ModelNet core nodes, which cooperate to subject the traffic to the
    bandwidth, congestion constraints, latency, and loss profile of a target
    network topology.This paper describes and evaluates the ModelNet
    architecture and its implementation, including novel techniques to balance
    emulation accuracy against scalability.  The current ModelNet prototype is
    able to accurately subject thousands of instances of a distrbuted
    application to Internet-like conditions with gigabits of bisection
    bandwidth.  Experiments with several large-scale distributed services
    demonstrate the generality and effectiveness of the infrastructure.", 
  location     = "https://doi.org/10.1145/844128.844154", 
  location     = "https://mathcs.holycross.edu/~kwalsh/papers/modelnet.pdf"
}

@Article{aieefdsan,
  author       = "Brian White and Jay Lepreau and Leigh Stoller and Robert Ricci and Shashi Guruprasad and Mac Newbold and Mike Hibler and Chad Barb and Abhijeet Joglekar",
  title        = "An integrated experimental environment for distributed systems and networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "255--270",
  month        = "Winter",
  keywords     = "network emulation, network simulation, network testbeds",
  abstract     = "Three experimental environments traditionally support network
    and distributed systems research: network emulators, network simulators,
    and live networks.  The continued use of multiple approaches highlights
    both the value and inadequacy of each.  Netbed, a descendant of Emulab,
    provides an experimentation facility that integrates these approaches,
    allowing researchers to configure and access networks composed of emulated,
    simulated, and wide-area nodes and links.  Netbed's primary goals are ease
    of use, control, and realism, achieved through consistent use of
    virtualization and abstraction.By providing operating system-like services,
    such as resource allocation and scheduling, and by virtualizing
    heterogeneous resources, Netbed acts as a virtual machine for network
    experimentation.  This paper presents Netbed's overall design and
    implementation and demonstrates its ability to improve experimental
    automation and efficiency.  These, in turn, lead to new methods of
    experimentation, including automated parameter-space studies within
    emulation and straightforward comparisons of simulated, emulated, and
    wide-area scenarios.", 
  location     = "https://doi.org/10.1145/844128.844152", 
  location     = "https://www.flux.utah.edu/download?uid=122"
}

@Article{roaapishp,
  author       = "Bhuvan Urgaonkar and Prashant Shenoy and Timothy Roscoe",
  title        = "Resource overbooking and application profiling in shared hosting platforms",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "239--254",
  month        = "Winter",
  keywords     = "qos management, yield management, cluster management, node
    assignment, resource demand modeling",
  abstract     = "In this paper, we present techniques for provisioning CPU and
    network resources in shared hosting platforms running potentially
    antagonistic third-party applications.  The primary contribution of our
    work is to demonstrate the feasibility and benefits of overbooking
    resources in shared platforms, to maximize the platform yield: the revenue
    generated by the available resources.  We do this by first deriving an
    accurate estimate of application resource needs by profiling applications
    on dedicated nodes, and then using these profiles to guide the placement of
    application components onto shared nodes.  By overbooking cluster resources
    in a controlled fashion, our platform can provide performance guarantees to
    applications even when overbooked, and combine these techniques with
    commonly used QoS resource allocation mechanisms to provide application
    isolation and performance guarantees at run-time.  When compared to
    provisioning based on the worst-case, the efficiency (and consequently
    revenue) benefits from controlled overbooking of resources can be dramatic.
    Specifically, experiments on our Linux cluster implementation indicate that
    overbooking resources by as little as 1% can increase the utilization of
    the cluster by a factor of two, and a 5% overbooking yields a 300--500%
    improvement, while still providing useful resource guarantees to
    applications.", 
  location     = "https://doi.org/10.1145/844128.844151", 
  location     = "https://people.inf.ethz.ch/troscoe/pubs/OSDI2002.pdf"
}

@Article{adotcs,
  author       = "Edsger Dijkstra",
  title        = "{A} Debate on Teaching Computer Science (On the Cruelty of Really Teaching Computer Science)",
  journal      = cacm,
  year         = 1989,
  volume       = 32,
  number       = 12,
  pages        = "1397--1414",
  month        = dec,
  keywords     = "teaching, reasoning, computer science",
  abstract     = "At the ACM Computer Science Conference last February, Edsger
    Dijkstra gave an invited talk A called “On the Cruelty of Really Teaching
    Computing Science.” He challenged some of the basic assumptions on which
    our curricula are based and provoked a lot of discussion. The editors of
    Communications received several recommendations to publish his talk in
    these pages. His comments brought into the foreground some of the
    background of controversy that surrounds the issue of what belongs in the
    core of a computer science curriculum. To give full airing to the
    controversy, we invited Dijkstra to engage in a debate with selected
    colleagues, each of whom would contribute a short critique of his position,
    with Dijkstra himself making a closing statement. He graciously accepted
    this offer. We invited people from a variety of specialties, backgrounds,
    and interpretations to provide their comments. David Parnas is a noted
    software engineer who was outspoken in his criticism of the proposed
    Strategic Defense Initiative. William Scherlis is known for his articulate
    advocacy of formal methods in computer science. M. H. van Emden is known
    for his contributions in programming languages and philosophical insights
    into science. Jacques Cohen is known for his work with programming
    languages and logic programming and is a member of the Editorial Panel of
    this magazine. Richard Hamming received the Turing Award in 1968 and is
    well known for his work in communications and coding theory. Richard
    M. Karp received the Turing Award in 1985 and is known for his
    contributions in the design of algorithms. Terry Winograd is well known for
    his early work in artificial intelligence and recent work in the principles
    of design. I am grateful to these people for participating in this debate
    and to Professor Dijkstra for creating the opening." 
}

@Article{irmfcbis,
  author       = "Kai Shen and Hong Tang and Tao Yang and Lingkun Chu",
  title        = "Integrated resource management for cluster-based Internet services",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "225--238",
  month        = "Winter",
  keywords     = "resource allocation, quality of service, service
    differentiation, yield management, service scheduling",
  abstract     = "Client request rates for Internet services tend to be bursty
    and thus it is important to maintain efficient resource utilization under a
    wide range of load conditions. Network service clients typically seek
    services interactively and maintaining reasonable response time is often
    imperative for such services. In addition, providing differentiated service
    qualities and resource allocation to multiple service classes can also be
    desirable at times. This paper presents an integrated resource management
    framework (part of Neptune system) that provides flexible service quality
    specification, efficient resource utilization, and service differentiation
    for cluster-based services. This framework introduces the metric of
    quality-aware service yield to combine the overall system efficiency and
    individual service response time in one flexible model. Resources are
    managed through a two-level request distribution and scheduling scheme. At
    the cluster level, a fully decentralized request distribution architecture
    is employed to achieve high scalability and availability. Inside each
    service node, an adaptive scheduling policy maintains efficient resource
    utilization under a wide range of load conditions. Our trace-driven
    evaluations demonstrate the performance, scalability, and service
    differentiation achieved by the proposed techniques.", 
  location     = "https://doi.org/10.1145/844128.844150", 
  location     = "https://www.cs.rochester.edu/u/kshen/papers/osdi2002_html/osdi02.html"
}

@Article{reiatvmlar,
  author       = "George~W. Dunlap and Samuel~T. King and Sukru Cinar and Murtaza~A. Basrai and Peter~M. Chen",
  title        = "{ReVirt}: enabling intrusion analysis through virtual-machine logging and replay",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "211--224",
  month        = "Winter",
  keywords     = "virtual machines, linux, trusted computing base, logging,
    replay, compression, attack analysis",
  abstract     = "Current system loggers have two problems: they depend on the
    integrity of the operating system being logged, and they do not save
    sufficient information to replay and analyze attacks that include any
    non-deterministic events. ReVirt removes the dependency on the target
    operating system by moving it into a virtual machine and logging below the
    virtual machine. This allows ReVirt to replay the system's execution
    before, during, and after an intruder compromises the system, even if the
    intruder replaces the target operating system. ReVirt logs enough
    information to replay a long-term execution of the virtual machine
    instruction-by-instruction. This enables it to provide arbitrarily detailed
    observations about what transpired on the system, even in the presence of
    non-deterministic attacks and executions. ReVirt adds reasonable time and
    space overhead. Overheads due to virtualization are imperceptible for
    interactive use and CPU-bound workloads, and 13--58% for kernel-intensive
    workloads. Logging adds 0--8% overhead, and logging traffic for our
    workloads can be stored on a single disk for several months.", 
  location     = "https://doi.org/10.1145/844128.844148", 
  location     = "https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F14/handouts/papers/dunlap02.pdf"
}

@Article{sapitdik,
  author       = "Andrew Whitaker and Marianne Shaw and Steven~D. Gribble",
  title        = "Scale and performance in the {Denali} isolation kernel",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "195--209",
  month        = "Winter",
  keywords     = "isolation kernels, exokernels, abstraction levels, interrupt
    architectures",
  abstract     = "This paper describes the Denali isolation kernel, an
    operating system architecture that safely multiplexes a large number of
    untrusted Internet services on shared hardware. Denali's goal is to allow
    new Internet services to be 'pushed' into third party infrastructure,
    relieving Internet service authors from the burden of acquiring and
    maintaining physical infrastructure. Our isolation kernel exposes a virtual
    machine abstraction, but unlike conventional virtual machine monitors,
    Denali does not attempt to emulate the underlying physical architecture
    precisely, and instead modifies the virtual architecture to gain scale,
    performance, and simplicity of implementation. In this paper, we first
    discuss design principles of isolation kernels, and then we describe the
    design and implementation of Denali. Following this, we present a detailed
    evaluation of Denali, demonstrating that the overhead of virtualization is
    small, that our architectural choices are warranted, and that we can
    successfully scale to more than 10,000 virtual machines on commodity
    hardware.", 
  location     = "https://doi.org/10.1145/844128.844147", 
  location     = "https://www.usenix.org/conference/osdi-02/scale-and-performance-denali-isolation-kernel"
}

@Article{mrmives,
  author       = "Carl~A. Waldspurger",
  title        = "Memory resource management in {VMware ESX} server",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "181--194",
  month        = "Winter",
  keywords     = "memory virtualization, resource reclamation, page
    replacement, ballooning, storage sharing, working sets",
  abstract     = "VMware ESX Server is a thin software layer designed to
    multiplex hardware resources efficiently among virtual machines running
    unmodified commodity operating systems. This paper introduces several novel
    ESX Server mechanisms and policies for managing memory. A ballooning
    technique reclaims the pages considered least valuable by the operating
    system running in a virtual machine. An idle memory tax achieves efficient
    memory utilization while maintaining performance isolation
    guarantees. Content-based page sharing and hot I/O page remapping exploit
    transparent page remapping to eliminate redundancy and reduce copying
    overheads. These techniques are combined to efficiently support virtual
    machine workloads that overcommit memory.", 
  location     = "https://doi.org/10.1145/844128.844146", 
  location     = "https://www.vmware.com/pdf/usenix_resource_mgmt.pdf"
}

@Article{stsaoaco,
  author       = "Ashvin Goel and Luca Abeni and Charles Krasic and Jim Snow and Jonathan Walpole",
  title        = "Supporting time-sensitive applications on a commodity {OS}",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "165--180",
  month        = "Winter",
  keywords     = "timer management, kernel interrupt architecture, process
    scheduling, real-time systems",
  abstract     = "Commodity operating systems are increasingly being used for
    serving time-sensitive applications. These applications require low-latency
    response from the kernel and from other system-level services. In this
    paper, we explore various operating systems techniques needed to support
    time-sensitive applications and describe the design of our Time-Sensitive
    Linux (TSL) system. We show that the combination of a high-precision timing
    facility, a well-designed preemptible kernel and the use of appropriate
    scheduling techniques is the basis for a low-latency response system and
    such a system can have low overhead. We evaluate the behavior of realistic
    time-sensitive user- and kernel-level applications on our system and show
    that, in practice, it is possible to satisfy the constraints of
    time-sensitive applications in a commodity operating system without
    significantly compromising the performance of throughput-oriented
    applications.", 
  location     = "https://doi.org/10.1145/844128.844144", 
  location     = "https://www.eecg.utoronto.ca/~ashvin/publications/osdi2002.pdf"
}

@Article{fgntsurb,
  author       = "Jeremy Elson and Lewis Girod and Deborah Estrin",
  title        = "Fine-grained network time synchronization using reference broadcasts",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "147--163",
  month        = "Winter",
  keywords     = "time synchronization, broadcast synchronization, relative
    synchronization, clock skew",
  abstract     = "Recent advances in miniaturization and low-cost, low-power
    design have led to active research in large-scale networks of small,
    wireless, low-power sensors and actuators. Time synchronization is critical
    in sensor networks for diverse purposes including sensor data fusion,
    coordinated actuation, and power-efficient duty cycling. Though the clock
    accuracy and precision requirements are often stricter than in traditional
    distributed systems, strict energy constraints limit the resources
    available to meet these goals.We present Reference-Broadcast
    Synchronization, a scheme in which nodes send reference beacons to their
    neighbors using physical-layer broadcasts. A reference broadcast does not
    contain an explicit timestamp; instead, receivers use its arrival time as a
    point of reference for comparing their clocks. In this paper, we use
    measurements from two wireless implementations to show that removing the
    sender's nondeterminism from the critical path in this way produces
    high-precision clock agreement (1.85 ± 1.28μsec, using off-the-shelf 802.11
    wireless Ethernet), while using minimal energy. We also describe a novel
    algorithm that uses this same broadcast property to federate clocks across
    broadcast domains with a slow decay in precision (3.68 ± 2.57μsec after 4
    hops). RBS can be used without external references, forming a precise
    relative timescale, or can maintain microsecond-level synchronization to an
    external timescale such as UTC. We show a significant improvement over the
    Network Time Protocol (NTP) under similar conditions.", 
  location     = "https://doi.org/10.1145/844128.844143", 
  location     = "https://www.cs.ubc.ca/~krasic/cpsc538a/papers/broadcast-osdi.pdf"
}

@Article{tatasfahsn,
  author       = "Samuel Madden and Michael~J. Franklin and Joseph~M. Hellerstein and Wei Hong",
  title        = "{TAG}: a Tiny AGgregation service for ad-hoc sensor networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "131--146",
  month        = "Winter",
  keywords     = "distributed aggregation, sensor networks, query languages,
    grouping",
  abstract     = "We present the Tiny AGgregation (TAG) service for aggregation
    in low-power, distributed, wireless environments. TAG allows users to
    express simple, declarative queries and have them distributed and executed
    efficiently in networks of low-power, wireless sensors. We discuss various
    generic properties of aggregates, and show how those properties affect the
    performance of our in network approach. We include a performance study
    demonstrating the advantages of our approach over traditional centralized,
    out-of-network methods, and discuss a variety of optimizations for
    improving the performance and fault tolerance of the basic solution.", 
  location     = "https://doi.org/10.1145/844128.844142", 
  location     = "https://web.stanford.edu/class/cs240e/papers/madden_tag.pdf"
}

@Article{cioaniosfeaa,
  author       = "Andreas Weissel and Bj{\" o}rn Beutel and Frank Bellosa",
  title        = "Cooperative {I/O}: a novel {I/O} semantics for energy-aware applications",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "117--129",
  month        = "Winter",
  keywords     = "low-power operations, cooperative scheduling, power scheduling",
  abstract     = "In this paper we demonstrate the benefits of application
    involvement in operating system power management.  We present Coop-I/O, an
    approach to reduce the power consumption of devices while encompassing all
    levels of the system---from the hardware and OS to a new interface for
    cooperative I/O that can be used by energy-aware applications.  We assume
    devices which can be set to low-power operation modes if they are not
    accessed and where switching between modes consumes additional energy, e.g.
    devices with rotating components or network devices consuming energy for
    the establishment and shutdown of network connections.  In these cases
    frequent mode switches should be avoided.With Coop-I/O, applications can
    declare open, read and write operations as deferrable and even abortable by
    specifying a time-out and a cancel flag.  This information enables the
    operating system to delay and batch requests so that the number of power
    mode switches is reduced and the device can be kept longer in a low-power
    mode.  We have deployed our concept to the IDE hard disk driver and Ext2
    file system of Linux and to typical real-life programs so that they make
    use of the new cooperative I/O functions.  With energy savings of up to
    50%, the experimental results demonstrate the benefits of the concept.  We
    will show that Coop-I/O even outperforms the 'oracle' shutdown policy which
    defines the lower bound in power consumption if the timing of requests can
    not be influenced.", 
  location     = "https://doi.org/10.1145/844128.844140", 
  location     = "https://www.usenix.org/legacy/event/osdi02/tech/full_papers/weissel/weissel.pdf"
}

@Article{vapsfl,
  author       = "Kriszti{\' a}n Flautner and Trevor Mudge",
  title        = "Vertigo: automatic performance-setting for {Linux}",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "105--116",
  month        = "Winter",
  keywords     = "adaptive power management, performance-based scheduling,
    monitoring",
  abstract     = "Combining high performance with low power consumption is
    becoming one of the primary objectives of processor designs.  Instead of
    relying just on sleep mode for conserving power, an increasing number of
    processors take advantage of the fact that reducing the clock frequency and
    corresponding operating voltage of the CPU can yield quadratic decrease in
    energy use.  However, performance reduction can only be beneficial if it is
    done transparently, without causing the software to miss its deadlines.  In
    this paper, we describe the implementation and performance-setting
    algorithms used in Vertigo, our power management extensions for Linux.
    Vertigo makes its decisions automatically, without any application-specific
    involvement.  We describe how a hierarchy of performance-setting
    algorithms, each specialized for different workload characteristics, can be
    used for controlling the processor's performance.  The algorithms operate
    independently from one another and can be dynamically configured.  As a
    basis for comparison with conventional algorithms, we contrast measurements
    made on a Transmeta Crusoe-based computer using its built-in LongRun power
    manager with Vertigo running on the same system.  We show that unlike
    conventional interval-based algorithms like LongRun, Vertigo is successful
    at focusing in on a small range of performance levels that are sufficient
    to meet an application's deadlines.  When playing MPEG movies, this
    behavior translates into a 11%--35% reduction of mean performance level
    over LongRun, without any negative impact on the framerate.  The
    performance reduction can in turn yield significant power savings.", 
  location     = "https://doi.org/10.1145/844128.844139", 
  location     = "https://www.usenix.org/conference/osdi-02/vertigo-automatic-performance-setting-linux"
}

@Article{ptossfs,
  author       = "Juan Navarro and Sitararn Iyer and Peter Druschel and Alan Cox",
  title        = "Practical, transparent operating system support for superpages",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "89--104",
  month        = "Winter",
  keywords     = "tlb coverage, superpages, fragmentation, storage
    reservations, contiguous storage",
  abstract     = "Most general-purpose processors provide support for memory
    pages of large sizes, called superpages.  Superpages enable each entry in
    the translation lookaside buffer (TLB) to map a large physical memory
    region into a virtual address space.  This dramatically increases TLB
    coverage, reduces TLB misses, and promises performance improvements for
    many applications.  However, supporting superpages poses several challenges
    to the operating system, in terms of superpage allocation and promotion
    tradeoffs, fragmentation control, etc.  We analyze these issues, and
    propose the design of an effective superpage management system.  We
    implement it in FreeBSD on the Alpha CPU, and evaluate it on real workloads
    and benchmarks.  We obtain substantial performance benefits, often
    exceeding 30%; these benefits are sustained even under stressful workload
    scenarios.", 
  location     = "https://doi.org/10.1145/844128.844138", 
  location     = "https://www.usenix.org/conference/osdi-02/practical-transparent-operating-system-support-superpages"
}

@Article{capatmcrc,
  author       = "Madanlal Musuvathi and David Y.~W. Park and Andy Chou and Dawson~R. Engler and David~L. Dill",
  title        = "{CMC}: a pragmatic approach to model checking real code",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "75--88",
  month        = "Winter",
  keywords     = "model checking, abstraction, annotations, model building,
    derivable models, aodv",
  abstract     = "Many system errors do not emerge unless some intricate
    sequence of events occurs.  In practice, this means that most systems have
    errors that only trigger after days or weeks of execution.  Model checking
    [4] is an effective way to find such subtle errors.  It takes a simplified
    description of the code and exhaustively tests it on all inputs, using
    techniques to explore vast state spaces efficiently.  Unfortunately, while
    model checking systems code would be wonderful, it is almost never done in
    practice: building models is just too hard.  It can take significantly more
    time to write a model than it did to write the code.  Furthermore, by
    checking an abstraction of the code rather than the code itself, it is easy
    to miss errors.The paper's first contribution is a new model checker, CMC,
    which checks C and C++ implementations directly, eliminating the need for a
    separate abstract description of the system behavior.  This has two major
    advantages: it reduces the effort to use model checking, and it reduces
    missed errors as well as time-wasting false error reports resulting from
    inconsistencies between the abstract description and the actual
    implementation.  In addition, changes in the implementation can be checked
    immediately without updating a high-level description.The paper's second
    contribution is demonstrating that CMC works well on real code by applying
    it to three implementations of the Ad-hoc On-demand Distance Vector (AODV)
    networking protocol [7].  We found 34 distinct errors (roughly one bug per
    328 lines of code), including a bug in the AODV specification itself.
    Given our experience building systems, it appears that the approach will
    work well in other contexts, and especially well for other networking
    protocols.", 
  location     = "https://doi.org/10.1145/844128.844136",
  location     = "https://web.stanford.edu/~engler/osdi2002.ps"
}

@Article{umctddf,
  author       = "Sanjeev Kumar and Kai Li",
  title        = "Using model checking to debug device firmware",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "61--74",
  month        = "Winter",
  keywords     = "domain specific languages, esp, device drivers, state-machine
    programming, spin, derived models",
  abstract     = "Device firmware is a piece of concurrent software that achieves high performance at the cost of software complexity.  They contain subtle race conditions that make them difficult to debug using traditional debugging techniques.  The problem is further compounded by the lack of debugging support on the devices.  This is a serious problem because the device firmware is trusted by the operating system.Model checkers are designed to systematically verify properties of concurrent systems.  Therefore, model checking is a promising approach to debugging device firmware.  However, model checking involves an exponential search.  Consequently, the models have to be small to allow effective model checking.This paper describes the abstraction techniques used by the ESP compiler to extract abstract models from device firmware written in ESP.  The abstract models are small because they discard some of the details in the firmware that is irrelevant to the particular property being verified.  The programmer is required to specify the abstractions to be performed.  The ESP compiler uses the abstraction specification to extract models conservatively.  Therefore, every bug in the original program will be present in the extracted model.This paper also presents our experience with using Spin model checker to develop and debug VMMC firmware for the Myrinet network interfaces.  An earlier version of the ESP compiler yielded models that were too large to check for system-wide properties like absence of deadlocks.  The new version of the compiler generated abstract models that were used to identify several subtle bugs in the firmware.  So far, we have not encountered any bugs that were not caught by Spin.",
  location     = "https://doi.org/10.1145/844128.844135",
  location     = "https://www.usenix.org/conference/osdi-02/using-model-checking-debug-device-firmware"
}

@Article{dpuaattbdrs2002,
  author       = "Xiaohu Qie and Ruoming Pang and Larry Peterson",
  title        = "Defensive programming: using an annotation toolkit to build {DoS}-resistant software",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "45--60",
  month        = "Winter",
  keywords     = "denial-of-service attacks, resource hogging, tempo attacks,
    annotations, resource management, sensors and actuators",
  abstract     = "This paper describes a toolkit to help improve the robustness
    of code against DoS attacks.  We observe that when developing software,
    programmers primarily focus on functionality.  Protecting code from attacks
    is often considered the responsibility of the OS, firewalls and intrusion
    detection systems.  As a result, many DoS vulnerabilities are not
    discovered until the system is attacked and the damage is done.  Instead of
    reacting to attacks after the fact, this paper argues that a better
    solution is to make software defensive by systematically injecting
    protection mechanisms into the code itself.  Our toolkit provides an API
    that programmers use to annotate their code.  At runtime, these annotations
    serve as both sensors and actuators: watching for resource abuse and taking
    the appropriate action should abuse be detected.  This paper presents the
    design and implementation of the toolkit, as well as evaluation of its
    effectiveness with three widely-deployed network services.", 
  location     = "https://doi.org/10.1145/844128.844134",
  location     = "https://www.cs.princeton.edu/research/techreps/TR-658-02"
}

@Article{iarwptpfs,
  author       = "Athicha Muthitacharoen and Robert Morris and Thomer~M. Gil and Benjie Chen",
  title        = "Ivy: a read/write peer-to-peer file system",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "31--44",
  month        = "Winter",
  keywords     = "logging, block chain, reconstruction, distributed hash
    tables, peer-to-peer file systems",
  abstract     = "Ivy is a multi-user read/write peer-to-peer file system.  Ivy
    has no centralized or dedicated components, and it provides useful
    integrity properties without requiring users to fully trust either the
    underlying peer-to-peer storage system or the other users of the file
    system.An Ivy file system consists solely of a set of logs, one log per
    participant.  Ivy stores its logs in the DHash distributed hash table.
    Each participant finds data by consuiting all logs, but performs
    modifications by appending only to its own log.  This arrangement allows
    Ivy to maintain meta-data consistency without locking.  Ivy users can
    choose which other logs to trust, an appropriate arrangement in a semi-open
    peer-to-peer system.Ivy presents applications with a conventional file
    system interface.  When the underlying network is fully connected, Ivy
    provides NFS-like semantics, such as close-to-open consistency.  Ivy
    detects conflicting modifications made during a partition, and provides
    relevant version information to application-specific conflict resolvers.
    Performance measurements on a wide-area network show that Ivy is two to
    three times slower than NFS.", 
  location     = "https://doi.org/10.1145/844128.844132",
  location     = "https://pdos.csail.mit.edu/archive/ivy/"
}

@Article{ffaarsfaite,
  author       = "Atul Adya and William~J. Bolosky and Miguel Castro and Gerald Cermak and Ronnie Chaiken and John~R. Douceur and Jon Howell and Jacob~R. Lorch and Marvin Theimer and Roger~P. Wattenhofer",
  title        = "Farsite: federated, available, and reliable storage for an incompletely trusted environment",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "1--14",
  month        = "Winter",
  keywords     = "peer-to-peer systems, distributed file systems, security,
    consistency, scalability",
  abstract     = "Farsite is a secure, scalable file system that logically
    functions as a centralized file server but is physically distributed among
    a set of untrusted computers.  Farsite provides file availability and
    reliability through randomized replicated storage; it ensures the secrecy
    of file contents with cryptographic techniques; it maintains the integrity
    of file and directory data with a Byzantine-fault-tolerant protocol; it is
    designed to be scalable by using a distributed hint mechanism and
    delegation certificates for pathname translations; and it achieves good
    performance by locally caching file data, lazily propagating file updates,
    and varying the duration and granularity of content leases.  We report on
    the design of Farsite and the lessons we have learned by implementing much
    of that design.", 
  location     = "https://doi.org/10.1145/844128.844130",
  location     = "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/farsite-osdi2002.pdf"
}

@Article{taritpwafs,
  author       = "Yasushi Saito and Christos Karamanolis and Magnus Karlsson and Mallik Mahalingam",
  title        = "Taming aggressive replication in the {Pangaea} wide-area file system",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "15--30",
  month        = "Winter",
  keywords     = "replication, nfs, update propagation, failure recovery, hinting",
  abstract     = "Pangaea is a wide-area file system that supports data sharing
    among a community of widely distributed users.  It is built on a
    symmetrically decentralized infrastructure that consists of commodity
    computers provided by the end users.  Computers act autonomously to serve
    data to their local users.  When possible, they exchange data with nearby
    peers to improve the system's overall performance, availability, and
    network economy.  This approach is realized by aggressively creating a
    replica of a file whenever and wherever it is accessed.This paper presents
    the design, implementation, and evaluation of the Pangaea file system.
    Pangaea offers efficient, randomized algorithms to manage highly dynamic
    and potentially large groups of file replicas.  It applies optimistic
    consistency semantics to replica contents, but it also offers stronger
    guarantees when required by the users.  The evaluation demonstrates that
    Pangaea outperforms existing distributed file systems in large
    heterogeneous environments, typical of the Internet and of large corporate
    intranets.", 
  location     = "https://doi.org/10.1145/844128.844131",
  location     = "https://www.usenix.org/conference/osdi-02/taming-aggressive-replication-pangaea-wide-area-file-system"
}

@Article{bbrwbp,
  author       = "Miguel Castro and Manuel Costa and Jean-Philippe Martin",
  title        = "Better Bug Reporting With Better Privacy",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "319--328",
  month        = mar,
  keywords     = "error reports, privacy, symbolic execution, constraint
    solving, path conditions, minimal conditions, satisfiability modulo
    theories", 
  abstract     = "Software vendors collect bug reports from customers to
    improve the quality of their software.  These reports should include the
    inputs that make the software fail, to enable vendors to reproduce the bug.
    However, vendors rarely include these inputs in reports because they may
    contain private user data.  We describe a solution to this problem that
    provides software vendors with new input values that satisfy the conditions
    required to make the software follow the same execution path until it
    fails, but are otherwise unrelated with the original inputs.  These new
    inputs allow vendors to reproduce the bug while revealing less private
    information than existing approaches.  Additionally, we provide a mechanism
    to measure the amount of information revealed in an error report.  This
    mechanism allows users to perform informed decisions on whether or not to
    submit reports.  We implemented a prototype of our solution and evaluated
    it with real errors in real programs.  The results show that we can produce
    error reports that allow software vendors to reproduce bugs while revealing
    almost no private information.", 
  location     = "https://doi.org/10.1145/1353535.1346322", 
  location     = "https://www.microsoft.com/en-us/research/publication/better-bug-reporting-with-better-privacy/"
}

@Article{pscoch,
  author       = "Edmund~B. Nightingale and Daniel Peek and Peter~M. Chen and Jason Flinn",
  title        = "Parallelizing Security Checks on Commodity Hardware",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "308--318",
  month        = mar,
  keywords     = "operating systems, security, performance, parallel,
    speculative execution, speck, replay, dynamic binary rewriting, storage
    analysis, system-call analysis, taint analysis, logging",
  abstract     = "Speck (Speculative Parallel Check) is a system that
    accelerates powerful security checks on commodity hardware by executing
    them in parallel on multiple cores.  Speck provides an infrastructure that
    allows sequential invocations of a particular security check to run in
    parallel without sacrificing the safety of the system.  Speck creates
    parallelism in two ways.  First, Speck decouples a security check from an
    application by continuing the application, using speculative execution,
    while the security check executes in parallel on another core.  Second,
    Speck creates parallelism between sequential invocations of a security
    check by running later checks in parallel with earlier ones.  Speck
    provides a process-level replay system to deterministically and efficiently
    synchronize state between a security check and the original process.We use
    Speck to parallelize three security checks: sensitive data analysis,
    on-access virus scanning, and taint propagation.  Running on a 4-core and
    an 8-core computer, Speck improves performance 4x and 7.5x for the
    sensitive data analysis check, 3.3x and 2.8x for theon-access virus
    scanning check, and 1.6x and 2x for the taint propagation check.", 
  location     = "https://doi.org/10.1145/1353535.1346321", 
  location     = "https://www.microsoft.com/en-us/research/publication/parallelizing-security-checks-on-commodity-hardware/"
}

@Article{spgpmpus,
  author       = "Jayanth Gummaraju and Joel Coburn and Yoshio Turner and Mendel Rosenblum",
  title        = "Streamware: programming general-purpose multicore processors using streams",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "297--307",
  month        = mar,
  keywords     = "streams, general-purpose multicore processors, stream
    programming, runtime systems, intermediate representations, hardware
    mapping, cache aware dynamic scheduling",
  abstract     = "Recently, the number of cores on general-purpose processors
    has been increasing rapidly.  Using conventional programming models, it is
    challenging to effectively exploit these cores for maximal performance.  An
    interesting alternative candidate for programming multiple cores is the
    stream programming model, which provides a framework for writing programs
    in a sequential-style while greatly simplifying the task of automatic
    parallelization.  It has been shown that not only traditional media/image
    applications but also more general-purpose data-intensive applications can
    be expressed in the stream programming style.In this paper, we investigate
    the potential to use the stream programming model to efficiently utilize
    commodity multicore general-purpose processors (e.g., Intel/AMD).  Although
    several stream languages and stream compilers have recently been developed,
    they typically target special-purpose stream processors.  In contrast, we
    propose a flexible software system, Streamware, which automatically maps
    stream programs onto a wide variety of general-purpose multicore processor
    configurations.  We leverage existing compilation framework for stream
    processors and design a runtime environment which takes as input the output
    of these stream compilers in the form of machine-independent stream virtual
    machine code.  The runtime environment assigns work to processor cores
    considering processor/cache configurations and adapts to workload
    variations.  We evaluate this approach for a few general-purpose scientific
    applications on real hardware and a cycle-level simulator set-up to
    showcase scaling and contention issues.  The results show that the stream
    programming model is a good choice for efficiently exploiting modern and
    future multicore CPUs for an important class of applications.", 
  location     = "https://doi.org/10.1145/1353535.1346319",
  location     = "http://mesl.ucsd.edu/joel/papers/streamware_asplos08.pdf"
}

@Article{mapmfhmcs,
  author       = "Michael~D. Linderman and Jamison~D. Collins and Hong Wang and Teresa~H. Meng",
  title        = "Merge: a programming model for heterogeneous multi-core systems",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "287--296",
  month        = mar,
  keywords     = "heterogeneous multi-core, gpgpu, predicate dispatch,
    map-reduce, libraries",
  abstract     = "In this paper we propose the Merge framework, a general
    purpose programming model for heterogeneous multi-core systems.  The Merge
    framework replaces current ad hoc approaches to parallel programming on
    heterogeneous platforms with a rigorous, library-based methodology that can
    automatically distribute computation across heterogeneous cores to achieve
    increased energy and performance efficiency.  The Merge framework provides
    (1) a predicate dispatch-based library system for managing and invoking
    function variants for multiple architectures; (2) a high-level,
    library-oriented parallel language based on map-reduce; and (3) a compiler
    and runtime which implement the map-reduce language pattern by dynamically
    selecting the best available function implementations for a given input and
    machine configuration.  Using a generic sequencer architecture interface
    for heterogeneous accelerators, the Merge framework can integrate function
    variants for specialized accelerators, offering the potential for
    to-the-metal performance for a wide range of heterogeneous architectures,
    all transparent to the user.  The Merge framework has been prototyped on a
    heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core
    32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous
    32-way Unisys SMP system with Intel Xeon processors.  We implemented a set
    of benchmarks using the Merge framework and enhanced the library with X3000
    specific implementations, achieving speedups of 3.6x -- 8.5x using the
    X3000 and 5.2x -- 22x using the 32-way system relative to the straight C
    reference implementation on a single IA32 core.", 
  location     = "https://doi.org/10.1145/1353535.1346318",
  location     = "https://cs.uwaterloo.ca/~brecht/courses/epfl/Possible-Readings/programming-models/merge-programming-model-hetero-multicore-asplos-2008.pdf"
}

@Article{fdtpeahpeomtwoc,
  author       = "M.~Aater Suleman and Moinuddin~K. Qureshi and Yale~N. Patt",
  title        = "Feedback-driven threading: power-efficient and high-performance execution of multi-threaded workloads on {CMP}s",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "277--286",
  month        = mar,
  keywords     = "multi-threading, cmp, data synchronization overhead,
    off-chip bus bandwidth, adaptive threading, synchronization-aware
    threading, bandwidth-aware threading",
  abstract     = "Extracting high-performance from the emerging Chip
    Multiprocessors (CMPs) requires that the application be divided into
    multiple threads.  Each thread executes on a separate core thereby
    increasing concurrency and improving performance.  As the number of cores
    on a CMP continues to increase, the performance of some multi-threaded
    applications will benefit from the increased number of threads, whereas,
    the performance of other multi-threaded applications will become limited by
    data-synchronization and off-chip bandwidth.  For applications that get
    limited by data-synchronization, increasing the number of threads
    significantly degrades performance and increases on-chip power.  Similarly,
    for applications that get limited by off-chip bandwidth, increasing the
    number of threads increases on-chip power without providing any performance
    improvement.  Furthermore, whether an application gets limited by
    data-synchronization, or bandwidth, or neither depends not only on the
    application but also on the input set and the machine configuration.
    Therefore, controlling the number of threads based on the run-time behavior
    of the application can significantly improve performance and reduce
    power.This paper proposes Feedback-Driven Threading (FDT), a framework to
    dynamically control the number of threads using run-time information.  FDT
    can be used to implement Synchronization-Aware Threading (SAT), which
    predicts the optimal number of threads depending on the amount of
    data-synchronization.  Our evaluation shows that SAT can reduce both
    execution time and power by up to 66% and 78% respectively.  Similarly, FDT
    can be used to implement Bandwidth-Aware Threading (BAT), which predicts
    the minimum number of threads required to saturate the off-chip bus.  Our
    evaluation shows that BAT reduces on-chip power by up to 78%.  When SAT and
    BAT are combined, the average execution time reduces by 17% and power
    reduces by 59%.  The proposed techniques leverage existing performance
    counters and require minimal support from the threading library.",
  location     = "https://doi.org/10.1145/1353535.1346317",
  location     = "https://researcher.watson.ibm.com/files/us-moinqureshi/papers-fdt.pdf"
}

@Article{utpohetsaifrsd,
  author       = "Man-Lap Li and Pradeep Ramachandran and Swarup Kumar Sahoo and Sarita~V. Adve and Vikram~S. Adve and Yuanyuan Zhou",
  title        = "Understanding the propagation of hard errors to software and implications for resilient system design",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "265--276",
  month        = mar,
  keywords     = "error detection, architecture, permanent fault, fault
    injection, fault tolerance",
  abstract     = "With continued CMOS scaling, future shipped hardware will be
    increasingly vulnerable to in-the-field faults.  To be broadly deployable,
    the hardware reliability solution must incur low overheads, precluding use
    of expensive redundancy.  We explore a cooperative hardware-software
    solution that watches for anomalous software behavior to indicate the
    presence of hardware faults.  Fundamental to such a solution is a
    characterization of how hardware faults indifferent microarchitectural
    structures of a modern processor propagate through the application and
    OS.This paper aims to provide such a characterization, resulting in
    identifying low-cost detection methods and providing guidelines for
    implementation of the recovery and diagnosis components of such a
    reliability solution.  We focus on hard faults because they are
    increasingly important and have different system implications than the much
    studied transients.  We achieve our goals through fault injection
    experiments with a microarchitecture-level full system timing simulator.
    Our main results are: (1) we are able to detect 95% of the unmasked faults
    in 7 out of 8 studied microarchitectural structures with simple detectors
    that incur zero to little hardware overhead; (2) over 86% of these
    detections are within latencies that existing hardware checkpointing
    schemes can handle, while others require software checkpointing; and (3) a
    large fraction of the detected faults corrupt OS state, but almost all of
    these are detected with latencies short enough to use hardware
    checkpointing, thereby enabling OS recovery in virtually all such cases.",
  location     = "https://doi.org/10.1145/1353535.1346315",
  location     = "http://rsim.cs.illinois.edu/Pubs/08ASPLOS.pdf"
}

@Article{atifims,
  author       = "Philip~M. Wells and Koushik Chakraborty and Gurindar~S. Sohi",
  title        = "Adapting to intermittent faults in multicore systems",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "255--264",
  month        = mar,
  keywords     = "intermittent faults, over-committing resources, slack",
  abstract     = "Future multicore processors will be more susceptible to a
    variety of hardware failures.  In particular, intermittent faults, caused
    in part by manufacturing, thermal, and voltage variations, can cause bursts
    of frequent faults that last from several cycles to several seconds or
    more.  Due to practical limitations of circuit techniques, cost-effective
    reliability will likely require the ability to temporarily suspend
    execution on a core during periods of intermittent faults.We investigate
    three of the most obvious techniques for adapting to the dynamically
    changing resource availability caused by intermittent faults, and
    demonstrate their different system-level implications.  We show that system
    software reconfiguration has very high overhead, that temporarily pausing
    execution on a faulty core can lead to cascading livelock, and that using
    spare cores has high fault-free cost.  To remedy these and other drawbacks
    of the three baseline techniques, we propose using a thin hardware/firmware
    layer to manage an overcommitted system -- one where the OS is configured
    to use more virtual processors than the number of currently available
    physical cores.  We show that this proposed technique can gracefully
    degrade performance during intermittent faults of various duration with low
    overhead, without involving system software or requiring spare cores.", 
  location     = "https://doi.org/10.1145/1353535.1346314", 
  location     = "https://ftp.cs.wisc.edu/pub/sohi/papers/2008/asplos2008-intermittent.pdf"
}

@Article{xaeocfsp,
  author       = "Russ Cox and Tom Bergan and Austin~T. Clements and Frans Kaashoek and Eddie Kohler",
  title        = "Xoc, an extension-oriented compiler for systems programming",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "244--254",
  month        = mar,
  keywords     = "extension-oriented compilers, lazyness, ambiguity, syntax
    patterns, meta-programming, glr parsers",
  abstract     = "Today's system programmers go to great lengths to extend the
    languages in which they program.  For instance, system-specific compilers
    find errors in Linux and other systems, and add support for specialized
    control flow to Qt and event-based programs.  These compilers are difficult
    to build and cannot always understand each other's language changes.
    However, they can greatly improve code understandability and correctness,
    advantages that should be accessible to all programmers.We describe an
    extension-oriented compiler for C called xoc.  An extension-oriented
    compiler, unlike a conventional extensible compiler, implements new
    features via many small extensions that are loaded together as needed.  Xoc
    gives extension writers full control over program syntax and semantics
    while hiding many compiler internals.  Xoc programmers concisely define
    powerful compiler extensions that, by construction, can be combined; even
    some parts of the base compiler, such as GNU C compatibility, are
    structured as extensions.Xoc is based on two key interfaces.  Syntax
    patterns allow extension writers to manipulate language fragments using
    concrete syntax.  Lazy computation of attributes allows extension writers
    to use the results of analyses by other extensions or the core without
    needing to worry about pass scheduling.Extensions built using xoc include
    xsparse, a 345-line extension mimicing Sparse, Linux's C front end, and
    xlambda, a 170-line extension adding function expressions to C.  An
    evaluation of xoc using these and 13 other extensions shows xoc extensions
    are typically more concise than equivalent extensions written for
    conventional extensible compilers and that extensions can be composed.",
  location     = "https://doi.org/10.1145/1353535.1346312",
  location     = "https://pdos.csail.mit.edu/papers/xoc:asplos08.pdf"
}

@Article{opbfdp,
  author       = "Milind Kulkarni and Keshav Pingali and Ganesh Ramanarayanan and Bruce Walter and Kavita Bala and L.~Paul Chew",
  title        = "Optimistic parallelism benefits from data partitioning",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "233--243",
  month        = mar,
  keywords     = "optimistic parallelism, irregular programs, data
    partitioning, locality, lock coarsening, over-decomposition, galois,
    abstract domains",
  abstract     = "Recent studies of irregular applications such as
    finite-element mesh generators and data-clustering codes have shown that
    these applications have a generalized data parallelism arising from the use
    of iterative algorithms that perform computations on elements of worklists.
    In some irregular applications, the computations on different elements are
    independent.  In other applications, there may be complex patterns of
    dependences between these computations.The Galois system was designed to
    exploit this kind of irregular data parallelism on multicore processors.
    Its main features are (i) two kinds of set iterators for expressing
    worklist-based data parallelism, and (ii) a runtime system that performs
    optimistic parallelization of these iterators, detecting conflicts and
    rolling back computations as needed.  Detection of conflicts and rolling
    back iterations requires information from class implementors.In this paper,
    we introduce mechanisms to improve the execution efficiency of Galois
    programs: data partitioning, data-centric work assignment, lock coarsening,
    and over-decomposition.  These mechanisms can be used to exploit locality
    of reference, reduce mis-speculation, and lower synchronization overhead.
    We also argue that the design of the Galois system permits these mechanisms
    to be used with relatively little modification to the user code.  Finally,
    we present experimental results that demonstrate the utility of these
    mechanisms.", 
  location     = "https://doi.org/10.1145/1353535.1346311", 
  location     = "https://www.cs.cornell.edu/~kb/publications/asplos08.pdf"
}

@Article{cofgmtis,
  author       = "Guilherme Ottoni and David~I. August",
  title        = "Communication optimizations for global multi-threaded instruction scheduling",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "222--232",
  month        = mar,
  keywords     = "multi-threading, instruction scheduling, communication,
  synchronization, daa-flow analysis, graph min-cut, communication
  optimizationc",
  abstract     = "The recent shift in the industry towards chip multiprocessor
    (CMP) designs has brought the need for multi-threaded applications to
    mainstream computing.  As observed in several limit studies, most of the
    parallelization opportunities require looking for parallelism beyond local
    regions of code.  To exploit these opportunities, especially for sequential
    applications, researchers have recently proposed global multi-threaded
    instruction scheduling techniques, including DSWP and GREMIO.  These
    techniques simultaneously schedule instructions from large regions of code,
    such as arbitrary loop nests or whole procedures, and have been shown to be
    effective at extracting threads for many applications.  A key enabler of
    these global instruction scheduling techniques is the Multi-Threaded Code
    Generation (MTCG) algorithm proposed in [16], which generates
    multi-threaded code for any partition of the instructions into threads.
    This algorithm inserts communication and synchronization instructions to
    satisfy all inter-thread dependences.  We present a general compiler
    framework, COCO, to optimize the communication and synchronization
    instructions inserted by the MTCG algorithm.  This framework, based on
    thread-aware data-flow analyses and graph min-cut algorithms, appropriately
    models andoptimizes all kinds of inter-thread dependences, including
    register, memory, and control dependences.  Our experiments, using a fully
    automatic compiler implementation of these techniques, demonstrate
    significant reductions (about 30% on average) in the number of dynamic
    communication instructions in code parallelized with DSWP and GREMIO.  This
    reduction in communication translates to performance gains of up to 40%.", 
  location     = "https://doi.org/10.1145/1353535.1346310", 
  location     = "http://liberty.princeton.edu/Publications/asplos08_mtcgco.pdf"
}

@Article{uavfswdft,
  author       = "Shashidhar Mysore and Bita Mazloom and Banit Agrawal and Timothy Sherwood",
  title        = "Understanding and visualizing full systems with data flow tomography",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "211--221",
  month        = mar,
  keywords     = "data-flow tracking, tomographic analysis, data tracing,
    visualization, data tagging", 
  abstract     = "It is common for modern systems to be composed of many
    interacting services running across multiple machines in such a way that
    most developers do not understand the whole system.  By layering
    abstractions, developers compose systems of extraordinary complexity with
    relative ease.  However, many software properties, especially those that
    cut across abstraction layers, become difficult to understand in such
    compositions.  The communication patterns involved, the privacy of critical
    data, and the provenance of information, can be difficult to find and
    understand, even with access to all of the source code.  The goal of Data
    Flow Tomography is to use the inherent information flow of such systems to
    help visualize the interactions between complex and interwoven components
    across multiple layers of abstraction.  In the same way that the injection
    of short-lived radioactive isotopes help doctors trace problems in the
    cardiovascular system, the use of 'data tagging' can help developers slice
    through the extraneous layers of software and pin-point those portions of
    the system interacting with the data of interest.  To demonstrate the
    feasibility of this approach we have developed a prototype system in which
    tags are tracked both through the machine and in between machines over the
    network, and from which novel visualizations of the whole system can be
    derived.  We describe the system-level challenges in creating a working
    system tomography tool and we qualitatively evaluate our system by
    examining several example real world scenarios.", 
  location     = "https://doi.org/10.1145/1353535.1346308", 
  location     = "http://www.cs.ucsb.edu/~sherwood/pubs/ASPLOS-08-systemtomography.pdf"
}

@Article{dpaabtcm,
  author       = "Luk Van Ertvelde and Lieven Eeckhout",
  title        = "Dispersing proprietary applications as benchmarks through code mutation",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "201--210",
  month        = mar,
  keywords     = "benchmark generation, code mutation, black-box analysis,
    benchmarking, execution profiles, binary rewriting, mutation efficacy",
  abstract     = "Industry vendors hesitate to disseminate proprietary applications to academia and third party vendors.  By consequence, the benchmarking process is typically driven by standardized, open-source benchmarks which may be very different from and likely not representative of the real-life applications of interest.This paper proposes code mutation, a novel technique that mutates a proprietary application to complicate reverse engineering so that it can be distributed as a benchmark.  The benchmark mutant then serves as a proxy for the proprietary application.  The key idea in the proposed code mutation approach is to preserve the proprietary application's dynamic memory access and/or control flow behavior in the benchmark mutant while mutating the rest of the application code.  To this end, we compute program slices for memory access operations and/or control flow operationstrimmed through constant value and branch profiles; and subsequently mutate the instructions not appearing in these slices through binary rewriting.Our experimental results using SPEC CPU2000 and MiBench benchmarks show that code mutation is a promising technique that mutates up to 90% of the static binary, up to 50% of the dynamically executed instructions, and up to 35% of the at run time exposed inter-operation data dependencies.  The performance characteristics of the mutant are very similar to those of the proprietary application across a wide range of microarchitectures and hardware implementations.",
  location     = "https://doi.org/10.1145/1353535.1346307"
}

@Article{hcdotfrs,
  author       = "Kai Shen and Ming Zhong and Sandhya Dwarkadas and Chuanpeng Li and Christopher Stewart and Xiao Zhang",
  title        = "Hardware counter driven on-the-fly request signatures",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "189--200",
  month        = mar,
  keywords     = "operating system adaptation, hardware counters, server
    systems, request classification, anomaly detection",
  abstract     = "Today's processors provide a rich source of statistical
    information on application execution through hardware counters.  In this
    paper, we explore the utilization of these statistics as request
    signaturesin server applications for identifying requests and inferring
    high-level request properties (e.g., CPU and I/O resource needs).  Our key
    finding is that effective request signatures may be constructed using a
    small amount of hardware statistics while the request is still in an early
    stage of its execution.  Such on-the-fly request identification and
    property inference allow guided operating system adaptation at request
    granularity (e.g., resource-aware request scheduling and on-the-fly request
    classification).  We address the challenges of selecting hardware counter
    metrics for signature construction and providing necessary operating system
    support for per-request statistics management.  Our implementation in the
    Linux 2.6.10 kernel suggests that our approach requires low overhead
    suitable for runtime deployment.  Our on-the-fly request resource
    consumption inference (averaging 7%, 3%, 20%, and 41% prediction errors for
    four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven
    index search, respectively) is much more accurate than the online
    running-average based prediction (73-82% errors).  Its use for
    resource-aware request scheduling results in a 15-70% response time
    reduction for three CPU-bound applications.  Its use for on-the-fly request
    classification and anomaly detection exhibits high accuracy for the TPC-H
    workload with synthetically generated anomalous requests following a
    typical SQL-injection attack pattern.", 
  location     = "https://doi.org/10.1145/1353535.1346306",
  location     = "https://www.cs.rochester.edu/~kshen/papers/asplos2008.pdf"
}

@Article{titfocoossfpd,
  author       = "Yaron Weinsberg and Danny Dolev and Tal Anker and Muli Ben-Yehuda and Pete Wyckoff",
  title        = "Tapping into the fountain of {CPU}s: on operating system support for programmable devices",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "179--188",
  month        = mar,
  keywords     = "offloading, operating systems, programming models, peripheral
    processors",
  abstract     = "The constant race for faster and more powerful CPUs is
    drawing to a close.  No longer is it feasible to significantly increase the
    speed of the CPU without paying a crushing penalty in power consumption and
    production costs.  Instead of increasing single thread performance, the
    industry is turning to multiple CPU threads or cores (such as SMT and CMP)
    and heterogeneous CPU architectures (such as the Cell Broadband Engine).
    While this is a step in the right direction, in every modern PC there is a
    wealth of untapped compute resources.  The NIC has a CPU; the disk
    controller is programmable; some high-end graphics adaptersare already more
    powerful than host CPUs.  Some of these CPUs can perform some functions
    more efficiently than the host CPUs.  Our operating systems and programming
    abstractions should be expanded to let applications tap into these
    computational resources and make the best use of them.Therefore, we propose
    the HYDRA framework, which lets application developers use the combined
    power of every compute resource in a coherent way.  HYDRA is a programming
    model and a runtime support layer which enables utilization of host
    processors as well as various programmable peripheral devices' processors.
    We present the frameworkand its application for a demonstrative use-case,
    as well as provide a thorough evaluation of its capabilities.  Using HYDRA
    we were able to cut down the development cost of a system that uses
    multiple heterogenous compute resources significantly.", 
  location     = "https://doi.org/10.1145/1353535.1346304", 
  location     = "http://noodle.cs.huji.ac.il/~dolev/pubs/aspolos08-yaron--selfcopy.pdf"
}

@Article{tdaiom,
  author       = "Vinod Ganapathy and Matthew~J. Renzelmann and Arini Balakrishnan and Michael~M. Swift and Somesh Jha",
  title        = "The design and implementation of microdrivers",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "168--178",
  month        = mar,
  keywords     = "device drivers, reliability, program partitioning,
    device-driver development, user-kernel split, semantic rpc, driver slicing",
  abstract     = "Device drivers commonly execute in the kernel to achieve high 
    performance and easy access to kernel services.  However, this comes at the
    price of decreased reliability and increased programming difficulty.
    Driver programmers are unable to use user-mode development tools and must
    instead use cumbersome kernel tools.  Faults in kernel drivers can cause
    the entire operating system to crash.  User-mode drivers have long been
    seen as a solution to this problem, but suffer from either poor performance
    or new interfaces that require a rewrite of existing drivers.This paper
    introduces the Microdrivers architecture that achieves high performance and
    compatibility by leaving critical path code in the kernel and moving the
    rest of the driver code to a user-mode process.  This allows data-handling
    operations critical to I/O performance to run at full speed, while
    management operations such as initialization and configuration run at
    reduced speed in user-level.  To achieve compatibility, we present
    DriverSlicer, a tool that splits existing kernel drivers into a
    kernel-level component and a user-level component using a small number of
    programmer annotations.  Experiments show that as much as 65% of driver
    code can be removed from the kernel without affecting common-case
    performance, and that only 1-6 percent of the code requires annotations.", 
  location     = "https://doi.org/10.1145/1353535.1346303", 
  location     = "http://pages.cs.wisc.edu/~swift/papers/microdrivers-asplos08.pdf"
}

@Article{pv2008,
  author       = "Ioana Burcea and Stephen Somogyi and Andreas Moshovos and Babak Falsafi",
  title        = "Predictor virtualization",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "157--167",
  month        = mar,
  keywords     = "predictor virtualization, caches, memory hierarchy, metadata",
  abstract     = "Many hardware optimizations rely on collecting information
    about program behavior at runtime.  This information is stored in lookup
    tables.  To be accurate and effective, these optimizations usually require
    large dedicated on-chip tables.  Although technology advances offer an
    increased amount of on-chip resources, these resources are allocated to
    increase the size of on-chip conventional cache hierarchies.This work
    proposes Predictor Virtualization, a technique that uses the existing
    memory hierarchy to emulate large predictor tables.  We demonstrate the
    benefits of this technique by virtualizing a state-of-the-art data
    prefetcher.  Full-system, cycle-accurate simulations demonstrate that the
    virtualized prefetcher preserves the performance benefits of the original
    design, while reducing the on-chip storage dedicated to the predictor table
    from 60KB down to less than one kilobyte.", 
  location     = "https://doi.org/10.1145/1353535.1346301"
}

@Article{ssehsfcaao,
  author       = "James Tuck and Wonsun Ahn and Luis Ceze and Josep Torrellas",
  title        = "{SoftSig}: software-exposed hardware signatures for code analysis and optimization",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "145--156",
  month        = mar,
  keywords     = "memory disambiguation, multi-core architectures, runtime
    optimization, memoization, bloom filters, function signatures, signature caches",
  abstract     = "Many code analysis techniques for optimization, debugging, or 
    parallelization need to perform runtime disambiguation of sets of
    addresses.  Such operations can be supported efficiently and with low
    complexity with hardware signatures.To enable flexible use of signatures,
    this paper proposes to expose a Signature Register File to the software
    through a rich ISA.  The software has great flexibility to decide, for each
    signature,which addresses to collect and which addresses to disambiguate
    against.  We call this architecture SoftSig.  In addition, as an example of
    SoftSig use, we show how to detect redundant function calls efficiently and
    eliminate them dynamically.  We call this algorithm MemoiSE.  On average
    for five popular applications, MemoiSE reduces the number of dynamic
    instructions by 9.3%, thereby reducing the execution time of the
    applications by 9%.", 
  location     = "https://doi.org/10.1145/1353535.1346300",
  location     = "https://iacoma.cs.uiuc.edu/iacoma-papers/PRES/present_asplos08.pdf"
}

@Article{aspmscicm,
  author       = "Shekhar Srikantaiah and Mahmut Kandemir and Mary Jane Irwin",
  title        = "Adaptive set pinning: managing shared caches in chip multiprocessors",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "135--144",
  month        = mar,
  keywords     = "shared cache, set pinning, cmp, inter-processor conflicts,
    intra-processor conflicts, taxonomies, cache behavior",
  abstract     = "As part of the trend towards Chip Multiprocessors (CMPs) for
    greater computing performance, many architectures have explored
    sharing the last level of cache among different processors for improved
    performance-cost ratio and resource allocation.  Shared cache
    management is a crucial CMP design aspect for system 
    performance.  This paper first presents a new classification of cache misses -
    CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with
    shared caches to provide a better understanding of the interactions between
    memory transactions of different processors at the level of shared cache in
    a CMP.  We then propose a novel approach, called set pinning, for
    eliminating inter-processor misses and reducing intra-processor misses in a
    shared cache.  Furthermore, we show that an adaptive set pinning scheme
    improves over the benefits obtained by the set pinning scheme by
    significantly reducing the number of off-chip accesses.  Extensive analysis
    of these approaches with SPEComp 2001 benchmarks is performed using a full
    system simulator.  Our experiments indicate that the set pinning scheme
    achieves an average improvement of 22.18% in the L2 miss rate while the
    adaptive set pinning scheme reduces the miss rates by an average of 47.94%
    as compared to the traditional shared cache scheme.  They also improve the
    performance by 7.24% and 17.88% respectively.", 
  location     = "https://doi.org/10.1145/1353535.1346299", 
  location     = "https://cs.uwaterloo.ca/~brecht/courses/856-Topics-In-Computer-Systems-2009/Possible-Readings/scheduling/adaptive-set-pinning-asplos-2008-srikantaiah.pdf"
}

@Article{abpfst,
  author       = "Bumyong Choi and Leo Porter and Dean~M. Tullsen",
  title        = "Accurate branch prediction for short threads",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "125--134",
  month        = mar,
  keywords     = "chip multiprocessors, branch prediction, speculative
    multithreading, global history register, continuity, context switching",
  abstract     = "Multi-core processors, with low communication costs and high
    availability of execution cores, will increase the use of execution and
    compilation models that use short threads to expose parallelism.  Current
    branch predictors seek to incorporate large amounts of control flow history
    to maximize accuracy.  However, when that history is absent the predictor
    fails to work as intended.  Thus, modern predictors are almost useless for
    threads below a certain length.Using a Speculative Multithreaded (SpMT)
    architecture as an example of a system which generates shorter threads,
    this work examines techniques to improve branch prediction accuracy when a
    new thread begins to execute on a different core.  This paper proposes a
    minor change to the branch predictor that gives virtually the same
    performance on short threads as an idealized predictor that incorporates
    unknowable pre-history of a spawned speculative thread.  At the same time,
    strong performance on long threads is preserved.  The proposed technique
    sets the global history register of the spawned thread to the initial value
    of the program counter.  This novel and simple design reduces branch
    mispredicts by 29% and provides as much as a 13% IPC improvement on
    selected SPEC2000 benchmarks.", 
  location     = "https://doi.org/10.1145/1353535.1346298",
  location     = "http://cseweb.ucsd.edu/~tullsen/asplos08.pdf"
}

@Article{atasfras,
  author       = "Vitaliy~B. Lvin and Gene Novark and Emery~D. Berger and Benjamin~G. Zorn",
  title        = "Archipelago: trading address space for reliability and security",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "115--124",
  month        = mar,
  keywords     = "archipelago, buffer overflow, dynamic memory allocation,
    memory errors, probabilistic memory safety, randomized algorithms, virtual
    memory, page mapping", 
  abstract     = "Memory errors are a notorious source of security
    vulnerabilities that can lead to service interruptions, information leakage
    and unauthorized access.  Because such errors are also difficult to debug,
    the absence of timely patches can leave users vulnerable to attack for long
    periods of time.  A variety of approaches have been introduced to combat
    these errors, but these often incur large runtime overheads and generally
    abort on errors, threatening availability.This paper presents Archipelago,
    a runtime system that takes advantage of available address space to
    substantially reduce the likelihood that a memory error will affect program
    execution.  Archipelago randomly allocates heap objects far apart in
    virtual address space, effectively isolating each object from buffer
    overflows.  Archipelago also protects against dangling pointer errors by
    preserving the contents of freed objects after they are freed.  Archipelago
    thus trades virtual address space---a plentiful resource on 64-bit
    systems---for significantly improved program reliability and security,
    while limiting physical memory consumption by tracking the working set of
    an application and compacting cold objects.  We show that Archipelago
    allows applications to continue to run correctly in the face of thousands
    of memory errors.  Across a suite of server applications, Archipelago's
    performance overhead is 6% on average (between -7% and 22%), making it
    especially suitable to protect servers that have known security
    vulnerabilities due to heap memory errors.",
  location     = "https://doi.org/10.1145/1353535.1346296",
  location     = "http://www.cs.umass.edu/~emery/pubs/archipelago.pdf"
}

@Article{hasfssotcpl,
  author       = "Joe Devietti and Colin Blundell and Milo M.~K. Martin and Steve Zdancewic",
  title        = "{HardBound}: architectural support for spatial safety of the {C} programming language",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "103--114",
  month        = mar,
  keywords     = "spatial memory safety, c programming language, fat pointers,
    bounded pointers, hardware support, compiler support",
  abstract     = "The C programming language is at least as well known for its
    absence of spatial memory safety guarantees (i.e., lack of bounds checking)
    as it is for its high performance.  C's unchecked pointer arithmetic and
    array indexing allow simple programming mistakes to lead to erroneous
    executions, silent data corruption, and security vulnerabilities.  Many
    prior proposals have tackled enforcing spatial safety in C programs by
    checking pointer and array accesses.  However, existing software-only
    proposals have significant drawbacks that may prevent wide adoption,
    including: unacceptably high run-time overheads, lack of completeness,
    incompatible pointer representations, or need for non-trivial changes to
    existing C source code and compiler infrastructure.Inspired by the promise
    of these software-only approaches, this paper proposes a hardware bounded
    pointer architectural primitive that supports cooperative hardware/software
    enforcement of spatial memory safety for C programs.  This bounded pointer
    is a new hardware primitive datatype for pointers that leaves the standard
    C pointer representation intact, but augments it with bounds information
    maintained separately and invisibly by the hardware.  The bounds are
    initialized by the software, and they are then propagated and enforced
    transparently by the hardware, which automatically checks a pointer's
    bounds before it is dereferenced.  One mode of use requires instrumenting
    only malloc, which enables enforcement of perallocation spatial safety for
    heap-allocated objects for existing binaries.  When combined with simple
    intraprocedural compiler instrumentation, hardware bounded pointers enable
    a low-overhead approach for enforcing complete spatial memory safety in
    unmodified C programs.", 
  location     = "https://doi.org/10.1145/1353535.1346295",
  location     = "http://acg.cis.upenn.edu/papers/asplos08_hardbound.pdf"
}

@Article{tmcvmsfgpacc,
  author       = "Michal Wegiel and Chandra Krintz",
  title        = "The {Mapping Collector}: virtual memory support for generational, parallel, and concurrent compaction",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "91--102",
  month        = mar,
  keywords     = "virtual memory, compaction, parallel gc, concurrent gc,
    page-oriented gc, stop-the-world gc",
  abstract     = "Parallel and concurrent garbage collectors are increasingly
    employed by managed runtime environments (MREs) to maintain scalability, as
    multi-core architectures and multi-threaded applications become pervasive.
    Moreover, state-of-the-art MREs commonly implement compaction to eliminate
    heap fragmentation and enable fast linear object allocation.Our empirical
    analysis of object demographics reveals that unreachable objects in the
    heap tend to form clusters large enough to be effectively managed at the
    granularity of virtual memory pages.  Even though processes can manipulate
    the mapping of the virtual address space through the standard operating
    system (OS) interface on most platforms, extant parallel/concurrent
    compactors do not do so to exploit this clustering behavior and instead
    achieve compaction by performing, relatively expensive, object moving and
    pointer adjustment.We introduce the Mapping Collector (MC), which leverages
    virtual memory operations to reclaim and consolidate free space without
    moving objects and updating pointers.  MC is a nearly-single-phase
    compactor that is simpler and more efficient than previously reported
    compactors that comprise two to four phases.  Through effective MRE-OS
    coordination, MC maintains the simplicity of a non-moving collector while
    providing efficient parallel and concurrent compaction.We implement both
    stop-the-world and concurrent MC in a generational garbage collection
    framework within the open-source HotSpot Java Virtual Machine.  Our
    experimental evaluation using a multiprocessor indicates that MC
    significantly increases throughput and scalability as well as reduces pause
    times, relative to state-of-the-art, parallel and concurrent compactors.", 
  location     = "https://doi.org/10.1145/1353535.1346294",
  location     = "https://sites.cs.ucsb.edu/~ckrintz/papers/mappinggc-asplos08.pdf"
}

@Article{itpooolwdpoij,
  author       = "Jose~A. Joao and Onur Mutlu and Hyesoon Kim and Rishi Agarwal and Yale~N. Patt",
  title        = "Improving the performance of object-oriented languages with dynamic predication of indirect jumps",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "80--90",
  month        = mar,
  keywords     = "dynamic prediction, indirect jumps, virtual functions,
    object-oriented languages, predicated execution, adaptive algorithms,
    speculative execution",
  abstract     = "Indirect jump instructions are used to implement
    increasingly-common programming constructs such as virtual function calls,
    switch-case statements, jump tables, and interface calls.  The performance
    impact of indirect jumps is likely to increase because indirect jumps with
    multiple targets are difficult to predict even with specialized
    hardware.This paper proposes a new way of handling hard-to-predict indirect
    jumps: dynamically predicating them.  The compiler (static or dynamic)
    identifies indirect jumps that are suitable for predication along with
    their control-flow merge (CFM) points.  The hardware predicates
    theinstructions between different targets of the jump and its CFM point if
    the jump turns out to be hard-to-predict at run time.  If the jump would
    actually have been mispredicted, its dynamic predication eliminates a
    pipeline flush, thereby improving performance.Our evaluations show that
    Dynamic Indirect jump Predication (DIP) improves the performance of a set
    of object-oriented applications including the Java DaCapo benchmark suite
    by 37.8% compared to a commonly-used branch target buffer based predictor,
    while also reducing energy consumption by 24.8%.  We compare DIP to three
    previously proposed indirect jump predictors and find that it provides the
    best performance and energy-efficiency.", 
  location     = "https://doi.org/10.1145/1353535.1346293",
  location     = "https://hps.ece.utexas.edu/pub/joao_asplos08.pdf"
}

@Article{pmupptcdfs,
  author       = "Arindam Mallik and Jack Cosgrove and Robert~P. Dick and Gokhan Memik and Peter Dinda",
  title        = "{PICSEL}: measuring user-perceived performance to control dynamic frequency scaling",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "70--79",
  month        = mar,
  keywords     = "user-perceived performance, dynamic voltage and frequency
  scaling, power management, thermal emergency, display analysis, human factors",
  abstract     = "The ultimate goal of a computer system is to satisfy its
    users.  The success of architectural or system-level optimizations depends
    largely on having accurate metrics for user satisfaction.  We propose to
    derive such metrics from information that is 'close to flesh' and apparent
    to the user rather than from information that is 'close to metal' and
    hidden from the user.  We describe and evaluate PICSEL, a dynamic voltage
    and frequency scaling (DVFS) technique that uses measurements of variations
    in the rate of change of a computer's video output to estimate
    user-perceived performance.  Our adaptive algorithms, one conservative and
    one aggressive, use these estimates to dramatically reduce operating
    frequencies and voltages for graphically-intensive applications while
    maintaining performance at a satisfactory level for the user.  We evaluate
    PICSEL through user studies conducted on a Pentium M laptop running Windows
    XP.  Experiments performed with 20 users executing three applications
    indicate that the measured laptop power can be reduced by up to 12.1%,
    averaged across all of our users and applications, compared to the default
    Windows XP DVFS policy.  User studies revealed that the difference in
    overall user satisfaction between the more aggressive version of PICSEL and
    Windows DVFS were statistically insignificant, whereas the conservative
    version of PICSEL actually improved user satisfaction when compared to
    Windows DVFS.", 
  location     = "https://doi.org/10.1145/1353535.1346291",
  location     = "http://ziyang.eecs.umich.edu/publications/cosgrove08apr.pdf"
}

@Article{easapbtrspicm,
  author       = "Chinnakrishnan~S. Ballapuram and Ahmad Sharif and Hsien-Hsin~S. Lee",
  title        = "Exploiting access semantics and program behavior to reduce snoop power in chip multiprocessors",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "60--69",
  month        = mar,
  keywords     = "cache management, snoopy consistency protocols, snoop
    suppression, modified code, ",
  abstract     = "Integrating more processor cores on-die has become the
    unanimous trend in the microprocessor industry.  Most of the current
    research thrusts using chip multiprocessors (CMPs) as the baseline to
    analyze problems in various domains.  One of the main design issues facing
    CMP systems is the growing number of snoops required to maintain cache
    coherency and to support self/cross-modifying code that leads to power and
    performance limitations.  In this paper, we analyze the internal and
    external snoop behavior in a CMP system and relax the snoopy cache
    coherence protocol based on the program semantics and properties of the
    shared variables for saving power.  Based on the observations and analyses,
    we propose two novel techniques: Selective Snoop Probe (SSP) and Essential
    Snoop Probe (ESP) to reduce power without compromising performance.  Our
    simulation results show that using the SSPtechnique, 5% to 65% data cache
    energy savings per core for different processor configurations can be
    achieved with 1% to 2% performance improvement.  We also show that 5% to
    82% of data cache energy per core is spent on the non-essential snoop
    probes that can be saved using the ESP technique.", 
  location     = "https://doi.org/10.1145/1353535.1346290"
}

@Article{npscmlpmftdc,
  author       = "Ramya Raghavendra and Parthasarathy Ranganathan and Vanish Talwar and Zhikui Wang and Xiaoyun Zhu",
  title        = "No ``power'' struggles: coordinated multi-level power management for the data center",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "48--59",
  month        = mar,
  keywords     = "data center, power management, coordination, efficiency,
  capping, virtualization, control theory, heterogeneity",
  abstract     = "Power delivery, electricity consumption, and heat management
    are becoming key challenges in data center environments.  Several past
    solutions have individually evaluated different techniques to address
    separate aspects of this problem, in hardware and software, and at local
    and global levels.  Unfortunately, there has been no corresponding work on
    coordinating all these solutions.  In the absence of such coordination,
    these solutions are likely to interfere with one another, in unpredictable
    (and potentially dangerous) ways.  This paper seeks to address this
    problem.  We make two key contributions.  First, we propose and validate a
    power management solution that coordinates different individual approaches.
    Using simulations based on 180 server traces from nine different real-world
    enterprises, we demonstrate the correctness, stability, and efficiency
    advantages of our solution.  Second, using our unified architecture as the
    base, we perform a detailed quantitative sensitivity analysis and draw
    conclusions about the impact of different architectures, implementations,
    workloads, and system design choices.", 
  location     = "https://doi.org/10.1145/1353535.1346289", 
  location     = "https://www.hpl.hp.com/techreports/2007/HPL-2007-194.pdf"
}

@Article{etalfcma,
  author       = "Benjamin~C. Lee and David Brooks",
  title        = "Efficiency trends and limits from comprehensive microarchitectural adaptivity",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "36--47",
  month        = mar,
  keywords     = "reconfiguability, adaptivity, microarchitecture, simulation,
  statistics, inference, regression, performance, power, efficiency, genetic
  algorithms, temporal sampling, spatial sampling, regression modeling",
  abstract     = "Increasing demand for power-efficient, high-performance
    computing requires tuning applications and/or the underlying hardware to
    improve the mapping between workload heterogeneity and computational
    resources.  To assess the potential benefits of hardware tuning, we propose
    a framework that leverages synergistic interactions between recent advances
    in (a) sampling, (b) predictive modeling, and (c) optimization heuristics.
    This framework enables qualitatively new capabilities in analyzing the
    performance and power characteristics of adaptive microarchitectures.  For
    the first time, we are able to simultaneously consider high temporal and
    comprehensive spatial adaptivity.  In particular, we optimize efficiency
    for many, short adaptive intervals and identify the best configuration of
    15 parameters, which define a space of 240B point.With frequent
    sub-application reconfiguration and a fully reconfigurable hardware
    substrate, adaptive microarchitectures achieve bips3/w efficiency gains of
    up to 5.3x (median 2.4x) relative to their static counterparts already
    optimized for a given application.  This 5.3x efficiency gain is derived
    from a 1.6x performance gain and 0.8x power reduction.  Although several
    applications achieve a significant fraction of their potential efficiency
    with as few as three adaptive parameters, the three most significant
    parameters differ across applications.  These differences motivate a
    hardware substrate capable of comprehensive adaptivity to meet these
    diverse application requirements.", 
  location     = "https://doi.org/10.1145/1353535.1346288", 
  location     = "https://www.eecs.harvard.edu/~dbrooks/lee2008_asplos.pdf"
}

@Article{atdpwfvs,
  author       = "Ravi Bhargava and Benjamin Serebrin and Francesco Spadini and Srilatha Manne",
  title        = "Accelerating two-dimensional page walks for virtualized systems",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "26--35",
  month        = mar,
  keywords     = "virtualization, tlb, memory management, nested paging, page
  walk caching, hypervisor, virtual machine monitor, amd, virtual page size",
  abstract     = "Nested paging is a hardware solution for alleviating the
    software memory management overhead imposed by system virtualization.
    Nested paging complements existing page walk hardware to form a
    two-dimensional (2D) page walk, which reduces the need for hypervisor
    intervention in guest page table management.  However, the extra dimension
    also increases the maximum number of architecturally-required page table
    references.This paper presents an in-depth examination of the 2D page table
    walk overhead and options for decreasing it.  These options include using
    the AMD Opteron processor's page walk cache to exploit the strong reuse of
    page entry references.  For a mix of server and SPEC benchmarks, the
    presented results show a 15%-38% improvement in guest performance by
    extending the existing page walk cache to also store the nested dimension
    of the 2D page walk.  Caching nested page table translations and skipping
    multiple page entry references produce an additional 3%-7% improvement.Much
    of the remaining 2D page walk overhead is due to low-locality nested page
    entry references, which result in additional memory hierarchy misses.  By
    using large pages, the hypervisor can eliminate many of these long-latency
    accesses and further improve the guest performance by 3%-22%.", 
  location     = "https://doi.org/10.1145/1353535.1346286", 
  location     = "https://pages.cs.wisc.edu/~remzi/Classes/838/Spring2013/Papers/p26-bhargava.pdf"
}

@Article{hlcygrfhsmtce,
  author       = "Jonathan~M. McCune and Bryan Parno and Adrian Perrig and Michael~K. Reiter and Arvind Seshadri",
  title        = "How low can you go?: recommendations for hardware-supported minimal {TCB} code execution",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "14--25",
  month        = mar,
  keywords     = "trusted computing, late launch, secure execution, access control",
  abstract     = "We explore the extent to which newly available CPU-based
    security technology can reduce the Trusted Computing Base (TCB) for
    security-sensitive applications.  We find that although this new technology
    represents a step in the right direction, significant performance issues
    remain.  We offer several suggestions that leverage existing processor
    technology, retain security, and improve performance.  Implementing these
    recommendations will finally allow application developers to focus
    exclusively on the security of their own code, enabling it to execute in
    isolation from the numerous vulnerabilities in the underlying layers of
    legacy code.", 
  location     = "https://doi.org/10.1145/1353535.1346285", 
  location     = "http://www.cs.cmu.edu/~arvinds/pubs/asplos08.pdf"
}

@Article{oavbatrpicos,
  author       = "Xiaoxin Chen and Tal Garfinkel and E.~Christopher Lewis and Pratap Subrahmanyam and Carl~A. Waldspurger and Dan Boneh and Jeffrey Dwoskin and Dan R.K.~Ports",
  title        = "Overshadow: a virtualization-based approach to retrofitting protection in commodity operating systems",
  journal      = asplos08,
  year         = 2008,
  volume       = 42,
  number       = 2,
  pages        = "2--13",
  month        = mar,
  keywords     = "virtual machine onitors, vmm, hypervisors, operating systems,
    memory protection, multi-shadowing, cloaking",
  abstract     = "Commodity operating systems entrusted with securing sensitive
    data are remarkably large and complex, and consequently, frequently prone
    to compromise.  To address this limitation, we introduce a
    virtual-machine-based system called Overshadow that protects the privacy
    and integrity of application data, even in the event of a total
    OScompromise.  Overshadow presents an application with a normal view of its
    resources, but the OS with an encrypted view.  This allows the operating
    system to carry out the complex task of managing an application's
    resources, without allowing it to read or modify them.  Thus, Overshadow
    offers a last line of defense for application data.Overshadow builds on
    multi-shadowing, a novel mechanism that presents different views of
    'physical' memory, depending on the context performing the access.  This
    primitive offers an additional dimension of protection beyond the
    hierarchical protection domains implemented by traditional operating
    systems and processor architectures.We present the design and
    implementation of Overshadow and show how its new protection semantics can
    be integrated with existing systems.  Our design has been fully implemented
    and used to protect a wide range of unmodified legacy applications running
    on an unmodified Linux operating system.  We evaluate the performance of
    our implementation, demonstrating that this approach is practical.", 
  location     = "https://doi.org/10.1145/1353535.1346284", 
  location     = "https://www.cs.utexas.edu/~shmat/courses/cs380s/overshadow.pdf"
}

@Article{emdfa,
  author       = "Judy~M. Bishop",
  title        = "Effective machine descriptors for {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "235--242",
  month        = nov,
  keywords     = "data descriptors, indexing, subranges, vectors, dynamic
    bounds, packing",
  abstract     = "With the impetus provided by the U.S.  Department of
    Defense's support for Ada, serious attention must once again be given to
    the design of computer architecture for the efficient execution of
    structured high level languages.  Descriptors are one of the more
    theoretically popular features in a structured machine, yet in practice
    they do not come up to expectations.  This paper examines the deficiencies
    of current descriptor implementations, and goes on to propose a new
    approach to descriptors.  The design takes into account the relative usage
    of scalar and structured data in structured programs, and pays due
    attention to the efficient representation of descriptors as well as to
    packing, the question of uninitialised values and the contribution which
    the compiler makes to range and index checking.", 
  location     = "https://doi.org/10.1145/948632.948664", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a169589.pdf"
}

@Article{tdoavmfa,
  author       = "L.~J. Groves and W.~J. Rogers",
  title        = "The design of a virtual machine for {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "223--234",
  month        = nov,
  keywords     = "virtual machine design, compile-time/run-time trade-offs,
    storage, data types, control flow, code structure, tasks",
  abstract     = "An implementation of Ada should be based on a
    machine-independent translator generating code for a Virtual Machine, which
    can be realised on a variety of machines.  This approach, which leads to a
    high degree of compiler portability, has been very successful in a number
    of recent language implementation projects and is the approach which has
    been specified by the U.  S.  Army and Air Force in their requirements for
    Ada implementations.This paper discusses the rationale, requirements and
    design of such a Virtual Machine for Ada.  The discussion concentrates on a
    number of fundamental areas in which problems arise: basic Virtual Machine
    structure, including storage structure and addressing; data storage and
    manipulation; flow of control; subprograms, blocks and exceptions; and task
    handling.", 
  location     = "https://doi.org/10.1145/948632.948663"
}

@Article{oafmottcia,
  author       = "Hans Henrik Løvengreen and Dines Bjørner",
  title        = "On a formal model of the tasking concept in {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "213--222",
  month        = nov,
  keywords     = "denotational semantics, meta-iv, formal methods, tasking",
  abstract     = "This paper describes the August 1980 state of the development
    of a formal model of the Ada tasking concept.  The model is being developed
    at the Department of Computer Science, Technical University of Denmark in
    collaboration with the Danish Datamatics Centre as part of a full Ada
    compiler development project summarized below.  The paper includes a short
    characterization of the (present) tasking concepts in Ada on which the
    model is based.  The model description is rather informal to let you have
    an idea about the modelling principles.  In appendix A a detailed example
    is given, unfolding the course of a rendezvous.  The paper ends with a
    sketch of how an implementation can be systematically derived from the
    formal model.", 
  location     = "https://doi.org/10.1145/800004.807952", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a169589.pdf"
}

@Article{ampiofdoais,
  author       = "F.~C. Belz and E.~K. Blum and D.~Heimbigner",
  title        = "{A} multi-processing implementation-oriented formal definition of {Ada} in {SEMANOL}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "202--212",
  month        = nov,
  keywords     = "formal methods, static semantics, concurrency semantics, exceptions",
  abstract     = "A formal definition of the syntax and semantics of
    Preliminary Ada has been designed and partially implemented as a
    metaprogram in the SEMANOL system.  The paper describes the design in
    detail and also presents excerpts of the actual SEMANOL metaprogram.
    Special attention is paid to the following aspects, lacking in the formal
    denotational definition of Ada: (1) formal definition of the connection
    between the concrete and abstract syntax of Ada, a necessary element of a
    formal definition if it is to be executable; (2) formal definition of the
    semantics of concurrency in Ada tasking; (3) formal definition of the
    semantics of exceptions and interrupts.  The complete SEMANOL metaprogram
    will be an executable formal definition.  In effect, it defines a
    multi-processing operating system which accepts an Ada program, as a set of
    compilation units, together with data, and causes the program to be
    executed in a concurrent mode in accordance with the semantics of tasks
    prescribed in the Ada reference manual and rationale.", 
  location     = "https://doi.org/10.1145/800004.807951"
}

@Article{tnatai,
  author       = "Robert B.~K. Dewar and Gerald~A. Fisher and Edmond Schonberg and Robert Froehlich and Stephen Bryant and Clinton~F. Goss and Michael Burke",
  title        = "The {NYU} {Ada} translator and interpreter",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "194--201",
  month        = nov,
  keywords     = "setl, very-high level languages, intermediate code, error
    recovery, interpreters, generics, tasking",
  abstract     = "The NYU-Ada project is engaged in the design and
    implementation of a translator-interpreter for the Ada language.  The
    objectives of this project are twofold: a) to provide an executable
    semantic model for the full Ada language, that can be used for teaching,
    and serve as a starting point for the design of an efficient Ada compiler;
    b) to serve as a testing ground for the software methodology that has
    emerged from our experience with the very-high level language SETL.  In
    accordance with these objectives, the NYU-Ada system is written in a
    particularly high-level, abstract SETL style that emphasizes clarity of
    design and user interface over speed and efficiency.  A number of unusual
    design features of the translator and interpreter follow from this
    emphasis.  Some of these features are described below.  We also discuss the
    question of semantic specification of programming languages, and the
    general methodology of “Software Prototyping” of which the NYU-Ada system
    is a sizeable example.", 
  location     = "https://doi.org/10.1145/800004.807950"
}

@Article{ststatpapta,
  author       = "Paul~F. Albrecht and Phillip~E. Garrison and Susan~L. Graham and Robert~H. Hyerle and Patricia Ip and Bernd Krieg Brückner",
  title        = "Source-to-source translation: {Ada} to {Pascal} and {Pascal} to {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "183--193",
  month        = nov,
  keywords     = "source-to-source transformations, sublanguages",
  abstract     = "An implementation of translators between Ada and Pascal is
    described. The method used is to define subsets of each language between
    which there is a straightforward translation and to translate each source
    program to its respective sublanguage by program transformations.  A common
    internal tree representation is used.  The underlying organization of the
    translators is described, and some of the difficulties we have confronted
    and solved are discussed.", 
  location     = "https://doi.org/10.1145/800004.807949"
}

@Article{aaastt,
  author       = "Gary~L. Filipski and Donald~R. Moore and John~E. Newton",
  title        = "{Ada} as a software transition tool",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "176--182",
  month        = nov,
  keywords     = "algol, portability, translation",
  abstract     = "Our agency plans to use the Ada programming language as a
    vehicle to transport a locally written inquiry system from Burroughs'
    equipment to another vendor's hardware.  This is being done in the
    following manner.  A bootstrap Ada translator has been written in Pascal to
    generate Burroughs' ALGOL.  The Ada translator will be recoded into Ada.
    The inquiry system will be rewritten into Ada from ALGOL and executed on
    the current Burroughs' machine.  When the new hardware is selected, the Ada
    translator will be retargeted to generate an efficient language on that
    machine, thereby transporting the inquiry system.  The Ada translator will
    be discarded for an Ada compiler on the new system.", 
  location     = "https://doi.org/10.1145/800004.807948", 
  location     = "https://apps.dtic.mil/dtic/tr/fulltext/u2/a169589.pdf"
}

@Article{aftam,
  author       = "D.~R. Stevenson",
  title        = "Algorithms for translating {Ada} multitasking",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "166--175",
  month        = nov,
  keywords     = "ada-m, modeling, tasking",
  abstract     = "Algorithms are presented for translating the multitasking
    constructs of Ada into the language Ada-M.  The purpose of the translation
    is to study various implementations of Ada tasking and their relative
    problems, merits, and efficiencies.  The multiprocessing constructs of
    Ada-M are lower level than those of Ada and, hence, flexible enough to
    permit development of a variety of compilation techniques for Ada tasking.
    Ada-M is sufficiently high-level, however, to permit the implementations to
    be developed quickly and understandably.  Requirements for data structures,
    scheduling, and other pertinant elements of Ada tasking compilation are
    identified by the translation.", 
  location     = "https://doi.org/10.1145/800004.807947"
}

@Article{tramcitaed,
  author       = "W.~Eventoff and D.~Harvey and R.~J. Price",
  title        = "The rendezvous and monitor concepts: Is there an efficiency difference?",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "156--165",
  month        = nov,
  keywords     = "concurrent pascal, tasking, monitors, rendezvous",
  abstract     = "The efficiency of Ada's rendezvous concept is compared with
    Concurrent Pascal's monitor concept.  The differences between the two
    approaches, as well as a number of issues relating to their implementation,
    are presented.  The results indicate that a concurrent programming language
    should provide both types of concepts.",
  location     = "https://doi.org/10.1145/800004.807946"
}

@Article{aacgfv17wu,
  author       = "Mark Sherman and Andy Hisgen and David Alex Lamb and Jonathan Rosenberg",
  title        = "An {Ada} code generator for {VAX} 11/780 with " # unix,
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "91--98",
  month        = nov,
  keywords     = "machine models, run-time management, unix, stack machines",
  abstract     = "This paper describes the final phase of an Ada compiler which
    produces code for the VAX 11/780 running the Unix operating system.
    Problems encountered in the implementation of subprogram calls, parameter
    passing, function return values, and exception handling are discussed and
    their solutions outlined.  An underlying requirement for the code generator
    has been speed of implementation consistent with being a test bed for an
    Ada implementation.  To accomplish this, a common model for the target
    environment has been assumed.  The assumptions include: the VAXis a stack
    machine, a single address space is used, only the general case is
    implemented (no optimization of special cases), the hardware does as much
    work as possible, run time routines for lengthy code sequences are
    acceptable, and the conventions given in the VAX architecture, hardware,
    and software manuals are used.  The code generator has been running on a
    PDP-10 with Tops-10, producing a VAX assembly language source program as
    output.  It has been available to local users since the beginning of
    1980.", 
  location     = "https://doi.org/10.1145/800004.807939"
}

@Article{taatmeotpac,
  author       = "Benjamin~M. Brosgol",
  title        = "{TCOL}${}_{\hbox{\rm Ada}}$ and the ``Middle End'' of the {PQCC Ada} compiler",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "101--112",
  month        = nov,
  keywords     = "intermediate representations, attributed graph
    representations, abstract representations, middle-end, generative code,
    generics",
  abstract     = "A compiler is traditionally partitioned into a (mostly)
    machine independent Front End which performs lexical, syntactic, and
    semantic analysis, and a machine dependent Back End which performs
    optimization and code generation.  In the Ada compiler being implemented at
    Carnegie-Mellon University in the PQCC project, it is useful to identify a
    set of phases occurring at the start of the Back End - i.e., “Middle End” -
    after semantic analysis but before optimization.  These phases, known
    collectively as “CWVM” (an abbreviation for “Compiler Writer's Virtual
    Machine”) make basic representational choices and reflect these in an
    expanded program tree.  This paper describes both TCOLAda - the
    intermediate language interface produced by the Front End - and the phases
    comprising CWVM.  TCOLAda is a graph structured high level representation
    of the source program which includes both the symbol table and the program
    tree.  The CWVM phases perform transformations of the TCOLAda graph which
    fall into three categories: language oriented (e.g., expansion of checking
    for constructs such as array indexing), virtual machine oriented (e.g.,
    translation of up-level addressing into “display” vector accesses), and
    actual machine oriented (e.g., expansion of component selection into
    address arithmetic).", 
  location     = "https://doi.org/10.1145/800004.807940"
}

@Article{apmodavapwp,
  author       = "David~C. Luckham and Wolfgang Polak",
  title        = "{A} practical method of documenting and verifying {Ada} programs with packages",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "113--122",
  month        = nov,
  keywords     = "modules, documentation, formal specifications, proof theory",
  abstract     = "We present a method of formal specification of Ada programs containing packages.  The method suggests concepts and guidelines useful for giving adequate informal documentation of packages by means of comments.  The method depends on (1) the standard inductive assertion technique for subprograms, (2) the use of history sequences in assertions specifying the declaration and use of packages, and (3) the addition of three categories of specifications to Ada package declarations: (a) visible specifications, (b) boundary specifications, (c) internal specifications.  Axioms and proof rules for the Ada package constructs (declaration, instantiation, and function and procedure call) are given in terms of history sequences and package specifications.  These enable us to construct formal proofs of the correctness of Ada programs with packages.  The axioms and proof rules are easy to implement in automated program checking systems.  The use of history sequences in both informal documentation and formal specifications and proofs is illustrated by examples.",
  location     = "https://doi.org/10.1145/800004.807941"
}

@Article{gavia,
  author       = "William~D. Young and Donald~I. Good",
  title        = "Generics and verification in {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "123--127",
  month        = nov,
  keywords     = "generics, proof modularity, generic verification",
  abstract     = "This paper explores the restrictions a mechanism in the style
    of the Ada generics facility would have to satisfy in order to be amenable
    to existing verification techniques.  “Generic verification” is defined and
    defended as the appropriate goal for any such facility.  Criteria are
    developed for generic verification to be possible and then Ada is evaluated
    with respect to these criteria.  An example of the application of these
    techniques to an Ada unit is presented to show that generic verification is
    possible at least on a subclass of Ada generic units.  Finally some
    potential applications of verified generic units are presented.",
  location     = "https://doi.org/10.1145/800004.807942"
}

@Article{atalfaap,
  author       = "Bernd Krieg Br{\" u}ckner and David~C. Luckham",
  title        = "{ANNA}: Towards a language for annotating {Ada} programs",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "128--138",
  month        = nov,
  keywords     = "formal specification, program development, annotations,
    state-based specifications",
  abstract     = "ANNA is a proposal to extend Ada to include facilities for
    formally specifying the intended behaviour of Ada programs (or portions
    thereof) at all stages of program development.  ANNA programs are Ada
    programs with formal comments.  Formal comments in ANNA consist of virtul
    Ada text and annotations.  The syntax and semantics of different kinds of
    annotations are defined: declarative annotations (for variables, subtypes,
    subprograms, and packages), statement annotations, exception annotations,
    and visibility annotations.  ANNA includes a small number of predefined
    attributes which may appear only in annotations, e.g., access type
    collections.  The lexical structure of ANNA is designed so that the
    extensions of Ada appear as Ada comments.  ANNA programs are therefore
    acceptable by Ada translators.  The semantics of annotations are defined in
    terms of Ada concepts, in particular many annotations are generalizations
    of the constraint concept.  It is therefore a simple step for the Ada
    programmer to use ANNA to give formal specifications of programs.  ANNA is
    intended to provide a formal framework within which different theories of
    formal specification may be applied to Ada.  Our proposal omits tasking for
    the time being.", 
  location     = "https://doi.org/10.1145/800004.807943"
}

@Article{niapiftb,
  author       = "Lori~A. Clarke and Jack~C. Wileden and Alexander~L. Wolf",
  title        = "Nesting in {Ada} programs is for the birds",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "139--145",
  month        = nov,
  keywords     = "scoping, program structure, packages, name-space control",
  abstract     = "Given a data abstraction construct like the Ada package and
    in light of current thoughts on programming methodology, we feel that
    nesting is an anachronism.  In this paper we propose a nest-free program
    style for Ada that eschews nested program units and declarations within
    blocks and instead heavily utilizes packages and context specifications as
    mechanisms for controlling visibility.  We view this proposal as a first
    step toward the development of programming methods that exploit the novel
    language features available in Ada.  Consideration of this proposal's
    ramifications for data flow, control flow, and overall program structure
    substantiates our contention that a tree structure is seldom a natural
    representation of a program and that nesting therefore generally interferes
    with program development and readability.", 
  location     = "https://doi.org/10.1145/800004.807944", 
  location     = "https://www.doc.ic.ac.uk/~alw/doc/papers/ada80.pdf"
}

@Article{etairts,
  author       = "Lee MacLaren",
  title        = "Evolving toward {Ada} in real time systems",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "146--155",
  month        = nov,
  keywords     = "real-time systems, scheduling, multitasking, cyclic execution",
  abstract     = "The Ada view of multitasking represents a radical departure
    from the traditional “cyclic executive” approach to real time operating
    systems.  Since system designers must by necessity be conservative, it
    would be unrealistic to expect an abrupt change of this magnitude in
    engineering practice.  Instead, this paper outlines a sequence of
    intermediate steps designed so that the advantages and familiar structures
    of cyclic systems may be retained, while the capabilities of Ada
    multitasking are gradually incorporated.  A scale of increasing scheduling
    complexity provides the justification for this sequence.  The discussion of
    each step then briefly mentions some of the related benefits and costs.
    The paper draws some conclusions about the use of Ada in real time
    systems.", 
  location     = "https://doi.org/10.1145/800004.807945"
}

@Article{arrfavat,
  author       = "Andy Hisgen and David Alex Lamb and Jonathan Rosenberg and Mark Sherman",
  title        = "{A} runtime representation for {Ada} variables and types",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "82--90",
  month        = nov,
  keywords     = "subtypes, constraints, dynamic arrays, discriminants, type
    descriptors, block operations",
  abstract     = "The type and subtype facilities of the Ada programming
    language permit some subtype information to be determined dynamically.
    This subtype information requires a runtime representation, and its dynamic
    nature influences the representation of variables.  In this paper, we first
    review Ada's types and subtypes to identify some of those aspects which
    affect runtime representation.  We then present the particular
    representation scheme which is used in the CHARRETTE Ada implementation.
    The scheme is straightforward and consistent in that a variable is
    represented the same way independently of where it appears, whether it is
    on the stack, on the heap, or a component of another variable.  The design
    treats Ada's discriminants and discriminant constraints as a form of
    parameterized types, where the parameterization permits different instances
    of a type to have different variants and different sizes for array fields.
    Composition of such parameterized types is supported.  We explain how
    several Ada operations are handled by our particular representation.  We
    briefly discuss some alternative approaches to Ada representation,
    comparing them to our design.", 
  location     = "https://doi.org/10.1145/800004.807938"
}

@Article{tcac,
  author       = "Jonathan Rosenberg and David Alex Lamb and Andy Hisgen and Mark Sherman",
  title        = "The Charrette {Ada} compiler",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "72--81",
  month        = nov,
  keywords     = "intermediate representations, parsing, run-time organization,
    declarations, translation",
  abstract     = "The Charrette Ada compiler is a working compiler for a
    substantial subset of the preliminary Ada language.  The Ada source program
    is translated into an equivalent program in an intermediate implementation
    language.  The result of the compilation process is machine language
    generated for this intermediate program.  This paper provides a brief
    overview of the compiler with special attention given to the primary
    translation phase.  Emphasis is placed on the transformation of Ada type
    and subtype information and the representation of objects.  The translation
    of several interesting statement and expression forms is also outlined.", 
  location     = "https://doi.org/10.1145/800004.807937", 
  location     = "https://kilthub.cmu.edu/articles/journal_contribution/The_Charrette_Ada_compiler/6610439"
}

@Article{afsafa,
  author       = "Mark~S. Sherman and Martha~S. Borkan",
  title        = "{A} flexible semantic analyzer for {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "62--69",
  month        = nov,
  keywords     = "semantic analyzers, simula, overload resolution, scope
    management, packages",
  abstract     = "A technique for writing semantic analysis phases of compilers
    is described.  The technique uses Simula classes and virtual procedures to
    create a flexible and modular program.  This technique is used to implement
    a semantic analysis phase of a compiler front end for the preliminary Ada
    language.  Because the design is extremely flexible and modular, the front
    end is able to accommodate changes in the Ada language and its semantics as
    they are published.  Several problems were encountered when implementing
    Ada's semantics.  These problems are described and their solutions
    presented.  The front end also produces TCOLAda, the specified intermediate
    language for various Ada compiler contracts.  This output has been used by
    an experimental compiler back end.  [9] The front end is written as two
    programs which perform lexical analysis, syntactic analysis, semantic
    analysis, and TCOLAda generation.  The front end is coded in Simula, and
    has been running on DEC Tops-10 and Tops-20 systems since September 1979.", 
  location     = "https://doi.org/10.1145/800004.807936"
}

@Article{triaair,
  author       = "Peter~A. Belmont",
  title        = "Type resolution in {Ada}: An implementation report",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "57--61",
  month        = nov,
  keywords     = "name resolution, type determination",
  abstract     = "Various features of Ada [Ichbiah 79, Honeywell 80] make type
    resolution an interesting and difficult as implemented in a semantic
    analyzer for Ada built in 1979-80.  First, a straightforward algorithm,
    similar to that of Ganzinger and Ripken [Ganzinger 80], is discussed.  Next
    an optimized version of this algorithm is presented.  The optimization is
    based on the idea that, in a tree-walking analysis in which the information
    developed on one branch can affect the analysis elsewhere, and where the
    difficulty of analysis is not uniform, analysis should be performed a
    little at a time wherever it is most likely to be useful rather than
    according to any data-independent, pre-planned method such as bottom-up or
    top-down.  This raises the likelihood that the analysis of “simple”
    branches will occur early and so ease the computational cost of analyzing
    the more difficult branches.", 
  location     = "https://doi.org/10.1145/800004.807935"
}

@Article{oipa,
  author       = "Guido Persch and Georg Winterstein and Manfred Dausmann and Sophia Drossopoulou",
  title        = "Overloading in preliminary {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "47--56",
  month        = nov,
  keywords     = "overloading, name resolution",
  abstract     = "Ada permits the overloading of enumeration literals,
    aggregates, subprograms and operators, i.e.  the declaration of the same
    designator with different meanings in the same scope.  This leads to
    difficulties during the semantic analysis of expressions and subprogram
    calls.  For selecting the meaning not only the designator but also the
    types of its operands or parameters and the type of its result must be
    used.  We show that the identification of expressions is possible in two
    passes, the first bottom-up, the second top-down.", 
  location     = "https://doi.org/10.1145/800004.807934"
}

@Article{tacfefa,
  author       = "Gerhard Goos and Georg Winterstein",
  title        = "Towards a compiler front-end for {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "36--46",
  month        = nov,
  keywords     = "bootstrapping, modularization, semantic analysis, separate
    compilation, aida, intermediate representations",
  abstract     = "This paper discusses the current development of a compiler
    front-end for Ada at the University of Karlsruhe.  The front-end is
    independent of the target-machine and will compile Ada into an intermediate
    language AIDA, essentially an attributed structure tree.  The front-end is
    written in its own language using Ada-0 and LIS as preliminary compilers
    for the bootstrap.  The compiler in its present form relies heavily on the
    formal definition of Ada which is under development at CII and IRIA.", 
  location     = "https://doi.org/10.1145/800004.807933"
}

@Article{uafiema,
  author       = "A.~G. Duncan and J.~S. Hutchison",
  title        = "Using {Ada} for industrial embedded microprocessor applications",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "26--35",
  month        = nov,
  keywords     = "microcontrollers, software development",
  abstract     = "This paper investigates the use of Ada as a high level
    implementation language for use on microprocessors embedded in industrial
    applications.  Many of these applications use microprocessors with minimal
    hardware, that is, no hardware support for a stack and possibly not even a
    hardware clock.  The use of minimal hardware is dictated by manufacturing
    economics.  If one can save $.25 per unit over a run of 100,000 units, the
    total savings will be $25,000.  An Ada implementation for such hardware
    will differ greatly from an implementation for a large mainframe.  For
    instance, the storage allocator cannot blithely allocate space for
    variables in activation records.  While these programs do not use many of
    Ada's powerful language features, the compiler must be able to generate
    highly optimized code for those parts of the language that are used.  Our
    discussion of Ada centers around a typical application, a program to
    control an automatic oven.  While this example is not one of our intended
    applications, it embodies most of the problems that we expect to find in
    practice.  The example was inspired by McCracken's furnace control case
    study [1], written in PL/M, and provides a good opportunity for comparing
    Ada with PL/M.  Applications like the oven control program require a number
    of sophisticated compiler optimizations.  In addition to performing the
    usual common subexpression analysis, removal of redundant and unreachable
    code, and sophisticated peephole optimizations, the compiler must be
    capable of complete static allocation of all data and must be able to
    perform significant amounts of verification to simplify generated code,
    especially with respect to error checks and exception handling.  Finally,
    we discuss some language problems and some pragmas that we feel are
    necessary for use of Ada in real-time and time-dependent situations.", 
  location     = "https://doi.org/10.1145/800004.807932"
}

@Article{adatse,
  author       = "Richard~E. Fairley",
  title        = "{Ada} debugging and testing support environments",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "16--25",
  month        = nov,
  keywords     = "testing, documentation, development support environments",
  abstract     = "This paper presents analysis and design considerations for
    Ada Programming Support Environments (APSEs) to support interactive
    debugging and testing of embedded, real time software at the Ada source
    code level.  The analysis is based on the “Stoneman” requirements
    specification for APSEs (1).  Important factors in the analysis and design
    of Ada debugging and testing support systems include the requirement for a
    source level system, the host machine-target machine configurations, the
    real time and concurrent nature of target software, and the KAPSE virtual
    machine interface to the APSE data base.  Although this paper is
    specifically concerned with debugging and testing issues, the methods
    utilized and the results obtained are of general applicability.  The
    following sections of the paper address general analysis considerations,
    source level support environments, design considerations for an interactive
    source level debugger, and KAPSE design considerations.", 
  location     = "https://doi.org/10.1145/800004.807931"
}

@Article{aewpia,
  author       = "David~S. Notkin",
  title        = "An experience with parallelism in {Ada}",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "9--15",
  month        = nov,
  keywords     = "gandalf, project management, package management, synchronization",
  abstract     = "One of the more interesting and controversial features of Ada
    is the tasking structure.  The Ada tasking facility provides high level
    mechanisms for communication and synchronization among tasks executing in
    parallel.  The open question about these mechanisms is whether they provide
    programmers with both the appropriate level of abstraction and also the
    necessary level of control.  This paper describes in detail the
    implementation of a system using parallelism that was written during the
    Ada test and evaluation process.", 
  location     = "https://doi.org/10.1145/800004.807930"
}

@Article{tacvc,
  author       = "John~B. Goodenough",
  title        = "The {Ada} Compiler Validation Capability",
  journal      = sapl80,
  year         = 1980,
  volume       = 15,
  number       = 11,
  pages        = "1--8",
  month        = nov,
  keywords     = "validation, compiler implementation, testing",
  abstract     = "The Ada Compiler Validation Capability consists of tests,
    tools, procedures, and documentation designed to enforce (and encourage)
    development of compilers that conform to the Ada language Standard.  In
    this paper, we discuss our approach to solving the principal problems faced
    in developing and using such a capability.", 
  location     = "https://doi.org/10.1145/800004.807929"
}

@Article{asfstpwpbstidp,
  author       = "Robert~D. Cameron and Dan Lin",
  title        = "Architectural Support for {SWAR} Text Processing with Parallel Bit Streams:  The Inductive Doubling Principle",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "325--335",
  month        = mar,
  keywords     = "inductive doubling, parallel bit streams, swar, bit twiddling",
  abstract     = "Parallel bit stream algorithms exploit the SWAR (SIMD within
    a register) capabilities of commodity processors in high-performance text
    processing applications such as UTF-8 to UTF-16 transcoding, XML parsing,
    string search and regular expression matching.  Direct architectural
    support for these algorithms in future SWAR instruction sets could further
    increase performance as well as simplifying the programming task.  A set of
    simple SWAR instruction set extensions are proposed for this purpose based
    on the principle of systematic support for inductive doubling as an
    algorithmic technique.  These extensions are shown to significantly reduce
    instruction count in core parallel bit stream algorithms, often providing a
    3X or better improvement.  The extensions are also shown to be useful for
    SWAR programming in other application areas, including providing a
    systematic treatment for horizontal operations.  An implementation model
    for these extensions involves relatively simple circuitry added to the
    operand fetch components in a pipelined processor.", 
  location     = "https://doi.org/10.1145/2528521.1508283"
}

@Article{sasfafcrt,
  author       = "Karthik Ramani and Christiaan~P. Gribble and Al Davis",
  title        = "{StreamRay}: a stream filtering architecture for coherent ray tracing",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "325--336",
  month        = mar,
  keywords     = "graphics processors, interactive rendering, ray tracing, wide
    simd architectures, coherence, scatter-gather",
  abstract     = "The wide availability of commodity graphics processors has
    made real-time graphics an intrinsic component of the human/computer
    interface.  These graphics cores accelerate the z-buffer algorithm and
    provide a highly interactive experience at a relatively low cost.  However,
    many applications in entertainment, science, and industry require high
    quality lighting effects such as accurate shadows, reflection, and
    refraction.  These effects can be difficult to achieve with z-buffer
    algorithms but are straightforward to implement using ray tracing.
    Although ray tracing is computationally more complex, the algorithm
    exhibits excellent scaling and parallelism properties.  Nevertheless, ray
    tracing memory access patterns are difficult to predict and the parallelism
    speedup promise is therefore hard to achieve.This paper highlights a novel
    approach to ray tracing based on stream filtering and presents StreamRay, a
    multicore wide SIMD microarchitecture that delivers interactive frame rates
    of 15-32 frames/second for scenes of high geometric complexity and exhibits
    high utilization for SIMD widths ranging from eight to 16 elements.
    StreamRay consists of two main components: the ray engine, which is
    responsible for stream assembly and employs address generation units that
    generate addresses to form large SIMD vectors, and the filter engine, which
    implements the ray tracing operations with programmable accelerators.
    Results demonstrate that separating address and data processing reduces
    data movement and resource contention.  Performance improves by 56% while
    simultaneously providing 11.63% power savings per accelerator core compared
    to a design which does not use separate resources for address and data
    computations.", 
  location     = "https://doi.org/10.1145/2528521.1508282"
}

@Article{pbavbtbd,
  author       = "Ioana Burcea and Andreas Moshovos",
  title        = "Phantom-{BTB}: a virtualized branch target buffer design",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "313--324",
  month        = mar,
  keywords     = "predictor metadata prefetching, predictor virtualization,
    branch target buffer, l2 cache storage, temporal coherence",
  abstract     = "Modern processors use branch target buffers (BTBs) to predict
    the target address of branches such that they can fetch ahead in the
    instruction stream increasing concurrency and performance.  Ideally, BTBs
    would be sufficiently large to capture the entire working set of the
    application and sufficiently small for fast access and practical on-chip
    dedicated storage.  Depending on the application, these requirements are at
    odds.This work introduces a BTB design that accommodates large instruction
    footprints without dedicating expensive onchip resources.  In the proposed
    Phantom-BTB (PBTB) design, a conventional BTB is augmented with a virtual
    table that collects branch target information as the application runs.  The
    virtual table does not have fixed dedicated storage.  Instead, it is
    transparently allocated, on demand, in the on-chip caches, at cache line
    granularity.  The entries in the virtual table are proactively prefetched
    and installed in the dedicated conventional BTB, thus, increasing its
    perceived capacity.  Experimental results with commercial workloads under
    full-system simulation demonstrate that PBTB improves IPC performance over
    a 1K-entry BTB by 6.9% on average and up to 12.7%, with a storage overhead
    of only 8%.  Overall, the virtualized design performs within 1% of a
    conventional 4K-entry, single-cycle access BTB, while the dedicated storage
    is 3.6 times smaller.", 
  location     = "https://doi.org/10.1145/2528521.1508281"
}

@Article{tsadofashndfgod,
  author       = "Aravind Menon and Simon Schubert and Willy Zwaenepoel",
  title        = "TwinDrivers: semi-automatic derivation of fast and safe hypervisor network drivers from guest {OS} drivers",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "301--312",
  month        = mar,
  keywords     = "performance, measurement, virtual machine managers,
    hypervisors, device drivers, kernel resident code",
  abstract     = "In a virtualized environment, device drivers are often run
    inside a virtual machine (VM) rather than in the hypervisor, for reasons of
    safety and reduction in software engineering effort.  Unfortunately, this
    approach results in poor performance for I/O-intensive devices such as
    network cards.  The alternative approach of running device drivers directly
    in the hypervisor yields better performance, but results in the loss of
    safety guarantees for the hypervisor and incurs additional software
    engineering costs.In this paper we present TwinDrivers, a framework which
    allows us to semi-automatically create safe and efficient hypervisor
    drivers from guest OS drivers.  The hypervisor driver runs directly in the
    hypervisor, but its data resides completely in the driver VM address space.
    A Software Virtual Memory mechanism allows the driver to access its VM data
    efficiently from the hypervisor running in any guest context, and also
    protects the hypervisor from invalid memory accesses from the driver.  An
    upcall mechanism allows the hypervisor to largely reuse the driver support
    infrastructure present in the VM.  The TwinDriver system thus combines most
    of the performance benefits of hypervisor-based driver approaches with the
    safety and software engineering benefits of VM-based driver
    approaches.Using the TwinDrivers hypervisor driver, we are able to improve
    the guest domain networking throughput in Xen by a factor of 2.4 for
    transmit workloads, and 2.1 for receive workloads, both in CPU-scaled
    units, and achieve close to 64-67 of native Linux throughput.", 
  location     = "https://doi.org/10.1145/2528521.1508279"
}

@Article{dpocyfmr,
  author       = "Michal Wegiel and Chandra Krintz",
  title        = "Dynamic prediction of collection yield for managed runtimes",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "289--300",
  month        = mar,
  keywords     = "garbage collection, predictive algorithms, managed run-times",
  abstract     = "The growth in complexity of modern systems makes it
    increasingly difficult to extract high-performance.  The software stacks
    for such systems typically consist of multiple layers and include managed
    runtime environments (MREs).  In this paper, we investigate techniques to
    improve cooperation between these layers and the hardware to increase the
    efficacy of automatic memory management in MREs.General-purpose MREs
    commonly implement parallel and/or concurrent garbage collection and employ
    compaction to eliminate heap fragmentation.  Moreover, most systems trigger
    collection based on the amount of heap a program uses.  Our analysis shows
    that in many cases this strategy leads to ineffective collections that are
    unable to reclaim sufficient space to justify the incurred cost.  To avoid
    such collections, we exploit the observation that dead objects tend to
    cluster together and form large, never-referenced, regions in the address
    space that correlate well with virtual pages that have not recently been
    referenced by the application.  We leverage this correlation to design a
    new, simple and light-weight, yield predictor that estimates the amount of
    reclaimable space in the heap using hardware page reference bits.  Our
    predictor allows MREs to avoid low-yield collections and thereby improve
    resource management.We integrate this predictor into three state-of-the-art
    parallel compactors, implemented in the HotSpot JVM, that represent
    distinct canonical heap layouts.  Our empirical evaluation, based on
    standard Java benchmarks and open-source applications, indicates that
    inexpensive and accurate yield prediction can improve performance
    significantly.", 
  location     = "https://doi.org/10.1145/2528521.1508278", 
  location     = "https://sites.cs.ucsb.edu/~ckrintz/papers/yieldpred-asplos09.pdf"
}

@Article{pwdwdaow,
  author       = "Todd Mytkowicz and Amer Diwan and Matthias Hauswirth and Peter~F. Sweeney",
  title        = "Producing wrong data without doing anything obviously wrong!",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "265--276",
  month        = mar,
  keywords     = "statistical analysis, biased measurements, benchmarking,
    randomization, causal analysis ",
  abstract     = "This paper presents a surprising result: changing a seemingly 
    innocuous aspect of an experimental setup can cause a systems researcher to
    draw wrong conclusions from an experiment.  What appears to be an innocuous
    aspect in the experimental setup may in fact introduce a significant bias
    in an evaluation.  This phenomenon is called measurement bias in the
    natural and social sciences.Our results demonstrate that measurement bias
    is significant and commonplace in computer system evaluation.  By
    significant we mean that measurement bias can lead to a performance
    analysis that either over-states an effect or even yields an incorrect
    conclusion.  By commonplace we mean that measurement bias occurs in all
    architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both
    compilers that we tried (gcc and Intel's C compiler), and most of the SPEC
    CPU2006 C programs.  Thus, we cannot ignore measurement bias.
    Nevertheless, in a literature survey of 133 recent papers from ASPLOS,
    PACT, PLDI, and CGO, we determined that none of the papers with
    experimental results adequately consider measurement bias.Inspired by
    similar problems and their solutions in other sciences, we describe and
    demonstrate two methods, one for detecting (causal analysis) and one for
    avoiding (setup randomization) measurement bias.", 
  location     = "https://doi.org/10.1145/2528521.1508275", 
  location     = "https://users.cs.northwestern.edu/~robby/courses/322-2013-spring/mytkowicz-wrong-data.pdf"
}

@Article{lp2009,
  author       = "Michael~D. Bond and Kathryn~S. McKinley",
  title        = "Leak pruning",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "277--288",
  month        = mar,
  keywords     = "storage leaks, garbage collection, error recovery, error toleration",
  abstract     = "Managed languages improve programmer productivity with type
    safety and garbage collection, which eliminate memory errors such as
    dangling pointers, double frees, and buffer overflows.  However, because
    garbage collection uses reachability to over-approximate live objects,
    programs may still leak memory if programmers forget to eliminate the last
    reference to an object that will not be used again.  Leaks slow programs by
    increasing collector workload and frequency.  Growing leaks eventually
    crash programs.This paper introduces leak pruning, which keeps programs
    running by predicting and reclaiming leaked objects at run time.  It
    predicts dead objects and reclaims them based on observing data structure
    usage patterns.  Leak pruning preserves semantics because it waits for heap
    exhaustion before reclaiming objects and poisons references to objects it
    reclaims.  If the program later tries to access a poisoned reference, the
    virtual machine (VM) throws an error.  We show leak pruning has low
    overhead in a Java VM and evaluate it on 10 leaking programs.  Leak pruning
    does not help two programs, executes five substantial programs 1.6-81X
    longer, and executes three programs, including a leak in Eclipse, for at
    least 24 hours.  In the worst case, leak pruning defers fatal errors.  In
    the best case, it keeps leaky programs running with preserved semantics and
    consistent throughput.", 
  location     = "https://doi.org/10.1145/2528521.1508277"
}

@Article{acsewamca,
  author       = "M.~Aater Suleman and Onur Mutlu and Moinuddin~K. Qureshi and Yale~N. Patt",
  title        = "Accelerating critical section execution with asymmetric multi-core architectures",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "253--264",
  month        = mar,
  keywords     = "cmp, critical sections, heterogeneous cores, multi-core,
    parallel programming, locking, amdahl's law, serialization, client-server
    architecture", 
  abstract     = "To improve the performance of a single application on Chip
    Multiprocessors (CMPs), the application must be split into threads which
    execute concurrently on multiple cores.  In multi-threaded applications,
    critical sections are used to ensure that only one thread accesses shared
    data at any given time.  Critical sections can serialize the execution of
    threads, which significantly reduces performance and scalability.This paper
    proposes Accelerated Critical Sections (ACS), a technique that leverages
    the high-performance core(s) of an Asymmetric Chip Multiprocessor (ACMP) to
    accelerate the execution of critical sections.  In ACS, selected critical
    sections are executed by a high-performance core, which can execute the
    critical section faster than the other, smaller cores.  As a result, ACS
    reduces serialization: it lowers the likelihood of threads waiting for a
    critical section to finish.  Our evaluation on a set of 12
    critical-section-intensive workloads shows that ACS reduces the average
    execution time by 34% compared to an equal-area 32T-core symmetric CMP and
    by 23% compared to an equal-area ACMP.  Moreover, for 7 out of the 12
    workloads, ACS improves scalability by increasing the number of threads at
    which performance saturates.", 
  location     = "https://doi.org/10.1145/2528521.1508274", 
  location     = "https://users.ece.cmu.edu/~omutlu/pub/acs_asplos09.pdf"
}

@Article{cafsplptstbp,
  author       = "Farhana Aleen and Nathan Clark",
  title        = "Commutativity analysis for software parallelization: letting program transformations see the big picture",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "241--252",
  month        = mar,
  keywords     = "automatic software parallelization, random interpretation,
    commutative functions, compiler analysis",
  abstract     = "Extracting performance from many-core architectures requires software engineers to create multi-threaded applications, which significantly complicates the already daunting task of software development.  One solution to this problem is automatic compile-time parallelization, which can ease the burden on software developers in many situations.  Clearly, automatic parallelization in its present form is not suitable for many application domains and new compiler analyses are needed address its shortcomings.In this paper, we present one such analysis: a new approach for detecting commutative functions.  Commutative functions are sections of code that can be executed in any order without affecting the outcome of the application, e.g., inserting elements into a set.  Previous research on this topic had one significant limitation, in that the results of a commutative functions must produce identical memory layouts.  This prevented previous techniques from detecting functions like malloc, which may return different pointers depending on the order in which it is called, but these differing results do not affect the overall output of the application.  Our new commutativity analysis correctly identify these situations to better facilitate automatic parallelization.  We demonstrate that this analysis can automatically extract significant amounts of parallelism from many applications, and where it is ineffective it can provide software developers a useful list of functions that may be commutative provided semantic program changes that are not automatable.",
  location     = "https://doi.org/10.1145/2528521.1508273"
}

@Article{daftledbscoplam,
  author       = "Aayush Gupta and Youngjae Kim and Bhuvan Urgaonkar",
  title        = "{DFTL}: a flash translation layer employing demand-based selective caching of page-level address mappings",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "229--240",
  month        = mar,
  keywords     = "flash management, flash translation layer, storage systems,
    dynamic ftl, simulation, enterprise storage systems, disk replacements",
  abstract     = "Recent technological advances in the development of
    flash-memory based devices have consolidated their leadership position as
    the preferred storage media in the embedded systems market and opened new
    vistas for deployment in enterprise-scale storage systems.  Unlike hard
    disks, flash devices are free from any mechanical moving parts, have no
    seek or rotational delays and consume lower power.  However, the internal
    idiosyncrasies of flash technology make its performance highly dependent on
    workload characteristics.  The poor performance of random writes has been a
    cause of major concern, which needs to be addressed to better utilize the
    potential of flash in enterprise-scale environments.  We examine one of the
    important causes of this poor performance: the design of the Flash
    Translation Layer (FTL), which performs the virtual-to-physical address
    translations and hides the erase-before-write characteristics of flash.  We
    propose a complete paradigm shift in the design of the core FTL engine from
    the existing techniques with our Demand-based Flash Translation Layer
    (DFTL), which selectively caches page-level address mappings.  We develop a
    flash simulation framework called FlashSim.  Our experimental evaluation
    with realistic enterprise-scale workloads endorses the utility of DFTL in
    enterprise-scale storage systems by demonstrating: (i) improved
    performance, (ii) reduced garbage collection overhead and (iii) better
    overload behavior compared to state-of-the-art FTL schemes.  For example, a
    predominantly random-write dominant I/O trace from an OLTP application
    running at a large financial institution shows a 78% improvement in average
    response time (due to a 3-fold reduction in operations of the garbage
    collector), compared to a state-of-the-art FTL scheme.  Even for the
    well-known read-dominant TPC-H benchmark, for which DFTL introduces
    additional overheads, we improve system response time by 56%.", 
  location     = "https://doi.org/10.1145/2528521.1508271", 
  location     = "http://www.cse.psu.edu/~buu1/papers/ps/dftl-asplos09.pdf"
}

@Article{gufmtbfpecfdia,
  author       = "Adrian~M. Caulfield and Laura~M. Grupp and Steven Swanson",
  title        = "Gordon: using flash memory to build fast, power-efficient clusters for data-intensive applications",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "217--228",
  month        = mar,
  keywords     = "cluster architecture, data centric computing, flash memory,
    solid-state storage, hadoop, design trade-offs",
  abstract     = "As our society becomes more information-driven, we have begun
    to amass data at an astounding and accelerating rate.  At the same time,
    power concerns have made it difficult to bring the necessary processing
    power to bear on querying, processing, and understanding this data.  We
    describe Gordon, a system architecture for data-centric applications that
    combines low-power processors, flash memory, and data-centric programming
    systems to improve performance for data-centric applications while reducing
    power consumption.  The paper presents an exhaustive analysis of the design
    space of Gordon systems, focusing on the trade-offs between power, energy,
    and performance that Gordon must make.  It analyzes the impact of
    flash-storage and the Gordon architecture on the performance and power
    efficiency of data-centric applications.  It also describes a novel flash
    translation layer tailored to data intensive workloads and large flash
    storage arrays.  Our data show that, using technologies available in the
    near future, Gordon systems can out-perform disk-based clusters by 1.5× and
    deliver up to 2.5× more performance per Watt.", 
  location     = "https://doi.org/10.1145/2528521.1508270", 
  location     = "http://cseweb.ucsd.edu/~acaulfie/papers/Asplos2009_Gordon.pdf"
}

@Article{pesip,
  author       = "David Meisner and Brian~T. Gold and Thomas~F. Wenisch",
  title        = "{PowerNap}: eliminating server idle power",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "205--216",
  month        = mar,
  keywords     = "power management, servers, binary devices, dvfs, efficiency
    ranges", 
  abstract     = "Data center power consumption is growing to unprecedented
    levels: the EPA estimates U.S.  data centers will consume 100 billion
    kilowatt hours annually by 2011.  Much of this energy is wasted in idle
    systems: in typical deployments, server utilization is below 30%, but idle
    servers still consume 60% of their peak power draw.  Typical idle periods
    though frequent--last seconds or less, confounding simple
    energy-conservation approaches.In this paper, we propose PowerNap, an
    energy-conservation approach where the entire system transitions rapidly
    between a high-performance active state and a near-zero-power idle state in
    response to instantaneous load.  Rather than requiring fine-grained
    power-performance states and complex load-proportional operation from each
    system component, PowerNap instead calls for minimizing idle power and
    transition time, which are simpler optimization goals.  Based on the
    PowerNap concept, we develop requirements and outline mechanisms to
    eliminate idle power waste in enterprise blade servers.  Because PowerNap
    operates in low-efficiency regions of current blade center power supplies,
    we introduce the Redundant Array for Inexpensive Load Sharing (RAILS), a
    power provisioning approach that provides high conversion efficiency across
    the entire range of PowerNap's power demands.  Using utilization traces
    collected from enterprise-scale commercial deployments, we demonstrate
    that, together, PowerNap and RAILS reduce average server power consumption
    by 74%.", 
  location     = "https://doi.org/10.1145/2528521.1508269", 
  location     = "https://web.eecs.umich.edu/~twenisch/papers/asplos09.pdf"
}

@Article{eovwde,
  author       = "Joseph Tucek and Weiwei Xiong and Yuanyuan Zhou",
  title        = "Efficient online validation with delta execution",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "193--204",
  month        = mar,
  keywords     = "delta execution, patch validation, testing, fork-join,
    redundant execution",
  abstract     = "Software systems are constantly changing.  Patches to fix
    bugs and patches to add features are all too common.  Every change risks
    breaking a previously working system.  Hence administrators loathe change,
    and are willing to delay even critical security patches until after fully
    validating their correctness.  Compared to off-line validation, on-line
    validation has clear advantages since it tests against real life workloads.
    Yet unfortunately it imposes restrictive overheads as it requires running
    the old and new versions side-by-side.  Moreover, due to spurious
    differences (e.g.  event timing, random number generation, and thread
    interleavings), it is difficult to compare the two for validation.To allow
    more effective on-line patch validation, we propose a new mechanism, called
    delta execution, that is based on the observation that most patches are
    small.  Delta execution merges the two side-by-side executions for most of
    the time and splits only when necessary, such as when they access different
    data or execute different code.  This allows us to perform on-line
    validation not only with lower overhead but also with greatly reduced
    spurious differences, allowing us to effectively validate changes.We first
    validate the feasibility of our idea by studying the characteristics of 240
    patches from 4 server programs; our examination shows that 77% of the
    changes should not be expected to cause large changes and are thereby
    feasible for Delta execution.  We then implemented Delta execution using
    dynamic instrumentation.  Using real world patches from 7 server
    applications and 3 other programs, we compared our implementation of Delta
    execution against a traditional side-by-side on-line validation.  Delta
    execution outperformed traditional validation by up to 128%; further, for 3
    of the changes, spurious differences caused the traditional validation to
    fail completely while Delta execution succeeded.  This demonstrates that
    Delta execution can allow administrators to use on-line validation to
    confidently ensure the correctness of the changes they apply.", 
  location     = "https://doi.org/10.1145/2528521.1508267", 
  location     = "http://opera.ucsd.edu/paper/asplos105-tucek.ps"
}

@Article{ideiicp,
  author       = "Sriram Rajamani and G.~Ramalingam and Venkatesh Prasad Ranganath and Kapil Vaswani",
  title        = "{ISOLATOR}: dynamically ensuring isolation in concurrent programs",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "181--192",
  month        = mar,
  keywords     = "concurrency, storage isolation, memory protection, optimistic
    concurrency control, noninterference",
  abstract     = "In this paper, we focus on concurrent programs that use locks
    to achieve isolation of data accessed by critical sections of code.  We
    present ISOLATOR, an algorithm that guarantees isolation for well-behaved
    threads of a program that obey a locking discipline even in the presence of
    ill-behaved threads that disobey the locking discipline.  ISOLATOR uses
    code instrumentation, data replication, and virtual memory protection to
    detect isolation violations and delays ill-behaved threads to ensure
    isolation.  Our instrumentation scheme requires access only to the code of
    well-behaved threads.  We have evaluated ISOLATOR on several benchmark
    programs and found that ISOLATOR can ensure isolation with reasonable
    runtime overheads.  In addition, we present three general desiderata -
    safety, isolation, and permissiveness - for any scheme that attempts to
    ensure isolation, and formally prove that ISOLATOR satisfies all of these
    desiderata.", 
  location     = "https://doi.org/10.1145/2528521.1508266", 
  location     = "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/Isolator20-20Dynamically20Ensuring20Isolation20in20Concurrent20Programs.pdf"
}

@Article{mmmr,
  author       = "Philip~M. Wells and Koushik Chakraborty and Gurindar~S. Sohi",
  title        = "Mixed-mode multicore reliability",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "169--180",
  month        = mar,
  keywords     = "multicore, dual-modular redundancy, performance-protection
    trade-off", 
  abstract     = "Future processors are expected to observe increasing rates of
    hardware faults.  Using Dual-Modular Redundancy (DMR), two cores of a
    multicore can be loosely coupled to redundantly execute a single software
    thread, providing very high coverage from many difference sources of
    faults.  This reliability, however, comes at a high price in terms of
    per-thread IPC and overall system throughput.We make the observation that a
    user may want to run both applications requiring high reliability, such as
    financial software, and more fault tolerant applications requiring high
    performance, such as media or web software, on the same machine at the same
    time.  Yet a traditional DMR system must fully operate in redundant mode
    whenever any application requires high reliability.This paper proposes a
    Mixed-Mode Multicore (MMM), which enables most applications, including the
    system software, to run with high reliability in DMR mode, while
    applications that need high performance can avoid the penalty of DMR.
    Though conceptually simple, two key challenges arise: 1) care must be taken
    to protect reliable applications from any faults occurring to applications
    running in high performance mode, and 2) the desire to execute additional
    independent software threads for a performance application complicates the
    scheduling of computation to cores.  After solving these issues, an MMM is
    shown to improve overall system performance, compared to a traditional DMR
    system, by approximately 2X when one reliable and one performance
    application are concurrently executing.", 
  location     = "https://doi.org/10.1145/2528521.1508265", 
  location     = "ftp://ftp.cs.wisc.edu/sohi/papers/2009/asplos2009-mixed-mode.pdf"
}

@Article{eewachtmi,
  author       = "Dave Dice and Yossi Lev and Mark Moir and Daniel Nussbaum",
  title        = "Early experience with a commercial hardware transactional memory implementation",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "157--168",
  month        = mar,
  keywords     = "hardware, transactional storage, synchronization, hardware
    design, error recovery",
  abstract     = "We report on our experience with the hardware transactional
    memory (HTM) feature of two pre-production revisions of a new commercial
    multicore processor.  Our experience includes a number of promising results
    using HTM to improve performance in a variety of contexts, and also
    identifies some ways in which the feature could be improved to make it even
    better.  We give detailed accounts of our experiences, sharing techniques
    we used to achieve the results we have, as well as describing challenges we
    faced in doing so.",
  location     = "https://doi.org/10.1145/2528521.1508263"
}

@Article{mbfamh,
  author       = "Owen~S. Hofmann and Christopher~J. Rossbach and Emmett Witchel",
  title        = "Maximum benefit from a minimal {HTM}",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "145--156",
  month        = mar,
  keywords     = "hardware transactional storage, linux kernels,
    synchronization, cooperative synchronization",
  abstract     = "A minimal, bounded hardware transactional memory
    implementation significantly improves synchronization performance when used
    in an operating system kernel.  We add HTM to Linux 2.4, a kernel with a
    simple, coarse-grained synchronization structure.  The transactional Linux
    2.4 kernel can improve performance of user programs by as much as 40% over
    the non-transactional 2.4 kernel.  It closes 68% of the performance gap
    with the Linux 2.6 kernel, which has had significant engineering effort
    applied to improve scalability.We then extend our minimal HTM to a fast,
    unbounded transactional memory with a novel technique for coordinating
    hardware transactions and software synchronization.  Overflowed
    transactions run in software, with only a minimal coupling between hardware
    and software systems.  There is no performance penalty for overflow rates
    of less than 1%.  In one instance, at 16 processors and an overflow rate of
    4%, performance degrades from an ideal 4.3x to 3.6x.", 
  location     = "https://doi.org/10.1145/2528521.1508262", 
  location     = "https://www.cs.utexas.edu/users/witchel/pubs/hofmann09asplos.pdf"
}

@Article{ptcaisp,
  author       = "Stijn Eyerman and Lieven Eeckhout",
  title        = "Per-thread cycle accounting in {SMT} processors",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "133--144",
  month        = mar,
  keywords     = "simultaneous multithreading, smt, cycle accounting,
    thread-progress aware fetch policy, measurement, estimation",
  abstract     = "This paper proposes a cycle accounting architecture for
    Simultaneous Multithreading (SMT) processors that estimates the execution
    times for each of the threads had they been executed alone, while they are
    running simultaneously on the SMT processor.  This is done by accounting
    each cycle to either a base, miss event or waiting cycle component during
    multi-threaded execution.  Single-threaded alone execution time is then
    estimated as the sum of the base and miss event components; the waiting
    cycle component represents the lost cycle count due to SMT execution.  The
    cycle accounting architecture incurs reasonable hardware cost (around 1KB
    of storage) and estimates single-threaded performance with average
    prediction errors around 7.2% for two-program workloads and 11.7% for
    four-program workloads.The cycle accounting architecture has several
    important applications to system software and its interaction with SMT
    hardware.  For one, the estimated single-thread alone execution time
    provides an accurate picture to system software of the actually consumed
    processor cycles per thread.  The alone execution time instead of the total
    execution time (timeslice) may make system software scheduling policies
    more effective.  Second, a new class of thread-progress aware SMT fetch
    policies based on per-thread progress indicators enable system software
    level priorities to be enforced at the hardware level.", 
  location     = "https://doi.org/10.1145/2528521.1508260", 
  location     = "https://www.elis.ugent.be/~leeckhou/papers/asplos09.pdf"
}

@Article{ralmrcocsfoo,
  author       = "David~K. Tam and Reza Azimi and Livio~B. Soares and Michael Stumm",
  title        = "{RapidMRC}: approximating {L2} miss rate curves on commodity systems for online optimizations",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "121--132",
  month        = mar,
  keywords     = "approximations, estimations, sampling, cache misses,
    predictions, cache management, ",
  abstract     = "Miss rate curves (MRCs) are useful in a number of contexts.
    In our research, online L2 cache MRCs enable us to dynamically identify
    optimal cache sizes when cache-partitioning a shared-cache multicore
    processor.  Obtaining L2 MRCs has generally been assumed to be expensive
    when done in software and consequently, their usage for online
    optimizations has been limited.  To address these problems and
    opportunities, we have developed a low-overhead software technique to
    obtain L2 MRCs online on current processors, exploiting features available
    in their performance monitoring units so that no changes to the application
    source code or binaries are required.  Our technique, called RapidMRC,
    requires a single probing period of roughly 221 million processor cycles
    (147 ms), and subsequently 124 million cycles (83 ms) to process the data.
    We demonstrate its accuracy by comparing the obtained MRCs to the actual L2
    MRCs of 30 applications taken from SPECcpu2006, SPECcpu2000, and
    SPECjbb2000.  We show that RapidMRC can be applied to sizing cache
    partitions, helping to achieve performance improvements of up to 27%.", 
  location     = "https://doi.org/10.1145/2528521.1508259"
}

@Article{ciftftgu,
  author       = "Mohit Tiwari and Hassan M.G.~Wassel and Bita Mazloom and Shashidhar Mysore and Frederic~T. Chong and Timothy Sherwood",
  title        = "Complete information flow tracking from the gates up",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "109--120",
  month        = mar,
  keywords     = "information flow, information tracking, tagged architecture,
    information leaks, non-turing architectures",
  abstract     = "For many mission-critical tasks, tight guarantees on the flow
    of information are desirable, for example, when handling important
    cryptographic keys or sensitive financial data.  We present a novel
    architecture capable of tracking all information flow within the machine,
    including all explicit data transfers and all implicit flows (those subtly
    devious flows caused by not performing conditional operations).  While the
    problem is impossible to solve in the general case, we have created a
    machine that avoids the general-purpose programmability that leads to this
    impossibility result, yet is still programmable enough to handle a variety
    of critical operations such as public-key encryption and authentication.
    Through the application of our novel gate-level information flow tracking
    method, we show how all flows of information can be precisely tracked.
    From this foundation, we then describe how a class of architectures can be
    constructed, from the gates up, to completely capture all information flows
    and we measure the impact of doing so on the hardware implementation, the
    ISA, and the programmer.", 
  location     = "https://doi.org/10.1145/2528521.1508258", 
  location     = "https://people.cs.uchicago.edu/~ftchong/papers/ASPLOS-09-glift.pdf"
}

@Article{kedmis,
  author       = "Marek Olszewski and Jason Ansel and Saman Amarasinghe",
  title        = "Kendo: efficient deterministic multithreading in software",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "97--108",
  month        = mar,
  keywords     = "deterministic multithreading, determinism, parallel
    programming, debugging, multicore, serialization, logical time, concurrency
    management",
  abstract     = "Although chip-multiprocessors have become the industry
    standard, developing parallel applications that target them remains a
    daunting task.  Non-determinism, inherent in threaded applications, causes
    significant challenges for parallel programmers by hindering their ability
    to create parallel applications with repeatable results.  As a consequence,
    parallel applications are significantly harder to debug, test, and maintain
    than sequential programs.This paper introduces Kendo: a new software-only
    system that provides deterministic multithreading of parallel applications.
    Kendo enforces a deterministic interleaving of lock acquisitions and
    specially declared non-protected reads through a novel dynamically
    load-balanced deterministic scheduling algorithm.  The algorithm tracks the
    progress of each thread using performance counters to construct a
    deterministic logical time that is used to compute an interleaving of
    shared data accesses that is both deterministic and provides good load
    balancing.  Kendo can run on today's commodity hardware while incurring
    only a modest performance cost.  Experimental results on the SPLASH-2
    applications yield a geometric mean overhead of only 16% when running on 4
    processors.  This low overhead makes it possible to benefit from Kendo even
    after an application is deployed.  Programmers can start using Kendo today
    to program parallel applications that are easier to develop, debug, and
    test.", 
  location     = "https://doi.org/10.1145/2528521.1508256", 
  location     = "http://groups.csail.mit.edu/commit/papers/09/asplos073-olszewski.pdf"
}

@Article{ddsmm,
  author       = "Joseph Devietti and Brandon Lucia and Luis Ceze and Mark Oskin",
  title        = "{DMP}: deterministic shared memory multiprocessing",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "85--96",
  month        = mar,
  keywords     = "deterministic parallel execution, deterministic smm,
    serialization",
  abstract     = "Current shared memory multicore and multiprocessor systems
    are nondeterministic.  Each time these systems execute a multithreaded
    application, even if supplied with the same input, they can produce a
    different output.  This frustrates debugging and limits the ability to
    properly test multithreaded code, becoming a major stumbling block to the
    much-needed widespread adoption of parallel programming.In this paper we
    make the case for fully deterministic shared memory multiprocessing (DMP).
    The behavior of an arbitrary multithreaded program on a DMP system is only
    a function of its inputs.  The core idea is to make inter-thread
    communication fully deterministic.  Previous approaches to coping with
    nondeterminism in multithreaded programs have focused on replay, a
    technique useful only for debugging.  In contrast, while DMP systems are
    directly useful for debugging by offering repeatability by default, we
    argue that parallel programs should execute deterministically in the field
    as well.  This has the potential to make testing more assuring and increase
    the reliability of deployed multithreaded software.  We propose a range of
    approaches to enforcing determinism and discuss their implementation
    trade-offs.  We show that determinism can be provided with little
    performance cost using our architecture proposals on future hardware, and
    that software-only approaches can be utilized on existing systems.", 
  location     = "https://doi.org/10.1145/2528521.1508255",
  location     = "http://www.cis.upenn.edu/~devietti/papers/devietti.dmp.toppicks.2010.pdf"
}

@Article{cashifpdmr,
  author       = "Pablo Montesinos and Matthew Hicks and Samuel~T. King and Josep Torrellas",
  title        = "Capo: a software-hardware interface for practical deterministic multiprocessor replay",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "73--84",
  month        = mar,
  keywords     = "capo, capoone, deterministic replay, replay sphere, divide
    and conquer, ",
  abstract     = "While deterministic replay of parallel programs is a powerful 
    technique, current proposals have shortcomings.  Specifically,
    software-based replay systems have high overheads on multiprocessors, while
    hardware-based proposals focus only on basic hardware-level mechanisms,
    ignoring the overall replay system.  To be practical, hardware-based replay
    systems need to support an environment with multiple parallel jobs running
    concurrently -- some being recorded, others being replayed and even others
    running without recording or replay.  Moreover, they need to manage
    limited-size log buffers.This paper addresses these shortcomings by
    introducing, for the first time, a set of abstractions and a
    software-hardware interface for practical hardware-assisted replay of
    multiprocessor systems.  The approach, called Capo, introduces the novel
    abstraction of the Replay Sphere to separate the responsibilities of the
    hardware and software components of the replay system.  In this paper, we
    also design and build CapoOne, a prototype of a deterministic
    multiprocessor replay system that implements Capo using Linux and simulated
    DeLorean hardware.  Our evaluation of 4-processor executions shows that
    CapoOne largely records with the efficiency of hardware-based schemes and
    the flexibility of software-based schemes.", 
  location     = "https://doi.org/10.1145/2528521.1508254", 
  location     = "https://iacoma.cs.uiuc.edu/iacoma-papers/asplos09.pdf"
}

@Article{abbpiavaaafsd,
  author       = "Martin Dimitrov and Huiyang Zhou",
  title        = "Anomaly-based bug prediction, isolation, and validation: an automated approach for software debugging",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "61--72",
  month        = mar,
  keywords     = "automated debugging, architectural support, anomaly
    detection, fault isolation, ",
  abstract     = "Software defects, commonly known as bugs, present a serious
    challenge for system reliability and dependability.  Once a program failure
    is observed, the debugging activities to locate the defects are typically
    nontrivial and time consuming.  In this paper, we propose a novel automated
    approach to pin-point the root-causes of software failures.Our proposed
    approach consists of three steps.  The first step is bug prediction, which
    leverages the existing work on anomaly-based bug detection as exceptional
    behavior during program execution has been shown to frequently point to the
    root cause of a software failure.  The second step is bug isolation, which
    eliminates false-positive bug predictions by checking whether the dynamic
    forward slices of bug predictions lead to the observed program failure.
    The last step is bug validation, in which the isolated anomalies are
    validated by dynamically nullifying their effects and observing if the
    program still fails.  The whole bug prediction, isolation and validation
    process is fully automated and can be implemented with efficient
    architectural support.  Our experiments with 6 programs and 7 bugs,
    including a real bug in the gcc 2.95.2 compiler, show that our approach is
    highly effective at isolating only the relevant anomalies.  Compared to
    state-of-art debugging techniques, our proposed approach pinpoints the
    defect locations more accurately and presents the user with a much smaller
    code set to analyze.", 
  location     = "https://doi.org/10.1145/2528521.1508252", 
  location     = "https://people.engr.ncsu.edu/hzhou/asplos09.pdf"
}

@Article{rdaopfros,
  author       = "Andrew Lenharth and Vikram~S. Adve and Samuel~T. King",
  title        = "Recovery domains: an organizing principle for recoverable operating systems",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "49--60",
  month        = mar,
  keywords     = "akeso, recovery domains, automatic fault recovery, rollback,
    fault isolation, logging",
  abstract     = "We describe a strategy for enabling existing commodity
    operating systems to recover from unexpected run-time errors in nearly any
    part of the kernel, including core kernel components.  Our approach is
    dynamic and request-oriented; it isolates the effects of a fault to the
    requests that caused the fault rather than to static kernel components.
    This approach is based on a notion of 'recovery domains,' an organizing
    principle to enable rollback of state affected by a request in a
    multithreaded system with minimal impact on other requests or threads.  We
    have applied this approach on v2.4.22 and v2.6.27 of the Linux kernel and
    it required 132 lines of changed or new code: the other changes are all
    performed by a simple instrumentation pass of a compiler.  Our experiments
    show that the approach is able to recover from otherwise fatal faults with
    minimal collateral impact during a recovery event.", 
  location     = "https://doi.org/10.1145/2528521.1508251", 
  location     = "https://llvm.org/pubs/2009-03-ASPLOS-Recovery.pdf"
}

@Article{aasshurp,
  author       = "Stelios Sidiroglou and Oren Laadan and Carlos Perez and Nicolas Viennot and Jason Nieh and Angelos~D. Keromytis",
  title        = "{ASSURE}: automatic software self-healing using rescue points",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "37--48",
  month        = mar,
  keywords     = "software self-healing, error recovery, reliable software,
    binary patching, checkpoint restart, rescue points, fault detection, fault
    replay",
  abstract     = "Software failures in server applications are a significant
    problem for preserving system availability.  We present ASSURE, a system
    that introduces rescue points that recover software from unknown faults
    while maintaining both system integrity and availability, by mimicking
    system behavior under known error conditions.  Rescue points are locations
    in existing application code for handling a given set of
    programmer-anticipated failures, which are automatically repurposed and
    tested for safely enabling fault recovery from a larger class of
    (unanticipated) faults.  When a fault occurs at an arbitrary location in
    the program, ASSURE restores execution to an appropriate rescue point and
    induces the program to recover execution by virtualizing the program's
    existing error-handling facilities.  Rescue points are identified using
    fuzzing, implemented using a fast coordinated checkpoint-restart mechanism
    that handles multi-process and multi-threaded applications, and, after
    testing, are injected into production code using binary patching.  We have
    implemented an ASSURE Linux prototype that operates without application
    source code and without base operating system kernel changes.  Our
    experimental results on a set of real-world server applications and bugs
    show that ASSURE enabled recovery for all of the bugs tested with fast
    recovery times, has modest performance overhead, and provides automatic
    self-healing orders of magnitude faster than current human-driven patch
    deployment methods.", 
  location     = "https://doi.org/10.1145/2528521.1508250",
  location     = "https://www.cs.columbia.edu/~angelos/Papers/2009/assure.pdf"
}

@Article{ceavbfthp,
  author       = "Soyeon Park and Shan Lu and Yuanyuan Zhou",
  title        = "{CTrigger}: exposing atomicity violation bugs from their hiding places",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "25--36",
  month        = mar,
  keywords     = "software testing, concurrency errors, unserializability,
    atomicity violations, stress testing, interference",
  abstract     = "Multicore hardware is making concurrent programs pervasive.
    Unfortunately, concurrent programs are prone to bugs.  Among different
    types of concurrency bugs, atomicity violation bugs are common and
    important.  Existing techniques to detect atomicity violation bugs suffer
    from one limitation: requiring bugs to manifest during monitored runs,
    which is an open problem in concurrent program testing.This paper makes two
    contributions.  First, it studies the interleaving characteristics of the
    common practice in concurrent program testing (i.e., running a program over
    and over) to understand why atomicity violation bugs are hard to expose.
    Second, it proposes CTrigger to effectively and efficiently expose
    atomicity violation bugs in large programs.  CTrigger focuses on a special
    type of interleavings (i.e., unserializable interleavings) that are
    inherently correlated to atomicity violation bugs, and uses trace analysis
    to systematically identify (likely) feasible unserializable interleavings
    with low occurrence-probability.  CTrigger then uses minimum execution
    perturbation to exercise low-probability interleavings and expose
    difficult-to-catch atomicity violation.We evaluate CTrigger with real-world
    atomicity violation bugs from four sever/desktop applications (Apache,
    MySQL, Mozilla, and PBZIP2) and three SPLASH2 applications on 8-core
    machines.  CTrigger efficiently exposes the tested bugs within 1--235
    seconds, two to four orders of magnitude faster than stress testing.
    Without CTrigger, some of these bugs do not manifest even after 7 full days
    of stress testing.  In addition, without deterministic replay support, once
    a bug is exposed, CTrigger can help programmers reliably reproduce it for
    diagnosis.  Our tested bugs are reproduced by CTrigger mostly within 5
    seconds, 300 to over 60000 times faster than stress testing.", 
  location     = "https://doi.org/10.1145/2528521.1508249",
  location     = "http://opera.ucsd.edu/paper/asplos092-zhou.pdf"
}

@Article{aionisac,
  author       = "Constantin Pistol and Wutichai Chongchitmate and Christopher Dwyer and Alvin~R. Lebeck",
  title        = "Architectural implications of nanoscale integrated sensing and computing",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "13--24",
  month        = mar,
  keywords     = "sensor networks, self-assembled systems, dna architectures,
    molecular isa",
  abstract     = "This paper explores the architectural implications of
    integrating computation and molecular probes to form nanoscale sensor
    processors (nSP).  We show how nSPs may enable new computing domains and
    automate tasks that currently require expert scientific training and costly
    equipment.  This new application domain severely constrains nSP size, which
    significantly impacts the architectural design space.  In this context, we
    explore nSP architectures and present an nSP design that includes a simple
    accumulator-based ISA, sensors, limited memory and communication
    transceivers.  To reduce the application memory footprint, we introduce the
    concept of instruction-fused sensing.  We use simulation and analytical
    models to evaluate nSP designs executing a representative set of target
    applications.  Furthermore, we propose a candidate nSP technology based on
    optical Resonance Energy Transfer (RET) logic that enables the small size
    required by the application domain; our smallest design is about the size
    of the largest known virus.  We also show laboratory results that
    demonstrate initial steps towards a prototype.",
  location     = "https://doi.org/10.1145/2528521.1508247",
  location     = "http://www.cs.duke.edu/~alvy/papers/asplos071-lebeck.pdf"
}

@Article{aeottcs,
  author       = "Mark Gebhart and Bertrand~A. Maher and Katherine~E. Coons and Jeff Diamond and Paul Gratz and Mario Marino and Nitya Ranganathan and Behnam Robatmili and Aaron Smith and James Burrill and Stephen~W. Keckler and Doug Burger and Kathryn~S. McKinley",
  title        = "An evaluation of the {TRIPS} computer system",
  journal      = asplos09,
  year         = 2009,
  volume       = 44,
  number       = 3,
  pages        = "1--12",
  month        = mar,
  keywords     = "performance, measurement, explicit data graph execution,
    superblocks, isa",
  abstract     = "The TRIPS system employs a new instruction set architecture (ISA) called Explicit Data Graph Execution (EDGE) that renegotiates the boundary between hardware and software to expose and exploit concurrency.  EDGE ISAs use a block-atomic execution model in which blocks are composed of dataflow instructions.  The goal of the TRIPS design is to mine concurrency for high performance while tolerating emerging technology scaling challenges, such as increasing wire delays and power consumption.  This paper evaluates how well TRIPS meets this goal through a detailed ISA and performance analysis.  We compare performance, using cycles counts, to commercial processors.  On SPEC CPU2000, the Intel Core 2 outperforms compiled TRIPS code in most cases, although TRIPS matches a Pentium 4.  On simple benchmarks, compiled TRIPS code outperforms the Core 2 by 10% and hand-optimized TRIPS code outperforms it by factor of 3.  Compared to conventional ISAs, the block-atomic model provides a larger instruction window, increases concurrency at a cost of more instructions executed, and replaces register and memory accesses with more efficient direct instruction-to-instruction communication.  Our analysis suggests ISA, microarchitecture, and compiler enhancements for addressing weaknesses in TRIPS and indicates that EDGE architectures have the potential to exploit greater concurrency in future technologies.",
  location     = "https://doi.org/10.1145/2528521.1508246"
}

@Article{osalosp,
  author       = "Li Wang and Xuejun Yang and Jingling Xue and Yu Deng and Xiaobo Yan and Tao Tang and Quan Hoang Nguyen",
  title        = "Optimizing Scientific Application Loops on Stream Processors",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "161--170",
  month        = jul,
  keywords     = "stream processor, streaming, loop optimization, data reuse,
    prefetching, graph coloring, software-managed cache",
  abstract     = "This paper describes a graph coloring compiler framework to
    allocate on-chip SRF (Stream Register File) storage for optimizing
    scientific applications on stream processors.  Our framework consists of
    first applying enabling optimizations such as loop unrolling to expose
    stream reuse and opportunities for maximizing parallelism, i.e.,
    overlapping kernel execution and memory transfers.Then the three SRF
    management tasks are solved in a unified manner via graph coloring: (1)
    placing streams in the SRF, (2) exploiting stream use, and (3) maximizing
    parallelism.  We evaluate the performance of our compiler framework by
    actually running nine representative scientific computing kernels on our
    FT64 stream processor.  Our preliminary results show that compiler
    management achieves an average speedup of 2.3x compared to First-Fit
    allocation.  In comparison with the performance results obtained from
    running these benchmarks on Itanium 2, an average speedup of 2.1x is
    observed.", 
  location     = "https://doi.org/10.1145/1379023.1375679"
}

@Article{parbrafcgra,
  author       = "Bjorn De Sutter and Paul Coene and Tom Vander Aa and Bingfeng Mei",
  title        = "Placement-and-routing-based register allocation for coarse-grained reconfigurable arrays",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "151--160",
  month        = jul,
  keywords     = "register allocation, placement and routing, coarse-grained
  parallelism, reconfigurable arrays",
  abstract     = "DSP architectures often feature multiple register files with
    sparse connections to a large set of ALUs.  For such DSPs, traditional
    register allocation algorithms suffer from a lot of problems, including a
    lack of retargetability and phase-ordering problems.  This paper studies
    alternative register allocation techniques based on placement and routing.
    Different register file models are studied and evaluated on a state-of-the
    art coarse-grained reconfigurable array DSP, together with a new post-pass
    register allocator for rotating register files.", 
  location     = "http://users.elis.ugent.be/~brdsutte/research/publications/2008LCTESdesutter.pdf"
}

@Article{pppratmlud,
  author       = "Mounira Bachir and Sid-Ahmed-Ali Touati and Albert Cohen",
  title        = "Post-pass periodic register allocation to minimise loop unrolling degree",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "141--150",
  month        = jul,
  keywords     = "periodic register allocation, software pipelining, loop
    unrolling, embedded code optimization",
  abstract     = "This paper solves an open problem regarding loop unrolling
    after periodic register allocation.  Although software pipelining is a
    powerful technique to extract fine-grain parallelism, it generates reuse
    circuits spanning multiple loop iterations.  These circuits require
    periodic register allocation, which in turn yield a code generation
    challenge, generally addressed through: (1) hardware support --- rotating
    register files --- deemed too expensive for embedded processors, (2)
    insertion of register moves with a high risk of reducing the computation
    throughput --- initiation interval (II) --- of software pipelining, and (3)
    post-pass loop unrolling that does not compromise throughput but often
    leads to unpractical code growth.  The latter approach relies on the proof
    that MAXLIVE registers are sufficient for periodic register allocation (2;
    3; 5); yet the only heuristic to control the amount of post-pass loop
    unrolling does not achieve this bound and leads to undesired register
    spills (4; 7).We propose a periodic register allocation technique allowing
    a software-only code generation that does not trade the optimality of the
    II for compactness of the generated code.  Our idea is based on using the
    remaining registers: calling Rarch the number of architectural registers of
    the target processor, then the number of remaining registers that can be
    used for minimising the unrolling degree is equal to Rarch-MAXLIVE.We
    provide a complete formalisation of the problem and algorithm, followed by
    extensive experiments.  We achieve practical loop unrolling degrees in most
    cases --- with no increase of the II --- while state-of-the-art techniques
    would either induce register spilling, degrade the II or lead to
    unacceptable code growth.",
  location     = "https://doi.org/10.1145/1379023.1375677",
  location     = "https://hal.inria.fr/inria-00637218/document"
}

@Article{daeoacfesp,
  author       = "Ryan~R. Newton and Lewis~D. Girod and Michael~B. Craig and Samuel~R. Madden and John Gregory Morrisett",
  title        = "Design and evaluation of a compiler for embedded stream programs",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "131--140",
  month        = jul,
  keywords     = "stream processing language, sensor networks, segmentation,
    scripting, optimizations, rewriting systems", 
  abstract     = "Applications that combine live data streams with embedded,
    parallel, and distributed processing are becoming more commonplace.
    WaveScript is a domain-specific language that brings high-level, type-safe,
    garbage-collected programming to these domains.  This is made possible by
    three primary implementation techniques, each of which leverages
    characteristics of the streaming domain.  First, we employ a novel
    evaluation strategy that uses a combination of interpretation and
    reification to partially evaluate programs into stream dataflow graphs.
    Second, we use profile-driven compilation to enable many optimizations that
    are normally only available in the synchronous (rather than asynchronous)
    dataflow domain.  Finally, we incorporate an extensible system for rewrite
    rules to capture algebraic properties in specific domains (such as signal
    processing).We have used our language to build and deploy a sensor network
    for the acoustic localization of wild animals, in particular, the
    Yellow-Bellied marmot.  We evaluate WaveScript's performance on this
    application, showing that it yields good performance on both embedded and
    desktop-class machines, including distributed execution and substantial
    parallel speedups.  Our language allowed us to implement the application
    rapidly, while outperforming a previous C implementation by over 35%, using
    fewer than half the lines of code.  We evaluate the contribution of our
    optimizations to this success.", 
  location     = "https://doi.org/10.1145/1379023.1375675", 
  location     = "http://db.csail.mit.edu/pubs/newton-lctes08.pdf"
}

@Article{cdmcgfsdfl,
  author       = "Dariusz Biernacki and Jean-Louis Colaço and Gregoire Hamon and Marc Pouzet",
  title        = "Clock-directed modular code generation for synchronous data-flow languages",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "121--130",
  month        = jul,
  keywords     = "real-time systems, synchronous languages, compilation,
    semantics, type systems, synchronization",
  abstract     = "The compilation of synchronous block diagrams into sequential
    imperative code has been addressed in the early eighties and can now be
    considered as folklore.  However, separate, or modular, code generation,
    though largely used in existing compilers and particularly in industrial
    ones, has never been precisely described or entirely formalized.  Such a
    formalization is now fundamental in the long-term goal to develop a
    mathematically certified compiler for a synchronous language as well as in
    simplifying existing implementations.This article presents in full detail
    the modular compilation of synchronous block diagrams into sequential code.
    We consider a first-order functional language reminiscent of LUSTRE, which
    it extends with a general n-ary merge operator, a reset construct, and a
    richer notion of clocks.  The clocks are used to express activation of
    computations in the program and are specifically taken into account during
    the compilation process to produce efficient imperative code.  We introduce
    a generic machine-based intermediate language to represent transition
    functions, and we present a concise clock-directed translation from the
    source to this intermediate language.  We address the target code
    generation phase by describing a translation from the intermediate language
    to JAVA and C.",
  location     = "https://doi.org/10.1145/1379023.1375674",
  location     = "https://www.di.ens.fr/~pouzet/bib/lctes08a.pdf"
}

@Article{eaeplborewa,
  author       = "Norman~H. Cohen and Karl Trygve Kalleberg",
  title        = "{EventScript}: an event-processing language based on regular expressions with actions",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "111--120",
  month        = jul,
  keywords     = "event processing, regular expressions, reactive programs,
    sensors, actuators",
  abstract     = "EventScript is a simple, powerful language for programming
    reactive processes.  An incoming-event stream is matched against a regular
    expression.  Actions embedded within the regular expression execute in
    response to pattern matching on events.  These actions include assigning
    computed values to variables and emitting output events.  The definition of
    EventScript presented a number of novel and interesting language-design
    choices.  EventScript has an efficient implementation, and has been used in
    a development environment for complex event-based applications.  We have
    used EventScript to program both small examples and large industrial
    applications.  Readers of EventScript programs find them easy to
    understand, and are comfortable with the familiar model of matching regular
    expressions.", 
  location     = "https://doi.org/10.1145/1379023.1375673"
}

@Article{atsftadohosdp,
  author       = "Gwenaël Delaval and Alain Girault and Marc Pouzet",
  title        = "{A} type system for the automatic distribution of higher-order synchronous dataflow programs",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "101--110",
  month        = jul,
  keywords     = "synchronous programming, distribution, type systems,
    functional programming, parallelism",
  abstract     = "We address the design of distributed systems with synchronous
    dataflow programming languages.  As modular design entails handling both
    architectural and functional modularity, our first contribution is to
    extend an existing synchronous dataflow programming language with
    primitives allowing the description of a distributed architecture and the
    localization of some expressions onto some processors.  We also present a
    distributed semantics to formalize the distributed execution of synchronous
    programs.  Our second contribution is to provide a type system, in order to
    infer the localization of non-annotated values by means of type inference
    and to ensure, at compilation time, the consistency of the distribution.
    Our third contribution is to provide a type-directed projection operation
    to obtain automatically, from a centralized typed program, the local
    program to be executed by each computing resource.  The type system as well
    as the automatic distribution mechanism has been fully implemented in the
    compiler of an existing synchronous data-flow programming language.", 
  location     = "https://doi.org/10.1145/1379023.1375672"
}

@Article{fafacaasfes,
  author       = "Jaejin Lee and Junghyun Kim and Choonki Jang and Seungkyun Kim and Bernhard Egger and Kwangsub Kim and SangYong Han",
  title        = "{FaCSim}: a fast and cycle-accurate architecture simulator for embedded systems",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "89--100",
  month        = jul,
  keywords     = "architecture simulator, simulator parallelization,
    cycle-accurate simulation, full-system simulation, virtual prototyping",
  abstract     = "There have been strong demands for a fast and cycle-accurate
    virtual platforms in the embedded systems area where developers can do
    meaningful software development including performance debugging in the
    context of the entire platform.  In this paper, we describe the design and
    implementation of a fast and cycle-accurate architecture simulator called
    FaCSim as a first step towards such a virtual platform.  FacSim accurately
    models the ARM9E-S processor core and ARM926EJ-S processor's memory
    subsystem.  It accurately simulates exceptions and interrupts to enable
    whole-system simulation including the OS.  Since it is implemented in a
    modular manner in C++, it can be easily extended with other system
    components by subclassing or adding new classes.  FaCSim is based on an
    interpretive simulation technique to provide flexibility, yet achieving
    high speed.  It enables fast cycle-accurate architecture simulation by
    means of three mechanisms.  First, it computes elapsed cycles in each
    pipeline stage as a chunk and incrementally adds it up to advance the core
    clock instead of performing cycle-by-cycle simulation.  Second, it uses a
    basic-block cache that caches decoded instructions at the basic-block
    level.  Finally, it is parallelized to exploit multicore systems that are
    available everywhere these days.  Using 21 applications from the EEMBC
    benchmark suite, FaCSim's accuracy is validated against the ARM926EJ-S
    development board from ARM, and is accurate in a ±7% error margin.  Due to
    basic-block level caching and parallelization, FaCSim is, on average, more
    than three times faster than ARMulator and more than six times faster than
    SimpleScalar.", 
  location     = "https://doi.org/10.1145/1379023.1375670"
}

@Article{adsifrc,
  author       = "Sanjay Rajopadhye and Gautam Gupta and Lakshminarayanan Renganarayana",
  title        = "A domain specific interconnect for reconfigurable computing",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "79--88",
  month        = jul,
  keywords     = "fpga, coarse-grain reconfiguration, silicon compilation",
  abstract     = "Affine Control Loops (ACLs) occur frequently in data- and 
    compute-intensive applications.  Implementing ACLs directly on dedicated
    hardware has the potential for spectacular performance improvement in area,
    time and energy.  An important challenge for such direct hardware
    compilation of ACLs is the interconnection between the different processing
    elements, which may be non-local as well as dynamic.  We propose a generic,
    reconfigurable interconnection fabric which can realize the data-path of
    any ACL and be dynamically reconfigured in constant time.  We have applied
    for a patent for this technology.", 
  location     = "https://doi.org/10.1145/1379023.1375669"
}

@Article{iparedwarrfoooep,
  author       = "Houman Homayoun and Sudeep Pasricha and Mohammad Makhzan and Alex Veidenbaum",
  title        = "Improving performance and reducing energy-delay with adaptive resource resizing for out-of-order embedded processors",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "71--78",
  month        = jul,
  keywords     = "adaptive architecture, performance, energy-delay, out-of-order
  embedded processor, cache latency, dynamic reconfiguration",
  abstract     = "While Ultra Deep Submicron (UDSM) CMOS scaling gives embedded
    processor designers ample silicon budget to increase processor resources to
    improve performance, restrictions with the power budget and practically
    achievable operating clock frequencies act as limiting factors.  In this
    paper we show how just increasing processor resource size is not effective
    in improving performance due to constraints on achievable operating clock
    frequency.  In response we propose two adaptive resource resizing
    techniques L2RS and L2ML1RS that adaptively resize resources by exploiting
    cache misses.  Our results show a significant performance improvement and
    overall energy-delay reduction of on average 9.2% (upto 34%) and 3.8%
    respectively across SPEC2K benchmarks for L2ML1RS.  Applying L2RS resulted
    in 6.8% performance improvement (upto 24%) and 4.6% energy-delay reduction.
    We also present the required circuit modification to apply these techniques
    which shown to be minimal.", 
  location     = "https://doi.org/10.1145/1379023.1375668"
}

@Article{rassaoes,
  author       = "Madhukar Anand and Insup Lee",
  title        = "Robust and sustainable schedulability analysis of embedded software",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "61--70",
  month        = jul,
  keywords     = "schedulability analysis, sustainable schedulability analysis,
    robust schedulability analysis, task management",
  abstract     = "For real-time systems, most of the analysis involves efficient or exact schedulability checking.  While this is important, analysis is often based on the assumption that the task parameters such as execution requirements and inter-arrival times between jobs are known exactly.  In most cases, however, only a worst-case estimate of these quantities is available at the time of analysis.  It is therefore imperative that schedulability analysis hold for better parameter values (Sustainable Analysis).  On the other hand, if the task or system parameters turn out to be worse off, then the analysis should tolerate some deterioration (Robust Analysis).  Robust analysis is especially important, because the implication of task schedulability is often weakened in the presence of optimizations that are performed on its code, or dynamic system parameters.In this work, we define and address sustainability and robustness questions for analysis of embedded real-time software that is modeled by conditional real-time tasks.  Specifically, we show that, while the analysis is sustainable for changes in the task such as lower job execution times and increased relative deadlines, it is not the case for code changes such as job splitting and reordering.  We discuss the impact of these results in the context of common compiler optimizations, and then develop robust schedulability techniques for operations where the original analysis is not sustainable.",
  location     = "https://doi.org/10.1145/1379023.1375666"
}

@Article{rcaocrp,
  author       = "Jan Reineke and Daniel Grund",
  title        = "Relative competitive analysis of cache replacement policies",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "51--60",
  month        = jul,
  keywords     = "cache performance, replacement policy, worst-case execution
    time, wcet analysis, predictability",
  abstract     = "Caches are commonly employed to hide the latency gap between memory and the CPU by exploiting locality in memory accesses.  On today's architectures a cache miss may cost several hundred CPU cycles.In order to fulfill stringent performance requirements, caches are now also used in hard real-time systems.  In such systems, upper and sometimes also lower bounds on the execution times of a task have to be computed.  To obtain tight bounds, timing analyses must take into account the cache architecture.  However, developing cache analyses -- analyses that determine whether a memory access is a hit or a miss -- is a difficult problem for some cache architectures.In this paper, we present a tool to automatically compute relative competitive ratios for a large class of replacement policies, including LRU, FIFO, and PLRU.  Relative competitive ratios bound the performance of one policy relative to the performance of another policy.These performance relations allow us to use cache-performance predictions for one policy to compute predictions for another, including policies that could previously not be dealt with.",
  location     = "https://doi.org/10.1145/1379023.1375665"
}

@Article{cddlofriaap,
  author       = "Doosan Cho and Sudeep Pasricha and Ilya Issenin and Nikil Dutt and Yunheung Paek and SunJun Ko",
  title        = "Compiler driven data layout optimization for regular/irregular array access patterns",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "41--50",
  month        = jul,
  keywords     = "compilers, memory hierarchy, energy consumption, data
    placement, lifetime analysis, dma preloading",
  abstract     = "Embedded multimedia applications consist of regular and
    irregular memory access patterns.  Particularly, irregular pattern are not
    amenable to static analysis for extraction of access patterns, and thus
    prevent efficient use of a Scratch Pad Memory (SPM) hierarchy for
    performance and energy improvements.  To resolve this, we present a
    compiler strategy to optimize data layout in regular/irregular multimedia
    applications running on embedded multiprocessor environments.  The goal is
    to maximize the amount of accesses to the SPM over the entire system which
    leads to a reduction in the energy consumption of the system.  This is
    achieved by optimizing data placement of application-wide reused data so
    that it resides in the SPMs of processing elements.  Specifically, our
    scheme is based on a profiling that generates a memory access footprint.
    The memory access footprint is used to identify data elements with fine
    granularity that can profitably be placed in the SPMs to maximize
    performance and energy gains.  We present a heuristic approach that
    efficiently exploits the SPMs using memory access footprint.  Our
    experimental results show that our approach is able to reduce energy
    consumption by 30% and improve performance by 18% over cache based memory
    subsystems for various multimedia applications.", 
  location     = "https://doi.org/10.1145/1379023.1375664", 
  location     = "http://www.engr.colostate.edu/~sudeep/wp-content/uploads/c18.pdf"
}

@Article{gisusg,
  author       = "Dietmar Ebner and Florian Brandner and Bernhard Scholz and Andreas Krall and Peter Wiedermann and Albrecht Kadlec",
  title        = "Generalized instruction selection using {SSA}-graphs",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "31--40",
  month        = jul,
  keywords     = "compilers, code generation, instruction selection, pbqp",
  abstract     = "Instruction selection is a well-studied compiler phase that 
    translates the compiler's intermediate representation of programs to a
    sequence of target-dependent machine instructions optimizing for various
    compiler objectives (e.g.  speed and space).  Most existing instruction
    selection techniques are limited to the scope of a single statement or a
    basic block and cannot cope with irregular instruction sets that are
    frequently found in embedded systems.We consider an optimal technique for
    instruction selection that uses Static Single Assignment (SSA) graphs as an
    intermediate representation of programs and employs the Partitioned Boolean
    Quadratic Problem (PBQP) for finding an optimal instruction selection.
    While existing approaches are limited to instruction patterns that can be
    expressed in a simple tree structure, we consider complex patterns
    producing multiple results at the same time including pre/post increment
    addressing modes, div-mod instructions, and SIMD extensions frequently
    found in embedded systems.  Although both instruction selection on
    SSA-graphs and PBQP are known to be NP-complete, the problem can be solved
    efficiently - even for very large instances.Our approach has been
    implemented in LLVM for an embedded ARMv5 architecture.  Extensive
    experiments show speedups of up to 57% on typical DSP kernels and up to 10%
    on SPECINT 2000 and MiBench benchmarks.  All of the test programs could be
    compiled within less than half a minute using a heuristic PBQP solver that
    solves 99.83% of all instances optimally.", 
  location     = "https://doi.org/10.1145/1379023.1375663", 
  location     = "https://www.complang.tuwien.ac.at/cd/ebner/lctes08_ebner_slides.pdf"
}

@Article{iojsoecirces,
  author       = "Carmen Badea and Alexandru Nicolau and Alexander~V. Veidenbaum",
  title        = "Impact of {JVM} superoperators on energy consumption in resource-constrained embedded systems",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "23--30",
  month        = jul,
  keywords     = "energy estimation, java virtual machine, superoperators,
    profile-guided optimization, embedded systems",
  abstract     = "Energy consumption is one of the most important issues in 
    resource-constrained embedded systems.  Many such systems run Java-based
    applications due to Java's architecture-independent format (bytecode).
    Standard techniques for executing bytecode programs, e.g.  interpretation
    or just-in-time compilation, have performance or memory issues that make
    them unsuitable for resource-constrained embedded systems.A
    superoperator-extended, lightweight Java Virtual Machine (JVM) can be used
    in resource-constrained embedded systems to improve performance and reduce
    memory consumption.  This paper shows that such a JVM also significantly
    reduces energy consumption.  This is due primarily to a considerable
    reduction in the number of memory accesses and thus in energy consumption
    in the instruction and data TLBs and caches and, in most cases, in DRAM
    energy consumption.  Since the fraction of processor energy dissipated in
    these units is approximately 60%, the energy savings achieved are
    significant.The paper evaluates the number of load, store, and
    computational instructions eliminated by the use of proposed superoperators
    as compared to a simple interpreter on a set of embedded benchmarks.  Using
    cache and DRAM per access energy we estimate the total processor/DRAM
    energy saved by using our JVM.  Our results show that with 32KB caches the
    reduction in energy consumption ranges from 40% to 60% of the overall
    processor, plus DRAM energy.  Even higher savings may be achieved with
    smaller caches and increased access to DRAM as DRAM access energy is fairly
    high.", 
  location     = "https://doi.org/10.1145/1379023.1375661"
}

@Article{ehsdhfejjitc,
  author       = "Seong-Won Lee and Soo-Mook Moon and Seong-Moo Kim",
  title        = "Enhanced hot spot detection heuristics for embedded {Java} just-in-time compilers",
  journal      = lctes08,
  year         = 2008,
  volume       = 43,
  number       = 7,
  pages        = "13--22",
  month        = jul,
  keywords     = "java, just-in-time compilation, hot-spot detection, sun's
    hotspot heuristic, java virtual machine, j2me cdc",
  abstract     = "Most Java just-in-time compilers (JITC) try to compile only
    hot methods since the compilation overhead is part of the running time.
    This requires precise and efficient hot spot detection, which includes
    distinguishing hot methods from cold methods, detecting them as early as
    possible, and paying a small runtime overhead for detection.  A hot method
    could be identified by measuring its running time during interpretation
    since a long-running method is likely to be a hot method.  However, precise
    measurement of the running time during execution is too expensive,
    especially in embedded systems, so many counter-based heuristics have been
    proposed to estimate it.  The Simple heuristic counts only method
    invocations without any consideration of loops [1], while Sun's HotSpot
    heuristic counts loop iterations as well, but does not consider loop sizes
    or method sizes [2,14].  The static analysis heuristic estimates the
    running time of a method by statically analyzing loops or heavy-cost
    bytecodes but does not measure their dynamic counts [3].  Although the
    overhead of these heuristics is low, they do not estimate the running time
    precisely, which may lead to imprecise hot spot detection.This paper
    proposes a new hot spot detection heuristic which can estimate the running
    time more precisely than others with a relatively low overhead.  It
    dynamically counts only important bytecodes interpreted, but with a simple
    arithmetic calculation it can obtain the precise count of all interpreted
    bytecodes.  We also propose employing a static analysis technique to
    predict those hot methods which spend a huge execution time once invoked.
    This static prediction can allow compiling these methods at their
    first-invocation, complementing the proposed dynamic estimation technique.
    We implemented both, which led to a performance benefit of 10% compared to
    the HotSpot heuristic.", 
  location     = "https://doi.org/10.1145/1379023.1375660"
}

@Article{vafefmm,
  author       = "Doe Hyun Yoon and Mattan Erez",
  title        = "Virtualized and Flexible {ECC} for Main Memory",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "397--408",
  month        = mar,
  keywords     = "fault tolerance, error correction, memory systems reliability",
  abstract     = "We present a general scheme for virtualizing main memory
    error-correction mechanisms, which map redundant information needed to
    correct errors into the memory namespace itself.  We rely on this basic
    idea, which increases flexibility to increase error protection
    capabilities, improve power efficiency, and reduce system cost; with only
    small performance overheads.  We augment the virtual memory system
    architecture to detach the physical mapping of data from the physical
    mapping of its associated ECC information.  We then use this mechanism to
    develop two-tiered error protection techniques that separate the process of
    detecting errors from the rare need to also correct errors, and thus save
    energy.  We describe how to provide strong chipkill and double-chip kill
    protection using existing DRAM and packaging technology.  We show how to
    maintain access granularity and redundancy overheads, even when using ×8
    DRAM chips.  We also evaluate error correction for systems that do not use
    ECC DIMMs.  Overall, analysis of demanding SPEC CPU 2006 and PARSEC
    benchmarks indicates that performance overhead is only 1% with ECC DIMMs
    and less than 10% using standard Non-ECC DIMM configurations, that DRAM
    power savings can be as high as 27%, and that the system energy-delay
    product is improved by 12% on average.", 
  location     = "https://doi.org/10.1145/1735971.1736064", 
  location     = "https://lph.ece.utexas.edu/users/dhyoon/pubs/virtualized_ecc_asplos10.pdf"
}

@Article{spserotc,
  author       = "Shuguang Feng and Shantanu Gupta and Amin Ansari and Scott Mahlke",
  title        = "Shoestring: probabilistic soft error reliability on the cheap",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "385--396",
  month        = mar,
  keywords     = "compiler analysis, error detection, fault injection",
  abstract     = "Aggressive technology scaling provides designers with an ever 
    increasing budget of cheaper and faster transistors.  Unfortunately, this
    trend is accompanied by a decline in individual device reliability as
    transistors become increasingly susceptible to soft errors.  We are quickly
    approaching a new era where resilience to soft errors is no longer a luxury
    that can be reserved for just processors in high-reliability,
    mission-critical domains.  Even processors used in mainstream computing
    will soon require protection.  However, due to tighter profit margins,
    reliable operation for these devices must come at little or no cost.  This
    paper presents Shoestring, a minimally invasive software solution that
    provides high soft error coverage with very little overhead, enabling its
    deployment even in commodity processors with 'shoestring' reliability
    budgets.  Leveraging intelligent analysis at compile time, and exploiting
    low-cost, symptom-based error detection, Shoestring is able to focus its
    efforts on protecting statistically-vulnerable portions of program code.
    Shoestring effectively applies instruction duplication to protect only
    those segments of code that, when subjected to a soft error, are likely to
    result in user-visible faults without first exhibiting symptomatic
    behavior.  Shoestring is able to recover from an additional 33.9% of soft
    errors that are undetected by a symptom-only approach, achieving an overall
    user-visible failure rate of 1.6%.  This reliability improvement comes at a
    modest performance overhead of 15.8%.", 
  location     = "https://doi.org/10.1145/1735971.1736063", 
  location     = "https://llvm.org/pubs/2010-03-ASPLOS-Shoestring.pdf"
}

@Article{oesipomc,
  author       = "Ruirui Huang and Daniel~Y. Deng and G.~Edward Suh",
  title        = "Orthrus: efficient software integrity protection on multi-cores",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "371--384",
  month        = mar,
  keywords     = "memory protection, ulti-core architecture, software diversity
    and redundancy, replication-aware architecture, software security",
  abstract     = "This paper proposes an efficient hardware/software system
    that significantly enhances software security through diversified
    replication on multi-cores.  Recent studies show that a large class of
    software attacks can be detected by running multiple versions of a program
    simultaneously and checking the consistency of their behaviors.  However,
    execution of multiple replicas incurs significant overheads on today's
    computing platforms, especially with fine-grained comparisons necessary for
    high security.  Orthrus exploits similarities in automatically generated
    replicas to enable simultaneous execution of those replicas with minimal
    overheads; the architecture reduces memory and bandwidth overheads by
    compressing multiple memory spaces together, and additional power
    consumption and silicon area by eliminating redundant computations.
    Utilizing the hardware architecture, Orthrus implements a fine-grained
    memory layout diversification with the LLVM compiler and can detect
    corruptions in both pointers and critical data.  Experiments indicate that
    the Orthrus architecture incurs minimal overheads and provides a protection
    against a broad range of attacks.", 
  location     = "https://doi.org/10.1145/1735971.1736062", 
  location     = "https://llvm.org/pubs/2010-03-ASPLOS-Orthrus.pdf"
}

@Article{icctfcm,
  author       = "Abhishek Bhattacharjee and Margaret Martonosi",
  title        = "Inter-core cooperative {TLB} for chip multiprocessors",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "359--370",
  month        = mar,
  keywords     = "translation lookaside buffer, parallelism, prefetching",
  abstract     = "Translation Lookaside Buffers (TLBs) are commonly employed in
    modern processor designs and have considerable impact on overall system
    performance.  A number of past works have studied TLB designs to lower
    access times and miss rates, specifically for uniprocessors.  With the
    growing dominance of chip multiprocessors (CMPs), it is necessary to
    examine TLB performance in the context of parallel workloads.This work is
    the first to present TLB prefetchers that exploit commonality in TLB miss
    patterns across cores in CMPs.  We propose and evaluate two Inter-Core
    Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness
    at eliminating TLB misses both individually and together.  Our results show
    these approaches require at most modest hardware and can collectively
    eliminate 19% to 90% of data TLB (D-TLB) misses across the surveyed
    parallel workloads.We also compare performance improvements across a range
    of hardware and software implementation possibilities.  We find that while
    a fully-hardware implementation results in average performance improvements
    of 8-46% for a range of TLB sizes, a hardware/software approach yields
    improvements of 4-32%.  Overall, our work shows that TLB prefetchers
    exploiting inter-core correlations can effectively eliminate TLB misses.", 
  location     = "https://doi.org/10.1145/1735971.1736060", 
  location     = "https://ur.booksc.eu/dl/44912658/2eae71"
}

@Article{aadsmmfhps,
  author       = "Isaac Gelado and John~E. Stone and Javier Cabezas and Sanjay Patel and Nacho Navarro and Wen-mei~W. Hwu",
  title        = "An asymmetric distributed shared memory model for heterogeneous parallel systems",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "347--358",
  month        = mar,
  keywords     = "heterogeneous systems, data-centric programming models,
    asymmetric distributed shared memory",
  abstract     = "Heterogeneous computing combines general purpose CPUs with
    accelerators to efficiently execute both sequential control-intensive and
    data-parallel phases of applications.  Existing programming models for
    heterogeneous computing rely on programmers to explicitly manage data
    transfers between the CPU system memory and accelerator memory.This paper
    presents a new programming model for heterogeneous computing, called
    Asymmetric Distributed Shared Memory (ADSM), that maintains a shared
    logical memory space for CPUs to access objects in the accelerator physical
    memory but not vice versa.  The asymmetry allows light-weight
    implementations that avoid common pitfalls of symmetrical distributed
    shared memory systems.  ADSM allows programmers to assign data objects to
    performance critical methods.  When a method is selected for accelerator
    execution, its associated data objects are allocated within the shared
    logical memory space, which is hosted in the accelerator physical memory
    and transparently accessible by the methods executed on CPUs.We argue that
    ADSM reduces programming efforts for heterogeneous computing systems and
    enhances application portability.  We present a software implementation of
    ADSM, called GMAC, on top of CUDA in a GNU/Linux environment.  We show that
    applications written in ADSM and running on top of GMAC achieve performance
    comparable to their counterparts using programmer-managed data transfers.
    This paper presents the GMAC system and evaluates different design choices.
    We suggest additional architectural support that will likely allow GMAC to
    achieve higher application performance than the current CUDA model.", 
  location     = "https://doi.org/10.1145/1735971.1736059"
}

@Article{fvstacahpfsfmcms,
  author       = "Eiman Ebrahimi and Chang Joo Lee and Onur Mutlu and Yale~N. Patt",
  title        = "Fairness via source throttling: a configurable and high-performance fairness substrate for multi-core memory systems",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "335--346",
  month        = mar,
  keywords     = "shared cmp systems, dynamic request throttling, storage
    traffic, adaptive algorithms",
  abstract     = "Cores in a chip-multiprocessor (CMP) system share multiple
    hardware resources in the memory subsystem.  If resource sharing is unfair,
    some applications can be delayed significantly while others are unfairly
    prioritized.  Previous research proposed separate fairness mechanisms in
    each individual resource.  Such resource-based fairness mechanisms
    implemented independently in each resource can make contradictory
    decisions, leading to low fairness and loss of performance.  Therefore, a
    coordinated mechanism that provides fairness in the entire shared memory
    system is desirable.This paper proposes a new approach that provides
    fairness in the entire shared memory system, thereby eliminating the need
    for and complexity of developing fairness mechanisms for each individual
    resource.  Our technique, Fairness via Source Throttling (FST), estimates
    the unfairness in the entire shared memory system.  If the estimated
    unfairness is above a threshold set by system software, FST throttles down
    cores causing unfairness by limiting the number of requests they can inject
    into the system and the frequency at which they do.  As such, our
    source-based fairness control ensures fairness decisions are made in tandem
    in the entire memory system.  FST also enforces thread priorities/weights,
    and enables system software to enforce different fairness objectives and
    fairness-performance tradeoffs in the memory system.Our evaluations show
    that FST provides the best system fairness and performance compared to four
    systems with no fairness control and with state-of-the-art fairness
    mechanisms implemented in both shared caches and memory controllers.", 
  location     = "https://doi.org/10.1145/1735971.1736058", 
  location     = "https://users.ece.cmu.edu/~omutlu/pub/fst_asplos10.pdfc"
}

@Article{sadvatamc,
  author       = "Bogdan~F. Romanescu and Alvin~R. Lebeck and Daniel~J. Sorin",
  title        = "Specifying and dynamically verifying address translation-aware memory consistency",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "323--334",
  month        = mar,
  keywords     = "memory consistency, virtual memory, address translation,
    dynamic verification",
  abstract     = "Computer systems with virtual memory are susceptible to
    design bugs and runtime faults in their address translation (AT) systems.
    Detecting bugs and faults requires a clear specification of correct
    behavior.  To address this need, we develop a framework for AT-aware memory
    consistency models.  We expand and divide memory consistency into the
    physical address memory consistency (PAMC) model that defines the behavior
    of operations on physical addresses and the virtual address memory
    consistency (VAMC) model that defines the behavior of operations on virtual
    addresses.  As part of this expansion, we show what AT features are
    required to bridge the gap between PAMC and VAMC.  Based on our AT-aware
    memory consistency specifications, we design efficient dynamic verification
    hardware detecting VAMC violations and thus detect design bugs and runtime
    faults, including most AT related bugs in published errata.",
  location     = "https://doi.org/10.1145/1735971.1736057",
  location     = "http://people.ee.duke.edu/~sorin/papers/asplos10_consistency.pdf"
}

@Article{fasffgs,
  author       = "Daniel Sanchez and Richard~M. Yoo and Christos Kozyrakis",
  title        = "Flexible architectural support for fine-grain scheduling",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "311--322",
  month        = mar,
  keywords     = "fine-grain parallelism, thread scheduling, asynchronous
    messaging, messaging hardware, virtualization",
  abstract     = "To make efficient use of CMPs with tens to hundreds of cores,
    it is often necessary to exploit fine-grain parallelism.  However, managing
    tasks of a few thousand instructions is particularly challenging, as the
    runtime must ensure load balance without compromising locality and
    introducing small overheads.  Software-only schedulers can implement
    various scheduling algorithms that match the characteristics of different
    applications and programming models, but suffer significant overheads as
    they synchronize and communicate task information over the deep cache
    hierarchy of a large-scale CMP.  To reduce these costs, hardware-only
    schedulers like Carbon, which implement task queuing and scheduling in
    hardware, have been proposed.  However, a hardware-only solution fixes the
    scheduling algorithm and leaves no room for other uses of the custom
    hardware.This paper presents a combined hardware-software approach to build
    fine-grain schedulers that retain the flexibility of software schedulers
    while being as fast and scalable as hardware ones.  We propose asynchronous
    direct messages (ADM), a simple architectural extension that provides
    direct exchange of asynchronous, short messages between threads in the CMP
    without going through the memory hierarchy.  ADM is sufficient to implement
    a family of novel, software-mostly schedulers that rely on low-overhead
    messaging to efficiently coordinate scheduling and transfer task
    information.  These schedulers match and often exceed the performance and
    scalability of Carbon when using the same scheduling algorithm.  When the
    ADM runtime tailors its scheduling algorithm to application
    characteristics, it outperforms Carbon by up to 70%.", 
  location     = "https://doi.org/10.1145/1735971.1736055", 
  location     = "http://csl.stanford.edu/~christos/publications/2010.adm.asplos.pdf"
}

@Article{capdpuigs,
  author       = "Dong Hyuk Woo and Hsien-Hsin~S. Lee",
  title        = "{COMPASS}: a programmable data prefetcher using idle {GPU} shaders",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "297--310",
  month        = mar,
  keywords     = "gpu, compute shaders, prefetching, repurposing",
  abstract     = "A traditional fixed-function graphics accelerator has evolved
    into a programmable general-purpose graphics processing unit over the last
    few years.  These powerful computing cores are mainly used for accelerating
    graphics applications or enabling low-cost scientific computing.  To
    further reduce the cost and form factor, an emerging trend is to integrate
    GPU along with the memory controllers onto the same die with the processor
    cores.  However, given such a system-on-chip, the GPU, while occupying a
    substantial part of the silicon, will sit idle and contribute nothing to
    the overall system performance when running non-graphics workloads or
    applications lack of data-level parallelism.  In this paper, we propose
    COMPASS, a compute shader-assisted data prefetching scheme, to leverage the
    GPU resource for improving single-threaded performance on an integrated
    system.  By harnessing the GPU shader cores with very lightweight
    architectural support, COMPASS can emulate the functionality of a
    hardware-based prefetcher using the idle GPU and successfully improve the
    memory performance of single-thread applications.  Moreover, thanks to its
    flexibility and programmability, one can implement the best performing
    prefetch scheme to improve each specific application as demonstrated in
    this paper.  With COMPASS, we envision that a future application vendor can
    provide a custom-designed COMPASS shader bundled with its software to be
    loaded at runtime to optimize the performance.  Our simulation results show
    that COMPASS can improve the single-thread performance of memory-intensive
    applications by 68% on average.", 
  location     = "https://doi.org/10.1145/1735971.1736054"
}

@Article{mmsosa,
  author       = "Amir~H. Hormati and Yoonseo Choi and Mark Woh and Manjunath Kudlur and Rodric Rabbah and Trevor Mudge and Scott Mahlke",
  title        = "{MacroSS}: macro-{SIMDization} of streaming applications",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "285--296",
  month        = mar,
  keywords     = "streaming, compiler, simd architecture, optimization",
  abstract     = "SIMD (Single Instruction, Multiple Data) engines are an
    essential part of the processors in various computing markets, from servers
    to the embedded domain.  Although SIMD-enabled architectures have the
    capability of boosting the performance of many application domains by
    exploiting data-level parallelism, it is very challenging for compilers and
    also programmers to identify and transform parts of a program that will
    benefit from a particular SIMD engine.  The focus of this paper is on the
    problem of SIMDization for the growing application domain of streaming.
    Streaming applications are an ideal solution for targeting multi-core
    architectures, such as shared/distributed memory systems, tiled
    architectures, and single-core systems.  Since these architectures, in most
    cases, provide SIMD acceleration units as well, it is highly beneficial to
    generate SIMD code from streaming programs.  Specifically, we introduce
    MacroSS, which is capable of performing macro-SIMDization on high-level
    streaming graphs.  Macro-SIMDization uses high-level information such as
    execution rates of actors and communication patterns between them to
    transform the graph structure, vectorize actors of a streaming program, and
    generate intermediate code.  We also propose low-overhead architectural
    modifications that accelerate shuffling of data elements between the scalar
    and vectorized parts of a streaming program.  Our experiments show that
    MacroSS is capable of generating code that, on average, outperforms scalar
    code compiled with the current state-of-art auto-vectorizing compilers by
    54%.  Using the low-overhead data shuffling hardware, performance is
    improved by an additional 8% with less than 1% area overhead.",
  location     = "https://doi.org/10.1145/1735971.1736053",
  location     = "http://tnm.engin.umich.edu/wp-content/uploads/sites/353/2017/12/2010.03.macross-macro-SIMD.pdf"
}

@Article{baadatdpm,
  author       = "Michelle~L. Goodstein and Evangelos Vlachos and Shimin Chen and Phillip~B. Gibbons and Michael~A. Kozuch and Todd~C. Mowry",
  title        = "Butterfly analysis: adapting dataflow analysis to dynamic parallel monitoring",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "257--270",
  month        = mar,
  keywords     = "dataflow analysis, static analysis, parallel programming,
    dynamic program monitoring, inter-thread data dependencies, epochal analysis",
  abstract     = "Online program monitoring is an effective technique for
    detecting bugs and security attacks in running applications.  Extending
    these tools to monitor parallel programs is challenging because the tools
    must account for inter-thread dependences and relaxed memory consistency
    models.  Existing tools assume sequential consistency and often slow down
    the monitored program by orders of magnitude.  In this paper, we present a
    novel approach that avoids these pitfalls by not relying on strong
    consistency models or detailed inter-thread dependence tracking.  Instead,
    we only assume that events in the distant past on all threads have become
    visible; we make no assumptions on (and avoid the overheads of tracking)
    the relative ordering of more recent events on other threads.  To overcome
    the potential state explosion of considering all the possible orderings
    among recent events, we adapt two techniques from static dataflow analysis,
    reaching definitions and reaching expressions, to this new domain of
    dynamic parallel monitoring.  Significant modifications to these techniques
    are proposed to ensure the correctness and efficiency of our approach.  We
    show how our adapted analysis can be used in two popular memory and
    security tools.  We prove that our approach does not miss errors, and
    sacrifices precision only due to the lack of a relative ordering among
    recent events.  Moreover, our simulation study on a collection of Splash-2
    and Parsec 2.0 benchmarks running a memory-checking tool on a
    hardware-assisted logging platform demonstrates the potential benefits in
    trading off a very low false positive rate for (i) reduced overhead and
    (ii) the ability to run on relaxed consistency models.", 
  location     = "https://doi.org/10.1145/1735971.1736050", 
  location     = "https://www.cs.cmu.edu/~mgoodste/research/butterfly_asplos10.pdf"
}

@Article{peaaopmoma,
  author       = "Evangelos Vlachos and Michelle~L. Goodstein and Michael~A. Kozuch and Shimin Chen and Babak Falsafi and Phillip~B. Gibbons and Todd~C. Mowry",
  title        = "{ParaLog}: enabling and accelerating online parallel monitoring of multithreaded applications",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "271--284",
  month        = mar,
  keywords     = "online parallel monitoring, hardware debugging support,
    instruction-grain lifeguards, shadow storage",
  abstract     = "Instruction-grain lifeguards monitor the events of a running
    application at the level of individual instructions in order to identify
    and help mitigate application bugs and security exploits.  Because such
    lifeguards impose a 10-100X slowdown on existing platforms, previous
    studies have proposed hardware designs to accelerate lifeguard processing.
    However, these accelerators are either tailored to a specific class of
    lifeguards or suitable only for monitoring singlethreaded programs.We
    present ParaLog, the first design of a system enabling fast online parallel
    monitoring of multithreaded parallel applications.  ParaLog supports a
    broad class of software-defined lifeguards.  We show how three existing
    accelerators can be enhanced to support online multithreaded monitoring,
    dramatically reducing lifeguard overheads.  We identify and solve several
    challenges in monitoring parallel applications and/or parallelizing these
    accelerators, including (i) enforcing inter-thread data dependences, (ii)
    dealing with inter-thread effects that are not reflected in coherence
    traffic, (iii) dealing with unmonitored operating system activity, and (iv)
    ensuring lifeguards can access shared metadata with negligible
    synchronization overheads.  We present our system design for both
    Sequentially Consistent and Total Store Ordering processors.  We implement
    and evaluate our design on a 16 core simulated CMP, using benchmarks from
    SPLASH-2 and PARSEC and two lifeguards: a data-flow tracking lifeguard and
    a memory-access checker lifeguard.  Our results show that (i) our parallel
    accelerators improve performance by 2-9X and 1.13-3.4X for our two
    lifeguards, respectively, (ii) we are 5-126X faster than the time-slicing
    approach required by existing techniques, and (iii) our average overheads
    for applications with eight threads are 51% and 28% for the two lifeguards,
    respectively.", 
  location     = "https://doi.org/10.1145/1735971.1736051",
  location     = "http://www.cs.cmu.edu/~lba/papers/LBA-ParaLog-asplos10.pdf"
}

@Article{jooiacpidcwmrt,
  author       = "Faraz Ahmad and T.~N. Vijaykumar",
  title        = "Joint optimization of idle and cooling power in data centers while maintaining response time",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "243--256",
  month        = mar,
  keywords     = "data center, power management, idle power, cooling power,
    response time",
  abstract     = "Server power and cooling power amount to a significant
    fraction of modern data centers' recurring costs.  While data centers
    provision enough servers to guarantee response times under the maximum
    loading, data centers operate under much less loading most of the times
    (e.g., 30-70% of the maximum loading).  Previous server-power proposals
    exploit this under-utilization to reduce the server idle power by keeping
    active only as many servers as necessary and putting the rest into
    low-power standby modes.  However, these proposals incur higher cooling
    power due to hot spots created by concentrating the data center loading on
    fewer active servers, or degrade response times due to standby-to-active
    transition delays, or both.  Other proposals optimize the cooling power but
    incur considerable idle power.  To address the first issue of power, we
    propose PowerTrade, which trades-off idle power and cooling power for each
    other, thereby reducing the total power.  To address the second issue of
    response time, we propose SurgeGuard to overprovision the number of active
    servers beyond that needed by the current loading so as to absorb future
    increases in the loading.  SurgeGuard is a two-tier scheme which uses
    well-known over-provisioning at coarse time granularities (e.g., one hour)
    to absorb the common, smooth increases in the loading, and a novel
    fine-grain replenishment of the over-provisioned reserves at fine time
    granularities (e.g., five minutes) to handle the uncommon, abrupt loading
    surges.  Using real-world traces, we show that combining PowerTrade and
    SurgeGuard reduces total power by 30% compared to previous low-power
    schemes while maintaining response times within 1.7%.", 
  location     = "https://doi.org/10.1145/1735971.1736048", 
  location     = "https://engineering.purdue.edu/~vijay/papers/2010/powertrade.pdf"
}

@Article{prdppitdc,
  author       = "Steven Pelley and David Meisner and Pooya Zandevakili and Thomas~F. Wenisch and Jack Underwood",
  title        = "Power routing: dynamic power provisioning in the data center",
  journal      = asplos10,
  year         = 2010,
  volume       = 45,
  number       = 3,
  pages        = "231--242",
  month        = mar,
  keywords     = "power infrastructure, data centers, throttling, power distribution",
  abstract     = "Data center power infrastructure incurs massive capital
    costs, which typically exceed energy costs over the life of the facility.
    To squeeze maximum value from the infrastructure, researchers have proposed
    over-subscribing power circuits, relying on the observation that peak loads
    are rare.  To ensure availability, these proposals employ power capping,
    which throttles server performance during utilization spikes to enforce
    safe power budgets.  However, because budgets must be enforced locally --
    at each power distribution unit (PDU) -- local utilization spikes may force
    throttling even when power delivery capacity is available elsewhere.
    Moreover, the need to maintain reserve capacity for fault tolerance on
    power delivery paths magnifies the impact of utilization spikes.In this
    paper, we develop mechanisms to better utilize installed power
    infrastructure, reducing reserve capacity margins and avoiding performance
    throttling.  Unlike conventional high-availability data centers, where
    collocated servers share identical primary and secondary power feeds, we
    reorganize power feeds to create shuffled power distribution topologies.
    Shuffled topologies spread secondary power feeds over numerous PDUs,
    reducing reserve capacity requirements to tolerate a single PDU failure.
    Second, we propose Power Routing, which schedules IT load dynamically
    across redundant power feeds to: (1) shift slack to servers with growing
    power demands, and (2) balance power draw across AC phases to reduce
    heating and improve electrical stability.  We describe efficient heuristics
    for scheduling servers to PDUs (an NP-complete problem).  Using data
    collected from nearly 1000 servers in three production facilities, we
    demonstrate that these mechanisms can reduce the required power
    infrastructure capacity relative to conventional high-availability data
    centers by 32% without performance degradation.", 
  location     = "https://doi.org/10.1145/1735971.1736047", 
  location     = "https://web.eecs.umich.edu/~twenisch/papers/asplos10.pdf"
}

@TechReport{oem2fblis,
  author       = "Rovner, Paul and Levin, Roy and Wick, John",
  title        = "On Extending {Modula}-2 for Building Large, Integrated Systems",
  institution  = "DEC System Research Center",
  year         = 1985,
  number       = "SRC-RR-3",
  address      = paca,
  month        = "11 " # jan,
  keywords     = "modula-2, types, exception handling, concurrency management,
    programming style, system development",
  abstract     = "Modula-2 has been chosen as SRC's primary programming
    language for the next few years.  This report addresses some of the
    problems of using Modula-2 for building large, integrated systems.  The
    report has three sections: Section 1 outlines a set of extensions to the
    language.  (The extended language is called Modula-2+.) Section 2 (with
    Appendix b) provides a complete description of the Modula-2+ type-checking
    rules.  Section 3 offers some guidelines for programming in Modula-2+.  Our
    implementation of Modula-2+ is based on the Modula-2 compiler written by
    Mike Powell at the DEC Western Research Laboratory.  Our extensions include
    features for exceptions and finalization, garbage collection, and
    concurrency.", 
  location     = "http://bitsavers.trailing-edge.com/pdf/dec/tech_reports/SRC-RR-3.pdf"
}

@InProceedings{tehrfsfrbd,
  author       = "Daniel Rosenband",
  title        = "The Ephemeral History Register:  Flexible Scheduling for Rule-Based Designs",
  booktitle    = pot # "Second ACM/IEEE International Conference on Formal Methods and Models for Co-Design (MEMOCODE '04)",
  year         = 2004,
  pages        = "189--198",
  publisher    = "IEEE Computer Society",
  month        = jan,
  keywords     = "hardware synthesis, processor pipelines, rule-based synthesis",
  abstract     = " The quality of high-level synthesis results is strongly
    dependant on the concurrency that can be found in designs.  In this paper
    we introduce the ephemeral history register (EHR), a new primitive state
    element that enables concurrent scheduling of arbitrary rules in a
    rule-based design framework.  The key properties of the EHR are that it
    allows multiple operations to write to the same state simultaneously, and
    that the EHR maintains a history of all writes that occur within a
    clock-cycle.  Using the EHR, we present an algorithm that takes as input a
    design and a desired schedule, and produces a functionally equivalent
    design that satisfies the desired concurrency and ordering of operations.
    A processor pipeline is used to illustrate the effectiveness of the EHR and
    scheduling algorithm, and shows how this approach significantly improves on
    previous synthesis algorithms for rule-based designs.", 
  location     = "https://doi.org/10.1109/MEMCOD.2004.1459853", 
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-479/memo479.pdf"
}
		  
% Local Variables:
% eval: (set-register ?b "  journal      = asplos06,\n  year         = 2006,\n  volume       = 41,\n  number       = 11,\n  pages        = \"--\",\n  month        = oct,\n")
% End:

