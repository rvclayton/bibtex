.so bibtex.header

@string{asplos04 = sigplan # " (" # pot # "Eleventh International Conference on " # asplos # ", ASPLOS XI)"}
@string{asplos06 = sigplan # " (" # pot # "Twelfth International Conference on " # asplos # ", ASPLOS XII)"}
@string{sosp01    = osr # " (" # pot # "Eighteenth" # sosp # ", SOSP '01)"}
@string{osdi02    = osr # " (" # pot # "Fifth " # osdi # ")"}
@string{oopsla86    = sigplan # " (Conference Proceedings on Object-Oriented Programming Systems, Languages And Applications, OOPSLA '86)"}

@Book{tsbotw,
  author       = "Victor Pelevin",
  title        = "The Sacred Book of the Werewolf",
  publisher    = "Viking",
  year         = 2005,
  address      = nyny,
  keywords     = "philosophy, were-creatures, post-soviet russia",
  location     = "PG 3485.E38 S8713"
}

@Book{hps1984,
  author       = "Pamela Sargent",
  title        = "Homesmind",
  subtitle     = "The Watchstar Trilogy Book III",
  publisher    = "Harper \& Row",
  year         = 1984,
  address      = nyny,
  keywords     = "survival",
  location     = "PZ 7.S2472 Ho 1984"
}

@Book{sewpr,
  author       = "Matthias Felleisen and Robert Bruce Findler and Matthew Flatt",
  title        = "Semantics Engineering with {PLT Redex}",
  publisher    = mitp,
  year         = 2009,
  address      = cama,
  keywords     = "reduction semantics, language semantics",
  location     = "QA 76.73.R227 F45"
}

@Book{dhwj,
  author       = "Joe Bageant",
  title        = "Deer Hunting with Jesus",
  subtitle     = "Dispatches from America's Class War",
  publisher    = "Crown",
  year         = 2007,
  address      = nyny,
  keywords     = "working class, class war, economics, politics, virginia",
  location     = "HN 90.S6 B32 2007"
}

@Book{gsrtp,
  author       = "Thomas Pynchon",
  title        = "Gravity's Rainbow",
  publisher    = "Bantam Books",
  year         = 1974,
  address      = nyny,
  price        = "$2.50",
  keywords     = "wwii, rocketry",
  location     = ""
}

@Book{thhgnr,
  author       = "Gerald~N. Rosenberg",
  title        = "The Hollow Hope",
  subtitle     = "Can the Courts Bring About Social Change?",
  publisher    = ucp,
  year         = 2008,
  address      = chil,
  edition      = "second",
  keywords     = "u.s. courts, judicial power, social change, politics, civil
    rights, women's rights, gay rights, environment, reapportionment, criminal
    law", 
  location     = "KF 8700.R66"
}

@Book{ilitec,
  author       = "Rajiv Chandrasekaran",
  title        = "Imperial Life in the Emerald City",
  subtitle     = "Inside Iraq's Green Zone",
  publisher    = "Vintage",
  year         = 2006,
  address      = nyny,
  keywords     = "occupation, development, iraq",
  location     = "DS 79.769.C53"
}

@Book{gleh,
  author       = "Elizabeth Hand",
  title        = "Generation Loss",
  publisher    = "Small Beer Press",
  year         = 2007,
  address      = noma,
  keywords     = "art photographers, maine, ritual sacrifice",
  location     = "PS 3558.A4619 G46"
}

@Book{tuws,
  author       = "Will Storr",
  title        = "The Unpersuadables",
  subtitle     = "Adventures with the Enemies of Science",
  publisher    = "The Overlook Press",
  year         = 2014,
  address      = nyny,
  keywords     = "skepticism, fundamentalism, psi research, holocaust denial",
  location     = "Q 172.5.H47 S76"
}

@Book{ditfp,
  author       = "Edgar Box",
  title        = "Death in the Fifth Position",
  publisher    = "Dutton",
  year         = 1952,
  address      = nyny,
  keywords     = "ballet, fellow travelers",
  location     = "PS 3543.I26 D4"
}

@Book{atdah,
  author       = "Bethany McLean and Joe Nocera",
  title        = "All the Devils are Here",
  subtitle     = "The Hidden History of the Financial Crisis",
  publisher    = "Portfolio\slash Penguin",
  year         = 2010,
  address      = nyny,
  keywords     = "the mortgage crisis, mortgage-backed securities, subprime
    loans, regulation, the financial crisis",
  location     = "HB 3717 2008.M35"
}

@Book{iadm,
  author       = "Donald MacKenzie",
  title        = "Inventing Accuracy",
  subtitle     = "A Historical Sociology of Nuclear Missile Guidance",
  publisher    = mitp,
  year         = 1990,
  address      = cama,
  keywords     = "ballistic missiles, nuclear weapons, military policy, nuclear
    warfare, politics, guidance systems, charles stark draper",
  location     = "UG 1312.B34 M33"
}

@Book{twohk,
  author       = "Sharon Ghamari-Tabrizi ",
  title        = "The Worlds of Herman Kahn",
  subtitle     = "The Intuitive Science of Thermonuclear War",
  publisher    = hup,
  year         = 2005,
  address      = cma,
  keywords     = "rand, on thermonuclear war, the ol' razzle-dazzle",
  location     = "U 263.G49"
}

@Book{dlih,
  author       = "Edgar Box",
  title        = "Death Likes It Hot",
  publisher    = "Dutton",
  year         = 1954,
  address      = nyny,
  keywords     = "the long island life, murrdaar",
  location     = "PS 3543.I26 D44"
}

@Book{tdossf,
  author       = "Shulamith Firestone",
  title        = "The Dialectic of Sex",
  subtitle     = "The Case for Feminist Revolution",
  publisher    = "Morrow",
  year         = 1970,
  address      = nyny,
  keywords     = "feminism, family, children, sex class, the patriarchy, culture",
  location     = "HQ 1190.F57"
}

@Book{lmma18,
  title        = "Lost Mars",
  subtitle     = "Stories From the Golden Age of the Red Planet",
  publisher    = ucp,
  year         = 2018,
  editor       = "Mike Ashley",
  address      = chil,
  keywords     = "the red planet, science fiction, short stories",
  location     = "PN 3433.6.L68"
}

@Book{tdapr,
  author       = "Philip Roth",
  title        = "The Dying Animal",
  publisher    = "Vintage International",
  year         = 2002,
  address      = nyny,
  keywords     = "teaching, student-teacher relations",
  location     = "PS 3568.O855 D95"
}

@Book{votdjs,
  author       = "Jeff Salyards",
  title        = "Veil of the Deserters",
  publisher    = "Night Shade Books",
  year         = 2014,
  address      = nyny,
  keywords     = "memory witches, betrayal, a questing we will go",
  location     = "PS 3619.A44266 V45"
}

@Article{hasfadsafes,
  author       = "John Regehr and Alastair Reid",
  title        = "{HOIST}:  {A} System for Automatically Deriving Static Analyzers for Embedded Systems",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "133--143",
  month        = nov,
  keywords     = "abstract interpretation, static analysis, program
    verification, object code, binary decision diagrams",
  abstract     = "Embedded software must meet conflicting requirements such as
    high reliability, running on resource-constrained platforms, and rapid
    development.  Static program analysis can help meet all of these goals.
    People developing analyzers for embedded object code face a difficult
    problem: writing an abstract version of each instruction in the target
    architecture(s).  This is currently done by hand, resulting in abstract
    operations that are both buggy and im-precise.  We have developed Hoist: a
    novel system that solves these problems by automatically constructing
    abstract operations using a microprocessor (or simulator) as its own
    specification.  With almost no input from a human, Hoist generates a
    collection of C functions ready to be linked into an abstract interpreter.
    We demonstrate that Hoist generates abstract operations that are correct,
    having been extensively tested, sufficiently fast, and substantially more
    precise than manually written abstract operations. Hoist is currently
    limited to eight-bit machines due to costs exponential in the word size of
    the target architecture.  It is essential to be able to analyze software
    running on these small processors: they are important and ubiquitous, with
    many embedded and safety-critical systems being based on them.",
  location     = "https://doi.org/10.1145/1037187.1024410",
  location     = "https://www.cs.utah.edu/flux/papers/hoist-asplos04.pdf"
}

@Article{htvvmoaei2pbp,
  author       = "Perry~H. Wang and Jamison~D. Collins and Hong Wang and Dongkeun Kim and Bill Greene and Kai-Ming Chan and Aamir~B. Yunus and Terry Sych and Stephen~F. Moore and John~P. Shen",
  title        = "Helper Threads via Virtual Multithreading on an Experimental {Itanium} 2 Processor-Based Platform",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "144--155",
  month        = nov,
  keywords     = "helper thread, cache miss prefetching, multithreading,
    switch-on-event, itanium processor, pal, db2 database",
  abstract     = "Helper threading is a technology to accelerate a program by
    exploiting a processor's multithreading capability to run ``assist''
    threads.  Previous experiments on hyper-threaded processors have
    demonstrated significant speedups by using helper threads to prefetch
    hard-to-predict delinquent data accesses.  In order to apply this technique
    to processors that do not have built-in hardware support for
    multithreading, we introduce virtual multithreading (VMT), a novel form of
    switch-on-event user-level multithreading, capable of fly-weight
    multiplexing of event-driven thread executions on a single processor
    without additional operating system support.  The compiler plays a key role
    in minimizing synchronization cost by judiciously partitioning register
    usage among the user-level threads.  The VMT approach makes it possible to
    launch dynamic helper thread instances in response to long-latency cache
    miss events, and to run helper threads in the shadow of cache misses when
    the main thread would be otherwise stalled.The concept of VMT is prototyped
    on an Itanium ® 2 processor using features provided by the Processor
    Abstraction Layer (PAL) firmware mechanism already present in currently
    shipping processors.  On a 4-way MP physical system equipped with
    VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism
    can achieve significant performance gains for a diverse set of real-world
    workloads, ranging from single-threaded workstation benchmarks to heavily
    multithreaded large scale decision support systems (DSS) using the IBM DB2
    Universal Database.  We measure a wall-clock speedup of 5.8% to 38.5% for
    the workstation benchmarks, and 5.0% to 12.7% on DSS workload queries.",
  location     = "https://doi.org/10.1145/1037947.1024411"
}

@Article{lomlduasp,
  author       = "Matthias Hauswirth and Trishul~M. Chilimbi",
  title        = "Low-overhead memory leak detection using adaptive statistical profiling",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "156--164",
  month        = nov,
  keywords     = "low-overhead monitoring, runtime analysis, memory leaks, ",
  abstract     = "Sampling has been successfully used to identify performance
    optimization opportunities.  We would like to apply similar techniques to
    check program correctness.  Unfortunately, sampling provides poor coverage
    of infrequently executed code, where bugs often lurk.  We describe an
    adaptive profiling scheme that addresses this by sampling executions of
    code segments at a rate inversely proportional to their execution
    frequency.  To validate our ideas, we have implemented SWAT, a novel memory
    leak detection tool.  SWAT traces program allocations/ frees to construct a
    heap model and uses our adaptive profiling infrastructure to monitor
    loads/stores to these objects with low overhead.  SWAT reports stale
    objects that have not been accessed for a long time as leaks.  This
    allows it to find all leaks that manifest during the current program
    execution.  Since SWAT has low runtime overhead (‹5%), and low space
    overhead (‹10% in most cases and often less than 5%), it can be used to
    track leaks in production code that take days to manifest.  In addition to
    identifying the allocations that leak memory, SWAT exposes where the
    program last accessed the leaked data, which facilitates debugging and
    fixing the leak.  SWAT has been used by several product groups at Microsoft
    for the past 18 months and has proved effective at detecting leaks with a
    low false positive rate (‹10%).",
  location     = "https://doi.org/10.1145/1037947.1024412",
  location     = "https://www.microsoft.com/en-us/research/publication/low-overhead-memory-leak-detection-using-adaptive-statistical-profiling/"
}

@Article{lpp04,
  author       = "Xipeng Shen and Yutao Zhong and Chen Ding",
  title        = "Locality phase prediction",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "165--176",
  month        = nov,
  keywords     = "program phase analysis and prediction, phrase hierarchy,
    locality analysis and optimization, reconfigurable architecture, dynamic
    optimization",
  abstract     = "As computer memory hierarchy becomes adaptive, its
    performance increasingly depends on forecasting the dynamic program
    locality.  This paper presents a method that predicts the locality phases
    of a program by a combination of locality profiling and run-time
    prediction.  By profiling a training input, it identifies locality phases
    by sifting through all accesses to all data elements using
    variable-distance sampling, wavelet filtering, and optimal phase
    partitioning.  It then constructs a phase hierarchy through grammar
    compression.  Finally, it inserts phase markers into the program using
    binary rewriting.  When the instrumented program runs, it uses the first
    few executions of a phase to predict all its later executions.Compared with
    existing methods based on program code and execution intervals, locality
    phase prediction is unique because it uses locality profiles, and it marks
    phase boundaries in program code.  The second half of the paper presents a
    comprehensive evaluation.  It measures the accuracy and the coverage of the
    new technique and compares it with best known run-time methods.  It
    measures its benefit in adaptive cache resizing and memory remapping.
    Finally, it compares the automatic analysis with manual phase marking.  The
    results show that locality phase prediction is well suited for identifying
    large, recurring phases in complex programs.", 
  location     = "https://doi.org/10.1145/1037947.1024414", 
  location     = "http://www.cs.rochester.edu/~cding/Documents/Publications/asplos04.pdf"
}

@Article{dtopmrcfmm,
  author       = "Pin Zhou and Vivek Pandey and Jagadeesan Sundaresan and Anand Raghuraman and Yuanyuan Zhou and Sanjeev Kumar",
  title        = "Dynamic tracking of page miss ratio curve for memory management",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "177--188",
  month        = nov,
  keywords     = "memory management, power management, resource allocation",
  abstract     = "Memory can be efficiently utilized if the dynamic memory
    demands of applications can be determined and analyzed at run-time.  The
    page miss ratio curve(MRC), i.e.  page miss rate vs.  memory size curve, is
    a good performance-directed metric to serve this purpose.  However,
    dynamically tracking MRC at run time is challenging in systems with virtual
    memory because not every memory reference passes through the operating
    system (OS).This paper proposes two methods to dynamically track MRC of
    applications at run time.  The first method is using a hardware MRC monitor
    that can track MRC at fine time granularity.  Our simulation results show
    that this monitor has negligible performance and energy overheads.  The
    second method is an OS-only implementation that can track MRC at coarse
    time granularity.  Our implementation results on Linux show that it adds
    only 7--10% overhead.We have also used the dynamic MRC to guide both memory
    allocation for multiprogramming systems and memory energy management.  Our
    real system experiments on Linux with applications including Apache Web
    Server show that the MRC-directed memory allocation can speed up the
    applications' execution/response time by up to a factor of 5.86 and reduce
    the number of page faults by up to 63.1%.  Our execution-driven simulation
    results with SPEC2000 benchmarks show that the MRC-directed memory energy
    management can improve the Energy * Delay metric by 27--58% over previously
    proposed static and dynamic schemes.", 
  location     = "https://doi.org/10.1145/1037947.1024415", 
  location     = "http://opera.ucsd.edu/paper/ASPLOS04-Zhou.pdf"
}

@Article{copvsap,
  author       = "Rodric~M. Rabbah and Hariharan Sandanagobalane and Mongkol Ekpanyapong and Weng-Fai Wong",
  title        = "Compiler orchestrated prefetching via speculation and predication",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "189--198",
  month        = nov,
  keywords     = "precomputation, speculation, predicted execution, prefetching",
  abstract     = "This paper introduces a compiler orchestrated prefetching
    system as a unified framework geared toward ameliorating the gap between
    processing speeds and memory access latencies.  We focus the scope of the
    optimization on specific subsets of the program dependence graph that
    succinctly characterize the memory access pattern of both regular
    array-based applications and irregular pointer-intensive programs.  We
    illustrate how program embedded precomputation via speculative execution
    can accurately predict and effectively prefetch future memory references
    with negligible overhead.  The proposed techniques reduce the total running
    time of seven SPEC benchmarks and two OLDEN benchmarks by 27% on an Itanium
    2 processor.  The improvements are in addition to several state-of-the-art
    optimizations including software pipelining and data prefetching.  In
    addition, we use cycle-accurate simulations to identify important and
    lightweight architectural innovations that further mitigate the memory
    system bottleneck.  We focus on the notoriously challenging class of
    pointer-chasing applications, and demonstrate how they may benefit from a
    novel scheme of it sentineled prefetching.  Our results for twelve SPEC
    benchmarks demonstrate that 45% of the processor stalls that are caused by
    the memory system are avoidable.  The techniques in this paper can
    effectively mask long memory latencies with little instruction overhead,
    and can readily contribute to the performance of processors today.", 
  location     = "https://doi.org/10.1145/1037947.1024416", 
  location     = "https://cs.uwaterloo.ca/~brecht/courses/702/Possible-Readings/prefetching-to-cache/compiler-orch-via-spec-and-prefetch-p189-rabbah-apslos-2004.pdf"
}

@Article{spfmsgchaasr,
  author       = "Chen-Yong Cher and Antony~L. Hosking and T.~N. Vijaykumar",
  title        = "Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "199--210",
  month        = nov,
  keywords     = "cache architecture, prefetching, garbage collection,
    mark-sweep collection, prefetch-on-grey, buffered prefetch, depth-first
    sweeps, breadth-first sweeps",
  abstract     = "Tracing garbage collectors traverse references from live
    program variables, transitively tracing out the closure of live objects.
    Memory accesses incurred during tracing are essentially random: a given
    object may contain references to any other object.  Since application heaps
    are typically much larger than hardware caches, tracing results in many
    cache misses.  Technology trends will make cache misses more important, so
    tracing is a prime target for prefetching.Simulation of Java benchmarks
    running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a
    projected hardware platform reveal high tracing overhead (up to 65% of
    elapsed time), and that cache misses are a problem.  Applying Boehm's
    default prefetching strategy yields improvements in execution time (16% on
    average with incremental/generational collection for GC-intensive
    benchmarks), but analysis shows that his strategy suffers from significant
    timing problems: prefetches that occur too early or too late relative to
    their matching loads.  This analysis drives development of a new
    prefetching strategy that yields up to three times the performance
    improvement of Boehm's strategy for GC-intensive benchmark (27% average
    speedup), and achieves performance close to that of perfect timing ie, few
    misses for tracing accesses) on some benchmarks.  Validating these
    simulation results with live runs on current hardware produces average
    speedup of 6% for the new strategy on GC-intensive benchmarks with a GC
    configuration that tightly controls heap growth.  In contrast, Boehm's
    default prefetching strategy is ineffective on this platform.", 
  location     = "https://doi.org/10.1145/1037947.1024417", 
  location     = "ftp://ftp.cs.purdue.edu/pub/hosking/papers/asplos04.pdf"
}

@Article{dsa,
  author       = "Timothy~E. Denehy and John Bent and Florentina~I. Popovici and Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Deconstructing storage arrays",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "59--71",
  month        = nov,
  keywords     = "storage, raid storage, statistical analysis, fingerprinting,
    reverse engineering, shear",
  abstract     = "We introduce Shear, a user-level software tool that
    characterizes RAID storage arrays.  Shear employs a set of controlled
    algorithms combined with statistical techniques to automatically determine
    the important properties of a RAID system, including the number of disks,
    chunk size, level of redundancy, and layout scheme.  We illustrate the
    correctness of Shear by running it upon numerous simulated configurations,
    and then verify its real-world applicability by running Shear on both
    software-based and hardware-based RAID systems.  Finally, we demonstrate
    the utility of Shear through three case studies.  First, we show how Shear
    can be used in a storage management environment to verify RAID construction
    and detect failures.  Second, we demonstrate how Shear can be used to
    extract detailed characteristics about the individual disks within an
    array.  Third, we show how an operating system can use Shear to
    automatically tune its storage subsystems to specific RAID
    configurations.",
  location     = "https://doi.org/10.1145/1037947.1024401",
  location     = "https://research.cs.wisc.edu/wind/Publications/shear-asplos04.ps"
}

@Article{fbdedafcc,
  author       = "Yasushi Saito and Svend Frølund and Alistair Veitch and Arif Merchant and Susan Spence",
  title        = "{FAB}: building distributed enterprise disk arrays from commodity components",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "48--58",
  month        = nov,
  keywords     = "disk storage, paxos voting, load balancing, erasure coding,
    replication, timestamping, dynamic reconfiguration, redundancy",
  abstract     = "This paper describes the design, implementation, and
    evaluation of a Federated Array of Bricks (FAB), a distributed disk array
    that provides the reliability of traditional enterprise arrays with lower
    cost and better scalability.  FAB is built from a collection of bricks,
    small storage appliances containing commodity disks, CPU, NVRAM, and
    network interface cards.  FAB deploys a new majority-voting-based algorithm
    to replicate or erasure-code logical blocks across bricks and a
    reconfiguration algorithm to move data in the background when bricks are
    added or decommissioned.  We argue that voting is practical and necessary
    for reliable, high-throughput storage systems such as FAB.  We have
    implemented a FAB prototype on a 22-node Linux cluster.  This prototype
    sustains 85MB/second of throughput for a database workload, and
    270MB/second for a bulk-read workload.  In addition, it can outperform
    traditional master-slave replication through performance decoupling and can
    handle brick failures and recoveries smoothly without disturbing client
    requests.",
  location     = "https://doi.org/10.1145/1037947.1024400"
}

@Article{haifepilotab,
  author       = "Xiaotong Zhuang and Tao Zhang and Santosh Pande",
  title        = "{HIDE}: an infrastructure for efficiently protecting information leakage on the address bus",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "72--84",
  month        = nov,
  keywords     = "secure processor, address bus leakage protection, random
    permutations, basic blocks, control flow graphs",
  abstract     = "XOM-based secure processor has recently been introduced as a
    mechanism to provide copy and tamper resistant execution.  XOM provides
    support for encryption/decryption and integrity checking.  However, neither
    XOM nor any other current approach adequately addresses the problem of
    information leakage via the address bus.  This paper shows that without
    address bus protection, the XOM model is severely crippled.  Two realistic
    attacks are shown and experiments show that 70% of the code might be
    cracked and sensitive data might be exposed leading to serious security
    breaches.Although the problem of address bus leakage has been widely
    acknowledged both in industry and academia, no practical solution has ever
    been proposed that can provide an adequate security guarantee.  The main
    reason is that the problem is very difficult to solve in practice due to
    severe performance degradation which accompanies most of the solutions.
    This paper presents an infrastructure called HIDE (Hardware-support for
    leakage-Immune Dynamic Execution) which provides a solution consisting of
    chunk-level protection with hardware support and a flexible interface which
    can be orchestrated through the proposed compiler optimization and user
    specifications that allow utilizing underlying hardware solution more
    efficiently to provide better security guarantees.Our results show that
    protecting both data and code with a high level of security guarantee is
    possible with negligible performance penalty (1.3% slowdown).", 
  location     = "https://doi.org/10.1145/1037947.1024403", 
  location     = "http://www.cs.wisc.edu/~rajwar/papers/asplos04.pdf"
}

@Article{spevdift,
  author       = "G.~Edward Suh and Jae~W. Lee and David Zhang and Srinivas Devadas",
  title        = "Secure program execution via dynamic information flow tracking",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "85--96",
  month        = nov,
  keywords     = "buffer overflow, format strings, hardware tagging, stack
    smashing, information flow analysis",
  abstract     = "We present a simple architectural mechanism called dynamic
    information flow tracking that can significantly improve the security of
    computing systems with negligible performance overhead.  Dynamic
    information flow tracking protects programs against malicious software
    attacks by identifying spurious information flows from untrusted I/O and
    restricting the usage of the spurious information.Every security attack to
    take control of a program needs to transfer the program's control to
    malevolent code.  In our approach, the operating system identifies a set of
    input channels as spurious, and the processor tracks all information flows
    from those inputs.  A broad range of attacks are effectively defeated by
    checking the use of the spurious values as instructions and pointers.Our
    protection is transparent to users or application programmers; the
    executables can be used without any modification.  Also, our scheme only
    incurs, on average, a memory overhead of 1.4% and a performance overhead of
    1.1%.", 
  location     = "https://doi.org/10.1145/1037947.1024404", 
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-467/memo-467.pdf"
}

@Article{cdmuoi,
  author       = "Jaehyuk Huh and Jichuan Chang and Doug Burger and Gurindar~S. Sohi",
  title        = "Coherence decoupling: making use of incoherence",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "97--106",
  month        = nov,
  keywords     = "coherence decoupling, speculative cache lookup, coherence
    misses, false sharing, caching protocols",
  abstract     = "This paper explores a new technique called coherence
    decoupling, which breaks a traditional cache coherence protocol into two
    protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing
    coherence protocol.  The SCL protocol produces a speculative load value,
    typically from an invalid cache line, permitting the processor to compute
    with incoherent data.  In parallel, the coherence protocol obtains the
    necessary coherence permissions and the correct value.  Eventually, the
    speculative use of the incoherent data can be verified against the coherent
    data.  Thus, coherence decoupling can greatly reduce --- if not eliminate
    --- the effects of false sharing.  Furthermore, coherence decoupling can
    also reduce latencies incurred by true sharing.  SCL protocols reduce those
    latencies by speculatively writing updates into invalid lines, thereby
    increasing the accuracy of speculation, without complicating the simple,
    underlying coherence protocol that guarantees correctness.The performance
    benefits of coherence decoupling are evaluated using a full-system
    simulator and a mix of commercial and scientific benchmarks.  Our results
    show that 40% to 90% of all coherence misses can be speculated correctly,
    and therefore their latencies partially or fully hidden.  This capability
    results in performance improvements ranging from 3% to over 16%, in most
    cases where the latencies of coherence misses have an effect on
    performance.", 
  location     = "https://doi.org/10.1145/1037947.1024406", 
  location     = "http://pages.cs.wisc.edu/~mscalar/papers/2004/asplos04-coherence-decoupling.pdf"
}

@Article{cfp,
  author       = "Srikanth~T. Srinivasan and Ravi Rajwar and Haitham Akkary and Amit Gandhi and Mike Upton",
  title        = "Continual flow pipelines",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "107--119",
  month        = nov,
  keywords     = "non-blocking, instruction window, latency tolerance, cfp,
    storage latency, instruction scheduling, register assignment, cache
    hierarchies", 
  abstract     = "Increased integration in the form of multiple processor cores
    on a single die, relatively constant die sizes, shrinking power envelopes,
    and emerging applications create a new challenge for processor architects.
    How to build a processor that provides high single-thread performance and
    enables multiple of these to be placed on the same die for high throughput
    while dynamically adapting for future applications? Conventional approaches
    for high single-thread performance rely on large and complex cores to
    sustain a large instruction window for memory tolerance, making them
    unsuitable for multi-core chips.  We present Continual Flow Pipelines (CFP)
    as a new non-blocking processor pipeline architecture that achieves the
    performance of a large instruction window without requiring cycle-critical
    structures such as the scheduler and register file to be large.  We show
    that to achieve benefits of a large instruction window, inefficiencies in
    management of both the scheduler and register file must be addressed, and
    we propose a unified solution.  The non-blocking property of CFP keeps key
    processor structures affecting cycle time and power (scheduler, register
    file), and die size (second level cache) small.  The memory
    latency-tolerant CFP core allows multiple cores on a single die while
    outperforming current processor cores for single-thread applications.", 
  location     = "https://doi.org/10.1145/1037947.1024407"
}

@Article{ssrefea,
  author       = "Rajagopalan Desikan and Simha Sethumadhavan and Doug Burger and Stephen~W. Keckler",
  title        = "Scalable selective re-execution for {EDGE} architectures",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "120--132",
  month        = nov,
  keywords     = "mis-speculation recovery, selective re-execution, dependence
    prediction, edge architectures, microarchitecture, speculation, speculative
    dataflow machines",
  abstract     = "Pipeline flushes are becoming increasingly expensive in
    modern microprocessors with large instruction windows and deep pipelines.
    Selective re-execution is a technique that can reduce the penalty of
    mis-speculations by re-executing only instructions affected by the
    mis-speculation, instead of all instructions.  In this paper we introduce a
    new selective re-execution mechanism that exploits the properties of a
    dataflow-like Explicit Data Graph Execution (EDGE) architecture to support
    efficient mis-speculation recovery, while scaling to window sizes of
    thousands of instructions with high performance.  This distributed
    selective re-execution (DSRE) protocol permits multiple speculative waves
    of computation to be traversing a dataflow graph simultaneously, with a
    commit wave propagating behind them to ensure correct execution.  We
    evaluate one application of this protocol to provide efficient recovery for
    load-store dependence speculation.  Unlike traditional dataflow
    architectures which resorted to single-assignment memory semantics, the
    DSRE protocol combines dataflow execution with speculation to enable high
    performance and conventional sequential memory semantics.  Our experiments
    show that the DSRE protocol results in an average 17% speedup over the best
    dependence predictor proposed to date, and obtains 82% of the performance
    possible with a perfect oracle directing the issue of loads.", 
  location     = "https://doi.org/10.1145/1037947.1024408", 
  location     = "https://www.cs.utexas.edu/~skeckler/pubs/asplos04.pdf"
}

@Article{dsdrdibbss,
  author       = "Christopher~R. Lumb and Richard Golding",
  title        = "{D}-{SPTF}: decentralized request distribution in brick-based storage systems",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "37--47",
  month        = nov,
  keywords     = "storage systems, brick-based storage, distributed systems,
    disk scheduling, decentralized systems, load balancing, global cache
    management",
  abstract     = "Distributed Shortest-Positioning Time First (D-SPTF) is a
    request distribution protocol for decentralized systems of storage servers.
    D-SPTF exploits high-speed interconnects to dynamically select which
    server, among those with a replica, should service each read request.  In
    doing so, it simultaneously balances load, exploits the aggregate cache
    capacity, and reduces positioning times for cache misses.  For network
    latencies expected in storage clusters (e.g., 10--200μs), D-SPTF performs
    as well as would a hypothetical centralized system with the same collection
    of CPU, cache, and disk resources.  Compared to popular decentralized
    approaches, D-SPTF achieves up to 65% higher throughput and adapts more
    cleanly to heterogenous server capabilities.", 
  location     = "https://doi.org/10.1145/1037947.1024399"
}

@Article{aulppfsn,
  author       = "Virantha Ekanayake and Clinton Kelly and Rajit Manohar",
  title        = "An ultra low-power processor for sensor networks",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "27--36",
  month        = nov,
  keywords     = "low-energy hardware, sensor networks, asynchronous circuits,
    wireless, sensor network processor, even-driven execution, picojoule
    computing",
  abstract     = "We present a novel processor architecture designed
    specifically for use in low-power wireless sensor-network nodes.  Our
    sensor network asynchronous processor (SNAP/LE) is based on an asynchronous
    data-driven 16-bit RISC core with an extremely low-power idle state, and a
    wakeup response latency on the order of tens of nanoseconds.  The processor
    instruction set is optimized for sensor-network applications, with support
    for event scheduling, pseudo-random number generation, bitfield operations,
    and radio/sensor interfaces.  SNAP/LE has a hardware event queue and event
    coprocessors, which allow the processor to avoid the overhead of operating
    system software (such as task schedulers and external interrupt servicing),
    while still providing a straightforward programming interface to the
    designer.  The processor can meet performance levels required for data
    monitoring applications while executing instructions with tens of
    picojoules of energy.We evaluate the energy consumption of SNAP/LE with
    several applications representative of the workload found in data-gathering
    wireless sensor networks.  We compare our architecture and software against
    existing platforms for sensor networks, quantifying both the software and
    hardware benefits of our approach.", 
  location     = "https://doi.org/10.1145/1037947.1024397"
}

@Article{sc2004,
  author       = "Mihai Budiu and Girish Venkataramani and Tiberiu Chelcea and Seth Copen Goldstein",
  title        = "Spatial computation",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "14--26",
  month        = nov,
  keywords     = "spatial computation, dataflow machine, application-specific
    hardware, low-power computing, hardware compilation, intermediate
    representation, superscalar architecture",
  abstract     = "This paper describes a computer architecture, Spatial
    Computation (SC), which is based on the translation of high-level language
    programs directly into hardware structures.  SC program implementations are
    completely distributed, with no centralized control.  SC circuits are
    optimized for wires at the expense of computation units.In this paper we
    investigate a particular implementation of SC: ASH (Application-Specific
    Hardware).  Under the assumption that computation is cheaper than
    communication, ASH replicates computation units to simplify interconnect,
    building a system which uses very simple, completely dedicated
    communication channels.  As a consequence, communication on the datapath
    never requires arbitration; the only arbitration required is for accessing
    memory.  ASH relies on very simple hardware primitives, using no
    associative structures, no multiported register files, no scheduling logic,
    no broadcast, and no clocks.  As a consequence, ASH hardware is fast and
    extremely power efficient.In this work we demonstrate three features of
    ASH: (1) that such architectures can be built by automatic compilation of C
    programs; (2) that distributed computation is in some respects
    fundamentally different from monolithic superscalar processors; and (3)
    that ASIC implementations of ASH use three orders of magnitude less energy
    compared to high-end superscalar processors, while being on average only
    33% slower in performance (3.5x worst-case).", 
  location     = "https://doi.org/10.1145/1037947.1024396", 
  location     = "https://acg.media.mit.edu/people/simong/thesis/SpatialComputing.pdf"
}

@Article{pwtcact,
  author       = "Lance Hammond and Brian~D. Carlstrom and Vicky Wong and Ben Hertzberg and Mike Chen and Christos Kozyrakis and Kunle Olukotun",
  title        = "Programming with transactional coherence and consistency ({TCC})",
  journal      = asplos04,
  year         = 2004,
  volume       = 39,
  number       = 11,
  pages        = "1--13",
  month        = nov,
  keywords     = "transactions, feedback optimization, multiprocessor
    architecture, parallelization",
  abstract     = "Transactional Coherence and Consistency (TCC) offers a way to
    simplify parallel programming by executing all code within transactions.
    In TCC systems, transactions serve as the fundamental unit of parallel
    work, communication and coherence.  As each transaction completes, it
    writes all of its newly produced state to shared memory atomically, while
    restarting other processors that have speculatively read stale data.  With
    this mechanism, a TCC-based system automatically handles data
    synchronization correctly, without programmer intervention.  To gain the
    benefits of TCC, programs must be decomposed into transactions.  We
    describe two basic programming language constructs for decomposing programs
    into transactions, a loop conversion syntax and a general
    transaction-forking mechanism.  With these constructs, writing correct
    parallel programs requires only small, incremental changes to correct
    sequential programs.  The performance of these programs may then easily be
    optimized, based on feedback from real program execution, using a few
    simple techniques.", 
  location     = "https://doi.org/10.1145/1037947.1024395", 
  location     = "http://csl.stanford.edu/~christos/publications/2004.tcc.asplos.slides.pdf"
}

@Article{saafwcsis,
  author       = "Matt Welsh and David Culler and Eric Brewer",
  title        = "{SEDA}: an architecture for well-conditioned, scalable internet services",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "230--243",
  month        = dec,
  keywords     = "string of pearls, event-driven architecture, pipeline
    architecture, threading, autonomic scalability",
  abstract     = "We propose a new design for highly concurrent Internet
    services, which we call the staged event-driven architecture (SEDA).  SEDA
    is intended to support massive concurrency demands and simplify the
    construction of well-conditioned services.  In SEDA, applications consist
    of a network of event-driven stages connected by explicit queues.  This
    architecture allows services to be well-conditioned to load, preventing
    resources from being overcommitted when demand exceeds service capacity.
    SEDA makes use of a set of dynamic resource controllers to keep stages
    within their operating regime despite large fluctuations in load.  We
    describe several control mechanisms for automatic tuning and load
    conditioning, including thread pool sizing, event batching, and adaptive
    load shedding.  We present the SEDA design and an implementation of an
    Internet services platform based on this architecture.  We evaluate the use
    of SEDA through two applications: a high-performance HTTP server and a
    packet router for the Gnutella peer-to-peer file sharing network.  These
    results show that SEDA applications exhibit higher performance than
    traditional service designs, and are robust to huge variations in load.", 
  location     = "https://doi.org/10.1145/502034.502057", 
  location     = "http://www.sosp.org/2001/papers/welsh.pdf"
}

@Article{barsbrunp,
  author       = "Tammo Spalink and Scott Karlin and Larry Peterson and Yitzchak Gottlieb",
  title        = "Building a robust software-based router using network processors",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "216--229",
  month        = dec,
  keywords     = "layered architecture, packet processing, network coprocessors",
  abstract     = "Recent efforts to add new services to the Internet have
    increased interest in software-based routers that are easy to extend and
    evolve.  This paper describes our experiences using emerging network
    processors---in particular, the Intel IXP1200---to implement a router.  We
    show it is possible to combine an IXP1200 development board and a PC to
    build an inexpensive router that forwards minimum-sized packets at a rate
    of 3.47Mpps.  This is nearly an order of magnitude faster than existing
    pure PC-based routers, and sufficient to support 1.77Gbps of aggregate link
    bandwidth.  At lesser aggregate line speeds, our design also allows the
    excess resources available on the IXP1200 to be used robustly for extra
    packet processing.  For example, with 8 × 100Mbps links, 240 register
    operations and 96 bytes of state storage are available for each 64-byte
    packet.  Using a hierarchical architecture we can guarantee line-speed
    forwarding rates for simple packets with the IXP1200, and still have extra
    capacity to process exceptional packets with the Pentium.  Up to 310Kpps of
    the traffic can be routed through the Pentium to receive 1510 cycles of
    extra per-packet processing.", 
  location     = "https://doi.org/10.1145/502034.502056", 
  location     = "https://people.eecs.berkeley.edu/~culler/courses/cs252-s05/papers/p216-spalink.pdf"
}

@Article{wacswc,
  author       = "Frank Dabek and M.~Frans Kaashoek and David Karger and Robert Morris and Ion Stoica",
  title        = "Wide-area cooperative storage with {CFS}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "202--215",
  month        = dec,
  keywords     = "peer-to-peer storage, chorus, distributed block storage,
    scalability, reliability, caching, distributed file systems, read-only file
    systems",
  abstract     = "The Cooperative File System (CFS) is a new peer-to-peer
    read-only storage system that provides provable guarantees for the
    efficiency, robustness, and load-balance of file storage and retrieval.
    CFS does this with a completely decentralized architecture that can scale
    to large systems.  CFS servers provide a distributed hash table (DHash) for
    block storage.  CFS clients interpret DHash blocks as a file system.  DHash
    distributes and caches blocks at a fine granularity to achieve load
    balance, uses replication for robustness, and decreases latency with server
    selection.  DHash finds blocks using the Chord location protocol, which
    operates in time logarithmic in the number of servers.CFS is implemented
    using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD.
    Experience on a globally deployed prototype shows that CFS delivers data to
    clients as fast as FTP.  Controlled tests show that CFS is scalable: with
    4,096 servers, looking up a block of data involves contacting only seven
    servers.  The tests also demonstrate nearly perfect robustness and
    unimpaired performance even when as many as half the servers fail.", 
  location     = "https://doi.org/10.1145/502034.502054", 
  location     = "https://pdos.csail.mit.edu/papers/cfs:sosp01/cfs_sosp.pdf"
}

@Article{oopwf,
  author       = "David~A. Moon",
  title        = "Object-Oriented Programming with {Flavors}",
  journal      = oopsla86,
  year         = 1986,
  volume       = 21,
  number       = 11,
  pages        = "1--8",
  month        = nov,
  keywords     = "lisp, symbolics, object-oriented programming, generic
    functions, data structures, modularity",
  abstract     = "This paper describes Symbolics' newly redesigned
    object-oriented programming system, Flavors.  Flavors encourages program
    modularity, eases the development of large, complex programs, and provides
    high efficiency at run time.  Flavors is integrated into Lisp and the
    Symbolics program development environment.  This paper describes the
    philosophy and some of the major characteristics of Symbolics' Flavors and
    shows how the above goals are addressed.  Full details of Flavors are left
    to the programmers' manual, Reference Guide to Symbolics Common Lisp.", 
  location     = "https://doi.org/10.1145/28697.28698", 
  location     = "https://www.cs.tufts.edu/comp/150FP/archive/david-moon/flavors.pdf"
}

@Article{asadsftodiisio,
  author       = "Sitaram Iyer and Peter Druschel",
  title        = "Anticipatory scheduling: a disk scheduling framework to overcome deceptive idleness in synchronous {I}/{O}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "117--130",
  month        = dec,
  keywords     = "",
  abstract     = "Disk schedulers in current operating systems are generally
    work-conserving, i.e., they schedule a request as soon as the previous
    request has finished.  Such schedulers often require multiple outstanding
    requests from each process to meet system-level goals of performance and
    quality of service.  Unfortunately, many common applications issue disk
    read requests in a synchronous manner, interspersing successive requests
    with short periods of computation.  The scheduler chooses the next request
    too early; this induces deceptive idleness, a condition where the scheduler
    incorrectly assumes that the last request issuing process has no further
    requests, and becomes forced to switch to a request from another process.We
    propose the anticipatory disk scheduling framework to solve this problem in
    a simple, general and transparent way, based on the non-work-conserving
    scheduling discipline.  Our FreeBSD implementation is observed to yield
    large benefits on a range of microbenchmarks and real workloads.  The
    Apache webserver delivers between 29% and 71% more throughput on a
    disk-intensive workload.  The Andrew filesystem benchmark runs faster by
    8%, due to a speedup of 54% in its read-intensive phase.  Variants of the
    TPC-B database benchmark exhibit improvements between 2% and 60%.
    Proportional-share schedulers are seen to achieve their contracts
    accurately and efficiently.", 
  location     = "https://doi.org/10.1145/502034.502046", 
  location     = "http://pdos.csail.mit.edu/6.824-2002/papers/iyer-scheduling.pdf"
}

@Article{ron,
  author       = "David Andersen and Hari Balakrishnan and Frans Kaashoek and Robert Morris",
  title        = "Resilient overlay networks",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "131--145",
  month        = dec,
  keywords     = "internet performance, failure detection, application-level
    networking, policy routing",
  abstract     = "A Resilient Overlay Network (RON) is an architecture that
    allows distributed Internet applications to detect and recover from path
    outages and periods of degraded performance within several seconds,
    improving over today's wide-area routing protocols that take at least
    several minutes to recover.  A RON is an application-layer overlay on top
    of the existing Internet routing substrate.  The RON nodes monitor the
    functioning and quality of the Internet paths among themselves, and use
    this information to decide whether to route packets directly over the
    Internet or by way of other RON nodes, optimizing application-specific
    routing metrics.Results from two sets of measurements of a working RON
    deployed at sites scattered across the Internet demonstrate the benefits of
    our architecture.  For instance, over a 64-hour sampling period in March
    2001 across a twelve-node RON, there were 32 significant outages, each
    lasting over thirty minutes, over the 132 measured paths.  RON's routing
    mechanism was able to detect, recover, and route around all of them, in
    less than twenty seconds on average, showing that its methods for fault
    detection and recovery work well at discovering alternate paths in the
    Internet.  Furthermore, RON was able to improve the loss rate, latency, or
    throughput perceived by data transfers; for example, about 5% of the
    transfers doubled their TCP throughput and 5% of our transfers saw their
    loss probability reduced by 0.05.  We found that forwarding packets via at
    most one intermediate RON node is sufficient to overcome faults and improve
    performance in most cases.  These improvements, particularly in the area of
    fault detection and recovery, demonstrate the benefits of moving some of
    the control over routing into the hands of end-systems.", 
  location     = "https://doi.org/10.1145/502034.502048", 
  location     = "http://nms.lcs.mit.edu/papers/ron-sosp2001.html"
}

@Article{measrihc,
  author       = "Jeffrey~S. Chase and Darrell~C. Anderson and Prachi~N. Thakar and Amin~M. Vahdat and Ronald~P. Doyle",
  title        = "Managing energy and server resources in hosting centers",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "103--116",
  month        = dec,
  keywords     = "energy management, resource provisioning, economic modeling,
    pricing",
  abstract     = "Internet hosting centers serve multiple service sites from a
    common hardware base.  This paper presents the design and implementation of
    an architecture for resource management in a hosting center operating
    system, with an emphasis on energy as a driving resource management issue
    for large server clusters.  The goals are to provision server resources for
    co-hosted services in a way that automatically adapts to offered load,
    improve the energy efficiency of server clusters by dynamically resizing
    the active server set, and respond to power supply disruptions or thermal
    events by degrading service in accordance with negotiated Service Level
    Agreements (SLAs).Our system is based on an economic approach to managing
    shared server resources, in which services 'bid' for resources as a
    function of delivered performance.  The system continuously monitors load
    and plans resource allotments by estimating the value of their effects on
    service performance.  A greedy resource allocation algorithm adjusts
    resource prices to balance supply and demand, allocating resources to their
    most efficient use.  A reconfigurable server switching infrastructure
    directs request traffic to the servers assigned to each service.
    Experimental results from a prototype confirm that the system adapts to
    offered load and resource availability, and can reduce server energy usage
    by 29% or more for a typical Web workload.", 
  location     = "https://doi.org/10.1145/502034.502045", 
  location     = "http://www.sosp.org/2001/papers/chase.pdf"
}

@Article{bewsnwlln,
  author       = "John Heidemann and Fabio Silva and Chalermek Intanagonwiwat and Ramesh Govindan and Deborah Estrin and Deepak Ganesan",
  title        = "Building efficient wireless sensor networks with low-level naming",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "145--159",
  month        = dec,
  keywords     = "attribute-based naming, sensor networks, in-network
    processing, directed diffusion, declarative routing",
  abstract     = "In most distributed systems, naming of nodes for low-level
    communication leverages topological location (such as node addresses) and
    is independent of any application.  In this paper, we investigate an
    emerging class of distributed systems where low-level communication does
    not rely on network topological location.  Rather, low-level communication
    is based on attributes that are external to the network topology and
    relevant to the application.  When combined with dense deployment of nodes,
    this kind of named data enables in-network processing for data aggregation,
    collaborative signal processing, and similar problems.  These approaches
    are essential for emerging applications such as sensor networks where
    resources such as bandwidth and energy are limited.  This paper is the
    first description of the software architecture that supports named data and
    in-network processing in an operational, multi-application sensor-network.
    We show that approaches such as in-network aggregation and nested queries
    can significantly affect network traffic.  In one experiment aggregation
    reduces traffic by up to 42% and nested queries reduce loss rates by 30%.
    Although aggregation has been previously studied in simulation, this paper
    demonstrates nested queries as another form of in-network processing, and
    it presents the first evaluation of these approaches over an operational
    testbed.", 
  location     = "https://doi.org/10.1145/502034.502049"
}

@Article{mbcrux,
  author       = "Alex~C. Snoeren and Kenneth Conley and David~K. Gifford",
  title        = "Mesh-based content routing using {XML}",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "160--173",
  month        = dec,
  keywords     = "overlay networks, mesh networks, routing, xml",
  abstract     = "We have developed a new approach for reliably multicasting
    time-critical data to heterogeneous clients over mesh-based overlay
    networks.  To facilitate intelligent content pruning, data streams are
    comprised of a sequence of XML packets and forwarded by application-level
    XML routers.  XML routers perform content-based routing of individual XML
    packets to other routers or clients based upon queries that describe the
    information needs of downstream nodes.  Our PC-based XML router prototype
    can route an 18 Mbit per second XML stream.Our routers use a novel
    Diversity Control Protocol (DCP) for router-to-router and router-to-client
    communication.  DCP reassembles a received stream of packets from one or
    more senders using the first copy of a packet to arrive from any sender.
    When each node is connected to n parents, the resulting network is
    resilient to (n − 1) router or independent link failures without repair.
    Associated mesh algorithms permit the system to recover to (n − 1)
    resilience after node and/or link failure.  We have deployed a distributed
    network of XML routers that streams real-time air traffic control data.
    Experimental results show multiple senders improve reliability and latency
    when compared to tree-based networks.", 
  location     = "https://doi.org/10.1145/502034.502050", 
  location     = "http://cseweb.ucsd.edu/~snoeren/papers/xml-sosp01.pdf"
}

@Article{albnfs,
  author       = "Athicha Muthitacharoen and Benjie Chen and David Mazi{\` e}res",
  title        = "{A} low-bandwidth network file system",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "174--187",
  month        = dec,
  keywords     = "redundancy, hash coding, file systems, low-bandwidth networks",
  abstract     = "Users rarely consider running network file systems over slow
    or wide-area networks, as the performance would be unacceptable and the
    bandwidth consumption too high.  Nonetheless, efficient remote file access
    would often be desirable over such networks---particularly when high
    latency makes remote login sessions unresponsive.  Rather than run
    interactive programs such as editors remotely, users could run the programs
    locally and manipulate remote files through the file system.  To do so,
    however, would require a network file system that consumes less bandwidth
    than most current file systems.This paper presents LBFS, a network file
    system designed for low-bandwidth networks.  LBFS exploits similarities
    between files or versions of the same file to save bandwidth.  It avoids
    sending data over the network when the same data can already be found in
    the server's file system or the client's cache.  Using this technique in
    conjunction with conventional compression and caching, LBFS consumes over
    an order of magnitude less bandwidth than traditional network file systems
    on common workloads.", 
  location     = "https://doi.org/10.1145/502034.502052", 
  location     = "https://pdos.csail.mit.edu/papers/lbfs:sosp01/lbfs.pdf"
}

@Article{smacipalspptpsu,
  author       = "Antony Rowstron and Peter Druschel",
  title        = "Storage management and caching in {PAST}, a large-scale, persistent peer-to-peer storage utility",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "188--201",
  month        = dec,
  keywords     = "pastry, storage management, hash routing, replica management,
    caching, peer-to-peer systems",
  abstract     = "This paper presents and evaluates the storage management and
    caching in PAST, a large-scale peer-to-peer persistent storage utility.
    PAST is based on a self-organizing, Internet-based overlay network of
    storage nodes that cooperatively route file queries, store multiple
    replicas of files, and cache additional copies of popular files.In the PAST
    system, storage nodes and files are each assigned uniformly distributed
    identifiers, and replicas of a file are stored at nodes whose identifier
    matches most closely the file's identifier.  This statistical assignment of
    files to storage nodes approximately balances the number of files stored on
    each node.  However, non-uniform storage node capacities and file sizes
    require more explicit storage load balancing to permit graceful behavior
    under high global storage utilization; likewise, non-uniform popularity of
    files requires caching to minimize fetch distance and to balance the query
    load.We present and evaluate PAST, with an emphasis on its storage
    management and caching system.  Extensive trace-driven experiments show
    that the system minimizes fetch distance, that it balances the query load
    for popular files, and that it displays graceful degradation of performance
    as the global storage utilization increases beyond 95%.", 
  location     = "https://doi.org/10.1145/502034.502053", 
  location     = "http://www.cs.cornell.edu/People/egs/615/past.pdf"
}

@Article{rtdvsflpeos,
  author       = "Padmanabhan Pillai and Kang~G. Shin",
  title        = "Real-time dynamic voltage scaling for low-power embedded operating systems",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "89--102",
  month        = dec,
  keywords     = "voltage scaling, power management",
  abstract     = "In recent years, there has been a rapid and wide spread of
    non-traditional computing platforms, especially mobile and portable
    computing devices.  As applications become increasingly sophisticated and
    processing power increases, the most serious limitation on these devices is
    the available battery life.  Dynamic Voltage Scaling (DVS) has been a key
    technique in exploiting the hardware characteristics of processors to
    reduce energy dissipation by lowering the supply voltage and operating
    frequency.  The DVS algorithms are shown to be able to make dramatic energy
    savings while providing the necessary peak computation power in
    general-purpose systems.  However, for a large class of applications in
    embedded real-time systems like cellular phones and camcorders, the
    variable operating frequency interferes with their deadline guarantee
    mechanisms, and DVS in this context, despite its growing importance, is
    largely overlooked/under-developed.  To provide real-time guarantees, DVS
    must consider deadlines and periodicity of real-time tasks, requiring
    integration with the real-time scheduler.  In this paper, we present a
    class of novel algorithms called real-time DVS (RT-DVS) that modify the
    OS's real-time scheduler and task management service to provide significant
    energy savings while maintaining real-time deadline guarantees.  We show
    through simulations and a working prototype implementation that these
    RT-DVS algorithms closely approach the theoretical lower bound on energy
    consumption, and can easily reduce energy consumption 20% to 40% in an
    embedded real-time system.", 
  location     = "https://doi.org/10.1145/502034.502044", 
  location     = "http://www.sosp.org/2001/papers/pillai.pdf"
}

@Article{aesoose,
  author       = "Andy Chou and Junfeng Yang and Benjamin Chelf and Seth Hallem and Dawson Engler",
  title        = "An empirical study of operating systems errors",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "73--88",
  month        = dec,
  keywords     = "error discovery, statistical analysis, error cliches, error
    metrics", 
  abstract     = "We present a study of operating system errors found by
    automatic, static, compiler analysis applied to the Linux and OpenBSD
    kernels.  Our approach differs from previous studies that consider errors
    found by manual inspection of logs, testing, and surveys because static
    analysis is applied uniformly to the entire kernel source, though our
    approach necessarily considers a less comprehensive variety of errors than
    previous studies.  In addition, automation allows us to track errors over
    multiple versions of the kernel source to estimate how long errors remain
    in the system before they are fixed.We found that device drivers have error
    rates up to three to seven times higher than the rest of the kernel.  We
    found that the largest quartile of functions have error rates two to six
    times higher than the smallest quartile.  We found that the newest quartile
    of files have error rates up to twice that of the oldest quartile, which
    provides evidence that code 'hardens' over time.  Finally, we found that
    bugs remain in the Linux kernel an average of 1.8 years before being
    fixed.", 
  location     = "https://doi.org/10.1145/502034.502042", 
  location     = "https://pdos.csail.mit.edu/archive/6.097/readings/osbugs.pdf"
}

@Article{badbagatieisc,
  author       = "Dawson Engler and David Yu Chen and Seth Hallem and Andy Chou and Benjamin Chelf",
  title        = "Bugs as deviant behavior: a general approach to inferring errors in systems code",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "57--72",
  month        = dec,
  keywords     = "static analysis, statistical sorting, error analysis",
  abstract     = "A major obstacle to finding program errors in a real system
    is knowing what correctness rules the system must obey.  These rules are
    often undocumented or specified in an ad hoc manner.  This paper
    demonstrates techniques that automatically extract such checking
    information from the source code itself, rather than the programmer,
    thereby avoiding the need for a priori knowledge of system rules.The
    cornerstone of our approach is inferring programmer 'beliefs' that we then
    cross-check for contradictions.  Beliefs are facts implied by code: a
    dereference of a pointer, p, implies a belief that p is non-null, a call to
    'unlock(1)' implies that 1 was locked, etc.  For beliefs we know the
    programmer must hold, such as the pointer dereference above, we immediately
    flag contradictions as errors.  For beliefs that the programmer may hold,
    we can assume these beliefs hold and use a statistical analysis to rank the
    resulting errors from most to least likely.  For example, a call to
    'spin_lock' followed once by a call to 'spin_unlock' implies that the
    programmer may have paired these calls by coincidence.  If the pairing
    happens 999 out of 1000 times, though, then it is probably a valid belief
    and the sole deviation a probable error.  The key feature of this approach
    is that it requires no a priori knowledge of truth: if two beliefs
    contradict, we know that one is an error without knowing what the correct
    belief is.Conceptually, our checkers extract beliefs by tailoring rule
    'templates' to a system --- for example, finding all functions that fit the
    rule template 'a must be paired with b.' We have developed six checkers
    that follow this conceptual framework.  They find hundreds of bugs in real
    systems such as Linux and OpenBSD.  From our experience, they give a
    dramatic reduction in the manual effort needed to check a large system.
    Compared to our previous work [9], these template checkers find ten to one
    hundred times more rule instances and derive properties we found
    impractical to specify manually.", 
  location     = "https://doi.org/10.1145/502034.502041", 
  location     = "https://web.stanford.edu/~engler/deviant-sosp-01.pdf"
}

@Article{iacigbs,
  author       = "Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Information and control in gray-box systems",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "43--56",
  month        = dec,
  keywords     = "experimentation, file-cache management, disk layouts,
    admission control",
  abstract     = "In modern systems, developers are often unable to modify the
    underlying operating system.  To build services in such an environment, we
    advocate the use of gray-box techniques.  When treating the operating
    system as a gray-box, one recognizes that not changing the OS restricts,
    but does not completely obviate, both the information one can acquire about
    the internal state of the OS and the control one can impose on the OS.  In
    this paper, we develop and investigate three gray-box Information and
    Control Layers (ICLs) for determining the contents of the file-cache,
    controlling the layout of files across local disk, and limiting process
    execution based on available memory.  A gray-box ICL sits between a client
    and the OS and uses a combination of algorithmic knowledge, observations,
    and inferences to garner information about or control the behavior of a
    gray-box system.  We summarize a set of techniques that are helpful in
    building gray-box ICLs and have begun to organize a 'gray toolbox' to ease
    the construction of ICLs.  Through our case studies, we demonstrate the
    utility of gray-box techniques, by implementing three useful 'OS-like'
    services without the modification of a single line of OS source code.", 
  location     = "https://doi.org/10.1145/502034.502040", 
  location     = "http://www.sosp.org/2001/papers/arpacidusseau.pdf"
}

@Article{tcaloafrs,
  author       = "Haifeng Yu and Amin Vahdat",
  title        = "The costs and limits of availability for replicated services",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "29--42",
  month        = dec,
  keywords     = "replication, performance, availability",
  abstract     = "As raw system and network performance continues to improve at
    exponential rates, the utility of many services is increasingly limited by
    availability rather than performance.  A key approach to improving
    availability involves replicating the service across multiple, wide-area
    sites.  However, replication introduces well-known tradeoffs between
    service consistency and availability.  Thus, this paper explores the
    benefits of dynamically trading consistency for availability using a
    continuous consistency model.  In this model, applications specify a
    maximum deviation from strong consistency on a per-replica basis.  In this
    paper, we: i) evaluate availability of a prototype replication system
    running across the Internet as a function of consistency level, consistency
    protocol, and failure characteristics, ii) demonstrate that simple
    optimizations to existing consistency protocols result in significant
    availability improvements (more than an order of magnitude in some
    scenarios), iii) use our experience with these optimizations to prove tight
    upper bounds on the availability of services, and iv) show that maximizing
    availability typically entails remaining as close to strong consistency as
    possible during times of good connectivity, resulting in a communication
    versus availability trade-off.", 
  location     = "https://doi.org/10.1145/502034.502038", 
  location     = "https://users.cs.duke.edu/~vahdat/ps/tr-cs-2001-03.pdf"
}

@Article{buatift,
  author       = "Rodrigo Rodrigues and Miguel Castro and Barbara Liskov",
  title        = "{BASE}: using abstraction to improve fault tolerance",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "15--28",
  month        = dec,
  keywords     = "Byzantine fault tolerance, middleware, abstraction",
  abstract     = "Software errors are a major cause of outages and they are
    increasingly exploited in malicious attacks.  Byzantine fault tolerance
    allows replicated systems to mask some software errors but it is expensive
    to deploy.  This paper describes a replication technique, BASE, which uses
    abstraction to reduce the cost of Byzantine fault tolerance and to improve
    its ability to mask software errors.  BASE reduces cost because it enables
    reuse of off-the-shelf service implementations.  It improves availability
    because each replica can be repaired periodically using an abstract view of
    the state stored by correct replicas, and because each replica can run
    distinct or non-deterministic service implementations, which reduces the
    probability of common mode failures.  We built an NFS service where each
    replica can run a different off-the-shelf file system implementation, and
    an object-oriented database where the replicas ran the same,
    non-deterministic implementation.  These examples suggest that our
    technique can be used in practice --- in both cases, the implementation
    required only a modest amount of new code, and our performance results
    indicate that the replicated services perform comparably to the
    implementations that they reuse.", 
  location     = "https://doi.org/10.1145/502034.502037", 
  location     = "http://www.cs.cornell.edu/People/egs/cornellonly/syslunch/fall01/sosp/rodrigues.pdf"
}

@Article{uhacspp,
  author       = "Steve Zdancewic and Lantian Zheng and Nathaniel Nystrom and Andrew~C. Myers",
  title        = "Untrusted hosts and confidentiality: secure program partitioning",
  journal      = sosp01,
  year         = 2001,
  volume       = 35,
  number       = 5,
  pages        = "1--14",
  month        = dec,
  keywords     = "security annotations, program partitioning, information flow,
    control transfer",
  abstract     = "This paper presents secure program partitioning, a
    language-based technique for protecting confidential data during
    computation in distributed systems containing mutually untrusted hosts.
    Confidentiality and integrity policies can be expressed by annotating
    programs with security types that constrain information flow; these
    programs can then be partitioned automatically to run securely on
    heterogeneously trusted hosts.  The resulting communicating subprograms
    collectively implement the original program, yet the system as a whole
    satisfies the security requirements of participating principals without
    requiring a universally trusted host machine.  The experience in applying
    this methodology and the performance of the resulting distributed code
    suggest that this is a promising way to obtain secure distributed
    computation.", 
  location     = "https://doi.org/10.1145/502034.502036", 
  location     = "https://www.cs.cornell.edu/andru/papers/sosp01/zznm01.pdf"
}

@Article{upbtm,
  author       = "Weihaw Chuang and Satish Narayanasamy and Ganesh Venkatesh and Jack Sampson and Michael Van Biesbrouck and Gilles Pokam and Brad Calder and Osvaldo Colavin",
  title        = "Unbounded Page-Based Transactional Memory",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "347--358",
  month        = oct,
  keywords     = "transactions, transactional memory, parallel programming,
    concurrency, virtual memory, transactional hardware",
  abstract     = "Exploiting thread level parallelism is paramount in the
    multicore era.  Transactions enable programmers to expose such parallelism
    by greatly simplifying the multi-threaded programming model.  Virtualized
    transactions (unbounded in space and time) are desirable, as they can
    increase the scope of transactions' use, and thereby further simplify a
    programmer's job.  However, hardware support is essential to support
    efficient execution of unbounded transactions.  In this paper, we introduce
    Page-based Transactional Memory to support unbounded transactions.  We
    combine transaction bookkeeping with the virtual memory system to support
    fast transaction conflict detection, commit, abort, and to maintain
    transactions' speculative data.", 
  location     = "https://doi.org/10.1145/1168919.1168901",
  location     = "https://cseweb.ucsd.edu/~calder/papers/ASPLOS-06-PTM.pdf"
}

@Article{sntmil,
  author       = "Michelle~J. Moravan and Jayaram Bobba and Kevin~E. Moore and Luke Yen and Mark~D. Hill and Ben Liblit and Michael~M. Swift and David~A. Wood",
  title        = "Supporting nested transactional memory in {LogTM}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "359--370",
  month        = nov,
  keywords     = "nested transactions, transactional storage, composition,
    partial aborts, open nesting, escaping actions, concurrency",
  abstract     = "Nested transactional memory (TM) facilitates software
    composition by letting one module invoke another without either knowing
    whether the other uses transactions.  Closed nested transactions extend
    isolation of an inner transaction until the toplevel transaction commits.
    Implementations may flatten nested transactions into the top-level one,
    resulting in a complete abort on conflict, or allow partial abort of inner
    transactions.  Open nested transactions allow a committing inner
    transaction to immediately release isolation, which increases parallelism
    and expressiveness at the cost of both software and hardware
    complexity.This paper extends the recently-proposed flat Log-based
    Transactional Memory (LogTM) with nested transactions.  Flat LogTM saves
    pre-transaction values in a log, detects conflicts with read (R) and write
    (W) bits per cache block, and, on abort, invokes a software handler to
    unroll the log.  Nested LogTM supports nesting by segmenting the log into a
    stack of activation records and modestly replicating R/W bits.  To
    facilitate composition with nontransactional code, such as language runtime
    and operating system services, we propose escape actions that allow trusted
    code to run outside the confines of the transactional memory system.", 
  location     = "https://doi.org/10.1145/1168857.1168902", 
  location     = "https://research.cs.wisc.edu/multifacet/papers/asplos06_nested_logtm.pdf"
}

@Article{titmv,
  author       = "JaeWoong Chung and Chi Cao Minh and Austen McDonald and Travis Skare and Hassan Chafi and Brian~D. Carlstrom and Christos Kozyrakis and Kunle Olukotun",
  title        = "Tradeoffs in transactional memory virtualization",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "371--381",
  month        = nov,
  keywords     = "chip multi-processors, os support, transactional memory,
    virtualization, virtualization",
  abstract     = "For transactional memory (TM) to achieve widespread
    acceptance, transactions should not be limited to the physical resources of
    any specific hardware implementation.  TM systems should guarantee correct
    execution even when transactions exceed scheduling quanta, overflow the
    capacity of hardware caches and physical memory, or include more
    independent nesting levels than what is supported in hardware.  Existing
    proposals for TM virtualization are either incomplete or rely on complex
    hardware implementations, which are an overkill if virtualization is
    invoked infrequently in the common case.We present eXtended Transactional
    Memory (XTM), the first TM virtualization system that virtualizes all
    aspects of transactional execution (time, space, and nesting depth).  XTM
    is implemented in software using virtual memory support.  It operates at
    page granularity, using private copies of overflowed pages to buffer memory
    updates until the transaction commits and snapshots of pages to detect
    interference between transactions.  We also describe two enhancements to
    XTM that use limited hardware support to address key performance
    bottlenecks.We compare XTM to hardwarebased virtualization using both real
    applications and synthetic microbenchmarks.  We show that despite being
    software-based, XTM and its enhancements are competitive with
    hardware-based alternatives.  Overall, we demonstrate that XTM provides a
    complete, flexible, and low-cost mechanism for practical TM
    virtualization.", 
  location     = "https://doi.org/10.1145/1168857.1168903", 
  location     = "http://csl.stanford.edu/~christos/publications/2006.tm_virtualization.asplos.pdf"
}

@Article{anirffehai,
  author       = "Motohiro Kawahito and Hideaki Komatsu and Takao Moriyama and Hiroshi Inoue and Toshio Nakatani",
  title        = "A new idiom recognition framework for exploiting hardware-assist instructions",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "382--393",
  month        = nov,
  keywords     = "idiom recognition, hardware-assist instructions, vmx,
    topological embedding, java, jits, code improvements, graph matching",
  abstract     = "Modern processors support hardware-assist instructions (such
    as TRT and TROT instructions on IBM zSeries) to accelerate certain
    functions such as delimiter search and character conversion.  Such special
    instructions have often been used in high performance libraries, but they
    have not been exploited well in optimizing compilers except for some
    limited cases.  We propose a new idiom recognition technique derived from a
    topological embedding algorithm [4] to detect idiom patterns in the input
    program more aggressively than in previous approaches.  Our approach can
    detect a pattern even if the code segment does not exactly match the idiom.
    For example, we can detect a code segment that includes additional code
    within the idiom pattern.  We implemented our new idiom recognition
    approach based on the Java Just-In-Time (JIT) compiler that is part of the
    J9 Java Virtual Machine, and we supported several important idioms for
    special hardware-assist instructions on the IBM zSeries and on some models
    of the IBM pSeries.  To demonstrate the effectiveness of our technique, we
    performed two experiments.  The first one is to see how many more patterns
    we can detect compared to the previous approach.  The second one is to see
    how much performance improvement we can achieve over the previous approach.
    For the first experiment, we used the Java Compatibility Kit (JCK) API
    tests.  For the second one we used IBM XML parser, SPECjvm98, and
    SPCjbb2000.  In summary, relative to a baseline implementation using exact
    pattern matching, our algorithm converted 75% more loops in JCK tests.  We
    also observed significant performance improvement of the XML parser by 64%,
    of SPECjvm98 by 1%, and of SPECjbb2000 by 2% on average on a z990.
    Finally, we observed the JIT compilation time increases by only 0.32% to
    0.44%.", 
  location     = "https://doi.org/10.1145/1168857.1168905"
}

@Article{agops,
  author       = "Sorav Bansal and Alex Aiken",
  title        = "Automatic generation of peephole superoptimizers",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "394--403",
  month        = nov,
  keywords     = "superoptimization, peephole optimization, code selection,
    searching, satisfiability solvers",
  abstract     = "Peephole optimizers are typically constructed using
    human-written pattern matching rules, an approach that requires expertise
    and time, as well as being less than systematic at exploiting all
    opportunities for optimization.  We explore fully automatic construction of
    peephole optimizers using brute force superoptimization.  While the
    optimizations discovered by our automatic system may be less general than
    human-written counterparts, our approach has the potential to automatically
    learn a database of thousands to millions of optimizations, in contrast to
    the hundreds found in current peephole optimizers.  We show experimentally
    that our optimizer is able to exploit performance opportunities not found
    by existing compilers; in particular, we show speedups from 1.7 to a factor
    of 10 on some compute intensive kernels over a conventional optimizing
    compiler.", 
  location     = "https://doi.org/10.1145/1168857.1168906", 
  location     = "https://theory.stanford.edu/~aiken/publications/papers/asplos06.pdf"
}

@Article{csffp,
  author       = "Armando Solar-Lezama and Liviu Tancau and Rastislav Bodik and Sanjit Seshia and Vijay Saraswat",
  title        = "Combinatorial sketching for finite programs",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "404--415",
  month        = nov,
  keywords     = "sketching, sat, program generation, template programming,
    code synthesis",
  abstract     = "Sketching is a software synthesis approach where the
    programmer develops a partial implementation - a sketch - and a separate
    specification of the desired functionality.  The synthesizer then completes
    the sketch to behave like the specification.  The correctness of the
    synthesized implementation is guaranteed by the compiler, which allows,
    among other benefits, rapid development of highly tuned implementations
    without the fear of introducing bugs.We develop SKETCH, a language for
    finite programs with linguistic support for sketching.  Finite programs
    include many highperformance kernels, including cryptocodes.  In contrast
    to prior synthesizers, which had to be equipped with domain-specific rules,
    SKETCH completes sketches by means of a combinatorial search based on
    generalized boolean satisfiability.  Consequently, our combinatorial
    synthesizer is complete for the class of finite programs: it is guaranteed
    to complete any sketch in theory, and in practice has scaled to realistic
    programming problems.Freed from domain rules, we can now write sketches as
    simpleto-understand partial programs, which are regular programs in which
    difficult code fragments are replaced with holes to be filled by the
    synthesizer.  Holes may stand for index expressions, lookup tables, or
    bitmasks, but the programmer can easily define new kinds of holes using a
    single versatile synthesis operator.We have used SKETCH to synthesize an
    efficient implementation of the AES cipher standard.  The synthesizer
    produces the most complex part of the implementation and runs in about an
    hour.", 
  location     = "https://doi.org/10.1145/1168857.1168907", 
  location     = "https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/combinatorial_sketching.pdf"
}

@Article{appafso,
  author       = "Jeff {Da Silva} and J.~Gregory Steffan",
  title        = "{A} probabilistic pointer analysis for speculative optimizations",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "416--425",
  month        = nov,
  keywords     = "dependence analysis, pointer analysis, speculative
    optimization",
  abstract     = "Pointer analysis is a critical compiler analysis used to
    disambiguate the indirect memory references that result from the use of
    pointers and pointer-based data structures.  A conventional pointer
    analysis deduces for every pair of pointers, at any program point, whether
    a points-to relation between them (i) definitely exists, (ii) definitely
    does not exist, or (iii) maybe exists.  Many compiler optimizations rely on
    accurate pointer analysis, and to ensure correctness cannot optimize in the
    maybe case.  In contrast, recently-proposed speculative optimizations can
    aggressively exploit the maybe case, especially if the likelihood that two
    pointers alias can be quantified.  This paper proposes a Probabilistic
    Pointer Analysis (PPA) algorithm that statically predicts the probability
    of each points-to relation at every program point.  Building on simple
    control-flow edge profiling, our analysis is both one-level context and
    flow sensitive-yet can still scale to large programs including the SPEC
    2000 integer benchmark suite.  The key to our approach is to compute
    points-to probabilities through the use of linear transfer functions that
    are efficiently encoded as sparse matrices.We demonstrate that our analysis
    can provide accurate probabilities, even without edge-profile information.
    We also find that-even without considering probability information-our
    analysis provides an accurate approach to performing pointer analysis.", 
  location     = "https://doi.org/10.1145/1168857.1168908"
}

@Article{spjfc,
  author       = "Jason~F. Cantin and Mikko~H. Lipasti and James~E. Smith",
  title        = "Stealth prefetching",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "274--282",
  month        = nov,
  keywords     = "prefetching, prefetch policy, exclusive access, precaching",
  abstract     = "Prefetching in shared-memory multiprocessor systems is an
    increasingly difficult problem.  As system designs grow to incorporate
    larger numbers of faster processors, memory latency and interconnect
    traffic increase.  While aggressive prefetching techniques can mitigate the
    increasing memory latency, they can harm performance by wasting precious
    interconnect bandwidth and prematurely accessing shared data, causing state
    downgrades at remote nodes that force later upgrades.This paper
    investigates Stealth Prefetching, a new technique that utilizes information
    from Coarse-Grain Coherence Tracking (CGCT) for prefetching data
    aggressively, stealthily, and efficiently in a broadcast-based
    shared-memory multiprocessor system.  Stealth Prefetching utilizes CGCT to
    identify regions of memory that are not shared by other processors,
    aggressively fetches these lines from DRAM in open-page mode, and moves
    them close to the processor in anticipation of future references.  Our
    analysis with commercial, scientific, and multiprogrammed workloads show
    that Stealth Prefetching provides an average speedup of 20% over an
    aggressive baseline system with conventional prefetching.", 
  location     = "https://doi.org/10.1145/1168857.1168892", 
  location     = "https://pharm.ece.wisc.edu/papers/asplos2006_final.pdf"
}

@Article{csehmtsccotf,
  author       = "Koushik Chakraborty and Philip~M. Wells and Gurindar~S. Sohi",
  title        = "Computation spreading: employing hardware migration to specialize {CMP} cores on-the-fly",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "283--292
    ",
  month        = nov,
  keywords     = "dynamic specialization, cache locality, code reuse, thread
    migration, thread assignment",
  abstract     = "In canonical parallel processing, the operating system (OS)
    assigns a processing core to a single thread from a multithreaded server
    application.  Since different threads from the same application often carry
    out similar computation, albeit at different times, we observe extensive
    code reuse among different processors, causing redundancy (e.g., in our
    server workloads, 45-65% of all instruction blocks are accessed by all
    processors).  Moreover, largely independent fragments of computation
    compete for the same private resources causing destructive interference.
    Together, this redundancy and interference lead to poor utilization of
    private microarchitecture resources such as caches and branch predictors.We
    present Computation Spreading (CSP), which employs hardware migration to
    distribute a thread's dissimilar fragments of computation across the
    multiple processing cores of a chip multiprocessor (CMP), while grouping
    similar computation fragments from different threads together.  This paper
    focuses on a specific example of CSP for OS intensive server applications:
    separating application level (user) computation from the OS calls it
    makes.When performing CSP, each core becomes temporally specialized to
    execute certain computation fragments, and the same core is repeatedly used
    for such fragments.  We examine two specific thread assignment policies for
    CSP, and show that these policies, across four server workloads, are able
    to reduce instruction misses in private L2 caches by 27-58%, private L2
    load misses by 0-19%, and branch mispredictions by 9-25%.", 
  location     = "https://doi.org/10.1145/1168857.1168893", 
  location     = "ftp://ftp.cs.wisc.edu/sohi/papers/2006/asplos2006-comp-spread.pdf"
}

@Article{sbicfep,
  author       = "Jason~E. Miller and Anant Agarwal",
  title        = "Software-based instruction caching for embedded processors",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "293--302",
  month        = nov,
  keywords     = "software caching, instruction caching, chaining,
  software-implemented caches, embedded processors, on-chip store",
  abstract     = "While hardware instruction caches are present in virtually
    all general-purpose and high-performance microprocessors today, many
    embedded processors use SRAM or scratchpad memories instead.  These are
    simple array memory structures that are directly addressed and explicitly
    managed by software.  Compared to hardware caches of the same data
    capacity, they are smaller, have shorter access times and consume less
    energy per access.  Access times are also easier to predict with simple
    memories since there is no possibility of a 'miss.' On the other hand, they
    are more difficult for the programmer to use since they are not
    automatically managed.In this paper, we present a software system that
    allows all or part of an SRAM or scratchpad memory to be automatically
    managed as a cache.  This system provides the programming convenience of a
    cache for processors that lack dedicated caching hardware.  It has been
    implemented for an actual processor and runs on real hardware.  Our results
    show that a software-based instruction cache can be built that provides
    performance within 10% of a traditional hardware cache on many benchmarks
    while using a cheaper, simpler, SRAM memory.  On these same benchmarks,
    energy consumption is up to 3% lower than it would be using a hardware
    cache.", 
  location     = "https://doi.org/10.1145/1168857.1168894", 
  location     = "https://groups.csail.mit.edu/cag/raw/documents/Miller-ASPLOS-2006.ps.Z"
}

@Article{meoamtep,
  author       = "Xin Li and Marian Boldt and Reinhard von Hanxleden",
  title        = "Mapping {Esterel} onto a multi-threaded embedded processor",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "303--314",
  month        = nov,
  keywords     = "reactive systems, concurrency, multi-threading, esterel,
    low-power processing",
  abstract     = "The synchronous language Esterel is well-suited for
    programming control-dominated reactive systems at the system level.  It
    provides non-traditional control structures, in particular concurrency and
    various forms of preemption, which allow to concisely express reactive
    behavior.  As these control structures cannot be mapped easily onto
    traditional, sequential processors, an alternative approach that has
    emerged recently makes use of special-purpose reactive processors.
    However, the designs proposed so far have limitations regarding
    completeness of the language support, and did not really take advantage of
    compile-time knowledge to optimize resource usage.This paper presents a
    reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its
    compiler.  The KEP3a improves on earlier designs in several areas; most
    notable are the support for exception handling and the provision of
    context-dependent preemption handling instructions.  The KEP3a compiler
    presented here is to our knowledge the first for multi-threaded reactive
    processors.  The translation of Esterel's preemption constructs onto KEP3a
    assembler is straightforward; however, a challenge is the correct and
    efficient representation of Esterel's concurrency.  The compiler generates
    code that respects data and control dependencies using the KEP3a
    priority-based scheduling mechanism.  We present a priority assignment
    approach that makes use of a novel concurrent control flow graph and has a
    complexity that in practice tends to be linear in the size of the program.
    Unlike earlier Esterel compilation schemes, this approach avoids
    unnecessary context switches by considering each thread's actual execution
    state at run time.  Furthermore, it avoids code replication present in
    other approaches.", 
  location     = "https://doi.org/10.1145/1168857.1168896"
}

@Article{inifhbti,
  author       = "Nathan~L. Binkert and Ali~G. Saidi and Steven~K. Reinhardt",
  title        = "Integrated network interfaces for high-bandwidth {TCP/IP}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "315--324",
  month        = nov,
  keywords     = "network interfaces, tcp/ip performance, zero-copy transfers,
    on-chip nics",
  abstract     = "This paper proposes new network interface controller (NIC)
    designs that take advantage of integration with the host CPU to provide
    increased flexibility for operating system kernel-based performance
    optimization.We believe that this approach is more likely to meet the needs
    of current and future high-bandwidth TCP/IP networking on end hosts than
    the current trend of putting more complexity in the NIC, while avoiding the
    need to modify applications and protocols.  This paper presents two such
    NICs.  The first, the simple integrated NIC (SINIC), is a minimally complex
    design that moves the responsibility for managing the network FIFOs from
    the NIC to the kernel.  Despite this closer interaction between the kernel
    and the NIC, SINIC provides performance equivalent to a conventional
    DMA-based NIC without increasing CPU overhead.  The second design, V-SINIC,
    adds virtual per-packet registers to SINIC, enabling parallel packet
    processing while maintaining a FIFO model.  V-SINIC allows the kernel to
    decouple examining a packet's header from copying its payload to memory.
    We exploit this capability to implement a true zero-copy receive
    optimization in the Linux 2.6 kernel, providing bandwidth improvements of
    over 50% on unmodified sockets-based receive-intensive benchmarks.", 
  location     = "https://doi.org/10.1145/1168857.1168897", 
  location     = "http://web.eecs.umich.edu/~saidi/pubs/asplos06-nic.pdf"
}

@Article{audptpgfgpu,
  author       = "David Tarditi and Sidd Puri and Jose Oglesby",
  title        = "Accelerator: using data parallelism to program {GPUs} for general-purpose uses",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "325--335",
  month        = nov,
  keywords     = "graphics processing units, data parallelism, jit compilation,
    gpu code generation",
  abstract     = "GPUs are difficult to program for general-purpose uses.
    Programmers can either learn graphics APIs and convert their applications
    to use graphics pipeline operations or they can use stream programming
    abstractions of GPUs.  We describe Accelerator, a system that uses data
    parallelism to program GPUs for general-purpose uses instead.  Programmers
    use a conventional imperative programming language and a library that
    provides only high-level data-parallel operations.  No aspects of GPUs are
    exposed to programmers.  The library implementation compiles the
    data-parallel operations on the fly to optimized GPU pixel shader code and
    API calls.We describe the compilation techniques used to do this.  We
    evaluate the effectiveness of using data parallelism to program GPUs by
    providing results for a set of compute-intensive benchmarks.  We compare
    the performance of Accelerator versions of the benchmarks against
    hand-written pixel shaders.  The speeds of the Accelerator versions are
    typically within 50% of the speeds of hand-written pixel shader code.  Some
    benchmarks significantly outperform C versions on a CPU: they are up to 18
    times faster than C code running on a CPU.",
  location     = "https://doi.org/10.1145/1168857.1168898",
  location     = "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-184.pdf"
}

@Article{htmpd,
  author       = "Peter Damron and Alexandra Fedorova and Yossi Lev and Victor Luchangco and Mark Moir and Daniel Nussbaum",
  title        = "Hybrid transactional memory",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "336--346",
  month        = nov,
  keywords     = "software transactional storage, hardware-assisted software,
    transactions", 
  abstract     = "Transactional memory (TM) promises to substantially reduce
    the difficulty of writing correct, efficient, and scalable concurrent
    programs.  But 'bounded' and 'best-effort' hardware TM proposals impose
    unreasonable constraints on programmers, while more flexible software TM
    implementations are considered too slow.  Proposals for supporting
    'unbounded' transactions in hardware entail significantly higher complexity
    and risk than best-effort designs.We introduce Hybrid Transactional Memory
    (HyTM), an approach to implementing TMin software so that it can use best
    effort hardware TM (HTM) to boost performance but does not depend on HTM.
    Thus programmers can develop and test transactional programs in existing
    systems today, and can enjoy the performance benefits of HTM support when
    it becomes available.We describe our prototype HyTM system, comprising a
    compiler and a library.  The compiler allows a transaction to be attempted
    using best-effort HTM, and retried using the software library if it fails.
    We have used our prototype to 'transactify' part of the Berkeley DB system,
    as well as several benchmarks.  By disabling the optional use of HTM, we
    can run all of these tests on existing systems.  Furthermore, by using a
    simulated multiprocessor with HTM support, we demonstrate the viability of
    the HyTM approach: it can provide performance and scalability approaching
    that of an unbounded HTM implementation, without the need to support all
    transactions with complicated HTM support.", 
  location     = "https://doi.org/10.1145/1168857.1168900", 
  location     = "https://www.ece.ubc.ca/~sasha/papers/asplos165-damron.pdf"
}

@Article{i3csm,
  author       = "Shashidhar Mysore and Banit Agrawal and Navin Srivastava and Sheng-Chih Lin and Kaustav Banerjee and Tim Sherwood",
  title        = "Introspective {3D} chips",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "264--273",
  month        = nov,
  keywords     = "introspection, hardware supported profiling, 3d
    architectures, configurable architectures, chip layout",
  abstract     = "While the number of transistors on a chip increases
    exponentially over time, the productivity that can be realized from these
    systems has not kept pace.  To deal with the complexity of modern systems,
    software developers are increasingly dependent on specialized development
    tools such as security profilers, memory leak identifiers, data flight
    recorders, and dynamic type analysis.  Many of these tools require
    full-system data which covers multiple interacting threads, processes, and
    processors.  Reducing the performance penalty and complexity of these
    software tools is critical to those developing next generation
    applications, and many researchers have proposed adding specialized
    hardware to assist in profiling and introspection.  Unfortunately, while
    this additional hardware would be incredibly beneficial to developers, the
    cost of this hardware must be paid on every single die that is
    manufactured.In this paper, we argue that a new way to attack this problem
    is with the addition of specialized analysis hardware built on separate
    active layers stacked vertically on the processor die using 3D IC
    technology.  This provides a modular 'snap-on' functionality that could be
    included with developer systems, and omitted from consumer systems to keep
    the cost impact to a minimum.  In this paper we describe the advantage of
    using inter-die vias for introspection and we quantify the impact they can
    have in terms of the area, power, temperature, and routability of the
    resulting systems.  We show that hardware stubs could be inserted into
    commodity processors at design time that would allow analysis layers to be
    bonded to development chips, and that these stubs would increase area and
    power by no more than 0.021mm2 and 0.9% respectively.", 
  location     = "https://doi.org/10.1145/1168857.1168890"
}

@Article{aptaasfqu,
  author       = "Ethan Schuchman and T.~N. Vijaykumar",
  title        = "{A} program transformation and architecture support for quantum uncomputation",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "252--263",
  month        = nov,
  keywords     = "quantum computing, uncomputation, qla, fault tolerance,
    quantum garbage management, entanglement and interference",
  abstract     = "Quantum computing's power comes from new algorithms that
    exploit quantum mechanical phenomena for computation.  Quantum algorithms
    are different from their classical counterparts in that quantum algorithms
    rely on algorithmic structures that are simply not present in classical
    computing.  Just as classical program transformations and architectures
    have been designed for common classical algorithm structures, quantum
    program transformations and quantum architectures should be designed with
    quantum algorithms in mind.  Because quantum algorithms come with these new
    algorithmic structures, resultant quantum program transformations and
    architectures may look very different from their classical
    counterparts.This paper focuses on uncomputation, a critical and prevalent
    structure in quantum algorithms, and considers how program transformations,
    and architecture support should be designed to accommodate uncomputation.
    In this paper,we show a simple quantum program transformation that exposes
    independence between uncomputation and later computation.  We then propose
    a multicore architecture tailored to this exposed parallelism and propose a
    scheduling policy that efficiently maps such parallelism to the multicore
    architecture.  Our policy achieves parallelism between uncomputation and
    later computation while reducing cumulative communication distance.  Our
    scheduling and architecture allows significant speedup of quantum programs
    (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while
    reducing cumulative communication distance 26%.", 
  location     = "https://doi.org/10.1145/1168857.1168889", 
  location     = "https://engineering.purdue.edu/~vijay/papers/2006/uncomputation.pdf"
}

@Article{adtsonsa,
  author       = "Jaidev~P. Patwardhan and Vijeta Johri and Chris Dwyer and Alvin~R. Lebeck",
  title        = "{A} defect tolerant self-organizing nanoscale {SIMD} architecture",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "241--251",
  month        = nov,
  keywords     = "self-organizing systems, simd architectures, data parallel
  computations, bit-serial assembly, defect tolerance, dna, nanocomputing",
  abstract     = "The continual decrease in transistor size (through either
    scaled CMOS or emerging nano-technologies) promises to usher in an era of
    tera to peta-scale integration.  However, this decrease in size is also
    likely to increase defect densities, contributing to the exponentially
    increasing cost of top-down lithography.  Bottom-up manufacturing
    techniques, like self assembly, may provide a viable lower-cost alternative
    to top-down lithography, but may also be prone to higher defects.
    Therefore, regardless of fabrication methodology, defect tolerant
    architectures are necessary to exploit the full potential of future
    increased device densities.This paper explores a defect tolerant SIMD
    architecture.  A key feature of our design is the ability of a large number
    of limited capability nodes with high defect rates (up to 30%) to
    self-organize into a set of SIMD processing elements.  Despite node
    simplicity and high defect rates, we show that by supporting the familiar
    data parallel programming model the architecture can execute a variety of
    programs.  The architecture efficiently exploits a large number of nodes
    and higher device densities to keep device switching speeds and power
    density low.  On a medium sized system (~1cm2 area), the performance of the
    proposed architecture on our data parallel programs matches or exceeds the
    performance of an aggressively scaled out-of-order processor (128-wide, 8k
    reorder buffer, perfect memory system).  For larger systems (&gt;1cm2), the
    proposed architecture can match the performance of a chip multiprocessor
    with 16 aggressively scaled out-of-order cores.", 
  location     = "https://doi.org/10.1145/1168857.1168888", 
  location     = "https://users.cs.duke.edu/~alvy/papers/sosa.pdf"
}

@Article{rsmdus,
  author       = "Satish Narayanasamy and Cristiano Pereira and Brad Calder",
  title        = "Recording shared memory dependencies using {Strata}",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "229--240",
  month        = nov,
  keywords     = "strata recorder, replay, multi-threaded debugging, logging, shared
    memory dependencies", 
  abstract     = "Significant time is spent by companies trying to reproduce
    and fix bugs.  BugNet and FDR are recent architecture proposals that
    provide architecture support for deterministic replay debugging.  They
    focus on continuously recording information about the program's execution,
    which can be communicated back to the developer.  Using that information,
    the developer can deterministically replay the program's execution to
    reproduce and fix the bugs.In this paper, we propose using Strata to
    efficiently capture the shared memory dependencies.  A stratum creates a
    time layer across all the logs for the running threads, which separates all
    the memory operations executed before and after the stratum.  A strata log
    allows us to determine all the shared memory dependencies during replay and
    thereby supports deterministic replay debugging for multi-threaded
    programs.", 
  location     = "https://doi.org/10.1145/1168857.1168886", 
  location     = "https://cseweb.ucsd.edu/~calder/papers/ASPLOS-06-Strata.pdf"
}

@Article{hihbbuad,
  author       = "Trishul~M. Chilimbi and Vinod Ganapathy",
  title        = "{HeapMD}: identifying heap-based bugs using anomaly detection",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "219--228",
  month        = nov,
  keywords     = "anomaly detection, pointer errors , heap storage modeling, graph
    metrics, debugging", 
  abstract     = "We present the design, implementation, and evaluation of
    HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly
    detection.  HeapMD is based upon the observation that, in spite of the
    evolving nature of the heap, several of its properties remain stable.
    HeapMD uses this observation in a novel way: periodically, during the
    execution of the program, it computes a suite of metrics which are
    sensitive to the state of the heap.  These metrics track heap behavior, and
    the stability of the heap reflects quantitatively in the values of these
    metrics.  The 'normal' ranges of stable metrics, obtained by running a
    program on multiple inputs, are then treated as indicators of correct
    behaviour, and are used in conjunction with an anomaly detector to find
    heap-based bugs.  Using HeapMD, we were able to find 40 heap-based bugs, 31
    of them previously unknown, in 5 large, commercial applications.", 
  location     = "https://doi.org/10.1145/1168857.1168885", 
  location     = "http://pages.cs.wisc.edu/~vg/papers/asplos2006/asplos2006.ps"
}

@Article{caepth,
  author       = "Mazen Kharbutli and Xiaowei Jiang and Yan Solihin and Guru Venkataramani and Milos Prvulovic",
  title        = "Comprehensively and efficiently protecting the heap",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "207--218",
  month        = nov,
  keywords     = "computer security, heap attacks, heap security, heap server",
  abstract     = "The goal of this paper is to propose a scheme that provides
    comprehensive security protection for the heap.  Heap vulnerabilities are
    increasingly being exploited for attacks on computer programs.  In most
    implementations, the heap management library keeps the heap meta-data (heap
    structure information) and the application's heap data in an interleaved
    fashion and does not protect them against each other.  Such implementations
    are inherently unsafe: vulnerabilities in the application can cause the
    heap library to perform unintended actions to achieve control-flow and
    non-control attacks.Unfortunately, current heap protection techniques are
    limited in that they use too many assumptions on how the attacks will be
    performed, require new hardware support, or require too many changes to the
    software developers' toolchain.  We propose Heap Server, a new solution
    that does not have such drawbacks.  Through existing virtual memory and
    inter-process protection mechanisms, Heap Server prevents the heap
    meta-data from being illegally overwritten, and heap data from being
    meaningfully overwritten.  We show that through aggressive optimizations
    and parallelism, Heap Server protects the heap with nearly-negligible
    performance overheads even on heap-intensive applications.  We also verify
    the protection against several real-world exploits and attack kernels.", 
  location     = "https://doi.org/10.1145/1168857.1168884", 
  location     = "https://www.cc.gatech.edu/~milos/kharbutli_asplos06.pdf"
}

@Article{eeadsvpm,
  author       = "Engin \.{I}pek and Sally~A. McKee and Rich Caruana and Bronis~R. {de Supinski} and Martin Schulz",
  title        = "Efficiently exploring architectural design spaces via predictive modeling",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "195--206",
  month        = nov,
  keywords     = "design space exploration, sensitivity studies, artificial
    neural networks, performance prediction, learning",
  abstract     = "Architects use cycle-by-cycle simulation to evaluate design
    choices and understand tradeoffs and interactions among design parameters.
    Efficiently exploring exponential-size design spaces with many interacting
    parameters remains an open problem: the sheer number of experiments renders
    detailed simulation intractable.  We attack this problem via an automated
    approach that builds accurate, confident predictive design-space models.
    We simulate sampled points, using the results to teach our models the
    function describing relationships among design parameters.  The models
    produce highly accurate performance estimates for other points in the
    space, can be queried to predict performance impacts of architectural
    changes, and are very fast compared to simulation, enabling efficient
    discovery of tradeoffs among parameters in different regions.  We validate
    our approach via sensitivity studies on memory hierarchy and CPU design
    spaces: our models generally predict IPC with only 1-2% error and reduce
    required simulation by two orders of magnitude.  We also show the efficacy
    of our technique for exploring chip multiprocessor (CMP) design spaces:
    when trained on a 1% sample drawn from a CMP design space with 250K points
    and up to 55x performance swings among different system configurations, our
    models predict performance with only 4-5% error on average.  Our approach
    combines with techniques to reduce time per simulation, achieving net time
    savings of three-four orders of magnitude.", 
  location     = "https://doi.org/10.1145/1168857.1168882", 
  location     = "http://www.cs.rochester.edu/~ipek/asplos06.pdf"
}

@Article{pu3stteaceecm,
  author       = "Taeho Kgil and Shaun D'Souza and Ali Saidi and Nathan Binkert and Ronald Dreslinski and Trevor Mudge and Steven Reinhardt and Krisztian Flautner",
  title        = "{PicoServer}: using {3D} stacking technology to enable a compact energy efficient chip multiprocessor",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "117--128",
  month        = nov,
  keywords     = "low power, tier 1 servers, web/file/streaming servers, 3d
    stacking technology, chip multiprocessor, full-system simulation, power
    trade-offs, data transport",
  abstract     = "In this paper, we show how 3D stacking technology can be used
    to implement a simple, low-power, high-performance chip multiprocessor
    suitable for throughput processing.  Our proposed architecture, PicoServer,
    employs 3D technology to bond one die containing several simple slow
    processing cores to multiple DRAM dies sufficient for a primary memory.
    The 3D technology also enables wide low-latency buses between processors
    and memory.  These remove the need for an L2 cache allowing its area to be
    re-allocated to additional simple cores.  The additional cores allow the
    clock frequency to be lowered without impairing throughput.  Lower clock
    frequency in turn reduces power and means that thermal constraints, a
    concern with 3D stacking, are easily satisfied.The PicoServer architecture
    specifically targets Tier 1 server applications, which exhibit a high
    degree of thread level parallelism.  An architecture targeted to efficient
    throughput is ideal for this application domain.  We find for a similar
    logic die area, a 12 CPU system with 3D stacking and no L2 cache
    outperforms an 8 CPU system with a large on-chip L2 cache by about 14%
    while consuming 55% less power.  In addition, we show that a PicoServer
    performs comparably to a Pentium 4-like class machine while consuming only
    about 1/10 of the power, even when conservative assumptions are made about
    the power consumption of the PicoServer.", 
  location     = "https://doi.org/10.1145/1168857.1168873", 
  location     = "http://web.eecs.umich.edu/~saidi/pres/picoserver.pdf"
}

@Article{aspsafea,
  author       = "Katherine~E. Coons and Xia Chen and Doug Burger and Kathryn~S. McKinley and Sundeep~K. Kushwaha",
  title        = "{A} spatial path scheduling algorithm for {EDGE} architectures",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "129--140",
  month        = nov,
  keywords     = "instruction  scheduling, path scheduling, simulated
    annealing, edge architecture",
  abstract     = "Growing on-chip wire delays are motivating architectural
    features that expose on-chip communication to the compiler.  EDGE
    architectures are one example of communication-exposed microarchitectures
    in which the compiler forms dataflow graphs that specify how the
    microarchitecture maps instructions onto a distributed execution substrate.
    This paper describes a compiler scheduling algorithm called spatial path
    scheduling that factors in previously fixed locations - called anchor
    points - for each placement.  This algorithm extends easily to different
    spatial topologies.  We augment this basic algorithm with three heuristics:
    (1) local and global ALU and network link contention modeling, (2) global
    critical path estimates, and (3) dependence chain path reservation.  We use
    simulated annealing to explore possible performance improvements and to
    motivate the augmented heuristics and their weighting functions.  We show
    that the spatial path scheduling algorithm augmented with these three
    heuristics achieves a 21% average performance improvement over the best
    prior algorithm and comes within an average of 5% of the annealed
    performance for our benchmarks.", 
  location     = "https://doi.org/10.1145/1168857.1168875", 
  location     = "http://www.cs.utexas.edu/users/mckinley/papers/sps-asplos-2006.pdf"
}

@Article{isfatda,
  author       = "Martha Mercaldi and Steven Swanson and Andrew Petersen and Andrew Putnam and Andrew Schwerin and Mark Oskin and Susan~J. Eggers",
  title        = "Instruction scheduling for a tiled dataflow architecture",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "141--150",
  month        = nov,
  keywords     = "dataflow architecture, instruction scheduling, tiled architectures",
  abstract     = "This paper explores hierarchical instruction scheduling for a
    tiled processor.  Our results show that at the top level of the hierarchy,
    a simple profile-driven algorithm effectively minimizes operand latency.
    After this schedule has been partitioned into large sections, the
    bottom-level algorithm must more carefully analyze program structure when
    producing the final schedule.Our analysis reveals that at this bottom
    level, good scheduling depends upon carefully balancing instruction
    contention for processing elements and operand latency between producer and
    consumer instructions.  We develop a parameterizable instruction scheduler
    that more effectively optimizes this trade-off.  We use this scheduler to
    determine the contention-latency sweet spot that generates the best
    instruction schedule for each application.  To avoid this
    application-specific tuning, we also determine the parameters that produce
    the best performance across all applications.  The result is a
    contention-latency setting that generates instruction schedules for all
    applications in our workload that come within 17% of the best schedule for
    each.", 
  location     = "https://doi.org/10.1145/1168857.1168876", 
  location     = "https://cseweb.ucsd.edu/~swanson/papers/ASPLOS2006WaveScalar.pdf"
}

@Article{ecgtdappisp,
  author       = "Michael~I. Gordon and William Thies and Saman Amarasinghe",
  title        = "Exploiting coarse-grained task, data, and pipeline parallelism in stream programs",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "151--162",
  month        = nov,
  keywords     = "coarse-grained dataflow, multicore, raw, software pipelining,
    streamit, stream processing, exotic compilation",
  abstract     = "As multicore architectures enter the mainstream, there is a
    pressing demand for high-level programming models that can effectively map
    to them.  Stream programming offers an attractive way to expose
    coarse-grained parallelism, as streaming applications (image, video, DSP,
    etc.) are naturally represented by independent filters that communicate
    over explicit data channels.In this paper, we demonstrate an end-to-end
    stream compiler that attains robust multicore performance in the face of
    varying application characteristics.  As benchmarks exhibit different
    amounts of task, data, and pipeline parallelism, we exploit all types of
    parallelism in a unified manner in order to achieve this generality.  Our
    compiler, which maps from the StreamIt language to the 16-core Raw
    architecture, attains a 11.2x mean speedup over a single-core baseline, and
    a 1.84x speedup over our previous work.", 
  location     = "https://doi.org/10.1145/1168857.1168877", 
  location     = "http://groups.csail.mit.edu/commit/papers/06/gordon-asplos06.pdf"
}

@Article{tescfwpe,
  author       = "Mahim Mishra and Timothy~J. Callahan and Tiberiu Chelcea and Girish Venkataramani and Seth~C. Goldstein and Mihai Budiu",
  title        = "Tartan: evaluating spatial computation for whole program execution",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "163--174",
  month        = nov,
  keywords     = "spatial computation, dataflow architectures, reconfigurable
    hardware, asynchronous circuits, low power, defect tolerance",
  abstract     = "Spatial Computing (SC) has been shown to be an
    energy-efficient model for implementing program kernels.  In this paper we
    explore the feasibility of using SC for more than small kernels.  To this
    end, we evaluate the performance and energy efficiency of entire
    applications on Tartan, a general-purpose architecture which integrates a
    reconfigurable fabric (RF) with a superscalar core.  Our compiler
    automatically partitions and compiles an application into an instruction
    stream for the core and a configuration for the RF.  We use a detailed
    simulator to capture both timing and energy numbers for all parts of the
    system.Our results indicate that a hierarchical RF architecture, designed
    around a scalable interconnect, is instrumental in harnessing the benefits
    of spatial computation.  The interconnect uses static configuration and
    routing at the lower levels and a packet-switched, dynamically-routed
    network at the top level.  Tartan is most energyefficient when almost all
    of the application is mapped to the RF, indicating the need for the RF to
    support most general-purpose programming constructs.  Our initial
    investigation reveals that such a system can provide, on average, an order
    of magnitude improvement in energy-delay compared to an aggressive
    superscalar core on single-threaded workloads.", 
  location     = "https://doi.org/10.1145/1168857.1168878", 
  location     = "http://www.cs.cmu.edu/~phoenix/papers/asplos06.pdf"
}

@Article{apcafcacc,
  author       = "Stijn Eyerman and Lieven Eeckhout and Tejas Karkhanis and James~E. Smith",
  title        = "{A} performance counter architecture for computing accurate {CPI} components",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "175--184",
  month        = nov,
  keywords     = "hardware performance counter architecture, superscalar
    processor performance modeling, cpi stacks, miss events",
  abstract     = "A common way of representing processor performance is to use
    Cycles per Instruction (CPI) `stacks' which break performance into a
    baseline CPI plus a number of individual miss event CPI components.  CPI
    stacks can be very helpful in gaining insight into the behavior of an
    application on a given microprocessor; consequently, they are widely used
    by software application developers and computer architects.  However,
    computing CPI stacks on superscalar out-of-order processors is challenging
    because of various overlaps among execution and miss events (cache misses,
    TLB misses, and branch mispredictions).This paper shows that meaningful and
    accurate CPI stacks can be computed for superscalar out-of-order
    processors.  Using interval analysis, a novel method for analyzing
    out-of-order processor performance, we gain understanding into the
    performance impact of the various miss events.  Based on this
    understanding, we propose a novel way of architecting hardware performance
    counters for building accurate CPI stacks.  The additional hardware for
    implementing these counters is limited and comparable to existing hardware
    performance counter architectures while being significantly more accurate
    than previous approaches.", 
  location     = "https://doi.org/10.1145/1168857.1168880", 
  location     = "https://www.elis.ugent.be/~leeckhou/papers/asplos06.pdf"
}

@Article{aaermfmpapp,
  author       = "Benjamin~C. Lee and David~M. Brooks",
  title        = "Accurate and efficient regression modeling for microarchitectural performance and power prediction",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "185--194",
  month        = nov,
  keywords     = "microarchitecture, simulation, statistics, interference,
    regression, prediction, modeling, evaluation",
  abstract     = "We propose regression modeling as an efficient approach for
    accurately predicting performance and power for various applications
    executing on any microprocessor configuration in a large microarchitectural
    design space.  This paper addresses fundamental challenges in
    microarchitectural simulation cost by reducing the number of required
    simulations and using simulated results more effectively via statistical
    modeling and inference.Specifically, we derive and validate regression
    models for performance and power.  Such models enable computationally
    efficient statistical inference, requiring the simulation of only 1 in 5
    million points of a joint microarchitecture-application design space while
    achieving median error rates as low as 4.1 percent for performance and 4.3
    percent for power.  Although both models achieve similar accuracy, the
    sources of accuracy are strikingly different.  We present optimizations for
    a baseline regression model to obtain (1) application-specific models to
    maximize accuracy in performance prediction and (2) regional power models
    leveraging only the most relevant samples from the microarchitectural
    design space to maximize accuracy in power prediction.  Assessing
    sensitivity to the number of samples simulated for model formulation, we
    find fewer than 4,000 samples from a design space of approximately 22
    billion points are sufficient.  Collectively, our results suggest
    significant potential in accurate and efficient statistical inference for
    microarchitectural design space exploration via regression models.", 
  location     = "https://doi.org/10.1145/1168857.1168881", 
  location     = "http://people.duke.edu/~bcl15/documents/lee2006-asplos.pdf"
}

@Article{cbatdg,
  author       = "DeMillo, Richard~A. and Offutt, A.~Jefferson",
  title        = "Constraint-Based Automatic Test Data Generation",
  journal      = tse,
  year         = 1991,
  volume       = 17,
  number       = 9,
  pages        = "900--910",
  month        = sep,
  keywords     = "constraint-based data generation, automatic test data
    generation, mutation analysis, relative adequacy, fault-based technique,
    algebraic constraints, Godzilla, module testing, Mothra testing system", 
  abstract     = "A novel technique for automatically generating test data is
    presented. The technique is based on mutation analysis and creates test
    data approximating relative adequacy.  Its  fault-based technique uses
    algebraic constraints to describe test cases designed to find particular
    fault types.  A set of tools collectively called Godzilla automatically
    generates constraints and solves them to create test cases for unit and
    module testing.  Godzilla has been integrated with the Mothra testing
    system and has been used as an effective way to generate test data to kill
    program mutants.  The authors present an initial list of constraints and
    discuss some of the problems solved to develop the complete implementation
    of the technique.",
  location     = "https://doi.org/10.1109/32.92910"
}

@Article{adavvaii,
  author       = "Shan Lu and Joseph Tucek and Feng Qin and Yuanyuan Zhou",
  title        = "{AVIO}: detecting atomicity violations via access interleaving invariants",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "37--48",
  month        = nov,
  keywords     = "concurrent programming, atomicity violation, concurrency bugs,
    bug detection, hardware support, atomicity invariants, invariant discovery,
    cache consistency protocols",
  abstract     = "Concurrency bugs are among the most difficult to test and
    diagnose of all software bugs.  The multicore technology trend worsens this
    problem.  Most previous concurrency bug detection work focuses on one bug
    subclass, data races, and neglects many other important ones such as
    atomicity violations, which will soon become increasingly important due to
    the emerging trend of transactional memory models.This paper proposes an
    innovative, comprehensive, invariantbased approach called AVIO to detect
    atomicity violations.  Our idea is based on a novel observation called
    access interleaving invariant, which is a good indication of programmers'
    assumptions about the atomicity of certain code regions.  By automatically
    extracting such invariants and detecting violations of these invariants at
    run time, AVIO can detect a variety of atomicity violations.Based on this
    idea, we have designed and built two implementations of AVIO and evaluated
    the trade-offs between them.  The first implementation, AVIO-S, is purely
    in software, while the second, AVIO-H, requires some simple extensions to
    the cache coherence hardware.  AVIO-S is cheaper and more accurate but
    incurs much higher overhead and thus more run-time perturbation than AVIOH.
    Therefore, AVIO-S is more suitable for in-house bug detection and
    postmortem bug diagnosis, while AVIO-H can be used for bug detection during
    production runs.We evaluate both implementations of AVIO using large
    realworld server applications (Apache and MySQL) with six representative
    real atomicity violation bugs, and SPLASH-2 benchmarks.  Our results show
    that AVIO detects more tested atomicity violations of various types and has
    25 times fewer false positives than previous solutions on average.", 
  location     = "https://doi.org/10.1145/1168857.1168864", 
  location     = "http://pages.cs.wisc.edu/~shanlu/paper/asplos062-lu.ps"
}

@Article{artrrflmrr,
  author       = "Min Xu and Mark~D. Hill and Rastislav Bodik",
  title        = "{A} regulated transitive reduction ({RTR}) for longer memory race recording",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "49--60",
  month        = nov,
  keywords     = "multithreading, determinism, race recording, data
    compression, approximations, consistency models",
  abstract     = "Multithreaded deterministic replay has important applications
    in cyclic debugging, fault tolerance and intrusion analysis.  Memory race
    recording is a key technology for multithreaded deterministic replay.  In
    this paper, we considerably improve our previous always-on Flight Data
    Recorder (FDR) in four ways: •Longer recording by reducing the log size
    growth rate to approximately one byte per thousand dynamic instructions.
    •Lower hardware cost by reducing the cost to 24 KB per processor core.
    •Simpler design by modifying only the cache coherence protocol, but not the
    cache.  •Broader applicability by supporting both Sequential Consistency
    (SC) and Total Store Order (TSO) memory consistency models (existing
    recorders support only SC).These improvements stem from several ideas: (1)
    a Regulated Transitive Reduction (RTR) recording algorithm that creates
    stricter and vectorizable dependencies to reduce the log growth rate; (2) a
    Set/LRU timestamp approximation method that better approximates timestamps
    of uncached memory locations to reduce the hardware cost; (3) an
    order-value-hybrid recording methodthat explicitly logs the value of
    potential SC-violating load instructions to support multiprocessor systems
    with TSO.", 
  location     = "https://doi.org/10.1145/1168857.1168865", 
  location     = "http://www.cs.wisc.edu/multifacet/papers/asplos06_rtr.pdf"
}

@Article{bbeomld,
  author       = "Michael~D. Bond and Kathryn~S. McKinley",
  title        = "Bell: bit-encoding online memory leak detection",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "61--72",
  month        = nov,
  keywords     = "memory leaks, low-overhead monitoring, probabilistic
    approaches, managed systems, one-bit hashes",
  abstract     = "Memory leaks compromise availability and security by
    crippling performance and crashing programs.  Leaks are difficult to
    diagnose because they have no immediate symptoms.  Online leak detection
    tools benefit from storing and reporting per-object sites (e.g., allocation
    sites) for potentially leaking objects.  In programs with many small
    objects, per-object sites add high space overhead, limiting their use in
    production environments.This paper introduces Bit-Encoding Leak Location
    (Bell), a statistical approach that encodes per-object sites to a single
    bit per object.  A bit loses information about a site, but given sufficient
    objects that use the site and a known, finite set of possible sites, Bell
    uses brute-force decoding to recover the site with high accuracy.We use
    this approach to encode object allocation and last-use sites in Sleigh, a
    new leak detection tool.  Sleigh detects stale objects (objects unused for
    a long time) and uses Bell decoding to report their allocation and last-use
    sites.  Our implementation steals four unused bits in the object header and
    thus incurs no per-object space overhead.  Sleigh's instrumentation adds
    29% execution time overhead, which adaptive profiling reduces to 11%.
    Sleigh's output is directly useful for finding and fixing leaks in SPEC
    JBB2000 and Eclipse, although sufficiently many objects must leak before
    Bell decoding can report sites with confidence.  Bell is suitable for other
    leak detection approaches that store per-object sites, and for other
    problems amenable to statistical per-object metadata.", 
  location     = "https://doi.org/10.1145/1168857.1168866", 
  location     = "https://www.cs.utexas.edu/users/mckinley/papers/bell-asplos-2006.pdf"
}

@Article{ulcdpfmp,
  author       = "Smitha Shyam and Kypros Constantinides and Sujay Phadke and Valeria Bertacco and Todd Austin",
  title        = "Ultra low-cost defect protection for microprocessor pipelines",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "73--82",
  month        = nov,
  keywords     = "reliability, defect protection, low cost, pipelines, hardware
    failures, cheap redundancy",
  abstract     = "The sustained push toward smaller and smaller technology
    sizes has reached a point where device reliability has moved to the
    forefront of concerns for next-generation designs.  Silicon failure
    mechanisms, such as transistor wearout and manufacturing defects, are a
    growing challenge that threatens the yield and product lifetime of future
    systems.  In this paper we introduce the BulletProof pipeline, the first
    ultra low-cost mechanism to protect a microprocessor pipeline and on-chip
    memory system from silicon defects.  To achieve this goal we combine
    area-frugal on-line testing techniques and system-level checkpointing to
    provide the same guarantees of reliability found in traditional solutions,
    but at much lower cost.  Our approach utilizes a microarchitectural
    checkpointing mechanism which creates coarse-grained epochs of execution,
    during which distributed on-line built in self-test (BIST) mechanisms
    validate the integrity of the underlying hardware.  In case a failure is
    detected, we rely on the natural redundancy of instructionlevel parallel
    processors to repair the system so that it can still operate in a degraded
    performance mode.  Using detailed circuit-level and architectural
    simulation, we find that our approach provides very high coverage of
    silicon defects (89%) with little area cost (5.8%).  In addition, when a
    defect occurs, the subsequent degraded mode of operation was found to have
    only moderate performance impacts, (from 4% to 18% slowdown).", 
  location     = "https://doi.org/10.1145/1168857.1168868", 
  location     = "http://www-personal.umich.edu/~kypros/shyam-asplos06.pdf"
}

@Article{upbprtflohcft,
  author       = "Vimal~K. Reddy and Eric Rotenberg and Sailashri Parthasarathy",
  title        = "Understanding prediction-based partial redundant threading for low-overhead, high-coverage fault tolerance",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "83--94",
  month        = nov,
  keywords     = "simultaneous multithreading, smt, chip multiprocessors, cmp,
    slipstream processor, transient faults, time redundancy, redundant
    multithreading, branch prediction, value prediction",
  abstract     = "Redundant threading architectures duplicate all instructions
    to detect and possibly recover from transient faults.  Several lighter
    weight Partial Redundant Threading (PRT) architectures have been proposed
    recently.  (i) Opportunistic Fault Tolerance duplicates instructions only
    during periods of poor single-thread performance.  (ii) ReStore does not
    explicitly duplicate instructions and instead exploits mispredictions among
    highly confident branch predictions as symptoms of faults.  (iii)
    Slipstream creates a reduced alternate thread by replacing many
    instructions with highly confident predictions.  We explore PRT as a
    possible direction for achieving the fault tolerance of full duplication
    with the performance of single-thread execution.  Opportunistic and ReStore
    yield partial coverage since they are restricted to using only partial
    duplication or only confident predictions, respectively.  Previous analysis
    of Slipstream fault tolerance was cursory and concluded that only
    duplicated instructions are covered.  In this paper, we attempt to better
    understand Slipstream's fault tolerance, conjecturing that the mixture of
    partial duplication and confident predictions actually closely approximates
    the coverage of full duplication.  A thorough dissection of prediction
    scenarios confirms that faults in nearly 100% of instructions are
    detectable.  Fewer than 0.1% of faulty instructions are not detectable due
    to coincident faults and mispredictions.  Next we show that the current
    recovery implementation fails to leverage excellent detection capability,
    since recovery sometimes initiates belatedly, after already retiring a
    detected faulty instruction.  We propose and evaluate a suite of simple
    microarchitectural alterations to recovery and checking.  Using the best
    alterations, Slipstream can recover from faults in 99% of instructions,
    compared to only 78% of instructions without alterations.  Both results are
    much higher than predicted by past research, which claims coverage for only
    duplicated instructions, or 65% of instructions.  On an 8-issue SMT
    processor, Slipstream performs within 1.3% of single-thread execution
    whereas full duplication slows performance by 14%.A key byproduct of this
    paper is a novel analysis framework in which every dynamic instruction is
    considered to be hypothetically faulty, thus not requiring explicit fault
    injection.  Fault coverage is measured in terms of the fraction of
    candidate faulty instructions that are directly or indirectly detectable
    before.", 
  location     = "https://doi.org/10.1145/1168857.1168869", 
  location     = "http://people.engr.ncsu.edu/ericro/publications/conference_ASPLOS-12.pdf"
}

@Article{ssbleferm,
  author       = "Angshuman Parashar and Anand Sivasubramaniam and Sudhanva Gurumurthi",
  title        = "{SlicK}: slice-based locality exploitation for efficient redundant multithreading",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "95--105",
  month        = nov,
  keywords     = "transient faults, redundant threading, backward slice
    extraction, microarchitecture",
  abstract     = "Transient faults are expected a be a major design
    consideration in future microprocessors.  Recent proposals for transient
    fault detection in processor cores have revolved around the idea of
    redundant threading, which involves redundant execution of a program across
    multiple execution contexts.  This paper presents a new approach to
    redundant threading by bringing together the concepts of slice-level
    execution and value and control-flow locality into a novel partial
    redundant threading mechanism called SlicK.The purpose of redundant
    execution is to check the integrity of the outputs propagating out of the
    core (typically through stores).  SlicK implements redundancy at the
    granularity of backward-slices of these output instructions and exploits
    value and control-flow locality to avoid redundantly executing slices that
    lead to predictable outputs, thereby avoiding redundant execution of a
    significant fraction of instructions while maintaining extremely low
    vulnerabilities for critical processor structures.We propose the
    microarchitecture of a backward-slice extractor called SliceEM that is able
    to identify backward slices without interrupting the instruction flow, and
    show how this extractor and a set of predictors can be integrated into a
    redundant threading mechanism to form SlicK.  Detailed simulations with
    SPEC CPU2000 benchmarks show that SlicK can provide around 10.2%
    performance improvement over a well known redundant threading mechanism,
    buying back over 50% of the loss suffered due to redundant execution.
    SlicK can keep the Architectural Vulnerability Factors of processor
    structures to typically 0%-2%.  More importantly, SlicK's slice-based
    mechanisms provide future opportunities for exploring interesting points in
    the performance-reliability design space based on market segment needs.", 
  location     = "https://doi.org/10.1145/1168857.1168870", 
  location     = "https://www.cs.virginia.edu/~gurumurthi/papers/asplos06.pdf"
}

@Article{mafteamfss,
  author       = "Taliver Heath and Ana Paula Centeno and Pradeep George and Luiz Ramos and Yogesh Jaluria and Ricardo Bianchini",
  title        = "{Mercury} and {Freon}: temperature emulation and management for server systems",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "106--116",
  month        = nov,
  keywords     = "temperature modeling, thermal management, energy
    conservation, server clusters",
  abstract     = "Power densities have been increasing rapidly at all levels of
    server systems.  To counter the high temperatures resulting from these
    densities, systems researchers have recently started work on softwarebased
    thermal management.  Unfortunately, research in this new area has been
    hindered by the limitations imposed by simulators and real measurements.
    In this paper, we introduce Mercury, a software suite that avoids these
    limitations by accurately emulating temperatures based on simple layout,
    hardware, and componentutilization data.  Most importantly, Mercury runs
    the entire software stack natively, enables repeatable experiments, and
    allows the study of thermal emergencies without harming hardware
    reliability.  We validate Mercury using real measurements and a widely used
    commercial simulator.  We use Mercury to develop Freon, a system that
    manages thermal emergencies in a server cluster without unnecessary
    performance degradation.  Mercury will soon become available from
    http://www.darklab.rutgers.edu.", 
  location     = "https://doi.org/10.1145/1168857.1168872", 
  location     = "https://people.cs.pitt.edu/~kirk/cs3150spring2010/p106-heath.pdf"
}

@Article{tsdhmtwvm,
  author       = "Jedidiah~R. Crandall and Gary Wassermann and Daniela A.~S. {de Oliveira} and Zhendong Su and S.~Felix Wu and Frederic~T. Chong",
  title        = "Temporal search: detecting hidden malware timebombs with virtual machines",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "25--36",
  month        = nov,
  keywords     = "worms, malware, virtual machines, temporal discovery, time
    bombs",
  abstract     = "Worms, viruses, and other malware can be ticking bombs
    counting down to a specific time, when they might, for example, delete
    files or download new instructions from a public web server.  We propose a
    novel virtual-machine-based analysis technique to automatically discover
    the timetable of a piece of malware, or when events will be triggered, so
    that other types of analysis can discern what those events are.  This
    information can be invaluable for responding to rapid malware, and
    automating its discovery can provide more accurate information with less
    delay than careful human analysis.Developing an automated system that
    produces the timetable of a piece of malware is a challenging research
    problem.  In this paper, we describe our implementation of a key component
    of such a system: the discovery of timers without making assumptions about
    the integrity of the infected system's kernel.  Our technique runs a
    virtual machine at slightly different rates of perceived time (time as seen
    by the virtual machine), and identifies time counters by correlating memory
    write frequency to timer interrupt frequency.We also analyze real malware
    to assess the feasibility of using full-system, machine-level symbolic
    execution on these timers to discover predicates.  Because of the
    intricacies of the Gregorian calendar (leap years, different number of days
    in each month, etc.) these predicates will not be direct expressions on the
    timer but instead an annotated trace; so we formalize the calculation of a
    timetable as a weakest precondition calculation.  Our analysis of six real
    worms sheds light on two challenges for future work: 1) time-dependent
    malware behavior often does not follow a linear timetable; and 2) that an
    attacker with knowledge of the analysis technique can evade analysis.  Our
    current results are promising in that with simple symbolic execution we are
    able to discover predicates on the day of the month for four real worms.
    Then through more traditional manual analysis we conclude that a more
    control-flow-sensitive symbolic execution implementation would discover all
    predicates for the malware we analyzed.", 
  location     = "https://doi.org/10.1145/1168857.1168862", 
  location     = "http://web.cs.ucdavis.edu/~su/publications/asplos06.pdf"
}

@Article{gmtbciavme,
  author       = "Stephen~T. Jones and Andrea~C. Arpaci-Dusseau and Remzi~H. Arpaci-Dusseau",
  title        = "Geiger: monitoring the buffer cache in a virtual machine environment",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "14--24",
  month        = nov,
  keywords     = "virtual machines, inferences, grey-box discovery, buffer
    cache management, virtual page management",
  abstract     = "Virtualization is increasingly being used to address server
    management and administration issues like flexible resource allocation,
    service isolation and workload migration.  In a virtualized environment,
    the virtual machine monitor (VMM) is the primary resource manager and is an
    attractive target for implementing system features like scheduling,
    caching, and monitoring.  However, the lackof runtime information within
    the VMM about guest operating systems, sometimes called the semantic gap,
    is a significant obstacle to efficiently implementing some kinds of
    services.In this paper we explore techniques that can be used by a VMM to
    passively infer useful information about a guest operating system's unified
    buffer cache and virtual memory system.  We have created a prototype
    implementation of these techniques inside the Xen VMM called Geiger and
    show that it can accurately infer when pages are inserted into and evicted
    from a system's buffer cache.  We explore several nuances involved in
    passively implementing eviction detection that have not previously been
    addressed, such as the importance of tracking disk block liveness, the
    effect of file system journaling, and the importance of accounting for the
    unified caches found in modern operating systems.Using case studies we show
    that the information provided by Geiger enables a VMM to implement useful
    VMM-level services.  We implement a novel working set size estimator which
    allows the VMM to make more informed memory allocation decisions.  We also
    show that a VMM can be used to drastically improve the hit rate in remote
    storage caches by using eviction-based cache placement without modifying
    the application or operating system storage interface.  Both case studies
    hint at a future where inference techniques enable a broad new class of
    VMM-level functionality.", 
  location     = "https://doi.org/10.1145/1168857.1168861", 
  location     = "https://research.cs.wisc.edu/wind/Publications/geiger-asplos06.pdf"
}

@Article{acosahtfxv,
  author       = "Keith Adams and Ole Agesen",
  title        = "{A} comparison of software and hardware techniques for x86 virtualization",
  journal      = asplos06,
  year         = 2006,
  volume       = 41,
  number       = 11,
  pages        = "2--13",
  month        = nov,
  keywords     = "virtualization, virtual machine monitor, dynamic binary
    translation, x86, vt, svm, mmu, tlb, nested paging",
  abstract     = "Until recently, the x86 architecture has not permitted
    classical trap-and-emulate virtualization.  Virtual Machine Monitors for
    x86, such as VMware ® Workstation and Virtual PC, have instead used binary
    translation of the guest kernel code.  However, both Intel and AMD have now
    introduced architectural extensions to support classical virtualization.We
    compare an existing software VMM with a new VMM designed for the emerging
    hardware support.  Surprisingly, the hardware VMM often suffers lower
    performance than the pure software VMM.  To determine why, we study
    architecture-level events such as page table updates, context switches and
    I/O, and find their costs vastly different among native, software VMM and
    hardware VMM execution.We find that the hardware support fails to provide
    an unambiguous performance advantage for two primary reasons: first, it
    offers no support for MMU virtualization; second, it fails to co-exist with
    existing software techniques for MMU virtualization.  We look ahead to
    emerging techniques for addressing this MMU virtualization problem in the
    context of hardware-assisted virtualization.", 
  location     = "https://doi.org/10.1145/1168857.1168860"
}

@Article{lafjps,
  author       = "Chris Hawblitzel and Thorsten {von Eicken}",
  title        = "Luna: a Flexible {Java} Protection System",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "391--403",
  month        = "Winter",
  keywords     = "type systems, distributed shared storage, remote pointers,
    pointer revocation",
  abstract     = "Extensible Java systems face a difficult trade-off between
    sharing and protection.  On one hand, Java's ability to run different
    protection domains in a single virtual machine enables domains to share
    data easily and communicate without address space switches.  On the other
    hand, unrestricted sharing blurs the boundaries between protection domains,
    making it difficult to terminate domains and enforce restrictions on
    resource usage.  Existing solutions to these problems restrict sharing in
    an ad-hoc fashion, ruling out many desirable programming styles.This paper
    presents an extension to Java's type system that systematically addresses
    the issues of data sharing, revocation, thread control, and resource
    control.  Multiple tasks running in a single virtual machines share data
    using special remote pointers, which have different types from local
    pointers.  The distinction between local and remote pointers allows the
    Java runtime system to mediate the communication between tasks without
    slowing down operations on ordinary pointers.  The extensions to Java are
    implemented by a system called Luna, based on the Guavac and Marmot
    compilers, extended with special optimizations to support both fast
    inter-task communication and dynamic access control.  The paper describes
    two applications written in Luna: a simple extensible web server, and an
    extension of the Squid web cache to support dynamic content generation.", 
  location     = "https://dl.acm.org/doi/abs/10.1145/844128.844164"
}

@Article{otmovc,
  author       = "Constantine~P. Sapuntzakis and Ramesh Chandra and Ben Pfaff and Jim Chow and Monica~S. Lam and Mendel Rosenblum",
  title        = "Optimizing the migration of virtual computers",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "377--390",
  month        = "Winter",
  keywords     = "process migration, state transfer, virtual machines, capsule",
  abstract     = "This paper shows how to quickly move the state of a running
    computer across a network, including the state in its disks, memory, CPU
    registers, and I/O devices.  We call this state a capsule.  Capsule state
    is hardware state, so it includes the entire operating system as well as
    applications and running processes.We have chosen to move x86 computer
    states because x86 computers are common, cheap, run the software we use,
    and have tools for migration.  Unfortunately, x86 capsules can be large,
    containing hundreds of megabytes of memory and gigabytes of disk data.  We
    have developed techniques to reduce the amount of data sent over the
    network: copy-on-write disks track just the updates to capsule disks,
    'ballooning' zeros unused memory, demand paging fetches only needed blocks,
    and hashing avoids sending blocks that already exist at the remote end.  We
    demonstrate these optimizations in a prototype system that uses VMware GSX
    Server virtual, machine monitor to create and run x86 capsules.  The system
    targets networks as slow as 384 kbps.Our experimental results suggest that
    efficient capsule migration can improve user mobility and system
    management.  Software updates or installations on a set of machines can be
    accomplished simply by distributing a capsule with the new changes.
    Assuming the presence of a prior capsule, the amount of traffic incurred is
    commensurate with the size of the update or installation package itself.
    Capsule migration makes it possible for machines to start running an
    application within 20 minutes on a 384 kbps link, without having to first
    install the application or even the underlying operating system.
    Furthermore, users' capsules can be migrated during a commute between home
    and work in even less time.", 
  location     = "https://doi.org/10.1145/844128.844163", 
  location     = "https://benpfaff.org/papers/migration.pdf"
}

@Article{tdaiozasfmce,
  author       = "Steven Osman and Dinesh Subhraveti and Gong Su and Jason Nieh",
  title        = "The design and implementation of {Zap}: a system for migrating computing environments",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "361--376",
  month        = "Winter",
  keywords     = "process migration, portability, system management, pods,
    virtualization",
  abstract     = "We have created Zap, a novel system for transparent migration
    of legacy and networked applications.  Zap provides a thin virtualization
    layer on top of the operating system that introduces pods, which are groups
    of processes that are provided a consistent, virtualized view of the
    system.  This decouples processes in pods from dependencies to the host
    operating system and other processes on the system.  By integrating Zap
    virtualization with a checkpoint-restart mechanism, Zap can migrate a pod
    of processes as a unit among machines running independent operating systems
    without leaving behind any residual state after migration.  We have
    implemented a Zap prototype in Linux that supports transparent migration of
    unmodified applications without any kernel modifications.  We demonstrate
    that our Linux Zap prototype can provide general-purpose process migration
    functionality with low overhead.  Our experimental results for migrating
    pods used for running a standard user's X windows desktop computing
    environment and for running an Apache web server show that these kinds of
    pods can be migrated with subsecond checkpoint and restart latencies.", 
  location     = "https://doi.org/10.1145/844128.844162", 
  location     = "https://www.cs.cmu.edu/~sosman/publications/osdi2002/osdi2002_zap.pdf"
}

@Article{teorrocr,
  author       = "Limin Wang and Vivek Pai and Larry Peterson",
  title        = "The effectiveness of request redirection on {CDN} robustness",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "345--360",
  month        = "Winter",
  keywords     = "content distribution, load balancing, replication, caching,
    address hashing, locality, distribution networks",
  abstract     = "It is becoming increasingly common to construct network
    services using redundant resources geographically distributed across the
    Internet.  Content Distribution Networks are a prime example.  Such systems
    distribute client requests to an appropriate server based on a variety of
    factors---e.g., server load, network proximity, cache locality--in an
    effort to reduce response time and increase the system capacity under load.
    This paper explores the design space of strategies employed to redirect
    requests, and defines a class of new algorithms that carefully balance
    load, locality, and proximity.  We use large-scale detailed simulations to
    evaluate the various strategies.  These simulations clearly demonstrate the
    effectiveness of our new algorithms, which yield a 60--91% improvement in
    system capacity when compared with the best published CDN technology, yet
    user-perceived response latency remains low and the system scales well with
    the number of servers.", 
  location     = "https://doi.org/10.1145/844128.844160"
}

@Article{tnamfbt,
  author       = "Arun Venkataramani and Ravi Kokku and Mike Dahlinabs",
  title        = "{TCP} Nice: a mechanism for background transfers",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "329--343",
  month        = "Winter",
  keywords     = "tcp, background transfers",
  abstract     = "Many distributed applications can make use of large
    background transfers--transfers of data that humans are not waiting for--to
    improve availability, reliability, latency or consistency.  However, given
    the rapid fluctuations of available network bandwidth and changing resource
    costs due to technology trends, hand tuning the aggressiveness of
    background transfers risks (1) complicating applications, (2) being too
    aggressive and interfering with other applications, and (3) being too timid
    and not gaining the benefits of background transfers.  Our goal is for the
    operating system to manage network resources in order to provide a simple
    abstraction of near zero-cost background transfers.  Our system, TCP Nice,
    can provably bound the interference inflicted by background flows on
    foreground flows in a restricted network model.  And our microbenchmarks
    and case study applications suggest that in practice it interferes little
    with foreground flows, reaps a large fraction of spare network bandwidth,
    and simplifies application construction and deployment.  For example, in
    our prefetching case study application, aggressive prefetching improves
    demand performance by a factor of three when Nice manages resources; but
    the same prefetching hurts demand performance by a factor of six under
    standard network congestion control.", 
  location     = "https://doi.org/10.1145/844128.844159", 
  location     = "http://www.cs.umass.edu/~arun/papers/tcp-nice-osdi.pdf"
}

@Article{aaoicds,
  author       = "Stefan Saroiu and Krishna~P. Gummadi and Richard~J. Dunn and Steven~D. Gribble and Henry~M. Levy",
  title        = "An analysis of Internet content delivery systems",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "315--327",
  month        = "Winter",
  keywords     = "content delivery systems, peer-to-peer systems, traffic
    analysis, caching, scalability",
  abstract     = "In the span of only a few years, the Internet has experienced
    an astronomical increase in the use of specialized content delivery
    systems, such as content delivery networks and peer-to-peer file sharing
    systems.  Therefore, an understanding of content delivery on the lnternet
    now requires a detailed understanding of how these systems are used in
    practice.This paper examines content delivery from the point of view of
    four content delivery systems: HTTP web traffic, the Akamai content
    delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic.
    We collected a trace of all incoming and outgoing network traffic at the
    University of Washington, a large university with over 60,000 students,
    faculty, and staff.  From this trace, we isolated and characterized traffic
    belonging to each of these four delivery classes.  Our results (1)
    quantify, the rapidly increasing importance of new content delivery
    systems, particularly peer-to-peer networks, (2) characterize the behavior
    of these systems from the perspectives of clients, objects, and servers,
    and (3) derive implications for caching in these systems.", 
  location     = "https://doi.org/10.1145/844128.844158", 
  location     = "https://www.gribble.org/papers/p2p_osdi.pdf"
}

@Article{srfsptpon,
  author       = "Miguel Castro and Peter Druschel and Ayalvadi Ganesh and Antony Rowstron and Dan~S. Wallach",
  title        = "Secure routing for structured peer-to-peer overlay networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "299--314",
  month        = "Winter",
  keywords     = "overlay routing, pastry, sybil attacks, neighborhood
    management, secure routing",
  abstract     = "Structured peer-to-peer overlay networks provide a substrate
    for the construction of large-scale, decentralized applications, including
    distributed storage, group communication, and content distribution.  These
    overlays are highly resilient; they can route messages correctly even when
    a large fraction of the nodes crash or the network partitions.  But current
    overlays are not secure; even a small fraction of malicious nodes can
    prevent correct message delivery throughout the overlay.  This problem is
    particularly serious in open peer-to-peer systems, where many diverse,
    autonomous parties without preexisting trust relationships wish to pool
    their resources.  This paper studies attacks aimed at preventing correct
    message delivery in structured peer-to-peer overlays and presents defenses
    to these attacks.  We describe and evaluate techniques that allow nodes to
    join the overlay, to maintain routing state, and to forward messages
    securely in the presence of malicious nodes.", 
  location     = "https://doi.org/10.1145/844128.844156", 
  location     = "https://www.cs.rice.edu/~dwallach/pub/osdi2002.pdf"
}

@Article{pmbcae,
  author       = "Landon~P. Cox and Christopher~D. Murray and Brian~D. Noble",
  title        = "Pastiche: making backup cheap and easy",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "285--298",
  month        = "Winter",
  keywords     = "backup, peer-to-peer systems, content-based indexing,
    fingerprinting, compression",
  abstract     = "Backup is cumbersome and expensive.  Individual users almost
    never back up their data, and backup is a significant cost in large
    organizations.  This paper presents Pastiche, a simple and inexpensive
    backup system.  Pastiche exploits excess disk capacity to perform
    peer-to-peer backup with no administrative costs.  Each node minimizes
    storage overhead by selecting peers that share a significant amount of
    data.  It is easy for common installations to find suitable peers, and
    peers with high overlap can be identified with only hundreds of bytes.
    Pastiche provides mechanisms for confidentiality, integrity, and detection
    of failed or malicious peers.  A Pastiche prototype suffers only 7.4%
    overhead for a modified Andrew Benchmark, and restore performance is
    comparable to cross-machine copy.", 
  location     = "https://doi.org/10.1145/844128.844155", 
  location     = "http://www.cs.fsu.edu/~awang/courses/cop5611_s2009/pastiche.pdf"
}

@Article{saaialsne,
  author       = "Amin Vahdat and Ken Yocum and Kevin Walsh and Priya Mahadevan and Dejan Kostić and Jeff Chase and David Becker",
  title        = "Scalability and accuracy in a large-scale network emulator",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "271--284",
  month        = "Winter",
  keywords     = "network emulation, scalability, virtual networking",
  abstract     = "This paper presents ModelNet, a scalable Internet emulation
    environment that enables researchers to deploy unmodified software
    prototypes in a configurable Internet-like environment and subject them to
    faults and varying network conditions.  Edge nodes running user-specified
    OS and application software are configured to route their packets through a
    set of ModelNet core nodes, which cooperate to subject the traffic to the
    bandwidth, congestion constraints, latency, and loss profile of a target
    network topology.This paper describes and evaluates the ModelNet
    architecture and its implementation, including novel techniques to balance
    emulation accuracy against scalability.  The current ModelNet prototype is
    able to accurately subject thousands of instances of a distrbuted
    application to Internet-like conditions with gigabits of bisection
    bandwidth.  Experiments with several large-scale distributed services
    demonstrate the generality and effectiveness of the infrastructure.", 
  location     = "https://doi.org/10.1145/844128.844154", 
  location     = "https://mathcs.holycross.edu/~kwalsh/papers/modelnet.pdf"
}

@Article{aieefdsan,
  author       = "Brian White and Jay Lepreau and Leigh Stoller and Robert Ricci and Shashi Guruprasad and Mac Newbold and Mike Hibler and Chad Barb and Abhijeet Joglekar",
  title        = "An integrated experimental environment for distributed systems and networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "255--270",
  month        = "Winter",
  keywords     = "network emulation, network simulation, network testbeds",
  abstract     = "Three experimental environments traditionally support network
    and distributed systems research: network emulators, network simulators,
    and live networks.  The continued use of multiple approaches highlights
    both the value and inadequacy of each.  Netbed, a descendant of Emulab,
    provides an experimentation facility that integrates these approaches,
    allowing researchers to configure and access networks composed of emulated,
    simulated, and wide-area nodes and links.  Netbed's primary goals are ease
    of use, control, and realism, achieved through consistent use of
    virtualization and abstraction.By providing operating system-like services,
    such as resource allocation and scheduling, and by virtualizing
    heterogeneous resources, Netbed acts as a virtual machine for network
    experimentation.  This paper presents Netbed's overall design and
    implementation and demonstrates its ability to improve experimental
    automation and efficiency.  These, in turn, lead to new methods of
    experimentation, including automated parameter-space studies within
    emulation and straightforward comparisons of simulated, emulated, and
    wide-area scenarios.", 
  location     = "https://doi.org/10.1145/844128.844152", 
  location     = "https://www.flux.utah.edu/download?uid=122"
}

@Article{roaapishp,
  author       = "Bhuvan Urgaonkar and Prashant Shenoy and Timothy Roscoe",
  title        = "Resource overbooking and application profiling in shared hosting platforms",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "239--254",
  month        = "Winter",
  keywords     = "qos management, yield management, cluster management, node
    assignment, resource demand modeling",
  abstract     = "In this paper, we present techniques for provisioning CPU and
    network resources in shared hosting platforms running potentially
    antagonistic third-party applications.  The primary contribution of our
    work is to demonstrate the feasibility and benefits of overbooking
    resources in shared platforms, to maximize the platform yield: the revenue
    generated by the available resources.  We do this by first deriving an
    accurate estimate of application resource needs by profiling applications
    on dedicated nodes, and then using these profiles to guide the placement of
    application components onto shared nodes.  By overbooking cluster resources
    in a controlled fashion, our platform can provide performance guarantees to
    applications even when overbooked, and combine these techniques with
    commonly used QoS resource allocation mechanisms to provide application
    isolation and performance guarantees at run-time.  When compared to
    provisioning based on the worst-case, the efficiency (and consequently
    revenue) benefits from controlled overbooking of resources can be dramatic.
    Specifically, experiments on our Linux cluster implementation indicate that
    overbooking resources by as little as 1% can increase the utilization of
    the cluster by a factor of two, and a 5% overbooking yields a 300--500%
    improvement, while still providing useful resource guarantees to
    applications.", 
  location     = "https://doi.org/10.1145/844128.844151", 
  location     = "https://people.inf.ethz.ch/troscoe/pubs/OSDI2002.pdf"
}

@Article{adotcs,
  author       = "Edsger Dijkstra",
  title        = "{A} Debate on Teaching Computer Science (On the Cruelty of Really Teaching Computer Science)",
  journal      = cacm,
  year         = 1989,
  volume       = 32,
  number       = 12,
  pages        = "1397--1414",
  month        = dec,
  keywords     = "teaching, reasoning, computer science",
  abstract     = "At the ACM Computer Science Conference last February, Edsger
    Dijkstra gave an invited talk A called “On the Cruelty of Really Teaching
    Computing Science.” He challenged some of the basic assumptions on which
    our curricula are based and provoked a lot of discussion. The editors of
    Communications received several recommendations to publish his talk in
    these pages. His comments brought into the foreground some of the
    background of controversy that surrounds the issue of what belongs in the
    core of a computer science curriculum. To give full airing to the
    controversy, we invited Dijkstra to engage in a debate with selected
    colleagues, each of whom would contribute a short critique of his position,
    with Dijkstra himself making a closing statement. He graciously accepted
    this offer. We invited people from a variety of specialties, backgrounds,
    and interpretations to provide their comments. David Parnas is a noted
    software engineer who was outspoken in his criticism of the proposed
    Strategic Defense Initiative. William Scherlis is known for his articulate
    advocacy of formal methods in computer science. M. H. van Emden is known
    for his contributions in programming languages and philosophical insights
    into science. Jacques Cohen is known for his work with programming
    languages and logic programming and is a member of the Editorial Panel of
    this magazine. Richard Hamming received the Turing Award in 1968 and is
    well known for his work in communications and coding theory. Richard
    M. Karp received the Turing Award in 1985 and is known for his
    contributions in the design of algorithms. Terry Winograd is well known for
    his early work in artificial intelligence and recent work in the principles
    of design. I am grateful to these people for participating in this debate
    and to Professor Dijkstra for creating the opening." 
}

@Article{irmfcbis,
  author       = "Kai Shen and Hong Tang and Tao Yang and Lingkun Chu",
  title        = "Integrated resource management for cluster-based Internet services",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "225--238",
  month        = "Winter",
  keywords     = "resource allocation, quality of service, service
    differentiation, yield management, service scheduling",
  abstract     = "Client request rates for Internet services tend to be bursty
    and thus it is important to maintain efficient resource utilization under a
    wide range of load conditions. Network service clients typically seek
    services interactively and maintaining reasonable response time is often
    imperative for such services. In addition, providing differentiated service
    qualities and resource allocation to multiple service classes can also be
    desirable at times. This paper presents an integrated resource management
    framework (part of Neptune system) that provides flexible service quality
    specification, efficient resource utilization, and service differentiation
    for cluster-based services. This framework introduces the metric of
    quality-aware service yield to combine the overall system efficiency and
    individual service response time in one flexible model. Resources are
    managed through a two-level request distribution and scheduling scheme. At
    the cluster level, a fully decentralized request distribution architecture
    is employed to achieve high scalability and availability. Inside each
    service node, an adaptive scheduling policy maintains efficient resource
    utilization under a wide range of load conditions. Our trace-driven
    evaluations demonstrate the performance, scalability, and service
    differentiation achieved by the proposed techniques.", 
  location     = "https://doi.org/10.1145/844128.844150", 
  location     = "https://www.cs.rochester.edu/u/kshen/papers/osdi2002_html/osdi02.html"
}

@Article{reiatvmlar,
  author       = "George~W. Dunlap and Samuel~T. King and Sukru Cinar and Murtaza~A. Basrai and Peter~M. Chen",
  title        = "{ReVirt}: enabling intrusion analysis through virtual-machine logging and replay",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "211--224",
  month        = "Winter",
  keywords     = "virtual machines, linux, trusted computing base, logging,
    replay, compression, attack analysis",
  abstract     = "Current system loggers have two problems: they depend on the
    integrity of the operating system being logged, and they do not save
    sufficient information to replay and analyze attacks that include any
    non-deterministic events. ReVirt removes the dependency on the target
    operating system by moving it into a virtual machine and logging below the
    virtual machine. This allows ReVirt to replay the system's execution
    before, during, and after an intruder compromises the system, even if the
    intruder replaces the target operating system. ReVirt logs enough
    information to replay a long-term execution of the virtual machine
    instruction-by-instruction. This enables it to provide arbitrarily detailed
    observations about what transpired on the system, even in the presence of
    non-deterministic attacks and executions. ReVirt adds reasonable time and
    space overhead. Overheads due to virtualization are imperceptible for
    interactive use and CPU-bound workloads, and 13--58% for kernel-intensive
    workloads. Logging adds 0--8% overhead, and logging traffic for our
    workloads can be stored on a single disk for several months.", 
  location     = "https://doi.org/10.1145/844128.844148", 
  location     = "https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F14/handouts/papers/dunlap02.pdf"
}

@Article{sapitdik,
  author       = "Andrew Whitaker and Marianne Shaw and Steven~D. Gribble",
  title        = "Scale and performance in the {Denali} isolation kernel",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "195--209",
  month        = "Winter",
  keywords     = "isolation kernels, exokernels, abstraction levels, interrupt
    architectures",
  abstract     = "This paper describes the Denali isolation kernel, an
    operating system architecture that safely multiplexes a large number of
    untrusted Internet services on shared hardware. Denali's goal is to allow
    new Internet services to be 'pushed' into third party infrastructure,
    relieving Internet service authors from the burden of acquiring and
    maintaining physical infrastructure. Our isolation kernel exposes a virtual
    machine abstraction, but unlike conventional virtual machine monitors,
    Denali does not attempt to emulate the underlying physical architecture
    precisely, and instead modifies the virtual architecture to gain scale,
    performance, and simplicity of implementation. In this paper, we first
    discuss design principles of isolation kernels, and then we describe the
    design and implementation of Denali. Following this, we present a detailed
    evaluation of Denali, demonstrating that the overhead of virtualization is
    small, that our architectural choices are warranted, and that we can
    successfully scale to more than 10,000 virtual machines on commodity
    hardware.", 
  location     = "https://doi.org/10.1145/844128.844147", 
  location     = "https://www.usenix.org/conference/osdi-02/scale-and-performance-denali-isolation-kernel"
}

@Article{mrmives,
  author       = "Carl~A. Waldspurger",
  title        = "Memory resource management in {VMware ESX} server",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "181--194",
  month        = "Winter",
  keywords     = "memory virtualization, resource reclamation, page
    replacement, ballooning, storage sharing, working sets",
  abstract     = "VMware ESX Server is a thin software layer designed to
    multiplex hardware resources efficiently among virtual machines running
    unmodified commodity operating systems. This paper introduces several novel
    ESX Server mechanisms and policies for managing memory. A ballooning
    technique reclaims the pages considered least valuable by the operating
    system running in a virtual machine. An idle memory tax achieves efficient
    memory utilization while maintaining performance isolation
    guarantees. Content-based page sharing and hot I/O page remapping exploit
    transparent page remapping to eliminate redundancy and reduce copying
    overheads. These techniques are combined to efficiently support virtual
    machine workloads that overcommit memory.", 
  location     = "https://doi.org/10.1145/844128.844146", 
  location     = "https://www.vmware.com/pdf/usenix_resource_mgmt.pdf"
}

@Article{stsaoaco,
  author       = "Ashvin Goel and Luca Abeni and Charles Krasic and Jim Snow and Jonathan Walpole",
  title        = "Supporting time-sensitive applications on a commodity {OS}",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "165--180",
  month        = "Winter",
  keywords     = "timer management, kernel interrupt architecture, process
    scheduling, real-time systems",
  abstract     = "Commodity operating systems are increasingly being used for
    serving time-sensitive applications. These applications require low-latency
    response from the kernel and from other system-level services. In this
    paper, we explore various operating systems techniques needed to support
    time-sensitive applications and describe the design of our Time-Sensitive
    Linux (TSL) system. We show that the combination of a high-precision timing
    facility, a well-designed preemptible kernel and the use of appropriate
    scheduling techniques is the basis for a low-latency response system and
    such a system can have low overhead. We evaluate the behavior of realistic
    time-sensitive user- and kernel-level applications on our system and show
    that, in practice, it is possible to satisfy the constraints of
    time-sensitive applications in a commodity operating system without
    significantly compromising the performance of throughput-oriented
    applications.", 
  location     = "https://doi.org/10.1145/844128.844144", 
  location     = "https://www.eecg.utoronto.ca/~ashvin/publications/osdi2002.pdf"
}

@Article{fgntsurb,
  author       = "Jeremy Elson and Lewis Girod and Deborah Estrin",
  title        = "Fine-grained network time synchronization using reference broadcasts",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "147--163",
  month        = "Winter",
  keywords     = "time synchronization, broadcast synchronization, relative
    synchronization, clock skew",
  abstract     = "Recent advances in miniaturization and low-cost, low-power
    design have led to active research in large-scale networks of small,
    wireless, low-power sensors and actuators. Time synchronization is critical
    in sensor networks for diverse purposes including sensor data fusion,
    coordinated actuation, and power-efficient duty cycling. Though the clock
    accuracy and precision requirements are often stricter than in traditional
    distributed systems, strict energy constraints limit the resources
    available to meet these goals.We present Reference-Broadcast
    Synchronization, a scheme in which nodes send reference beacons to their
    neighbors using physical-layer broadcasts. A reference broadcast does not
    contain an explicit timestamp; instead, receivers use its arrival time as a
    point of reference for comparing their clocks. In this paper, we use
    measurements from two wireless implementations to show that removing the
    sender's nondeterminism from the critical path in this way produces
    high-precision clock agreement (1.85 ± 1.28μsec, using off-the-shelf 802.11
    wireless Ethernet), while using minimal energy. We also describe a novel
    algorithm that uses this same broadcast property to federate clocks across
    broadcast domains with a slow decay in precision (3.68 ± 2.57μsec after 4
    hops). RBS can be used without external references, forming a precise
    relative timescale, or can maintain microsecond-level synchronization to an
    external timescale such as UTC. We show a significant improvement over the
    Network Time Protocol (NTP) under similar conditions.", 
  location     = "https://doi.org/10.1145/844128.844143", 
  location     = "https://www.cs.ubc.ca/~krasic/cpsc538a/papers/broadcast-osdi.pdf"
}

@Article{tatasfahsn,
  author       = "Samuel Madden and Michael~J. Franklin and Joseph~M. Hellerstein and Wei Hong",
  title        = "{TAG}: a Tiny AGgregation service for ad-hoc sensor networks",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "131--146",
  month        = "Winter",
  keywords     = "distributed aggregation, sensor networks, query languages,
    grouping",
  abstract     = "We present the Tiny AGgregation (TAG) service for aggregation
    in low-power, distributed, wireless environments. TAG allows users to
    express simple, declarative queries and have them distributed and executed
    efficiently in networks of low-power, wireless sensors. We discuss various
    generic properties of aggregates, and show how those properties affect the
    performance of our in network approach. We include a performance study
    demonstrating the advantages of our approach over traditional centralized,
    out-of-network methods, and discuss a variety of optimizations for
    improving the performance and fault tolerance of the basic solution.", 
  location     = "https://doi.org/10.1145/844128.844142", 
  location     = "https://web.stanford.edu/class/cs240e/papers/madden_tag.pdf"
}

@Article{cioaniosfeaa,
  author       = "Andreas Weissel and Bj{\" o}rn Beutel and Frank Bellosa",
  title        = "Cooperative {I/O}: a novel {I/O} semantics for energy-aware applications",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "117--129",
  month        = "Winter",
  keywords     = "low-power operations, cooperative scheduling, power scheduling",
  abstract     = "In this paper we demonstrate the benefits of application
    involvement in operating system power management.  We present Coop-I/O, an
    approach to reduce the power consumption of devices while encompassing all
    levels of the system---from the hardware and OS to a new interface for
    cooperative I/O that can be used by energy-aware applications.  We assume
    devices which can be set to low-power operation modes if they are not
    accessed and where switching between modes consumes additional energy, e.g.
    devices with rotating components or network devices consuming energy for
    the establishment and shutdown of network connections.  In these cases
    frequent mode switches should be avoided.With Coop-I/O, applications can
    declare open, read and write operations as deferrable and even abortable by
    specifying a time-out and a cancel flag.  This information enables the
    operating system to delay and batch requests so that the number of power
    mode switches is reduced and the device can be kept longer in a low-power
    mode.  We have deployed our concept to the IDE hard disk driver and Ext2
    file system of Linux and to typical real-life programs so that they make
    use of the new cooperative I/O functions.  With energy savings of up to
    50%, the experimental results demonstrate the benefits of the concept.  We
    will show that Coop-I/O even outperforms the 'oracle' shutdown policy which
    defines the lower bound in power consumption if the timing of requests can
    not be influenced.", 
  location     = "https://doi.org/10.1145/844128.844140", 
  location     = "https://www.usenix.org/legacy/event/osdi02/tech/full_papers/weissel/weissel.pdf"
}

@Article{vapsfl,
  author       = "Kriszti{\' a}n Flautner and Trevor Mudge",
  title        = "Vertigo: automatic performance-setting for {Linux}",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "105--116",
  month        = "Winter",
  keywords     = "adaptive power management, performance-based scheduling,
    monitoring",
  abstract     = "Combining high performance with low power consumption is
    becoming one of the primary objectives of processor designs.  Instead of
    relying just on sleep mode for conserving power, an increasing number of
    processors take advantage of the fact that reducing the clock frequency and
    corresponding operating voltage of the CPU can yield quadratic decrease in
    energy use.  However, performance reduction can only be beneficial if it is
    done transparently, without causing the software to miss its deadlines.  In
    this paper, we describe the implementation and performance-setting
    algorithms used in Vertigo, our power management extensions for Linux.
    Vertigo makes its decisions automatically, without any application-specific
    involvement.  We describe how a hierarchy of performance-setting
    algorithms, each specialized for different workload characteristics, can be
    used for controlling the processor's performance.  The algorithms operate
    independently from one another and can be dynamically configured.  As a
    basis for comparison with conventional algorithms, we contrast measurements
    made on a Transmeta Crusoe-based computer using its built-in LongRun power
    manager with Vertigo running on the same system.  We show that unlike
    conventional interval-based algorithms like LongRun, Vertigo is successful
    at focusing in on a small range of performance levels that are sufficient
    to meet an application's deadlines.  When playing MPEG movies, this
    behavior translates into a 11%--35% reduction of mean performance level
    over LongRun, without any negative impact on the framerate.  The
    performance reduction can in turn yield significant power savings.", 
  location     = "https://doi.org/10.1145/844128.844139", 
  location     = "https://www.usenix.org/conference/osdi-02/vertigo-automatic-performance-setting-linux"
}

@Article{ptossfs,
  author       = "Juan Navarro and Sitararn Iyer and Peter Druschel and Alan Cox",
  title        = "Practical, transparent operating system support for superpages",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "89--104",
  month        = "Winter",
  keywords     = "tlb coverage, superpages, fragmentation, storage
    reservations, contiguous storage",
  abstract     = "Most general-purpose processors provide support for memory
    pages of large sizes, called superpages.  Superpages enable each entry in
    the translation lookaside buffer (TLB) to map a large physical memory
    region into a virtual address space.  This dramatically increases TLB
    coverage, reduces TLB misses, and promises performance improvements for
    many applications.  However, supporting superpages poses several challenges
    to the operating system, in terms of superpage allocation and promotion
    tradeoffs, fragmentation control, etc.  We analyze these issues, and
    propose the design of an effective superpage management system.  We
    implement it in FreeBSD on the Alpha CPU, and evaluate it on real workloads
    and benchmarks.  We obtain substantial performance benefits, often
    exceeding 30%; these benefits are sustained even under stressful workload
    scenarios.", 
  location     = "https://doi.org/10.1145/844128.844138", 
  location     = "https://www.usenix.org/conference/osdi-02/practical-transparent-operating-system-support-superpages"
}

@Article{capatmcrc,
  author       = "Madanlal Musuvathi and David Y.~W. Park and Andy Chou and Dawson~R. Engler and David~L. Dill",
  title        = "{CMC}: a pragmatic approach to model checking real code",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "75--88",
  month        = "Winter",
  keywords     = "model checking, abstraction, annotations, model building,
    derivable models, aodv",
  abstract     = "Many system errors do not emerge unless some intricate
    sequence of events occurs.  In practice, this means that most systems have
    errors that only trigger after days or weeks of execution.  Model checking
    [4] is an effective way to find such subtle errors.  It takes a simplified
    description of the code and exhaustively tests it on all inputs, using
    techniques to explore vast state spaces efficiently.  Unfortunately, while
    model checking systems code would be wonderful, it is almost never done in
    practice: building models is just too hard.  It can take significantly more
    time to write a model than it did to write the code.  Furthermore, by
    checking an abstraction of the code rather than the code itself, it is easy
    to miss errors.The paper's first contribution is a new model checker, CMC,
    which checks C and C++ implementations directly, eliminating the need for a
    separate abstract description of the system behavior.  This has two major
    advantages: it reduces the effort to use model checking, and it reduces
    missed errors as well as time-wasting false error reports resulting from
    inconsistencies between the abstract description and the actual
    implementation.  In addition, changes in the implementation can be checked
    immediately without updating a high-level description.The paper's second
    contribution is demonstrating that CMC works well on real code by applying
    it to three implementations of the Ad-hoc On-demand Distance Vector (AODV)
    networking protocol [7].  We found 34 distinct errors (roughly one bug per
    328 lines of code), including a bug in the AODV specification itself.
    Given our experience building systems, it appears that the approach will
    work well in other contexts, and especially well for other networking
    protocols.", 
  location     = "https://doi.org/10.1145/844128.844136",
  location     = "https://web.stanford.edu/~engler/osdi2002.ps"
}

@Article{umctddf,
  author       = "Sanjeev Kumar and Kai Li",
  title        = "Using model checking to debug device firmware",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "61--74",
  month        = "Winter",
  keywords     = "domain specific languages, esp, device drivers, state-machine
    programming, spin, derived models",
  abstract     = "Device firmware is a piece of concurrent software that achieves high performance at the cost of software complexity.  They contain subtle race conditions that make them difficult to debug using traditional debugging techniques.  The problem is further compounded by the lack of debugging support on the devices.  This is a serious problem because the device firmware is trusted by the operating system.Model checkers are designed to systematically verify properties of concurrent systems.  Therefore, model checking is a promising approach to debugging device firmware.  However, model checking involves an exponential search.  Consequently, the models have to be small to allow effective model checking.This paper describes the abstraction techniques used by the ESP compiler to extract abstract models from device firmware written in ESP.  The abstract models are small because they discard some of the details in the firmware that is irrelevant to the particular property being verified.  The programmer is required to specify the abstractions to be performed.  The ESP compiler uses the abstraction specification to extract models conservatively.  Therefore, every bug in the original program will be present in the extracted model.This paper also presents our experience with using Spin model checker to develop and debug VMMC firmware for the Myrinet network interfaces.  An earlier version of the ESP compiler yielded models that were too large to check for system-wide properties like absence of deadlocks.  The new version of the compiler generated abstract models that were used to identify several subtle bugs in the firmware.  So far, we have not encountered any bugs that were not caught by Spin.",
  location     = "https://doi.org/10.1145/844128.844135",
  location     = "https://www.usenix.org/conference/osdi-02/using-model-checking-debug-device-firmware"
}

@Article{dpuaattbdrs2002,
  author       = "Xiaohu Qie and Ruoming Pang and Larry Peterson",
  title        = "Defensive programming: using an annotation toolkit to build {DoS}-resistant software",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "45--60",
  month        = "Winter",
  keywords     = "denial-of-service attacks, resource hogging, tempo attacks,
    annotations, resource management, sensors and actuators",
  abstract     = "This paper describes a toolkit to help improve the robustness
    of code against DoS attacks.  We observe that when developing software,
    programmers primarily focus on functionality.  Protecting code from attacks
    is often considered the responsibility of the OS, firewalls and intrusion
    detection systems.  As a result, many DoS vulnerabilities are not
    discovered until the system is attacked and the damage is done.  Instead of
    reacting to attacks after the fact, this paper argues that a better
    solution is to make software defensive by systematically injecting
    protection mechanisms into the code itself.  Our toolkit provides an API
    that programmers use to annotate their code.  At runtime, these annotations
    serve as both sensors and actuators: watching for resource abuse and taking
    the appropriate action should abuse be detected.  This paper presents the
    design and implementation of the toolkit, as well as evaluation of its
    effectiveness with three widely-deployed network services.", 
  location     = "https://doi.org/10.1145/844128.844134",
  location     = "https://www.cs.princeton.edu/research/techreps/TR-658-02"
}

@Article{iarwptpfs,
  author       = "Athicha Muthitacharoen and Robert Morris and Thomer~M. Gil and Benjie Chen",
  title        = "Ivy: a read/write peer-to-peer file system",
  journal      = osdi02,
  year         = 2002,
  volume       = 36,
  number       = "SI",
  pages        = "31--44",
  month        = "Winter",
  keywords     = "logging, block chain, reconstruction, distributed hash
    tables, peer-to-peer file systems",
  abstract     = "Ivy is a multi-user read/write peer-to-peer file system.  Ivy
    has no centralized or dedicated components, and it provides useful
    integrity properties without requiring users to fully trust either the
    underlying peer-to-peer storage system or the other users of the file
    system.An Ivy file system consists solely of a set of logs, one log per
    participant.  Ivy stores its logs in the DHash distributed hash table.
    Each participant finds data by consuiting all logs, but performs
    modifications by appending only to its own log.  This arrangement allows
    Ivy to maintain meta-data consistency without locking.  Ivy users can
    choose which other logs to trust, an appropriate arrangement in a semi-open
    peer-to-peer system.Ivy presents applications with a conventional file
    system interface.  When the underlying network is fully connected, Ivy
    provides NFS-like semantics, such as close-to-open consistency.  Ivy
    detects conflicting modifications made during a partition, and provides
    relevant version information to application-specific conflict resolvers.
    Performance measurements on a wide-area network show that Ivy is two to
    three times slower than NFS.", 
  location     = "https://doi.org/10.1145/844128.844132",
  location     = "https://pdos.csail.mit.edu/archive/ivy/"
}

@TechReport{oem2fblis,
  author       = "Rovner, Paul and Levin, Roy and Wick, John",
  title        = "On Extending {Modula}-2 for Building Large, Integrated Systems",
  institution  = "DEC System Research Center",
  year         = 1985,
  number       = "SRC-RR-3",
  address      = paca,
  month        = "11 " # jan,
  keywords     = "modula-2, types, exception handling, concurrency management,
    programming style, system development",
  abstract     = "Modula-2 has been chosen as SRC's primary programming
    language for the next few years.  This report addresses some of the
    problems of using Modula-2 for building large, integrated systems.  The
    report has three sections: Section 1 outlines a set of extensions to the
    language.  (The extended language is called Modula-2+.) Section 2 (with
    Appendix b) provides a complete description of the Modula-2+ type-checking
    rules.  Section 3 offers some guidelines for programming in Modula-2+.  Our
    implementation of Modula-2+ is based on the Modula-2 compiler written by
    Mike Powell at the DEC Western Research Laboratory.  Our extensions include
    features for exceptions and finalization, garbage collection, and
    concurrency.", 
  location     = "http://bitsavers.trailing-edge.com/pdf/dec/tech_reports/SRC-RR-3.pdf"
}

@InProceedings{tehrfsfrbd,
  author       = "Daniel Rosenband",
  title        = "The Ephemeral History Register:  Flexible Scheduling for Rule-Based Designs",
  booktitle    = pot # "Second ACM/IEEE International Conference on Formal Methods and Models for Co-Design (MEMOCODE '04)",
  year         = 2004,
  pages        = "189--198",
  publisher    = "IEEE Computer Society",
  month        = jan,
  keywords     = "hardware synthesis, processor pipelines, rule-based synthesis",
  abstract     = " The quality of high-level synthesis results is strongly
    dependant on the concurrency that can be found in designs.  In this paper
    we introduce the ephemeral history register (EHR), a new primitive state
    element that enables concurrent scheduling of arbitrary rules in a
    rule-based design framework.  The key properties of the EHR are that it
    allows multiple operations to write to the same state simultaneously, and
    that the EHR maintains a history of all writes that occur within a
    clock-cycle.  Using the EHR, we present an algorithm that takes as input a
    design and a desired schedule, and produces a functionally equivalent
    design that satisfies the desired concurrency and ordering of operations.
    A processor pipeline is used to illustrate the effectiveness of the EHR and
    scheduling algorithm, and shows how this approach significantly improves on
    previous synthesis algorithms for rule-based designs.", 
  location     = "https://doi.org/10.1109/MEMCOD.2004.1459853", 
  location     = "http://csg.csail.mit.edu/pubs/memos/Memo-479/memo479.pdf"
}
		  
% Local Variables:
% eval: (set-register ?b "  journal      = asplos06,\n  year         = 2006,\n  volume       = 41,\n  number       = 11,\n  pages        = \"--\",\n  month        = oct,\n")
% End:

